{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(\"..\")\n",
    "from LBFGS import FullBatchLBFGS\n",
    "from kernels import vvkernels as vvk, sep_vvkernels as svvk, vvk_rbfkernel as vvk_rbf\n",
    "from means import vvmeans as vvm\n",
    "from likelihood import vvlikelihood as vvll\n",
    "from mlikelihoods import MarginalLogLikelihood as exmll\n",
    "from predstrategies import GPprediction\n",
    "from utils import ObjFun, get_vertices, stopping_criteria\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=40)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=32)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=32)    # legend fontsize\n",
    "plt.rc('font', size=32)          # controls default text sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "\n",
    "We sample from $$V_1(x_1, x_2) = 3(1 - x_1)^2 e^{-x_1^2 - (x_2 +1)^2} - 10 (x_1/5 - x_1 ^3 - x_2^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- (x_1 + 2) ^2 - x_2^2} + 0.5(2x_1 + x_2)$$\n",
    "$$V_2(x_1, x_2) = 3(1 +x_2)^2 e^{-x_2^2 - (x_1 +1)^2} - 10 (-x_2/5 + x_2 ^3 + x_1^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- ( 2- x_2) ^2 - x_1^2} + 0.5(2x_1 + x_2)$$\n",
    "\n",
    "where $(x_1, x_2) \\in [-3, 3]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3380, 0.3502], dtype=torch.float32)\n",
      "tensor([[ 1.4946, -1.6349],\n",
      "        [ 1.6712, -1.4105],\n",
      "        [ 1.0285, -1.3352],\n",
      "        [ 1.6098, -1.2711]])\n"
     ]
    }
   ],
   "source": [
    "vf = ObjFun()\n",
    "f_target = vf.tgt_vec\n",
    "print(f_target)\n",
    "sample_size = 4\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -3.\n",
    "vf.high = 3.\n",
    "\n",
    "high_minus_low = vf.high- vf.low\n",
    "#high_minus_low = -\n",
    "def g_theta(sample_size, D):\n",
    "    loc_x = (2. - 1.0 )  * np.random.random_sample((sample_size,1)) + 1.0\n",
    "    \n",
    "    loc_y = (2.  -1.0)  * np.random.random_sample((sample_size,1)) - 2.\n",
    "    loc = np.concatenate((loc_x, loc_y), 1)\n",
    "    #loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "#train_x = Tensor([[-1.5, 1.5], [-1.5, 1.3]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    \n",
    "    out = vf(x[:,0], x[:,1]) + torch.randn(Tensor(vf(x[:,0], x[:,1])).size()) * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)\n",
    "\n",
    "# print(train_y)\n",
    "# train_y = (train_y - train_y.mean())/train_y.std(dim=-2, keepdim=True)\n",
    "# train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "# print(train_y)\n",
    "# print(train_y.std(dim=-2, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP model initialization\n",
    "We inialize the GP model following https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)  #vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)#vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)  # \n",
    "        base_kernels = [] #contain all the base kernels\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters oprimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels,noise_value, current_model = None, current_likelihood = None):\n",
    "    noises = torch.ones(agg_data.shape[0]) * (noise_value) #  torch.zeros(agg_data.shape[0]) # \n",
    "    noises = noises.reshape(g_theta1.shape[0], 2)\n",
    "    \n",
    "#     if (current_model is not None):\n",
    "#         likelihood = current_likelihood #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)  #\n",
    "\n",
    "#         model = current_model#.get_fantasy_model(g_theta1, agg_data) #MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#         model.set_train_data(g_theta1, agg_data,  strict=False)\n",
    "#     else:\n",
    "#         likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2)#vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #\n",
    "#         model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "        \n",
    "    likelihood =  vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2) #\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.double()\n",
    "    likelihood.double()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #, weight_decay=0.001)  # Includes GaussianLikelihood parameters\n",
    "    mll = exmll(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, chi_square = mll(agg_data,g_theta1, model, likelihood, noise_value)\n",
    "        loss = -1. * loss\n",
    "#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \n",
    "        #print('loss is %.3f' %loss)\n",
    "        df = agg_data.shape[0]\n",
    "        chi_square = chi_square.clone().detach()\n",
    "        \n",
    "        p_val = 1. - stats.chi2.cdf(chi_square, df)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "       # print(p_val)\n",
    "#         if (p_val > 0.99999):\n",
    "#             return model, likelihood\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    print('loss is %.3f' %loss)\n",
    "#     for params in model.named_parameters():\n",
    "#         print(params)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design parameters and sampling point optimization (where to explore?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': g_theta2, 'lr': 0.1},{'params': x_d, 'lr': 0.1}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss2)\n",
    "        scheduler.step()\n",
    "        \n",
    "    loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "    loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    #print(x_d)\n",
    "    return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conducting the TAD experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_size = 2\n",
    "#loc_sample0 = Tensor((2. - 1.5)  * np.random.random_sample((loc_size,2)) + 1.5)\n",
    "x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "x0 = x0.reshape(1,2)\n",
    "\n",
    "dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(loc_size) )\n",
    "                    #loc_size = 4\n",
    "loc_sample = dis_2sample.sample((loc_size + 1,))\n",
    "\n",
    "loc_sample0 = loc_sample.reshape(loc_size + 1, 2)\n",
    "#loc_sample0[-1] = train_x[-1] + 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9775,  1.9504],\n",
      "        [-1.7812,  2.0668],\n",
      "        [-2.0650,  2.0864]])\n",
      "0\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /lus/theta-fs0/software/thetagpu/conda/2022-07-01/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -3.200\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 283.314\n",
      "expected info is tensor([[1.8475]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(570.6480, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1287,  2.9150]])\n",
      "new data istensor([[ 0.0120, -0.0372]])\n",
      "g_theta2 istensor([[-1.2726,  1.8766],\n",
      "        [-1.0627,  2.1588],\n",
      "        [-1.3791,  2.2182]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 272.176810833421314\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "0\n",
      "Loss design: 141.693\n",
      "expected info is tensor([[1.0704]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(292.8185, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1877,  2.8392]])\n",
      "new data istensor([[ 0.0253, -0.0221]])\n",
      "g_theta2 istensor([[-2.9178,  1.6951],\n",
      "        [-1.7676,  1.6179],\n",
      "        [-2.4797,  1.4930]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 537.880691463420590\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is3\n",
      "acquiring 2, new size is7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "1\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.261\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 20.237\n",
      "expected info is tensor([[0.0230]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(48.1439, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.3933,  2.8073]])\n",
      "new data istensor([[ 0.1891, -0.1276]])\n",
      "g_theta2 istensor([[-2.4615,  1.5151],\n",
      "        [-2.3728,  1.4647],\n",
      "        [-2.5926,  1.5382]])\n",
      "p21val is 0.863298893659420\n",
      "pf12val is 0.000033665422023\n",
      "chi_f12 is 20.598078600199308\n",
      "p_val_ftarget is 3.5129676945189203e-11\n",
      "new 2 points\n",
      "tensor([[-2.1917,  1.7069],\n",
      "        [-1.5129,  2.0560],\n",
      "        [-0.3933,  2.8073]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.306\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 46.208\n",
      "expected info is tensor([[0.0451]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(101.0863, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.4353, 2.8249]])\n",
      "new data istensor([[ 0.2652, -0.1563]])\n",
      "g_theta2 istensor([[-2.4981,  1.5208],\n",
      "        [-1.6683,  1.4900],\n",
      "        [-0.9546,  2.2113]])\n",
      "p21val is 0.000000289847724\n",
      "pf12val is 0.899533695340671\n",
      "chi_f12 is 0.211757532429187\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "Loss design: 6230.666\n",
      "expected info is tensor([[0.6653]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(179.9282, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1547,  2.9697]])\n",
      "new data istensor([[ 0.0361, -0.0398]])\n",
      "g_theta2 istensor([[ 0.7906,  3.2479],\n",
      "        [-1.4689,  1.6078],\n",
      "        [-0.1607,  2.2938]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.279790536461048\n",
      "chi_f12 is 2.547428079670173\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is4\n",
      "acquiring 2, new size is14\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "3\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.252\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 60.160\n",
      "expected info is tensor([[0.0493]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(127.4038, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.9493,  2.7991]])\n",
      "new data istensor([[-0.0513, -0.0283]])\n",
      "g_theta2 istensor([[-1.9740,  1.4931],\n",
      "        [-1.0526,  1.6301],\n",
      "        [-0.0329,  2.4216]])\n",
      "p21val is 0.026878436518449\n",
      "pf12val is 0.177061590449219\n",
      "chi_f12 is 3.462515276591394\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.7195, -1.7714],\n",
      "        [-2.1626,  1.3383],\n",
      "        [-1.9494,  2.7992]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "4\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.420\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 124.233\n",
      "expected info is tensor([[0.0453]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(256.4433, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7564,  2.8647]])\n",
      "new data istensor([[-0.1627,  0.0249]])\n",
      "g_theta2 istensor([[-2.0886, -2.4996],\n",
      "        [-1.8237,  1.5541],\n",
      "        [-1.4557,  2.2660]])\n",
      "p21val is 0.429164737852665\n",
      "pf12val is 0.728439881294053\n",
      "chi_f12 is 0.633700361427319\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.2134, -1.1898],\n",
      "        [-1.6102, -2.5673],\n",
      "        [-2.7563,  2.8648]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "5\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.462\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 325.196\n",
      "expected info is tensor([[0.0630]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(658.8487, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0038,  2.9038]])\n",
      "new data istensor([[-0.1952, -0.0104]])\n",
      "g_theta2 istensor([[-0.4468, -2.0595],\n",
      "        [-1.7484, -2.5261],\n",
      "        [-2.2127,  2.2992]])\n",
      "p21val is 0.264947886144825\n",
      "pf12val is 0.375255583325761\n",
      "chi_f12 is 1.960295859259721\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3987, -0.8955],\n",
      "        [-0.6160,  2.5412],\n",
      "        [ 0.6437,  1.7134]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "6\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.433\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 25.155\n",
      "expected info is tensor([[0.0364]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(56.9776, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.2030, 1.1601]])\n",
      "new data istensor([[0.4281, 0.0919]])\n",
      "g_theta2 istensor([[-0.0074, -1.6491],\n",
      "        [-1.0843,  2.7350],\n",
      "        [ 0.0638,  2.3040]])\n",
      "p21val is 0.093074463833180\n",
      "pf12val is 0.000041926543177\n",
      "chi_f12 is 20.159182885769244\n",
      "samples escaped box\n",
      "p_val_ftarget is 4.241051954068098e-13\n",
      "new 2 points\n",
      "tensor([[-0.4531,  2.4290],\n",
      "        [-0.4754,  1.9188],\n",
      "        [ 1.2029,  1.1601]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.316\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2.477\n",
      "expected info is tensor([[0.0205]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(12.1578, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8061, 0.6179]])\n",
      "new data istensor([[ 0.4356, -0.2041]])\n",
      "g_theta2 istensor([[-0.9450,  2.8452],\n",
      "        [-0.5996,  1.9209],\n",
      "        [ 1.0902,  1.1129]])\n",
      "p21val is 0.008288897364681\n",
      "pf12val is 0.000603858825670\n",
      "chi_f12 is 14.824340239365698\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "Loss design: 10.693\n",
      "expected info is tensor([[0.3798]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(16.6672, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.7330, 0.6763]])\n",
      "new data istensor([[ 0.3947, -0.2086]])\n",
      "g_theta2 istensor([[ 2.7754, -0.4377],\n",
      "        [-2.3033,  3.0078],\n",
      "        [ 0.9911,  1.3774]])\n",
      "p21val is 0.358486431518938\n",
      "pf12val is 0.000012176072837\n",
      "chi_f12 is 22.632075549631942\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.00024030785065809468\n",
      "new 2 points\n",
      "tensor([[ 0.0572,  0.2470],\n",
      "        [-2.4322, -1.0830],\n",
      "        [ 1.7330,  0.6763]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "8\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.198\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 45.928\n",
      "expected info is tensor([[0.0260]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(100.2335, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.4213, 0.1301]])\n",
      "new data istensor([[ 0.3107, -0.2031]])\n",
      "g_theta2 istensor([[-0.4719, -0.0353],\n",
      "        [-2.9371, -1.7146],\n",
      "        [ 1.3241,  1.1589]])\n",
      "p21val is 0.271202843591279\n",
      "pf12val is 0.655976833502655\n",
      "chi_f12 is 0.843259610888336\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.4703, -1.2945],\n",
      "        [ 2.0116, -0.2273],\n",
      "        [ 2.4213,  0.1302]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.170\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 10.492\n",
      "expected info is tensor([[0.2445]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(27.1386, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9629, 0.6865]])\n",
      "new data istensor([[ 0.3906, -0.0142]])\n",
      "g_theta2 istensor([[-2.6934, -1.6308],\n",
      "        [ 1.4644, -0.7286],\n",
      "        [ 2.0377, -0.2542]])\n",
      "p21val is 0.000452588432328\n",
      "pf12val is 0.744507109065010\n",
      "chi_f12 is 0.590065756251893\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: 1261871.955\n",
      "expected info is tensor([[0.2385]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(23.3908, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9971, 0.7272]])\n",
      "new data istensor([[0.3578, 0.0047]])\n",
      "g_theta2 istensor([[-1.5480, -1.8409],\n",
      "        [ 6.5523,  2.6134],\n",
      "        [ 2.3149, -0.0955]])\n",
      "p21val is 0.487060837045191\n",
      "pf12val is 0.921260407321505\n",
      "chi_f12 is 0.164025077171087\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 8.332027688129351e-06\n",
      "new 2 points\n",
      "tensor([[-2.2116,  1.0429],\n",
      "        [-0.2363, -2.6614],\n",
      "        [ 2.9972,  0.7272]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "10\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.140\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5.376\n",
      "expected info is tensor([[0.0013]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(17.7235, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.7657, 1.4305]])\n",
      "new data istensor([[0.3983, 0.0440]])\n",
      "g_theta2 istensor([[-2.7455,  1.7100],\n",
      "        [-0.7494, -2.8848],\n",
      "        [ 2.5345,  0.3696]])\n",
      "p21val is 0.943823443826118\n",
      "pf12val is 0.546187712066081\n",
      "chi_f12 is 1.209585134656732\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0001417094444081668\n",
      "new 2 points\n",
      "tensor([[ 1.4090,  1.7170],\n",
      "        [-0.8454,  0.7318],\n",
      "        [ 2.7656,  1.4304]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "11\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.192\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1.349\n",
      "expected info is tensor([[0.1295]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9.7105, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.2139, 1.9922]])\n",
      "new data istensor([[0.3776, 0.0127]])\n",
      "g_theta2 istensor([[ 0.7133,  2.4010],\n",
      "        [-1.2778,  0.3244],\n",
      "        [ 2.8304,  1.0944]])\n",
      "p21val is 0.000001524556577\n",
      "pf12val is 0.716005357215304\n",
      "chi_f12 is 0.668135259810405\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "11\n",
      "Loss design: 3171301.530\n",
      "expected info is tensor([[0.1286]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9.9312, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.2289, 1.9843]])\n",
      "new data istensor([[ 0.3687, -0.0046]])\n",
      "g_theta2 istensor([[8.6314, 1.3984],\n",
      "        [1.4623, 0.3708],\n",
      "        [2.8695, 1.0979]])\n",
      "p21val is 0.214361802219057\n",
      "pf12val is 0.541926785840099\n",
      "chi_f12 is 1.225248736285296\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.006973838914874686\n",
      "new 2 points\n",
      "tensor([[2.5437, 2.2567],\n",
      "        [1.5910, 1.5274],\n",
      "        [2.2289, 1.9844]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "12\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.191\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 26.618\n",
      "expected info is tensor([[0.4114]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(63.1581, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.6750, 1.4617]])\n",
      "new data istensor([[0.3692, 0.0365]])\n",
      "g_theta2 istensor([[2.7954, 2.7384],\n",
      "        [1.1239, 1.0396],\n",
      "        [2.7433, 2.3697]])\n",
      "p21val is 0.039007535149228\n",
      "pf12val is 0.364322707006255\n",
      "chi_f12 is 2.019430492792142\n",
      "p_val_ftarget is 1.9317880628477724e-14\n",
      "new 2 points\n",
      "tensor([[1.1142, 0.6845],\n",
      "        [1.9043, 1.7269],\n",
      "        [1.6752, 1.4616]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "13\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.174\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 39.771\n",
      "expected info is tensor([[0.1241]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(90.3595, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.3320, 1.1605]])\n",
      "new data istensor([[ 0.3663, -0.0041]])\n",
      "g_theta2 istensor([[1.3071, 0.4835],\n",
      "        [2.1309, 2.1543],\n",
      "        [1.3618, 1.7587]])\n",
      "p21val is 0.020992183541823\n",
      "pf12val is 0.626201555894759\n",
      "chi_f12 is 0.936165970863708\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5640,  0.1108],\n",
      "        [-2.9232, -0.4147],\n",
      "        [ 2.3320,  1.1605]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "14\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.157\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 99.736\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(212.6762, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8309, 1.7861]])\n",
      "new data istensor([[0.3784, 0.0079]])\n",
      "g_theta2 istensor([[-0.1243, -0.6226],\n",
      "        [-2.7469, -1.3275],\n",
      "        [ 2.8444,  1.0688]])\n",
      "p21val is 0.000036283775114\n",
      "pf12val is 0.215110438175222\n",
      "chi_f12 is 3.073207433608787\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "14\n",
      "Loss design: 99.471\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(212.1407, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8289, 1.7888]])\n",
      "new data istensor([[0.3197, 0.0106]])\n",
      "g_theta2 istensor([[-0.3705, -0.6893],\n",
      "        [-2.7125, -1.8687],\n",
      "        [ 2.8458,  1.0749]])\n",
      "p21val is 0.000000023078550\n",
      "pf12val is 0.472175760282137\n",
      "chi_f12 is 1.500807978462552\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is5\n",
      "acquiring 2, new size is61\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "15\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.094\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7.378\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(22.1557, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.2358, 2.5408]])\n",
      "new data istensor([[ 0.3102, -0.0740]])\n",
      "g_theta2 istensor([[ 0.1157, -0.2918],\n",
      "        [-2.7461, -1.0650],\n",
      "        [ 2.7287,  0.8011]])\n",
      "p21val is 0.472040666294410\n",
      "pf12val is 0.628242120443229\n",
      "chi_f12 is 0.929659289418295\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 1.545056403129408e-05\n",
      "new 2 points\n",
      "tensor([[-1.5329, -1.4345],\n",
      "        [ 0.2298, -0.1507],\n",
      "        [ 1.2356,  2.5408]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "16\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.669\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5.084\n",
      "expected info is tensor([[0.1338]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(16.3663, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8056, 2.9451]])\n",
      "new data istensor([[ 0.3755, -0.0094]])\n",
      "g_theta2 istensor([[-0.8057, -1.9719],\n",
      "        [ 0.1893, -0.2390],\n",
      "        [ 1.0151,  2.1759]])\n",
      "p21val is 0.805811427266484\n",
      "pf12val is 0.723299435095673\n",
      "chi_f12 is 0.647863972273843\n",
      "p_val_ftarget is 0.00027932199063473995\n",
      "new 2 points\n",
      "tensor([[ 0.1361, -0.1531],\n",
      "        [ 1.7057,  2.7720],\n",
      "        [ 1.8055,  2.9451]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "17\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.122\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 24.404\n",
      "expected info is tensor([[0.0469]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(58.7478, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.3897, 2.8596]])\n",
      "new data istensor([[0.4134, 0.0252]])\n",
      "g_theta2 istensor([[ 0.1397, -0.2574],\n",
      "        [ 1.4421,  2.4263],\n",
      "        [ 1.4446,  2.5566]])\n",
      "p21val is 0.362711141220257\n",
      "pf12val is 0.877996833631740\n",
      "chi_f12 is 0.260224583391011\n",
      "samples escaped box\n",
      "p_val_ftarget is 1.7497114868092467e-13\n",
      "new 2 points\n",
      "tensor([[-0.0671, -1.1352],\n",
      "        [-1.7263,  0.3732],\n",
      "        [ 2.3898,  2.8596]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.457\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 16.502\n",
      "expected info is tensor([[0.0116]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(42.4092, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9578, 2.8750]])\n",
      "new data istensor([[ 0.5359, -0.0310]])\n",
      "g_theta2 istensor([[ 0.0799, -1.0197],\n",
      "        [-2.1118, -0.0784],\n",
      "        [ 1.8547,  2.4979]])\n",
      "p21val is 0.005359480230406\n",
      "pf12val is 0.553593835627169\n",
      "chi_f12 is 1.182648019614406\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "Loss design: 15.499\n",
      "expected info is tensor([[0.0140]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(40.2777, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9667, 2.8753]])\n",
      "new data istensor([[ 0.5171, -0.0080]])\n",
      "g_theta2 istensor([[ 0.2165, -0.2376],\n",
      "        [ 2.7984,  2.3139],\n",
      "        [ 1.8458,  2.4780]])\n",
      "p21val is 0.428519328891980\n",
      "pf12val is 0.819706814488393\n",
      "chi_f12 is 0.397617091929047\n",
      "p_val_ftarget is 1.7939150165346973e-09\n",
      "new 2 points\n",
      "tensor([[2.9642, 2.7154],\n",
      "        [2.3239, 1.9862],\n",
      "        [2.9665, 2.8754]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "19\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.135\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 92.376\n",
      "expected info is tensor([[0.0118]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(194.7384, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.7846, 3.0036]])\n",
      "new data istensor([[ 0.4791, -0.0344]])\n",
      "g_theta2 istensor([[2.6174, 2.3740],\n",
      "        [2.3815, 2.2543],\n",
      "        [2.5878, 2.5156]])\n",
      "p21val is 0.708032728149311\n",
      "pf12val is 0.357535715775625\n",
      "chi_f12 is 2.057040035141384\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8633, -2.5057],\n",
      "        [ 2.4628, -2.9316],\n",
      "        [-1.7424, -0.9427]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "20\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.265\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7.555\n",
      "expected info is tensor([[0.0087]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(21.8486, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.2364, -0.4588]])\n",
      "new data istensor([[-0.3636,  0.5801]])\n",
      "g_theta2 istensor([[ 1.3246, -2.7431],\n",
      "        [ 2.9242, -2.7552],\n",
      "        [-2.2607, -1.4504]])\n",
      "p21val is 0.959766541866555\n",
      "pf12val is 0.000005026529731\n",
      "chi_f12 is 24.401561452508098\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 1.8015088404155222e-05\n",
      "new 2 points\n",
      "tensor([[-1.4044, -2.3266],\n",
      "        [-0.4902, -2.9440],\n",
      "        [-1.2365, -0.4589]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "21\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.084\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4.520\n",
      "expected info is tensor([[0.1899]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(18.6818, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.7463, -0.9022]])\n",
      "new data istensor([[0.1795, 0.2151]])\n",
      "g_theta2 istensor([[-1.3996, -2.7958],\n",
      "        [-0.1616, -2.4355],\n",
      "        [-1.1801, -0.2500]])\n",
      "p21val is 0.017438912464162\n",
      "pf12val is 0.631364314827565\n",
      "chi_f12 is 0.919744443983492\n",
      "samples escaped box\n",
      "p_val_ftarget is 8.77599737498791e-05\n",
      "new 2 points\n",
      "tensor([[-1.1873, -1.1613],\n",
      "        [ 1.7479,  0.6884],\n",
      "        [-0.7463, -0.9022]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "22\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.174\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 26.702\n",
      "expected info is tensor([[0.0792]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(64.8718, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.6968, -0.5320]])\n",
      "new data istensor([[0.2136, 0.1236]])\n",
      "g_theta2 istensor([[-1.5874, -1.7740],\n",
      "        [ 2.0367,  1.2787],\n",
      "        [-0.3492, -0.7954]])\n",
      "p21val is 0.025336947617441\n",
      "pf12val is 0.002475662307698\n",
      "chi_f12 is 12.002494639379462\n",
      "p_val_ftarget is 8.215650382226158e-15\n",
      "new 2 points\n",
      "tensor([[ 0.0845,  0.1611],\n",
      "        [ 2.0617,  1.9790],\n",
      "        [-0.6967, -0.5321]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "23\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.185\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 40.701\n",
      "expected info is tensor([[0.0019]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(90.9770, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1989, -1.1366]])\n",
      "new data istensor([[-0.1317,  0.2458]])\n",
      "g_theta2 istensor([[ 0.3766, -0.3505],\n",
      "        [ 1.9042,  2.4723],\n",
      "        [-0.1938, -0.9714]])\n",
      "p21val is 0.549988058746650\n",
      "pf12val is 0.880919615263662\n",
      "chi_f12 is 0.253577799630661\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.4490,  2.1099],\n",
      "        [-1.5030, -1.9514],\n",
      "        [-1.1989, -1.1367]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "24\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.201\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 43.189\n",
      "expected info is tensor([[0.0097]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(94.0350, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8783, -0.7922]])\n",
      "new data istensor([[-0.4670,  0.3974]])\n",
      "g_theta2 istensor([[ 2.5270,  2.1491],\n",
      "        [-1.3317, -1.8893],\n",
      "        [-0.7312, -0.6412]])\n",
      "p21val is 0.181762709129845\n",
      "pf12val is 0.810839122026795\n",
      "chi_f12 is 0.419371228850557\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.0242,  1.1325],\n",
      "        [-2.5904, -1.5065],\n",
      "        [-1.8781, -0.7922]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "25\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.233\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 27.405\n",
      "expected info is tensor([[0.0507]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(61.3488, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.4468, -0.1897]])\n",
      "new data istensor([[-0.5670,  0.2005]])\n",
      "g_theta2 istensor([[ 0.6821,  1.7896],\n",
      "        [-2.0574, -0.9896],\n",
      "        [-1.5762, -1.0835]])\n",
      "p21val is 0.709633760417697\n",
      "pf12val is 0.550183376480686\n",
      "chi_f12 is 1.195007289083825\n",
      "samples escaped box\n",
      "p_val_ftarget is 4.7628567756419216e-14\n",
      "new 2 points\n",
      "tensor([[-0.4527,  0.6774],\n",
      "        [-1.2969, -1.3052],\n",
      "        [-2.4468, -0.1897]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "26\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.232\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 39.752\n",
      "expected info is tensor([[0.1093]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(85.7082, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.9882,  0.3629]])\n",
      "new data istensor([[-0.4290,  0.0265]])\n",
      "g_theta2 istensor([[-0.1085,  1.1248],\n",
      "        [-1.1515, -1.4086],\n",
      "        [-2.1097, -0.5381]])\n",
      "p21val is 0.644008525007449\n",
      "pf12val is 0.987939984970502\n",
      "chi_f12 is 0.024266654072480\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.7790,  1.0861],\n",
      "        [-2.6042, -0.8182],\n",
      "        [-2.9884,  0.3629]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "27\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.258\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 171.021\n",
      "expected info is tensor([[0.2946]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(350.6522, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.8934, -0.4044]])\n",
      "new data istensor([[-0.4657,  0.0316]])\n",
      "g_theta2 istensor([[-2.5263,  1.6668],\n",
      "        [-2.2629, -1.0772],\n",
      "        [-2.6307,  0.4565]])\n",
      "p21val is 0.641707516884161\n",
      "pf12val is 0.887493315733499\n",
      "chi_f12 is 0.238708578457635\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.0478,  1.6022],\n",
      "        [ 0.1219, -1.7085],\n",
      "        [-2.8934, -0.4045]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "28\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.276\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 370.610\n",
      "expected info is tensor([[0.0045]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(728.6425, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0109, -1.0771]])\n",
      "new data istensor([[-0.4601,  0.0389]])\n",
      "g_theta2 istensor([[-1.8620,  1.4994],\n",
      "        [-0.0041, -1.8759],\n",
      "        [-2.4712,  0.0793]])\n",
      "p21val is 0.326671604265471\n",
      "pf12val is 0.307510844936713\n",
      "chi_f12 is 2.358489848541195\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.4812,  0.7312],\n",
      "        [-0.5013,  2.8455],\n",
      "        [-2.7611, -0.1744]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "29\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.236\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 474.158\n",
      "expected info is tensor([[0.0368]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(941.6037, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0096, -0.0777]])\n",
      "new data istensor([[-0.4456, -0.0281]])\n",
      "g_theta2 istensor([[-2.0999,  1.4176],\n",
      "        [-0.3227,  2.5802],\n",
      "        [-2.2546, -0.6247]])\n",
      "p21val is 0.961287940626230\n",
      "pf12val is 0.332366681412234\n",
      "chi_f12 is 2.203032914900276\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6049, -2.7796],\n",
      "        [-2.2802,  1.1245],\n",
      "        [ 2.3450,  0.7285]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "30\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.333\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 181.076\n",
      "expected info is tensor([[0.0021]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(374.8963, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.2729, 0.7742]])\n",
      "new data istensor([[ 0.3501, -0.1154]])\n",
      "g_theta2 istensor([[ 0.5099, -2.6776],\n",
      "        [-1.8530,  1.3830],\n",
      "        [ 2.8939,  1.3024]])\n",
      "p21val is 0.360286741011044\n",
      "pf12val is 0.852647716426679\n",
      "chi_f12 is 0.318817620867887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3047, -1.4525],\n",
      "        [ 1.8635, -1.2983],\n",
      "        [ 2.2729,  0.7741]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "31\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.342\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 135.847\n",
      "expected info is tensor([[0.0159]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(285.4424, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.4386, 1.6315]])\n",
      "new data istensor([[0.3321, 0.0032]])\n",
      "g_theta2 istensor([[-0.2391, -1.7139],\n",
      "        [ 1.4706, -1.7161],\n",
      "        [ 2.1717,  0.8200]])\n",
      "p21val is 0.040568381110166\n",
      "pf12val is 0.265225270132714\n",
      "chi_f12 is 2.654351476266710\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[2.5712, 1.7397],\n",
      "        [2.4760, 1.2625],\n",
      "        [2.4384, 1.6314]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "32\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.349\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 125.193\n",
      "expected info is tensor([[0.0054]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(263.8234, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.7011, 1.9182]])\n",
      "new data istensor([[0.3212, 0.0003]])\n",
      "g_theta2 istensor([[2.9886, 2.1241],\n",
      "        [2.8624, 0.9061],\n",
      "        [2.9763, 1.1865]])\n",
      "p21val is 0.878668426729898\n",
      "pf12val is 0.740805631922800\n",
      "chi_f12 is 0.600033986382972\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6287,  2.3243],\n",
      "        [-0.3528, -2.1261],\n",
      "        [ 1.7012,  1.9183]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "33\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.383\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 235.072\n",
      "expected info is tensor([[0.0061]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(484.7870, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.9903, 1.6324]])\n",
      "new data istensor([[0.3247, 0.0099]])\n",
      "g_theta2 istensor([[ 0.0080,  2.9271],\n",
      "        [-0.4744, -2.2136],\n",
      "        [ 1.2083,  1.4413]])\n",
      "p21val is 0.678233872781101\n",
      "pf12val is 0.970138550866143\n",
      "chi_f12 is 0.060632763480627\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.0850, -0.5341],\n",
      "        [ 2.3863,  2.9618],\n",
      "        [ 1.9901,  1.6323]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "34\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.401\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 169.682\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(354.1474, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.3458, 0.9344]])\n",
      "new data istensor([[0.4623, 0.0522]])\n",
      "g_theta2 istensor([[ 0.8639, -1.0886],\n",
      "        [ 2.0495,  2.9645],\n",
      "        [ 2.2700,  1.9150]])\n",
      "p21val is 0.029606974569604\n",
      "pf12val is 0.205690070603788\n",
      "chi_f12 is 3.162769509477481\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5801,  1.4785],\n",
      "        [-2.2685, -1.1373],\n",
      "        [ 1.3458,  0.9345]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "35\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.400\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1.212\n",
      "expected info is tensor([[0.0181]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11.8978, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7572, 0.5748]])\n",
      "new data istensor([[0.2776, 0.3782]])\n",
      "g_theta2 istensor([[ 0.2453,  1.8230],\n",
      "        [-2.3017, -1.3533],\n",
      "        [ 1.1908,  1.4229]])\n",
      "p21val is 0.018097191368801\n",
      "pf12val is 0.007672403129148\n",
      "chi_f12 is 9.740250794544206\n",
      "p_val_ftarget is 0.002608675701389007\n",
      "new 2 points\n",
      "tensor([[-1.0283,  0.4923],\n",
      "        [-1.7395,  1.2315],\n",
      "        [ 0.7571,  0.5746]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "36\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.390\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.249\n",
      "expected info is tensor([[0.0145]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(0.3558, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7093, 0.7523]])\n",
      "new data istensor([[0.3118, 0.3440]])\n",
      "g_theta2 istensor([[-1.5828, -0.0651],\n",
      "        [-2.0944,  1.3916],\n",
      "        [ 1.2712,  0.4579]])\n",
      "p21val is 0.166984042867521\n",
      "pf12val is 0.844126480819214\n",
      "chi_f12 is 0.338905873646697\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.8370435744124989\n",
      "new 2 points\n",
      "tensor([[ 0.7339,  1.5861],\n",
      "        [ 0.7652, -1.5091],\n",
      "        [ 0.7093,  0.7522]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "37\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.398\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -7.085\n",
      "expected info is tensor([[0.0063]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(0.8319, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7845, 0.7457]])\n",
      "new data istensor([[0.3524, 0.3026]])\n",
      "g_theta2 istensor([[ 0.7765,  1.7472],\n",
      "        [ 0.3916, -1.4737],\n",
      "        [ 1.0071,  0.3094]])\n",
      "p21val is 0.001995510394805\n",
      "pf12val is 0.225154171260988\n",
      "chi_f12 is 2.981939811639013\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "37\n",
      "Loss design: -7.075\n",
      "expected info is tensor([[0.0044]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(0.8250, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7763, 0.7479]])\n",
      "new data istensor([[0.3239, 0.3306]])\n",
      "g_theta2 istensor([[ 0.0994, -0.2245],\n",
      "        [-0.2798, -2.4653],\n",
      "        [ 1.2160,  0.3378]])\n",
      "p21val is 0.264160279788190\n",
      "pf12val is 0.894518068370233\n",
      "chi_f12 is 0.222940353659236\n",
      "p_val_ftarget is 0.661979296818467\n",
      "new 2 points\n",
      "tensor([[1.4973, 2.6258],\n",
      "        [0.8800, 1.3062],\n",
      "        [0.7763, 0.7480]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "38\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.429\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.837\n",
      "expected info is tensor([[0.0210]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.2290, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7896, 0.7389]])\n",
      "new data istensor([[0.3417, 0.3266]])\n",
      "g_theta2 istensor([[1.6523, 2.4853],\n",
      "        [1.0700, 1.5359],\n",
      "        [1.1200, 0.4757]])\n",
      "p21val is 0.495113391032752\n",
      "pf12val is 0.773614224179612\n",
      "chi_f12 is 0.513363896033458\n",
      "p_val_ftarget is 0.32807960979164585\n",
      "new 2 points\n",
      "tensor([[0.5682, 0.5080],\n",
      "        [0.4310, 0.0716],\n",
      "        [0.7896, 0.7390]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "39\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.449\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.926\n",
      "expected info is tensor([[0.0067]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.5248, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7665, 0.7453]])\n",
      "new data istensor([[0.3444, 0.2916]])\n",
      "g_theta2 istensor([[ 0.0708,  0.1053],\n",
      "        [ 1.0723, -0.1095],\n",
      "        [ 1.2409,  0.2759]])\n",
      "p21val is 0.290054638111513\n",
      "pf12val is 0.104730756400943\n",
      "chi_f12 is 4.512724893665593\n",
      "p_val_ftarget is 0.28297787272611896\n",
      "new 2 points\n",
      "tensor([[1.5906, 0.9006],\n",
      "        [0.4769, 0.9205],\n",
      "        [0.7662, 0.7453]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "40\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.427\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.876\n",
      "expected info is tensor([[0.0537]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.8869, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8223, 0.6555]])\n",
      "new data istensor([[0.3155, 0.3596]])\n",
      "g_theta2 istensor([[1.4237, 0.8766],\n",
      "        [0.5031, 1.3510],\n",
      "        [0.9922, 1.0473]])\n",
      "p21val is 0.854979740507021\n",
      "pf12val is 0.550874259223365\n",
      "chi_f12 is 1.192497401096320\n",
      "p_val_ftarget is 0.23611364056186113\n",
      "new 2 points\n",
      "tensor([[1.0844, 0.6026],\n",
      "        [0.8756, 0.7583],\n",
      "        [0.8222, 0.6554]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "41\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.489\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -2.239\n",
      "expected info is tensor([[0.0138]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11.3481, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8389, 0.5277]])\n",
      "new data istensor([[0.3066, 0.3771]])\n",
      "g_theta2 istensor([[1.4539, 0.5712],\n",
      "        [1.4419, 1.1985],\n",
      "        [0.7419, 0.8968]])\n",
      "p21val is 0.153389870916908\n",
      "pf12val is 0.076447703717844\n",
      "chi_f12 is 5.142296766900010\n",
      "p_val_ftarget is 0.0034339055219810843\n",
      "new 2 points\n",
      "tensor([[1.6540, 0.9668],\n",
      "        [0.8293, 0.8873],\n",
      "        [0.8387, 0.5277]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "42\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.479\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.205\n",
      "expected info is tensor([[0.0027]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5.4238, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8116, 0.7071]])\n",
      "new data istensor([[0.3340, 0.3194]])\n",
      "g_theta2 istensor([[1.4168, 0.5178],\n",
      "        [0.6195, 1.1660],\n",
      "        [0.6699, 1.2117]])\n",
      "p21val is 0.534474794140525\n",
      "pf12val is 0.604422206213392\n",
      "chi_f12 is 1.006964616672521\n",
      "p_val_ftarget is 0.066411678368552\n",
      "new 2 points\n",
      "tensor([[0.4958, 0.5945],\n",
      "        [0.8030, 0.4015],\n",
      "        [0.8116, 0.7071]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "43\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.474\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.002\n",
      "expected info is tensor([[0.0194]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5.9065, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8362, 0.6639]])\n",
      "new data istensor([[0.3611, 0.3156]])\n",
      "g_theta2 istensor([[ 0.0644,  0.0814],\n",
      "        [ 1.0243, -0.0045],\n",
      "        [ 0.8416,  0.9685]])\n",
      "p21val is 0.032617540070011\n",
      "pf12val is 0.123498143997409\n",
      "chi_f12 is 4.183058302776189\n",
      "p_val_ftarget is 0.05216860466259976\n",
      "new 2 points\n",
      "tensor([[0.9058, 1.1621],\n",
      "        [0.6619, 2.0476],\n",
      "        [0.8361, 0.6639]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "44\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.513\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -5.013\n",
      "expected info is tensor([[0.0007]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7.3522, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8441, 0.5803]])\n",
      "new data istensor([[0.3105, 0.3760]])\n",
      "g_theta2 istensor([[0.5710, 1.3981],\n",
      "        [1.0751, 2.6870],\n",
      "        [1.0221, 1.0058]])\n",
      "p21val is 0.711991090438503\n",
      "pf12val is 0.193393519060544\n",
      "chi_f12 is 3.286056414139420\n",
      "p_val_ftarget is 0.025321754469662672\n",
      "new 2 points\n",
      "tensor([[ 0.6313,  0.8375],\n",
      "        [ 0.4970, -0.5563],\n",
      "        [ 0.8441,  0.5804]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "45\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.541\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -2.727\n",
      "expected info is tensor([[0.0135]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10.3667, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8288, 0.4370]])\n",
      "new data istensor([[0.3143, 0.3920]])\n",
      "g_theta2 istensor([[ 0.8299,  1.2859],\n",
      "        [ 0.2230, -0.3056],\n",
      "        [ 0.6430,  0.9112]])\n",
      "p21val is 0.234135565155742\n",
      "pf12val is 0.044313402068121\n",
      "chi_f12 is 6.232936235859029\n",
      "p_val_ftarget is 0.0056091117100214305\n",
      "new 2 points\n",
      "tensor([[1.2567, 1.7227],\n",
      "        [0.1844, 0.0299],\n",
      "        [0.8288, 0.4370]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "46\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.556\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.427\n",
      "expected info is tensor([[0.0013]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3.7809, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8480, 0.4534]])\n",
      "new data istensor([[0.3134, 0.3331]])\n",
      "g_theta2 istensor([[ 1.4040,  1.5218],\n",
      "        [-0.3261, -0.5439],\n",
      "        [ 1.2776,  0.8426]])\n",
      "p21val is 0.690069291636649\n",
      "pf12val is 0.847641852509409\n",
      "chi_f12 is 0.330594152282751\n",
      "p_val_ftarget is 0.15100369075573106\n",
      "new 2 points\n",
      "tensor([[-0.9635, -1.7607],\n",
      "        [ 1.5110,  1.4972],\n",
      "        [ 0.8480,  0.4535]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "47\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.573\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -3.786\n",
      "expected info is tensor([[0.0015]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11.1119, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7897, 0.7158]])\n",
      "new data istensor([[0.3193, 0.3533]])\n",
      "g_theta2 istensor([[-1.0478, -1.8670],\n",
      "        [ 1.4221,  1.3992],\n",
      "        [ 0.8171,  1.1712]])\n",
      "p21val is 0.352046712222290\n",
      "pf12val is 0.768871577865410\n",
      "chi_f12 is 0.525662644593973\n",
      "p_val_ftarget is 0.003864481179693735\n",
      "new 2 points\n",
      "tensor([[-0.4567, -0.9002],\n",
      "        [ 0.7859,  0.6131],\n",
      "        [ 0.7898,  0.7160]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "48\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.587\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -7.937\n",
      "expected info is tensor([[0.0020]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.5711, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8614, 0.6031]])\n",
      "new data istensor([[0.3389, 0.3405]])\n",
      "g_theta2 istensor([[-0.4050, -0.8348],\n",
      "        [ 0.5619,  0.7089],\n",
      "        [ 0.6588,  0.9013]])\n",
      "p21val is 0.565486893410468\n",
      "pf12val is 0.914246313450213\n",
      "chi_f12 is 0.179310508564237\n",
      "p_val_ftarget is 0.2765045952521795\n",
      "new 2 points\n",
      "tensor([[1.6972, 1.9647],\n",
      "        [1.2183, 1.2039],\n",
      "        [0.8614, 0.6032]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "49\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.607\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -5.876\n",
      "expected info is tensor([[0.0026]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5.4564, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8485, 0.4585]])\n",
      "new data istensor([[0.3368, 0.3217]])\n",
      "g_theta2 istensor([[1.7391, 2.0186],\n",
      "        [1.3933, 1.3755],\n",
      "        [1.0405, 0.9584]])\n",
      "p21val is 0.730186323227556\n",
      "pf12val is 0.547226170529400\n",
      "chi_f12 is 1.205786175115134\n",
      "p_val_ftarget is 0.0653362600563141\n",
      "new 2 points\n",
      "tensor([[1.6790, 1.9038],\n",
      "        [1.0164, 0.7236],\n",
      "        [0.8483, 0.4585]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "50\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.632\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -7.334\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4.2261, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8566, 0.6078]])\n",
      "new data istensor([[0.3121, 0.3432]])\n",
      "g_theta2 istensor([[2.0950, 2.2929],\n",
      "        [0.7226, 1.0786],\n",
      "        [1.2898, 1.0306]])\n",
      "p21val is 0.268381556183809\n",
      "pf12val is 0.704960422346460\n",
      "chi_f12 is 0.699227232522708\n",
      "p_val_ftarget is 0.12086837945898643\n",
      "new 2 points\n",
      "tensor([[ 1.8042,  1.8633],\n",
      "        [ 0.1006, -0.6194],\n",
      "        [ 0.8566,  0.6078]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "51\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.648\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -3.806\n",
      "expected info is tensor([[0.0048]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11.6545, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8139, 0.6977]])\n",
      "new data istensor([[0.2898, 0.3210]])\n",
      "g_theta2 istensor([[ 2.2763,  2.2725],\n",
      "        [-0.1680, -1.1036],\n",
      "        [ 0.3341,  1.2412]])\n",
      "p21val is 0.231265688120251\n",
      "pf12val is 0.161146765591432\n",
      "chi_f12 is 3.650879483433652\n",
      "p_val_ftarget is 0.002946097945096371\n",
      "new 2 points\n",
      "tensor([[1.2706, 1.5776],\n",
      "        [1.6813, 1.4166],\n",
      "        [0.8138, 0.6977]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "52\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.630\n",
      "END HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     76\u001b[0m likelihood\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 79\u001b[0m x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_design_opti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_theta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_design\u001b[49m\u001b[43m,\u001b[49m\u001b[43miter_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnoise_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m cur_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     82\u001b[0m cur_likelihood \u001b[38;5;241m=\u001b[39m likelihood\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mconduct_design_opti\u001b[0;34m(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new, noise_value)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( training_param_iter ):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m         loss2, pf1, Qf1, Qf12, data_fit, Q21 \u001b[38;5;241m=\u001b[39m \u001b[43mlikelihood\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ell\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_theta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_theta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         loss2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m*\u001b[39m loss2\n\u001b[1;32m     20\u001b[0m         loss2\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/tiany/Targeted_Adaptive_Design/code/jupyter/../likelihood/vvlikelihood.py:345\u001b[0m, in \u001b[0;36mFixedNoiseMultitaskGaussianLikelihood.get_ell\u001b[0;34m(self, agg_data, f_target, x_, g_theta1, model, likelihood, noise_value, g_theta2)\u001b[0m\n\u001b[1;32m    343\u001b[0m C2f \u001b[38;5;241m=\u001b[39m Cf2\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m#with gpytorch.settings.cholesky_jitter(self.custom_jitter):\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m second_term_Qf12 \u001b[38;5;241m=\u001b[39m  C2f \u001b[38;5;241m-\u001b[39m \u001b[43mC11\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC1f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC21\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m#with gpytorch.settings.cholesky_jitter(self.custom_jitter):\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     Qf12_sec_term \u001b[38;5;241m=\u001b[39m (Q21)\u001b[38;5;241m.\u001b[39minv_matmul(second_term_Qf12\u001b[38;5;241m.\u001b[39mevaluate(), second_term_Qf12\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mevaluate())\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1219\u001b[0m, in \u001b[0;36mLazyTensor.inv_matmul\u001b[0;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(), \u001b[38;5;28;01mFalse\u001b[39;00m, right_tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/functions/_inv_matmul.py:49\u001b[0m, in \u001b[0;36mInvMatmul.forward\u001b[0;34m(ctx, representation_tree, has_left, *args)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mhas_left:\n\u001b[1;32m     48\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([left_tensor\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), right_tensor], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m     solves \u001b[38;5;241m=\u001b[39m \u001b[43m_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlazy_tsr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     res \u001b[38;5;241m=\u001b[39m solves[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, left_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) :]\n\u001b[1;32m     51\u001b[0m     res \u001b[38;5;241m=\u001b[39m left_tensor \u001b[38;5;241m@\u001b[39m res\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/functions/_inv_matmul.py:17\u001b[0m, in \u001b[0;36m_solve\u001b[0;34m(lazy_tsr, rhs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lazy_tsr\u001b[38;5;241m.\u001b[39minv_matmul(rhs)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfast_computations\u001b[38;5;241m.\u001b[39msolves\u001b[38;5;241m.\u001b[39moff() \u001b[38;5;129;01mor\u001b[39;00m lazy_tsr\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_cholesky_size\u001b[38;5;241m.\u001b[39mvalue():\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlazy_tsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_cholesky_solve(rhs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1004\u001b[0m, in \u001b[0;36mLazyTensor.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\u001b[38;5;28mself\u001b[39m, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LazyTensor\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m        (LazyTensor) Cholesky factor (triangular, upper/lower depending on \"upper\" arg)\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1006\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:428\u001b[0m, in \u001b[0;36mLazyTensor._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sub_mat, KeOpsLazyTensor) \u001b[38;5;28;01mfor\u001b[39;00m sub_mat \u001b[38;5;129;01min\u001b[39;00m evaluated_kern_mat\u001b[38;5;241m.\u001b[39m_args):\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run Cholesky with KeOps: it will either be really slow or not work.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 428\u001b[0m evaluated_mat \u001b[38;5;241m=\u001b[39m \u001b[43mevaluated_kern_mat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# if the tensor is a scalar, we can just take the square root\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluated_mat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/sum_lazy_tensor.py:66\u001b[0m, in \u001b[0;36mSumLazyTensor.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlazy_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlazy_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/sum_lazy_tensor.py:66\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mlazy_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m lazy_tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_tensors)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1148\u001b[0m, in \u001b[0;36mLazyTensor.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     eye \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(num_cols, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1147\u001b[0m     eye \u001b[38;5;241m=\u001b[39m eye\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shape, num_cols, num_cols)\n\u001b[0;32m-> 1148\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1409\u001b[0m, in \u001b[0;36mLazyTensor.matmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatmul_lazy_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatmulLazyTensor\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatmulLazyTensor(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m-> 1409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatmul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/functions/_matmul.py:21\u001b[0m, in \u001b[0;36mMatmul.forward\u001b[0;34m(ctx, representation_tree, rhs, *matrix_args)\u001b[0m\n\u001b[1;32m     18\u001b[0m     is_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m lazy_tsr \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mrepresentation_tree(\u001b[38;5;241m*\u001b[39mmatrix_args)\n\u001b[0;32m---> 21\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mlazy_tsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m to_save \u001b[38;5;241m=\u001b[39m [orig_rhs] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(matrix_args)\n\u001b[1;32m     24\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mto_save)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/kronecker_product_lazy_tensor.py:237\u001b[0m, in \u001b[0;36mKroneckerProductLazyTensor._matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vec:\n\u001b[1;32m    235\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m rhs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 237\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vec:\n\u001b[1;32m    240\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/kronecker_product_lazy_tensor.py:41\u001b[0m, in \u001b[0;36m_matmul\u001b[0;34m(lazy_tensors, kp_shape, rhs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lazy_tensor \u001b[38;5;129;01min\u001b[39;00m lazy_tensors:\n\u001b[1;32m     40\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput_batch_shape, lazy_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     factor \u001b[38;5;241m=\u001b[39m \u001b[43mlazy_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     factor \u001b[38;5;241m=\u001b[39m factor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput_batch_shape, lazy_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_cols)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     43\u001b[0m     res \u001b[38;5;241m=\u001b[39m factor\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39moutput_batch_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_cols)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/non_lazy_tensor.py:44\u001b[0m, in \u001b[0;36mNonLazyTensor._matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\u001b[38;5;28mself\u001b[39m, rhs):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loc_sample = loc_sample0.clone()\n",
    "iter_hp = 50\n",
    "iter_design = 200\n",
    "iter_param = 200\n",
    "num_base_kernels = 2\n",
    "max_iter = 50\n",
    "\n",
    "\n",
    "f_target = f_target.reshape(2,1) \n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    " #np.random.random_sample((loc_size,2))\n",
    "#loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "#train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "\n",
    "#loc_sample = Tensor([[0.0, 0.1], [0.0, -0.1]]) #T\n",
    "# loc_x = (-1.5 + 2.)  * np.random.random_sample((loc_size,1)) +2.\n",
    "\n",
    "# # loc_y = (2. - 1.5)  * np.random.random_sample((loc_size,1)) - 1.5\n",
    "# # loc = np.concatenate((loc_x, loc_y), 1)\n",
    "print(loc_sample)\n",
    "\n",
    "\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten()\n",
    "\n",
    "data_fit_vec = torch.empty((1,1))\n",
    "entropy_vec = torch.empty((1,1))\n",
    "loss_vec = torch.empty((1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "vec_x = x0.clone() #Tensor(np.array([0.0,0.0])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "var_vec = torch.zeros([max_iter, 1])\n",
    "p21_vec = torch.empty((1,1))\n",
    "\n",
    "lr_new = .01\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "show_TTRBox = False\n",
    "iter = 0    \n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "patience = 0.0\n",
    "patience_f = 0.0\n",
    "patience_2 = 0.0\n",
    "checking_model = False\n",
    "model_double_check = False\n",
    "while(SUCCESS == False and FAILURE == False):\n",
    "    print(iter)\n",
    "    model_double_check = False\n",
    "    if (checking_model == False):\n",
    "        print('START HYPERPARAMETERS optimization')\n",
    "        if (iter == 0):\n",
    "            cur_model = None\n",
    "            cur_likelihood = None\n",
    "\n",
    "\n",
    "        loc_sample_old = loc_sample.clone()\n",
    "        x0_old = x0.clone()\n",
    "        model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "\n",
    "\n",
    "        print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "   \n",
    "    \n",
    "    x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new,noise_value)\n",
    "  \n",
    "    cur_model = model\n",
    "    cur_likelihood = likelihood\n",
    "    \n",
    "  \n",
    "    lower_bound = torch.zeros(pf1.shape)\n",
    "    upper_bound = torch.zeros(pf1.shape)\n",
    "        \n",
    "    for i in range(pf1.shape[0]):\n",
    "        lower_bound[i] = pf1[i] -  torch.sqrt(Qf12[i,i])\n",
    "        upper_bound[i] = pf1[i] +  torch.sqrt(Qf12[i,i])\n",
    "\n",
    "    SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    \n",
    "    \n",
    "    entropy = ( 0.5 * torch.log( torch.det(Qf1.evaluate()) / torch.det(Qf12.evaluate()) ) ).reshape(1,1)\n",
    "    \n",
    "    print('expected info is '+str(entropy))\n",
    "    print('mohabb disatance is' + str(Qf12.inv_quad(f_target - pf1)))\n",
    "    if not SUCCESS:\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        new_data = vfield_(g_theta2.detach())  \n",
    "        agg_data12 = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "        g_theta12= torch.cat([g_theta1, g_theta2.detach()], 0)\n",
    "        new_data_x = vfield_(x0_new.detach() )  \n",
    "        print('current sol is'+str(x0_new.detach()))\n",
    "        print('new data is' + str(new_data_x))\n",
    "        print('g_theta2 is' + str(g_theta2.detach()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            if iter >= 0:\n",
    "                \n",
    "                \n",
    "                p21 = likelihood.get_p21(g_theta1, g_theta2.detach(), agg_data, model, noise_value)\n",
    "                \n",
    "#                 Q21 = Q21 + noise_value*torch.eye(Q21.shape[0])\n",
    "                chi_21 = (Q21).inv_quad(new_data.flatten() - p21.reshape(new_data.flatten().shape))\n",
    "                p_val = 1. - stats.chi2.cdf(chi_21, Q21.shape[0])\n",
    "                pf12 = likelihood.get_pf12(Q21,g_theta1, g_theta2.detach(), x0_new.detach(), new_data.flatten(), pf1, p21, model, noise_value)\n",
    "               \n",
    "                #Qf12 = Qf12 + \n",
    "                chi_f12 = (Qf12 + noise_value*torch.eye(Qf12.shape[0])).inv_quad(new_data_x.flatten() - pf12.reshape(new_data_x.flatten().shape))\n",
    "                p_val_f12 = 1. - stats.chi2.cdf(chi_f12, Qf12.shape[0])\n",
    "                print('p21val is %.15f' %p_val)\n",
    "                p21_vec = torch.cat([p21_vec, Tensor([p_val]).reshape(1,1)], 0)\n",
    "                print('pf12val is %.15f' %p_val_f12)\n",
    "                print('chi_f12 is %.15f' %chi_f12 )\n",
    "                \n",
    "                if (p_val < 0.01):# or p_val_f12 < 0.001:\n",
    "                    model_double_check = True\n",
    "                    checking_model = True\n",
    "                    patience = patience+1\n",
    "                    print('patience is %.3f' %patience)\n",
    "\n",
    "                if (model_double_check == True):\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_sample.shape[0],2)) + vf.low)\n",
    "                    sum = torch.zeros(2, 2) #replace with num_tasks\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_old).t(), (g_theta2.detach()[i] - x0_old) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum #+ torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_old, covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_old],0)\n",
    "                    \n",
    "                    x0 = x0_old #Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    if (patience >= 2):# or patience_2 >= 2 or patience_f >= 2):\n",
    "                        PATH = \".//model_Carlo/model_update/model_base_\"+str(iter)+\".pt\"\n",
    "                        torch.save(model, PATH)\n",
    "                        entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                        data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                        iter = iter + 1\n",
    "                        patience = 0\n",
    "#                         patience_2 = 0\n",
    "#                         patience_f = 0\n",
    "                        model_double_check = False\n",
    "                        checking_model = False\n",
    "                        num_base_kernels = num_base_kernels + 1\n",
    "                        print('adding complexity to model')\n",
    "                        print('num base is' + str(num_base_kernels))\n",
    "#     #                         \n",
    "                        loc_sample = loc_sample_old\n",
    "                        #x0 = x0_old\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                        g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                        print('acquiring 2, new size is' + str(g_theta1.shape[0]))\n",
    "                 \n",
    "                    #iter_hp = iter_hp + 10\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "                    torch.save(model, PATH)\n",
    "                    vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                    loss_vec = torch.cat([loss_vec, -loss])\n",
    "                    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                    entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                    data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                    model_double_check = False\n",
    "                    iter = iter + 1\n",
    "                    patience = 0\n",
    "                    patience_2 = 0\n",
    "                    patience_f = 0\n",
    "                    checking_model = False\n",
    "                    if (entropy < 1e-4 * tol_vector[0,0]):\n",
    "                        FAILURE = True\n",
    "                    \n",
    "                    x0 = (x0_new.detach())# + torch.randn(x0_new.detach().size()) * .001)#/torch.norm(x0_new.detach())\n",
    "                    sum = torch.zeros(2, 2)\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum# + torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_new.detach(), covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = loc_sample#/torch.norm(loc_sample)\n",
    "                    #loc_sample = 2. *  (loc_sample - torch.min(loc_sample)) / (torch.max(loc_sample) - torch.min(loc_sample)) - 1.\n",
    "                    #loc_sample[0] = x0_new.detach() #+ torch.randn(x0_new.detach().size()) * .001 #g_theta2.detach() #loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_size,2)) + vf.low)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_new.detach()],0)\n",
    "                    for i in range(loc_sample.shape[0]):\n",
    "                        if loc_sample[i,0] < -3. or loc_sample[i,0] > 3. or loc_sample[i,1] < -3. or loc_sample[i,1] > 3.:\n",
    "                            print('samples escaped box')\n",
    "                            loc_sample[i] = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    \n",
    "                    \n",
    "#                     if p_val > 0.99 and p_val_f12 > 0.99:\n",
    "#                         num_base_kernels = max(num_base_kernels - 1, 3)\n",
    "                        #iter_hp = iter_hp - 10\n",
    "                    chi_f_target = (Qf12 ).inv_quad(f_target - pf1)\n",
    "                    p_val_f_target = 1. - stats.chi2.cdf(chi_f_target, Qf12.shape[0])\n",
    "                    print('p_val_ftarget is '+str(p_val_f_target))\n",
    "                    if (p_val_f_target > .95):\n",
    "                        print('acquiring target point becuse p_val_ftarget is '+str(p_val_f_target))\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "        \n",
    "\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                    else:\n",
    "#                         x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "#                         g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                       \n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                        \n",
    "                    if x0_new.detach()[0,0] < -3. or x0_new.detach()[0,0] > 3. or x0_new.detach()[0,1] < -3. or x0_new.detach()[0,1] > 3.:\n",
    "#                         x0 = Tensor(np.array([0.0,-1.0])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = x0.reshape(1,2) \n",
    "                        #x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = x0.reshape(1,2)\n",
    "                        #x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                 \n",
    "             #       loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "                        loc_sample[-1] = x0 #(x0_new.detach()) \n",
    "                    print('new 2 points')\n",
    "                    print(loc_sample)\n",
    "                  \n",
    " #                    agg_data  = (agg_data  - agg_data.mean())/agg_data .std(dim=-1, keepdim=True)\n",
    "#                     g_theta1 = (g_theta1 - g_theta1.mean())/g_theta1.std(dim=-2, keepdim=True)\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            #clear_output(wait=False)\n",
    "           \n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "torch.save(model, PATH)\n",
    "print('current sol is'+str(x0_new.detach()))\n",
    "    \n",
    "print('Success is ' + str(SUCCESS) + ' and failure is ' + str(FAILURE)+' after '+ str(iter) + ' iterations')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3197],\n",
      "        [0.3347]], grad_fn=<CopySlices>)\n",
      "tensor([[0.3295],\n",
      "        [0.3445]], grad_fn=<CopySlices>)\n",
      "tensor([[0.3370],\n",
      "        [0.3492]], dtype=torch.float32)\n",
      "tensor([[0.3390],\n",
      "        [0.3512]], dtype=torch.float32)\n",
      "tensor([[0.3246],\n",
      "        [0.3396]], grad_fn=<ReshapeAliasBackward0>)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target - 0.001)\n",
    "print(f_target + 0.001)\n",
    "print(pf1)\n",
    "print(num_base_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting model validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('ticks')\n",
    "#plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "p21_vec_plot = p21_vec.detach()[1:]\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "#print(p21_vec_plot)\n",
    "ax.plot(p21_vec_plot,'s',color = 'blue', markersize=6)\n",
    "ax.axhline(.01,linestyle = '--',color = 'red', markersize=12, alpha = 1.0)\n",
    "ax.set_xlim(-0.3, p21_vec_plot.shape[0])\n",
    "ax.set_ylim(-0.3, 1.)\n",
    "#ax.set_yscale('log')\n",
    "plt.xticks(np.arange(0, iter+7, step=5.))\n",
    "ax.tick_params(labelsize='small', width=3)\n",
    "ax.set_xlabel('# of Model Checks')\n",
    "ax.set_ylabel('p-value')\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.05, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(1.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.02, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(0.1,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.26, 0.34), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(6.2,        #x start point\n",
    "             0.14,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.07,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.29, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(7.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.57, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(14.15,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.69, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(17.18,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.32, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(10.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "# ############################\n",
    "\n",
    "# ############################\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.33, 0.34), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(11.25,        #x start point\n",
    "#              0.14,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.07,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# #################################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.38, 0.1), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(12.25,        #x start point\n",
    "#              -0.15,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.1,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.38, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(13.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.55, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(18.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')  \n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.49, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(17.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.55, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(19.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.94, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(33.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ################################\n",
    "\n",
    "ax.annotate('Threshold Line ($y = 0.01$)', xy=(1.0,0.01), xytext=(6,0), color='red', \n",
    "                xycoords = ax.get_yaxis_transform(), textcoords=\"offset points\",\n",
    "                size=14, va=\"center\")\n",
    "\n",
    "\n",
    "#ax.legend(['p-value'], loc = 'upper left', fontsize = 30)\n",
    "plt.savefig('figures_Carlo/qvalue_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting expected value and data fit term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (53,) and torch.Size([52, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m entropy_vec_plot \u001b[38;5;241m=\u001b[39m entropy_vec\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      4\u001b[0m f, (ax1,ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m8\u001b[39m), tight_layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43max1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mentropy_vec_plot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--o\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#ax1.set_yscale('log')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#ax1.set_yscale('log')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ax2\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)), data_fit_vec_plot, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--o\u001b[39m\u001b[38;5;124m'\u001b[39m, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, markersize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n",
      "File \u001b[0;32m/lus/theta-fs0/software/thetagpu/conda/2022-07-01/mconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/lus/theta-fs0/software/thetagpu/conda/2022-07-01/mconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/theta-fs0/software/thetagpu/conda/2022-07-01/mconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (53,) and torch.Size([52, 1])"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('ticks')\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(18, 8), tight_layout=True)\n",
    "\n",
    "ax1.plot(np.array(range(1,iter+2)), (entropy_vec_plot), '--o', color = 'blue', markersize=12)\n",
    "#ax1.set_yscale('log')\n",
    "# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\n",
    "#ax1.set_yscale('log')\n",
    "\n",
    "ax2.plot(np.array(range(1,iter+2)), data_fit_vec_plot, '--o', color = 'red', markersize=12)\n",
    "#ax2.set_ylim(-10, 600)\n",
    "ax1.set_xlabel('Iteration #', size=32)\n",
    "ax2.set_xlabel('Iteration #', size=32)\n",
    "ax1.set_ylabel('Expected Information', size = 32)\n",
    "ax2.set_ylabel('Log-Gaussian Term', size = 32)\n",
    "ax1.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "ax2.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "plt.savefig('figures_Carlo/exp_info/expectedinfo_vs_datafit_2_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving all data needed for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/vec_x_success_base.txt',vec_x.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta2_success_base.txt', g_theta2.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta1_success_base.txt', g_theta1.detach().numpy())\n",
    "np.savetxt('data_plots/x_train_ini_success_base.txt', x_train.detach().numpy())\n",
    "np.savetxt('data_plots/y_train_ini_success_base.txt', y_train.detach().numpy())\n",
    "np.savetxt('data_plots/entropy_vec_success_base.txt', entropy_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/datafit_success_base.txt', data_fit_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/p21_vec_success_base.txt',p21_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/loss_success_base.txt',loss.detach().numpy())\n",
    "np.savetxt('data_plots/pf1_success_base.txt',pf1.detach().numpy())\n",
    "np.savetxt('data_plots/Qf1_success_base.txt',Qf1.evaluate().detach().numpy())\n",
    "np.savetxt('data_plots/Qf12_success_base.txt', Qf12.evaluate().detach().numpy())\n",
    "np.savetxt( 'data_plots/Q21_success_base.txt', Q21.evaluate().detach().numpy())\n",
    "#np.savetxt('data_plots/iter_success.txt', iter+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2),2)\n",
    "torch.save(v2, 'data_plots/v2_success_base.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.set_xlim(-3.1, 3.2)\n",
    "ax.set_ylim(-3.1, 3.2)\n",
    "#ax.scatter(g_theta1[:, 0].detach(),g_theta1[:, 1].detach(), c=\"b\", alpha=0.8)\n",
    "ax.plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(vec_x[-1,0], vec_x[-1,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd', color = 'green',markersize=15)\n",
    "ax.set_title('Final TAD configuration', fontsize = 40)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.legend(['1-points', 'TAD solution', 'Target'])\n",
    "plt.savefig('figures_Carlo/strategies/tad_sol_all_2_base.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "#v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "ii = 0\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "########################\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "ax.plot(vec_x[ii,0], vec_x[ii,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o', color = 'blue', markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Initial Configuration', fontsize = 40)\n",
    "ax.legend(['Target', 'Initial Target Candidate', 'Initial 1-sample','Initial 2-sample'])\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_ini_2_base.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "#for iter_plt in range(1,iter):\n",
    "iter_plt = 13\n",
    "ii = 3 + (iter_plt - 1) * (loc_size + 1)\n",
    "#######################\n",
    "\n",
    " \n",
    "###########\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(14,14))\n",
    "\n",
    "#ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 'bv', markersize=8)\n",
    "for i in range(1, iter_plt):\n",
    "    ax.plot(vec_x[i-1:i,0], vec_x[i-1:i,1],'v', color = 'red', markersize=15, alpha=0.2, label = '_nolegend_')\n",
    "    \n",
    "ax.plot(vec_x[0:iter_plt,0], vec_x[0:iter_plt,1],'v', color = 'red', markersize=15, linewidth=15, alpha= 0.2)\n",
    "#ax.plot(g_theta1.detach()[:,0], g_theta.detach()[:,1], 'bv', markersize=8)\n",
    "ax.plot(v2.detach()[0:ii,0], v2.detach()[0:ii,1], 's', color = 'blue', markersize=15,alpha=0.2)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o',color = 'blue' , markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd',markersize=15)\n",
    "ax.plot(vec_x[iter_plt,0], vec_x[iter_plt,1],'v', color = 'red',markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "#ax.set_title('Iteration '+str(iter_plt), fontsize = 40)\n",
    "ax.set_title('Final Configuration')\n",
    "ax.legend([ 'Previous Target Candidates', 'Previous 2-samples', 'Initial 1-sample','Current 2-sample', 'Target', 'Current Target Candidate'], fontsize = 30)\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_base'+str(iter_plt)+'_final.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta2_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_pll(x0,f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    #g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': x_d, 'lr': 0.001}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        #print(g_theta)\n",
    "        loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood, noise_value)\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood ,noise_value)\n",
    "    #loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "   # print(optimizer.state_dict())\n",
    "    print(x_d)\n",
    "    return x_d, lower_bound, upper_bound\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(vf.low, vf.high, 15)\n",
    "y_plot = np.linspace(vf.low, vf.high, 15)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat = torch.zeros(n * n, 2)\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "\n",
    "g_theta_grid = x_concat\n",
    "agg_data1_grid = vfield_(g_theta_grid)\n",
    "agg_data1_grid = agg_data1_grid.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.,2.])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_grid = Tensor(np.array([0.0,0.0])) \n",
    "vec_x_grid = vec_x_grid.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "model_grid, likelihood_grid = hyper_opti(g_theta_grid,agg_data1_grid,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_grid.eval()\n",
    "likelihood_grid.eval()\n",
    "x0_new_grid,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_grid, agg_data1_grid, model_grid, likelihood_grid, iter_design, iter_param, lr_new,noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "#loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_grid)\n",
    "print(SUCCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_grid = x0_new.detach()\n",
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.plot(x_concat[:,0],x_concat[:,1], 's', color = 'blue', markersize=15, alpha = 0.2)\n",
    "ax.plot(x0_new_grid.detach()[0,0], x0_new_grid.detach()[0,1],'v',color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Grid Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/grid_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_hp = 30\n",
    "# iter_design = 40 \n",
    "# iter_param = 50\n",
    "# num_base_kernels = 3\n",
    "\n",
    "# f_target = Tensor(vf.tgt_vec) \n",
    "# f_target = f_target.reshape(f_target.shape[0],1) \n",
    "# tol_vector = 0.005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size_rdn = math.ceil(g_theta1.shape[0]) #(iter)*(loc_size+1) + sample_size\n",
    "\n",
    "loc_sample = high_minus_low  * np.random.random_sample((loc_size_rdn,2)) + vf.low #np.random.random_sample((loc_size_rdn,2))\n",
    "g_theta_ = (Tensor(loc_sample).clone())\n",
    "agg_data1 = vfield_(g_theta_)\n",
    "agg_data1 = agg_data1.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.0,2.0])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_rdn = Tensor(np.array([0.,0.])) \n",
    "vec_x_rdn = vec_x_rdn.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "\n",
    "model_rdn, likelihood_rdn = hyper_opti(g_theta_,agg_data1,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_rdn.eval()\n",
    "likelihood_rdn.eval()\n",
    "x0_new_rdn,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_, agg_data1, model_rdn, likelihood_rdn, iter_design, iter_param, lr_new, noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_rdn)\n",
    "print(SUCCESS)\n",
    "sol_rdn = x0_new_rdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/sol_rdn_success_base.txt', sol_rdn.detach().numpy())\n",
    "np.savetxt('data_plots/sol_grid_success_base.txt', x0_new_grid.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta_rdn_success_base.txt', g_theta_.detach().numpy())\n",
    "#np.savetxt('data_plots/g_theta_grid_success.txt', x_concat.detach().numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "\n",
    "ax.plot(g_theta_[:,0].detach(),g_theta_[:,1].detach(), 's', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(sol_rdn.detach()[0,0], sol_rdn.detach()[0,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$', fontsize = 32)\n",
    "ax.set_ylabel('$d_2$', fontsize = 32)\n",
    "ax.set_title('Uniformly Random Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/rdn_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizing Means and Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker\n",
    "class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "        self.oom = order\n",
    "        self.fformat = fformat\n",
    "        matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
    "    def _set_order_of_magnitude(self):\n",
    "        self.orderOfMagnitude = self.oom\n",
    "    def _set_format(self, vmin=None, vmax=None):\n",
    "        self.format = self.fformat\n",
    "        if self._useMathText:\n",
    "             self.format = r'$\\mathdefault{%s}$' % self.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "likelihood.eval()\n",
    "\n",
    "#noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "#print(x_concat_)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(True):\n",
    "    pred = GPprediction(model)\n",
    "    pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "    #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "    pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "    mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "    mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "    pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "    \n",
    "    var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "    var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "#     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "# #     print(pr.covariance_matrix.mean(axis=0))\n",
    "# #     print(AA)\n",
    "# #     print(pr.variance)\n",
    "# #     print((pr.covariance_matrix))\n",
    "# #     K = model.covar_module\n",
    "#     print((cov.diag()))\n",
    "#     print(pr_mean)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 2, figsize = (28, 24), tight_layout=True)\n",
    "diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "cs10 = ax1[0].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax1[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[0].set_title('$|v_1 - \\mu(v_1)|$', fontsize = 40)\n",
    "cbar10 = fig.colorbar(cs10, ax = ax1[0],format=OOMFormatter(0, mathText=False));\n",
    "\n",
    "ax1[0].set_xlabel('$d_1$')\n",
    "ax1[0].set_ylabel('$d_2$')\n",
    "diff_mean_v1 = torch.abs(v_1 - mean_v_1.detach())/torch.sqrt(var_v_1)\n",
    "#print(var_v_1)\n",
    "cs11 = ax1[1].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(diff_mean_v1.min(), diff_mean_v1.max(), 100), cmap = 'jet')\n",
    "ax1[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[1].set_title('$|v_1 - \\mu(v_1)|/\\sigma(v_1)$', fontsize = 40)\n",
    "# ax1[0].set_aspect('equal')\n",
    "# ax1[1].set_aspect('equal')\n",
    "cbar11 = fig.colorbar(cs11, ax = ax1[1],format=OOMFormatter(0, mathText=False));\n",
    "ax1[1].set_xlabel('$d_1$')\n",
    "ax1[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())\n",
    "cs20 = ax2[0].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax2[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[0].set_title('$|v_2 - \\mu(v_2)|$', fontsize = 40)\n",
    "cbar20 = fig.colorbar(cs20, ax = ax2[0],format=OOMFormatter(0, mathText=False));\n",
    "ax2[0].set_xlabel('$d_1$')\n",
    "ax2[0].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())/torch.sqrt(var_v_2)\n",
    "cs21 = ax2[1].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(diff_mean_v2.min(), diff_mean_v2.max(), 100), cmap = 'jet')\n",
    "ax2[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[1].set_title('$|v_2 - \\mu(v_2)|/\\sigma(v_2)$', fontsize = 40)\n",
    "cbar21 = fig.colorbar(cs21, ax = ax2[1],format=OOMFormatter(0, mathText=False));\n",
    "ax2[1].set_xlabel('$d_1$')\n",
    "ax2[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "# ax2[0].set_aspect('equal')\n",
    "# ax2[1].set_aspect('equal')\n",
    "\n",
    "plt.savefig('figures_Carlo/mean_var/mean_final_2.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "plot = [1, 6, 10, iter+1]\n",
    "\n",
    "for ii in plot:\n",
    "    try:\n",
    "        \n",
    "        PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "    except:\n",
    "        PATH = \".//model_Carlo/model_update/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "        \n",
    "    #model_16 = torch.load(PATH)\n",
    "    model_16.eval()\n",
    "\n",
    "    likelihood.eval()\n",
    "\n",
    "    #noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "    #print(x_concat_)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var(False):\n",
    "        pred = GPprediction(model_16)\n",
    "        pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "        #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "        pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "        mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "        mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "        pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "\n",
    "        var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "        var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "    #     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "    # #     print(pr.covariance_matrix.mean(axis=0))\n",
    "    # #     print(AA)\n",
    "    # #     print(pr.variance)\n",
    "    # #     print((pr.covariance_matrix))\n",
    "    # #     K = model.covar_module\n",
    "    #     print((cov.diag()))\n",
    "    #     print(pr_mean)\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (28, 12), tight_layout=True)\n",
    "    diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "    minn = torch.min(var_v_1.detach().min(), var_v_2.detach().min())\n",
    "    maxx = torch.min(var_v_1.detach().max(), var_v_2.detach().max())\n",
    "\n",
    "    cs11 = ax1.contourf(xv_plot, yv_plot, var_v_1.detach(), np.linspace(0.0,1.1, 100), cmap = 'jet')\n",
    "    ax1.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    \n",
    "    ax1.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "  \n",
    "    ax1.set_title('$\\sigma^2(v_1)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    # ax1[0].set_aspect('equal')\n",
    "    # ax1[1].set_aspect('equal')\n",
    "    cbar11 = fig.colorbar(cs11, ax = ax1,format=OOMFormatter(-0, mathText=False));\n",
    "    ax1.set_xlabel('$d_1$')\n",
    "    ax1.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    cs21 = ax2.contourf(xv_plot, yv_plot, var_v_2.detach(), np.linspace(0.0, 1.1, 100), cmap = 'jet')\n",
    "    ax2.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "    ax2.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    ax2.set_title('$\\sigma^2(v_2)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    cbar21 = fig.colorbar(cs21, ax = ax2,format=OOMFormatter(-0, mathText=False));\n",
    "    ax2.set_xlabel('$d_1$')\n",
    "    ax2.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "    # ax2[0].set_aspect('equal')\n",
    "    # ax2[1].set_aspect('equal')\n",
    "\n",
    "    plt.savefig('figures_Carlo/mean_var/var_iter_base'+str(ii)+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# # #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "# #                     #loc_size = 4\n",
    "# # loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# # loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# # g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# z = torch.zeros(n*n, 1)\n",
    "# for ii in range(n*n):\n",
    "#     print(ii)\n",
    "#     x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "#     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#     g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21 = conduct_2opt(agg_data,f_target,x0, g_theta1, model, likelihood, noise_value, g2, training_param_iter)\n",
    "#     z[ii] = loss2\n",
    "# z = z.reshape(n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "# loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# g2 = g_theta2.detach() #loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# plot = [1, 7, 17, 26]\n",
    "# zz = torch.zeros(n*n, 4)\n",
    "# kk = 0\n",
    "# for jj in plot:\n",
    "#     try:\n",
    "        \n",
    "#         PATH = \".//model_Carlo/model_goodmodel/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#     except:\n",
    "#         PATH = \".//model_Carlo/model_update/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#    # model_16 = torch.load(PATH)\n",
    "#     model_16.eval()\n",
    "\n",
    "#     likelihood.eval()\n",
    "#     g2 = v2.detach()[jj - 1 +loc_size+1 : jj - 1 +loc_size+1 +loc_size+1]\n",
    "#     g_theta1_cur = g_theta1[0:(4 + (jj - 1)* 3)]\n",
    "#     agg_data_cur = agg_data[0:2 * (4 + (jj - 1)* 3)]\n",
    "#     print(agg_data_cur.shape)\n",
    "#     for ii in range(n*n):\n",
    "#         print(ii)\n",
    "#         x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     #     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#     #                     #loc_size = 4\n",
    "#     #     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     #     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#         #g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "\n",
    "\n",
    "        \n",
    "#         loss2_, pf1_, Qf1_, Qf12_, data_fit_, Q21_ = likelihood.get_ell(agg_data_cur,f_target,x0, g_theta1_cur, model_16, likelihood, noise_value, g2)\n",
    "#         zz[ii, kk] = loss2_\n",
    "#     kk = kk+1\n",
    "# zz = zz.reshape(n,n, 4)\n",
    "# torch.save(zz.detach(), 'data_plots/zz_success.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot = [1, 5, 10, 20]\n",
    "\n",
    "# for jj in range(4):\n",
    "#     fig, ax = plt.subplots(figsize = (16,14))\n",
    "#     #\n",
    "#     cs = ax.contour(xv_plot, yv_plot,  zz[:,:,jj].detach(), np.linspace( zz[:,:,jj].detach().numpy().min(), zz[:,:,jj].detach().numpy().max(), 1000), cmap = 'jet')\n",
    "#     cbar = fig.colorbar(cs, ax = ax,format=OOMFormatter(0, mathText=False));\n",
    "#     ax.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "#     kk = plot[jj]\n",
    "#     if kk < plot[3]:\n",
    "#         ax.plot(vec_x[kk,0], vec_x[kk,1],'o', color = 'black',markersize=12)\n",
    "#     if kk == plot[3]:\n",
    "#         ax.plot(vec_x[- 1,0], vec_x[- 1,1],'o', color = 'black',markersize=12)\n",
    "#     ax.set_title('TAD Acquisition Function (Iteration '+str(kk)+')', fontsize = 40)\n",
    "#     ax.set_xlabel('$d_1$')\n",
    "#     ax.set_ylabel('$d_2$')\n",
    "    \n",
    "#     plt.savefig('figures_Carlo/tad_obj'+str(kk)+'.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p21_vec_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
