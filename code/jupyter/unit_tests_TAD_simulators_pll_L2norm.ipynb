{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import sep_vvkernels as svvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "from LBFGS import FullBatchLBFGS\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = VField()\n",
    "f_target = vf.tgt_vec\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = Tensor(vf(x[i])) #+ torch.randn(Tensor(vf(x[i])).size()) * 0.02\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(vvk_rbf.vvkRBFKernel())\n",
    "\n",
    "\n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "#\\         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "       # self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels):\n",
    "    noises = torch.ones(agg_data.shape[0]) * 0.001\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = FullBatchLBFGS(model.parameters(), lr=.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    def closure():\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "        #output_ll = likelihood(output)\n",
    "\n",
    "        #loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss = -mll(output, agg_data)\n",
    "        print('Loss gp: %.3f' % ( loss))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    loss = closure()\n",
    "    loss.backward()\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        options = {'closure': closure, 'current_loss': loss, 'max_ls': 10}\n",
    "        loss, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "\n",
    "        if fail:\n",
    "            print('Convergence reached!')\n",
    "            break\n",
    "        #print(i)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class design_opti_pll(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super(design_opti_pll, self).__init__()\n",
    "        #loc = np.random.random_sample((loc_size,2))\n",
    "        #self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "        self.x_design = nn.Parameter(Tensor(x))\n",
    "    def forward(self):\n",
    "       \n",
    "        #g_theta2_new = self.g_theta2 #filter_sample(self.g_theta2, 0.009)\n",
    "        \n",
    "        return self.x_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_pll(x0,f_target, g_theta, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new):\n",
    "    design = design_opti_pll(x0)\n",
    "    loc_sample0 = loc_sample\n",
    "    x_d = design.forward()\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta, agg_data, model, likelihood)\n",
    "        #loss2 = -1. * loss2\n",
    "        loss2.backward(retain_graph=True)\n",
    "#         print(x_d)\n",
    "#         print(lower_bound)\n",
    "#         print(upper_bound)\n",
    "       \n",
    "        return loss2\n",
    "        \n",
    "        \n",
    "        \n",
    "    optimizer = torch.optim.LBFGS(design.parameters(), lr=lr_new, history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    x_d = design.forward()\n",
    "    loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta, agg_data, model, likelihood)\n",
    "    #loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "   # print(optimizer.state_dict())\n",
    "    print(x_d)\n",
    "    return x_d, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.800\n",
      "Loss gp: 0.788\n",
      "Loss gp: 0.776\n",
      "Loss gp: 0.751\n",
      "Loss gp: 0.700\n",
      "Loss gp: 0.594\n",
      "Loss gp: 0.369\n",
      "Loss gp: -0.122\n",
      "Loss gp: -1.160\n",
      "Loss gp: -2.926\n",
      "Loss gp: -3.259\n",
      "Loss gp: -3.264\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.280\n",
      "Loss gp: -3.292\n",
      "Loss gp: -3.303\n",
      "Loss gp: -3.313\n",
      "Loss gp: -3.322\n",
      "Loss gp: -3.332\n",
      "Loss gp: -3.340\n",
      "Loss gp: -3.351\n",
      "Loss gp: -3.361\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.381\n",
      "Loss gp: -3.387\n",
      "Loss gp: -3.393\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.404\n",
      "Loss gp: -3.409\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.419\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.424\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.427\n",
      "Loss gp: -3.427\n",
      "Loss gp: -3.428\n",
      "Loss gp: -3.429\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.433\n",
      "Loss gp: -3.433\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.435\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.435\n",
      "Loss gp: -3.435\n",
      "Loss gp: -3.435\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/utils/cholesky.py:83: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: 0.000\n",
      "Parameter containing:\n",
      "tensor([[0.2012, 0.1003]], requires_grad=True)\n",
      "tensor([[0.4973],\n",
      "        [0.9966]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5025],\n",
      "        [1.0035]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2012, 0.1003]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "iter_hp = 75\n",
    "iter_design = 40 \n",
    "iter_param = 50\n",
    "num_base_kernels = 3\n",
    "\n",
    "f_target = Tensor(vf.tgt_vec) \n",
    "f_target = f_target.reshape(f_target.shape[0],1) \n",
    "tol_vector = 0.005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size = 260\n",
    "loc_sample = np.random.random_sample((loc_size,2))\n",
    "g_theta_ = (Tensor(loc_sample).clone())\n",
    "agg_data1 = vfield_(g_theta_)\n",
    "agg_data1 = agg_data1.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([0.5,0.7])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x = Tensor(np.array([0.5,0.7])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "model, likelihood = hyper_opti(g_theta_,agg_data1,iter_hp,num_base_kernels)\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "\n",
    "x0_new,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_, agg_data1, model, likelihood, iter_design, iter_param, lr_new)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "loc_sample = np.random.random_sample((loc_size,2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(x0_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
