{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import sep_vvkernels as svvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = VField()\n",
    "f_target = vf.tgt_vec\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "def g_theta(sample_size, D):\n",
    "    loc = np.random.random_sample((sample_size,D))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "train_y = torch.zeros([sample_size, N])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    train_y[i] = Tensor(vf(train_x[i]))# + torch.randn(Tensor(vf(train_x[i])).size()) * 0.005\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = Tensor(vf(x[i])) #+ torch.randn(Tensor(vf(x[i])).size()) * 0.005\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criteria(tol_vector, f_target, lower_bound, upper_bound):\n",
    "    lower_tol_vector = f_target - tol_vector\n",
    "    upper_tol_vector = f_target + tol_vector\n",
    "    SUCCESS = True\n",
    "    FAILURE = False\n",
    "    for i in range(f_target.shape[0]):\n",
    "            if (lower_bound[i] < lower_tol_vector[i]) or  (upper_bound[i] > upper_tol_vector[i]):\n",
    "                SUCCESS = False  \n",
    "            if ((lower_bound[i] > upper_tol_vector[i]) or  (upper_bound[i] < lower_tol_vector[i])):\n",
    "                FAILURE = True\n",
    "    return SUCCESS, FAILURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "        base_kernels = [vvk_rbf.vvkRBFKernel(), vvk_rbf.vvkRBFKernel(), vvk_rbf.vvkRBFKernel(), vvk_rbf.vvkRBFKernel(), vvk_rbf.vvkRBFKernel(), vvk_rbf.vvkRBFKernel(), vvk_rbf.vvkRBFKernel()]\n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "#\\         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "       # self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "        #output_ll = likelihood(output)\n",
    "\n",
    "        #loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss = -mll(output, agg_data)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        print('Iter %d/%d - Loss hyperparam: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class param_opti(nn.Module):\n",
    "    def __init__(self, sample):\n",
    "        super(param_opti, self).__init__()\n",
    "        #loc = np.random.random_sample((loc_size,2))\n",
    "        self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return (self.g_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_param_opti(x,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_iter):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    _par = param_opti(loc_sample)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        g_theta2 = _par.forward()\n",
    "\n",
    "        loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "        loss1 = -1 * loss1\n",
    "        \n",
    "        return loss1\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(_par.parameters(), lr=1., history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    g_theta2 = _par.forward()\n",
    "\n",
    "    loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "    loss1 = -1 * loss1\n",
    "    \n",
    "    print('Loss theta: %.3f' % ( loss1))\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(_par.parameters(), lr=0.007)\n",
    "\n",
    "# #     for j in range(training_iter):\n",
    "# #         optimizer.zero_grad()\n",
    "# #         g_theta2 = _par.forward()\n",
    "\n",
    "# #         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "# #         loss1 = -1 * loss1\n",
    "# #         loss1.backward(retain_graph=True)\n",
    "# #         print('Iter %d/%d - Loss theta2: %.3f' % (j + 1, training_iter, loss1.item()))\n",
    "# #         optimizer.step()\n",
    "          \n",
    "    return loss1, g_theta2, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class design_opti(nn.Module):\n",
    "    def __init__(self, _x):\n",
    "        super(design_opti, self).__init__()\n",
    "#         a = _x[0,0]\n",
    "#         b = _x[0,1]\n",
    "#         self.x_design_1 = nn.Parameter(a)\n",
    "#         self.x_design_2 = nn.Parameter(b)\n",
    "        self.x_design = nn.Parameter(_x)\n",
    "    def forward(self):\n",
    "#         x_design = torch.zeros(1,2)\n",
    "#         x_design[0,0] = self.x_design_1\n",
    "#         x_design[0,1] = self.x_design_2\n",
    "        return (self.x_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new):\n",
    "    design = design_opti(x0)\n",
    "    loc_sample0 = loc_sample\n",
    "    x_d = design.forward()\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss2, g_theta2_out,lower_bound, upper_bound = conduct_param_opti(x_d,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_param_iter)\n",
    "        loss2.backward(retain_graph=True)\n",
    "#         print(x_d)\n",
    "#         print(lower_bound)\n",
    "#         print(upper_bound)\n",
    "       \n",
    "        return loss2\n",
    "        \n",
    "        \n",
    "        \n",
    "    optimizer = torch.optim.LBFGS(design.parameters(), lr=lr_new, history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    x_d = design.forward()\n",
    "    loss2, g_theta2_out,lower_bound, upper_bound = conduct_param_opti(x_d,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_param_iter) ##takethis out\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    print(optimizer.state)\n",
    "    return x_d, g_theta2_out, lower_bound, upper_bound\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.905\n",
      "Iter 2/80 - Loss hyperparam: 0.859\n",
      "Iter 3/80 - Loss hyperparam: 0.813\n",
      "Iter 4/80 - Loss hyperparam: 0.768\n",
      "Iter 5/80 - Loss hyperparam: 0.722\n",
      "Iter 6/80 - Loss hyperparam: 0.677\n",
      "Iter 7/80 - Loss hyperparam: 0.631\n",
      "Iter 8/80 - Loss hyperparam: 0.586\n",
      "Iter 9/80 - Loss hyperparam: 0.540\n",
      "Iter 10/80 - Loss hyperparam: 0.495\n",
      "Iter 11/80 - Loss hyperparam: 0.450\n",
      "Iter 12/80 - Loss hyperparam: 0.404\n",
      "Iter 13/80 - Loss hyperparam: 0.359\n",
      "Iter 14/80 - Loss hyperparam: 0.313\n",
      "Iter 15/80 - Loss hyperparam: 0.268\n",
      "Iter 16/80 - Loss hyperparam: 0.222\n",
      "Iter 17/80 - Loss hyperparam: 0.177\n",
      "Iter 18/80 - Loss hyperparam: 0.131\n",
      "Iter 19/80 - Loss hyperparam: 0.085\n",
      "Iter 20/80 - Loss hyperparam: 0.038\n",
      "Iter 21/80 - Loss hyperparam: -0.009\n",
      "Iter 22/80 - Loss hyperparam: -0.056\n",
      "Iter 23/80 - Loss hyperparam: -0.104\n",
      "Iter 24/80 - Loss hyperparam: -0.152\n",
      "Iter 25/80 - Loss hyperparam: -0.201\n",
      "Iter 26/80 - Loss hyperparam: -0.250\n",
      "Iter 27/80 - Loss hyperparam: -0.299\n",
      "Iter 28/80 - Loss hyperparam: -0.348\n",
      "Iter 29/80 - Loss hyperparam: -0.397\n",
      "Iter 30/80 - Loss hyperparam: -0.447\n",
      "Iter 31/80 - Loss hyperparam: -0.496\n",
      "Iter 32/80 - Loss hyperparam: -0.545\n",
      "Iter 33/80 - Loss hyperparam: -0.594\n",
      "Iter 34/80 - Loss hyperparam: -0.643\n",
      "Iter 35/80 - Loss hyperparam: -0.691\n",
      "Iter 36/80 - Loss hyperparam: -0.740\n",
      "Iter 37/80 - Loss hyperparam: -0.789\n",
      "Iter 38/80 - Loss hyperparam: -0.838\n",
      "Iter 39/80 - Loss hyperparam: -0.887\n",
      "Iter 40/80 - Loss hyperparam: -0.935\n",
      "Iter 41/80 - Loss hyperparam: -0.984\n",
      "Iter 42/80 - Loss hyperparam: -1.032\n",
      "Iter 43/80 - Loss hyperparam: -1.080\n",
      "Iter 44/80 - Loss hyperparam: -1.128\n",
      "Iter 45/80 - Loss hyperparam: -1.176\n",
      "Iter 46/80 - Loss hyperparam: -1.224\n",
      "Iter 47/80 - Loss hyperparam: -1.272\n",
      "Iter 48/80 - Loss hyperparam: -1.320\n",
      "Iter 49/80 - Loss hyperparam: -1.368\n",
      "Iter 50/80 - Loss hyperparam: -1.415\n",
      "Iter 51/80 - Loss hyperparam: -1.463\n",
      "Iter 52/80 - Loss hyperparam: -1.510\n",
      "Iter 53/80 - Loss hyperparam: -1.557\n",
      "Iter 54/80 - Loss hyperparam: -1.604\n",
      "Iter 55/80 - Loss hyperparam: -1.650\n",
      "Iter 56/80 - Loss hyperparam: -1.697\n",
      "Iter 57/80 - Loss hyperparam: -1.743\n",
      "Iter 58/80 - Loss hyperparam: -1.788\n",
      "Iter 59/80 - Loss hyperparam: -1.834\n",
      "Iter 60/80 - Loss hyperparam: -1.879\n",
      "Iter 61/80 - Loss hyperparam: -1.924\n",
      "Iter 62/80 - Loss hyperparam: -1.968\n",
      "Iter 63/80 - Loss hyperparam: -2.012\n",
      "Iter 64/80 - Loss hyperparam: -2.055\n",
      "Iter 65/80 - Loss hyperparam: -2.099\n",
      "Iter 66/80 - Loss hyperparam: -2.141\n",
      "Iter 67/80 - Loss hyperparam: -2.183\n",
      "Iter 68/80 - Loss hyperparam: -2.224\n",
      "Iter 69/80 - Loss hyperparam: -2.265\n",
      "Iter 70/80 - Loss hyperparam: -2.305\n",
      "Iter 71/80 - Loss hyperparam: -2.345\n",
      "Iter 72/80 - Loss hyperparam: -2.382\n",
      "Iter 73/80 - Loss hyperparam: -2.419\n",
      "Iter 74/80 - Loss hyperparam: -2.458\n",
      "Iter 75/80 - Loss hyperparam: -2.493\n",
      "Iter 76/80 - Loss hyperparam: -2.529\n",
      "Iter 77/80 - Loss hyperparam: -2.563\n",
      "Iter 78/80 - Loss hyperparam: -2.597\n",
      "Iter 79/80 - Loss hyperparam: -2.630\n",
      "Iter 80/80 - Loss hyperparam: -2.661\n",
      "END HYPERPARAMETERS optimization\n",
      "0\n",
      "Loss theta: 67294.906\n",
      "Loss theta: 85.855\n",
      "Loss theta: 85.885\n",
      "Loss theta: 83.309\n",
      "Loss theta: 85.614\n",
      "Loss theta: 83.953\n",
      "Loss theta: 83.309\n",
      "Loss theta: 84.736\n",
      "Loss theta: 86.234\n",
      "Loss theta: 83.537\n",
      "Loss theta: 86.460\n",
      "Loss theta: 83.309\n",
      "Loss theta: 83.309\n",
      "Loss design: 83.309\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.4182,  0.6178]], requires_grad=True): {'func_evals': 12, 'n_iter': 3, 'al': [tensor(5.6861e-05), tensor(20.3040), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0062,  0.0002]), 't': 0, 'old_dirs': [tensor([-236686.2812,  -21124.6465]), tensor([-15.6349,  -8.4828])], 'old_stps': [tensor([-0.9179, -0.0821]), tensor([-2.6878e-04, -6.0014e-05])], 'ro': [tensor(4.5665e-06), tensor(212.2487)], 'H_diag': tensor(1.4890e-05), 'prev_flat_grad': tensor([340.0850,  70.8715]), 'prev_loss': 83.30894470214844}})\n",
      "tensor(0.9764, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.5241],\n",
      "        [0.6048]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6528],\n",
      "        [0.7271]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.4182,  0.6178]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.881\n",
      "Iter 2/80 - Loss hyperparam: 0.835\n",
      "Iter 3/80 - Loss hyperparam: 0.791\n",
      "Iter 4/80 - Loss hyperparam: 0.746\n",
      "Iter 5/80 - Loss hyperparam: 0.701\n",
      "Iter 6/80 - Loss hyperparam: 0.657\n",
      "Iter 7/80 - Loss hyperparam: 0.612\n",
      "Iter 8/80 - Loss hyperparam: 0.568\n",
      "Iter 9/80 - Loss hyperparam: 0.524\n",
      "Iter 10/80 - Loss hyperparam: 0.480\n",
      "Iter 11/80 - Loss hyperparam: 0.436\n",
      "Iter 12/80 - Loss hyperparam: 0.391\n",
      "Iter 13/80 - Loss hyperparam: 0.347\n",
      "Iter 14/80 - Loss hyperparam: 0.303\n",
      "Iter 15/80 - Loss hyperparam: 0.259\n",
      "Iter 16/80 - Loss hyperparam: 0.214\n",
      "Iter 17/80 - Loss hyperparam: 0.169\n",
      "Iter 18/80 - Loss hyperparam: 0.123\n",
      "Iter 19/80 - Loss hyperparam: 0.077\n",
      "Iter 20/80 - Loss hyperparam: 0.030\n",
      "Iter 21/80 - Loss hyperparam: -0.018\n",
      "Iter 22/80 - Loss hyperparam: -0.066\n",
      "Iter 23/80 - Loss hyperparam: -0.114\n",
      "Iter 24/80 - Loss hyperparam: -0.163\n",
      "Iter 25/80 - Loss hyperparam: -0.212\n",
      "Iter 26/80 - Loss hyperparam: -0.261\n",
      "Iter 27/80 - Loss hyperparam: -0.310\n",
      "Iter 28/80 - Loss hyperparam: -0.359\n",
      "Iter 29/80 - Loss hyperparam: -0.407\n",
      "Iter 30/80 - Loss hyperparam: -0.456\n",
      "Iter 31/80 - Loss hyperparam: -0.505\n",
      "Iter 32/80 - Loss hyperparam: -0.554\n",
      "Iter 33/80 - Loss hyperparam: -0.603\n",
      "Iter 34/80 - Loss hyperparam: -0.653\n",
      "Iter 35/80 - Loss hyperparam: -0.702\n",
      "Iter 36/80 - Loss hyperparam: -0.751\n",
      "Iter 37/80 - Loss hyperparam: -0.801\n",
      "Iter 38/80 - Loss hyperparam: -0.850\n",
      "Iter 39/80 - Loss hyperparam: -0.898\n",
      "Iter 40/80 - Loss hyperparam: -0.947\n",
      "Iter 41/80 - Loss hyperparam: -0.996\n",
      "Iter 42/80 - Loss hyperparam: -1.044\n",
      "Iter 43/80 - Loss hyperparam: -1.093\n",
      "Iter 44/80 - Loss hyperparam: -1.141\n",
      "Iter 45/80 - Loss hyperparam: -1.189\n",
      "Iter 46/80 - Loss hyperparam: -1.237\n",
      "Iter 47/80 - Loss hyperparam: -1.285\n",
      "Iter 48/80 - Loss hyperparam: -1.333\n",
      "Iter 49/80 - Loss hyperparam: -1.381\n",
      "Iter 50/80 - Loss hyperparam: -1.429\n",
      "Iter 51/80 - Loss hyperparam: -1.476\n",
      "Iter 52/80 - Loss hyperparam: -1.524\n",
      "Iter 53/80 - Loss hyperparam: -1.571\n",
      "Iter 54/80 - Loss hyperparam: -1.618\n",
      "Iter 55/80 - Loss hyperparam: -1.665\n",
      "Iter 56/80 - Loss hyperparam: -1.712\n",
      "Iter 57/80 - Loss hyperparam: -1.758\n",
      "Iter 58/80 - Loss hyperparam: -1.804\n",
      "Iter 59/80 - Loss hyperparam: -1.850\n",
      "Iter 60/80 - Loss hyperparam: -1.895\n",
      "Iter 61/80 - Loss hyperparam: -1.940\n",
      "Iter 62/80 - Loss hyperparam: -1.984\n",
      "Iter 63/80 - Loss hyperparam: -2.029\n",
      "Iter 64/80 - Loss hyperparam: -2.073\n",
      "Iter 65/80 - Loss hyperparam: -2.116\n",
      "Iter 66/80 - Loss hyperparam: -2.160\n",
      "Iter 67/80 - Loss hyperparam: -2.202\n",
      "Iter 68/80 - Loss hyperparam: -2.244\n",
      "Iter 69/80 - Loss hyperparam: -2.285\n",
      "Iter 70/80 - Loss hyperparam: -2.326\n",
      "Iter 71/80 - Loss hyperparam: -2.365\n",
      "Iter 72/80 - Loss hyperparam: -2.404\n",
      "Iter 73/80 - Loss hyperparam: -2.441\n",
      "Iter 74/80 - Loss hyperparam: -2.479\n",
      "Iter 75/80 - Loss hyperparam: -2.516\n",
      "Iter 76/80 - Loss hyperparam: -2.553\n",
      "Iter 77/80 - Loss hyperparam: -2.588\n",
      "Iter 78/80 - Loss hyperparam: -2.623\n",
      "Iter 79/80 - Loss hyperparam: -2.654\n",
      "Iter 80/80 - Loss hyperparam: -2.686\n",
      "END HYPERPARAMETERS optimization\n",
      "1\n",
      "Loss theta: 86.323\n",
      "Loss theta: 2.715\n",
      "Loss theta: 2.398\n",
      "Loss theta: 1.971\n",
      "Loss theta: 0.533\n",
      "Loss theta: 0.127\n",
      "Loss theta: -0.074\n",
      "Loss theta: -0.122\n",
      "Loss theta: -0.140\n",
      "Loss theta: -0.160\n",
      "Loss theta: -0.204\n",
      "Loss theta: -0.252\n",
      "Loss theta: -0.405\n",
      "Loss theta: -0.358\n",
      "Loss theta: -0.368\n",
      "Loss theta: -0.359\n",
      "Loss theta: -0.369\n",
      "Loss theta: -0.354\n",
      "Loss theta: -0.369\n",
      "Loss theta: -0.369\n",
      "Loss theta: 1.705\n",
      "Loss theta: 1.705\n",
      "Loss theta: 3.180\n",
      "Loss theta: -0.627\n",
      "Loss theta: -0.812\n",
      "Loss theta: 4.129\n",
      "Loss theta: -1.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss theta: 2.568\n",
      "Loss theta: -0.741\n",
      "Loss theta: -0.979\n",
      "Loss theta: -0.921\n",
      "Loss theta: -0.901\n",
      "Loss theta: -0.882\n",
      "Loss theta: -0.874\n",
      "Loss theta: -1.001\n",
      "Loss theta: -1.001\n",
      "Loss theta: -1.001\n",
      "Loss theta: -1.001\n",
      "Loss design: -1.001\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.8883, -1.3567]], requires_grad=True): {'func_evals': 37, 'n_iter': 14, 'al': [tensor(4.1472e-06), tensor(0.0009), tensor(0.0007), tensor(0.0037), tensor(0.0147), tensor(0.0875), tensor(0.2917), tensor(0.6815), tensor(0.9625), tensor(-2.1203), tensor(34.2799), tensor(-3.6196), tensor(6.5339), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 5.7285, -0.6627]), 't': 0, 'old_dirs': [tensor([-379.6473,  -76.9005]), tensor([-2.7246, -0.5344]), tensor([-5.5660, -1.2006]), tensor([-1.8029, -0.4030]), tensor([-1.2102, -0.2805]), tensor([-0.4022, -0.1058]), tensor([-0.2653, -0.0751]), tensor([-0.1939, -0.0662]), tensor([-0.2558, -0.1163]), tensor([-0.3454, -0.3077]), tensor([0.0306, 0.0515]), tensor([ 0.0525, -0.2717]), tensor([-0.0291, -0.3630])], 'old_stps': [tensor([-0.8309, -0.1691]), tensor([-0.0676, -0.0160]), tensor([-0.2268, -0.0654]), tensor([-0.1456, -0.0540]), tensor([-0.1441, -0.0756]), tensor([-0.0713, -0.0706]), tensor([-0.0443, -0.1021]), tensor([-0.0110, -0.1442]), tensor([ 0.0377, -0.3254]), tensor([ 0.1394, -0.5122]), tensor([ 0.1406, -0.0551]), tensor([ 0.6261, -0.3435]), tensor([ 0.1277, -0.0414])], 'ro': [tensor(0.0030), tensor(5.1860), tensor(0.7457), tensor(3.5177), tensor(5.1125), tensor(27.6652), tensor(51.5161), tensor(85.6412), tensor(35.4901), tensor(9.1345), tensor(684.6923), tensor(7.9250), tensor(88.2519)], 'H_diag': tensor(0.0855), 'prev_flat_grad': tensor([-0.8667, -0.8841]), 'prev_loss': -1.0008063316345215}})\n",
      "tensor(1.2957, grad_fn=<CopyBackwards>)\n",
      "tensor([[-1.5740],\n",
      "        [-0.9715]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6398],\n",
      "        [1.1275]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.8883, -1.3567]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.872\n",
      "Iter 2/80 - Loss hyperparam: 0.828\n",
      "Iter 3/80 - Loss hyperparam: 0.784\n",
      "Iter 4/80 - Loss hyperparam: 0.740\n",
      "Iter 5/80 - Loss hyperparam: 0.696\n",
      "Iter 6/80 - Loss hyperparam: 0.652\n",
      "Iter 7/80 - Loss hyperparam: 0.608\n",
      "Iter 8/80 - Loss hyperparam: 0.563\n",
      "Iter 9/80 - Loss hyperparam: 0.519\n",
      "Iter 10/80 - Loss hyperparam: 0.475\n",
      "Iter 11/80 - Loss hyperparam: 0.432\n",
      "Iter 12/80 - Loss hyperparam: 0.388\n",
      "Iter 13/80 - Loss hyperparam: 0.344\n",
      "Iter 14/80 - Loss hyperparam: 0.300\n",
      "Iter 15/80 - Loss hyperparam: 0.255\n",
      "Iter 16/80 - Loss hyperparam: 0.211\n",
      "Iter 17/80 - Loss hyperparam: 0.165\n",
      "Iter 18/80 - Loss hyperparam: 0.119\n",
      "Iter 19/80 - Loss hyperparam: 0.072\n",
      "Iter 20/80 - Loss hyperparam: 0.025\n",
      "Iter 21/80 - Loss hyperparam: -0.022\n",
      "Iter 22/80 - Loss hyperparam: -0.070\n",
      "Iter 23/80 - Loss hyperparam: -0.118\n",
      "Iter 24/80 - Loss hyperparam: -0.167\n",
      "Iter 25/80 - Loss hyperparam: -0.216\n",
      "Iter 26/80 - Loss hyperparam: -0.265\n",
      "Iter 27/80 - Loss hyperparam: -0.314\n",
      "Iter 28/80 - Loss hyperparam: -0.363\n",
      "Iter 29/80 - Loss hyperparam: -0.413\n",
      "Iter 30/80 - Loss hyperparam: -0.462\n",
      "Iter 31/80 - Loss hyperparam: -0.512\n",
      "Iter 32/80 - Loss hyperparam: -0.561\n",
      "Iter 33/80 - Loss hyperparam: -0.611\n",
      "Iter 34/80 - Loss hyperparam: -0.660\n",
      "Iter 35/80 - Loss hyperparam: -0.709\n",
      "Iter 36/80 - Loss hyperparam: -0.759\n",
      "Iter 37/80 - Loss hyperparam: -0.808\n",
      "Iter 38/80 - Loss hyperparam: -0.858\n",
      "Iter 39/80 - Loss hyperparam: -0.907\n",
      "Iter 40/80 - Loss hyperparam: -0.957\n",
      "Iter 41/80 - Loss hyperparam: -1.006\n",
      "Iter 42/80 - Loss hyperparam: -1.055\n",
      "Iter 43/80 - Loss hyperparam: -1.104\n",
      "Iter 44/80 - Loss hyperparam: -1.152\n",
      "Iter 45/80 - Loss hyperparam: -1.201\n",
      "Iter 46/80 - Loss hyperparam: -1.249\n",
      "Iter 47/80 - Loss hyperparam: -1.298\n",
      "Iter 48/80 - Loss hyperparam: -1.346\n",
      "Iter 49/80 - Loss hyperparam: -1.394\n",
      "Iter 50/80 - Loss hyperparam: -1.442\n",
      "Iter 51/80 - Loss hyperparam: -1.490\n",
      "Iter 52/80 - Loss hyperparam: -1.538\n",
      "Iter 53/80 - Loss hyperparam: -1.586\n",
      "Iter 54/80 - Loss hyperparam: -1.633\n",
      "Iter 55/80 - Loss hyperparam: -1.680\n",
      "Iter 56/80 - Loss hyperparam: -1.727\n",
      "Iter 57/80 - Loss hyperparam: -1.774\n",
      "Iter 58/80 - Loss hyperparam: -1.820\n",
      "Iter 59/80 - Loss hyperparam: -1.866\n",
      "Iter 60/80 - Loss hyperparam: -1.912\n",
      "Iter 61/80 - Loss hyperparam: -1.958\n",
      "Iter 62/80 - Loss hyperparam: -2.002\n",
      "Iter 63/80 - Loss hyperparam: -2.047\n",
      "Iter 64/80 - Loss hyperparam: -2.091\n",
      "Iter 65/80 - Loss hyperparam: -2.135\n",
      "Iter 66/80 - Loss hyperparam: -2.178\n",
      "Iter 67/80 - Loss hyperparam: -2.221\n",
      "Iter 68/80 - Loss hyperparam: -2.263\n",
      "Iter 69/80 - Loss hyperparam: -2.304\n",
      "Iter 70/80 - Loss hyperparam: -2.345\n",
      "Iter 71/80 - Loss hyperparam: -2.385\n",
      "Iter 72/80 - Loss hyperparam: -2.425\n",
      "Iter 73/80 - Loss hyperparam: -2.464\n",
      "Iter 74/80 - Loss hyperparam: -2.501\n",
      "Iter 75/80 - Loss hyperparam: -2.539\n",
      "Iter 76/80 - Loss hyperparam: -2.572\n",
      "Iter 77/80 - Loss hyperparam: -2.608\n",
      "Iter 78/80 - Loss hyperparam: -2.643\n",
      "Iter 79/80 - Loss hyperparam: -2.675\n",
      "Iter 80/80 - Loss hyperparam: -2.708\n",
      "END HYPERPARAMETERS optimization\n",
      "2\n",
      "Loss theta: -0.306\n",
      "Loss theta: -0.279\n",
      "Loss theta: -0.375\n",
      "Loss theta: -0.410\n",
      "Loss theta: -0.440\n",
      "Loss theta: -0.409\n",
      "Loss theta: -0.412\n",
      "Loss theta: -0.429\n",
      "Loss theta: -0.440\n",
      "Loss theta: -0.440\n",
      "Loss design: -0.440\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.5779, -1.4870]], requires_grad=True): {'func_evals': 9, 'n_iter': 4, 'al': [tensor(-0.0075), tensor(-0.1524), tensor(0.1678), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0243,  0.0167]), 't': 0, 'old_dirs': [tensor([ 0.0179, -0.6852]), tensor([0.3171, 0.0445]), tensor([0.0495, 0.1867])], 'old_stps': [tensor([ 0.1580, -0.1947]), tensor([0.1545, 0.0029]), tensor([-0.0021,  0.0615])], 'ro': [tensor(7.3404), tensor(20.3572), tensor(87.9522)], 'H_diag': tensor(0.3049), 'prev_flat_grad': tensor([ 0.0401, -0.0297]), 'prev_loss': -0.44032981991767883}})\n",
      "tensor(0.7466, grad_fn=<CopyBackwards>)\n",
      "tensor([[-1.9111],\n",
      "        [-1.1767]], grad_fn=<CopySlices>)\n",
      "tensor([[0.1072],\n",
      "        [0.7394]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.5779, -1.4870]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.858\n",
      "Iter 2/80 - Loss hyperparam: 0.814\n",
      "Iter 3/80 - Loss hyperparam: 0.771\n",
      "Iter 4/80 - Loss hyperparam: 0.728\n",
      "Iter 5/80 - Loss hyperparam: 0.685\n",
      "Iter 6/80 - Loss hyperparam: 0.642\n",
      "Iter 7/80 - Loss hyperparam: 0.599\n",
      "Iter 8/80 - Loss hyperparam: 0.556\n",
      "Iter 9/80 - Loss hyperparam: 0.512\n",
      "Iter 10/80 - Loss hyperparam: 0.469\n",
      "Iter 11/80 - Loss hyperparam: 0.426\n",
      "Iter 12/80 - Loss hyperparam: 0.382\n",
      "Iter 13/80 - Loss hyperparam: 0.338\n",
      "Iter 14/80 - Loss hyperparam: 0.293\n",
      "Iter 15/80 - Loss hyperparam: 0.248\n",
      "Iter 16/80 - Loss hyperparam: 0.203\n",
      "Iter 17/80 - Loss hyperparam: 0.157\n",
      "Iter 18/80 - Loss hyperparam: 0.110\n",
      "Iter 19/80 - Loss hyperparam: 0.063\n",
      "Iter 20/80 - Loss hyperparam: 0.016\n",
      "Iter 21/80 - Loss hyperparam: -0.032\n",
      "Iter 22/80 - Loss hyperparam: -0.080\n",
      "Iter 23/80 - Loss hyperparam: -0.128\n",
      "Iter 24/80 - Loss hyperparam: -0.176\n",
      "Iter 25/80 - Loss hyperparam: -0.225\n",
      "Iter 26/80 - Loss hyperparam: -0.274\n",
      "Iter 27/80 - Loss hyperparam: -0.323\n",
      "Iter 28/80 - Loss hyperparam: -0.373\n",
      "Iter 29/80 - Loss hyperparam: -0.422\n",
      "Iter 30/80 - Loss hyperparam: -0.472\n",
      "Iter 31/80 - Loss hyperparam: -0.521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32/80 - Loss hyperparam: -0.571\n",
      "Iter 33/80 - Loss hyperparam: -0.620\n",
      "Iter 34/80 - Loss hyperparam: -0.670\n",
      "Iter 35/80 - Loss hyperparam: -0.719\n",
      "Iter 36/80 - Loss hyperparam: -0.769\n",
      "Iter 37/80 - Loss hyperparam: -0.818\n",
      "Iter 38/80 - Loss hyperparam: -0.868\n",
      "Iter 39/80 - Loss hyperparam: -0.917\n",
      "Iter 40/80 - Loss hyperparam: -0.967\n",
      "Iter 41/80 - Loss hyperparam: -1.016\n",
      "Iter 42/80 - Loss hyperparam: -1.065\n",
      "Iter 43/80 - Loss hyperparam: -1.114\n",
      "Iter 44/80 - Loss hyperparam: -1.163\n",
      "Iter 45/80 - Loss hyperparam: -1.212\n",
      "Iter 46/80 - Loss hyperparam: -1.261\n",
      "Iter 47/80 - Loss hyperparam: -1.309\n",
      "Iter 48/80 - Loss hyperparam: -1.358\n",
      "Iter 49/80 - Loss hyperparam: -1.406\n",
      "Iter 50/80 - Loss hyperparam: -1.455\n",
      "Iter 51/80 - Loss hyperparam: -1.503\n",
      "Iter 52/80 - Loss hyperparam: -1.551\n",
      "Iter 53/80 - Loss hyperparam: -1.599\n",
      "Iter 54/80 - Loss hyperparam: -1.647\n",
      "Iter 55/80 - Loss hyperparam: -1.694\n",
      "Iter 56/80 - Loss hyperparam: -1.741\n",
      "Iter 57/80 - Loss hyperparam: -1.788\n",
      "Iter 58/80 - Loss hyperparam: -1.835\n",
      "Iter 59/80 - Loss hyperparam: -1.881\n",
      "Iter 60/80 - Loss hyperparam: -1.928\n",
      "Iter 61/80 - Loss hyperparam: -1.973\n",
      "Iter 62/80 - Loss hyperparam: -2.019\n",
      "Iter 63/80 - Loss hyperparam: -2.063\n",
      "Iter 64/80 - Loss hyperparam: -2.108\n",
      "Iter 65/80 - Loss hyperparam: -2.152\n",
      "Iter 66/80 - Loss hyperparam: -2.196\n",
      "Iter 67/80 - Loss hyperparam: -2.238\n",
      "Iter 68/80 - Loss hyperparam: -2.280\n",
      "Iter 69/80 - Loss hyperparam: -2.322\n",
      "Iter 70/80 - Loss hyperparam: -2.363\n",
      "Iter 71/80 - Loss hyperparam: -2.403\n",
      "Iter 72/80 - Loss hyperparam: -2.442\n",
      "Iter 73/80 - Loss hyperparam: -2.482\n",
      "Iter 74/80 - Loss hyperparam: -2.519\n",
      "Iter 75/80 - Loss hyperparam: -2.557\n",
      "Iter 76/80 - Loss hyperparam: -2.593\n",
      "Iter 77/80 - Loss hyperparam: -2.627\n",
      "Iter 78/80 - Loss hyperparam: -2.661\n",
      "Iter 79/80 - Loss hyperparam: -2.695\n",
      "Iter 80/80 - Loss hyperparam: -2.726\n",
      "END HYPERPARAMETERS optimization\n",
      "3\n",
      "Loss theta: -0.345\n",
      "Loss theta: -0.321\n",
      "Loss theta: -0.342\n",
      "Loss theta: -0.337\n",
      "Loss theta: -0.339\n",
      "Loss theta: -0.336\n",
      "Loss theta: -0.345\n",
      "Loss theta: -0.345\n",
      "Loss design: -0.345\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.5779, -1.4870]], requires_grad=True): {'func_evals': 7, 'n_iter': 1, 'd': tensor([-0.0563, -0.1352]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([0.0563, 0.1352]), 'prev_loss': -0.34474149346351624}})\n",
      "tensor(0.6710, grad_fn=<CopyBackwards>)\n",
      "tensor([[-1.9901],\n",
      "        [-1.2526]], grad_fn=<CopySlices>)\n",
      "tensor([[0.0346],\n",
      "        [0.6706]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.5779, -1.4870]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.860\n",
      "Iter 2/80 - Loss hyperparam: 0.817\n",
      "Iter 3/80 - Loss hyperparam: 0.773\n",
      "Iter 4/80 - Loss hyperparam: 0.730\n",
      "Iter 5/80 - Loss hyperparam: 0.686\n",
      "Iter 6/80 - Loss hyperparam: 0.643\n",
      "Iter 7/80 - Loss hyperparam: 0.599\n",
      "Iter 8/80 - Loss hyperparam: 0.555\n",
      "Iter 9/80 - Loss hyperparam: 0.511\n",
      "Iter 10/80 - Loss hyperparam: 0.467\n",
      "Iter 11/80 - Loss hyperparam: 0.423\n",
      "Iter 12/80 - Loss hyperparam: 0.379\n",
      "Iter 13/80 - Loss hyperparam: 0.335\n",
      "Iter 14/80 - Loss hyperparam: 0.291\n",
      "Iter 15/80 - Loss hyperparam: 0.246\n",
      "Iter 16/80 - Loss hyperparam: 0.201\n",
      "Iter 17/80 - Loss hyperparam: 0.155\n",
      "Iter 18/80 - Loss hyperparam: 0.109\n",
      "Iter 19/80 - Loss hyperparam: 0.062\n",
      "Iter 20/80 - Loss hyperparam: 0.014\n",
      "Iter 21/80 - Loss hyperparam: -0.034\n",
      "Iter 22/80 - Loss hyperparam: -0.082\n",
      "Iter 23/80 - Loss hyperparam: -0.131\n",
      "Iter 24/80 - Loss hyperparam: -0.179\n",
      "Iter 25/80 - Loss hyperparam: -0.228\n",
      "Iter 26/80 - Loss hyperparam: -0.277\n",
      "Iter 27/80 - Loss hyperparam: -0.326\n",
      "Iter 28/80 - Loss hyperparam: -0.375\n",
      "Iter 29/80 - Loss hyperparam: -0.425\n",
      "Iter 30/80 - Loss hyperparam: -0.475\n",
      "Iter 31/80 - Loss hyperparam: -0.524\n",
      "Iter 32/80 - Loss hyperparam: -0.574\n",
      "Iter 33/80 - Loss hyperparam: -0.624\n",
      "Iter 34/80 - Loss hyperparam: -0.674\n",
      "Iter 35/80 - Loss hyperparam: -0.724\n",
      "Iter 36/80 - Loss hyperparam: -0.774\n",
      "Iter 37/80 - Loss hyperparam: -0.824\n",
      "Iter 38/80 - Loss hyperparam: -0.874\n",
      "Iter 39/80 - Loss hyperparam: -0.923\n",
      "Iter 40/80 - Loss hyperparam: -0.973\n",
      "Iter 41/80 - Loss hyperparam: -1.022\n",
      "Iter 42/80 - Loss hyperparam: -1.072\n",
      "Iter 43/80 - Loss hyperparam: -1.121\n",
      "Iter 44/80 - Loss hyperparam: -1.170\n",
      "Iter 45/80 - Loss hyperparam: -1.220\n",
      "Iter 46/80 - Loss hyperparam: -1.269\n",
      "Iter 47/80 - Loss hyperparam: -1.317\n",
      "Iter 48/80 - Loss hyperparam: -1.366\n",
      "Iter 49/80 - Loss hyperparam: -1.415\n",
      "Iter 50/80 - Loss hyperparam: -1.464\n",
      "Iter 51/80 - Loss hyperparam: -1.512\n",
      "Iter 52/80 - Loss hyperparam: -1.560\n",
      "Iter 53/80 - Loss hyperparam: -1.608\n",
      "Iter 54/80 - Loss hyperparam: -1.656\n",
      "Iter 55/80 - Loss hyperparam: -1.704\n",
      "Iter 56/80 - Loss hyperparam: -1.751\n",
      "Iter 57/80 - Loss hyperparam: -1.798\n",
      "Iter 58/80 - Loss hyperparam: -1.845\n",
      "Iter 59/80 - Loss hyperparam: -1.892\n",
      "Iter 60/80 - Loss hyperparam: -1.938\n",
      "Iter 61/80 - Loss hyperparam: -1.984\n",
      "Iter 62/80 - Loss hyperparam: -2.030\n",
      "Iter 63/80 - Loss hyperparam: -2.075\n",
      "Iter 64/80 - Loss hyperparam: -2.120\n",
      "Iter 65/80 - Loss hyperparam: -2.164\n",
      "Iter 66/80 - Loss hyperparam: -2.208\n",
      "Iter 67/80 - Loss hyperparam: -2.250\n",
      "Iter 68/80 - Loss hyperparam: -2.293\n",
      "Iter 69/80 - Loss hyperparam: -2.335\n",
      "Iter 70/80 - Loss hyperparam: -2.376\n",
      "Iter 71/80 - Loss hyperparam: -2.416\n",
      "Iter 72/80 - Loss hyperparam: -2.455\n",
      "Iter 73/80 - Loss hyperparam: -2.495\n",
      "Iter 74/80 - Loss hyperparam: -2.533\n",
      "Iter 75/80 - Loss hyperparam: -2.569\n",
      "Iter 76/80 - Loss hyperparam: -2.607\n",
      "Iter 77/80 - Loss hyperparam: -2.641\n",
      "Iter 78/80 - Loss hyperparam: -2.676\n",
      "Iter 79/80 - Loss hyperparam: -2.710\n",
      "Iter 80/80 - Loss hyperparam: -2.742\n",
      "END HYPERPARAMETERS optimization\n",
      "4\n",
      "Loss theta: -0.289\n",
      "Loss theta: -0.179\n",
      "Loss theta: -0.303\n",
      "Loss theta: -0.296\n",
      "Loss theta: -0.296\n",
      "Loss theta: -0.297\n",
      "Loss theta: -0.303\n",
      "Loss theta: -0.303\n",
      "Loss design: -0.303\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.6099, -1.5777]], requires_grad=True): {'func_evals': 7, 'n_iter': 2, 'al': [tensor(-0.0613), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([0.0033, 0.0051]), 't': 0, 'old_dirs': [tensor([-0.1477, -0.3992])], 'old_stps': [tensor([-0.0320, -0.0907])], 'ro': [tensor(24.4324)], 'H_diag': tensor(0.2259), 'prev_flat_grad': tensor([-0.0148, -0.0224]), 'prev_loss': -0.3030795454978943}})\n",
      "tensor(0.7238, grad_fn=<CopyBackwards>)\n",
      "tensor([[-2.1311],\n",
      "        [-1.3885]], grad_fn=<CopySlices>)\n",
      "tensor([[0.0830],\n",
      "        [0.7196]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.6099, -1.5777]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.844\n",
      "Iter 2/80 - Loss hyperparam: 0.801\n",
      "Iter 3/80 - Loss hyperparam: 0.758\n",
      "Iter 4/80 - Loss hyperparam: 0.716\n",
      "Iter 5/80 - Loss hyperparam: 0.673\n",
      "Iter 6/80 - Loss hyperparam: 0.630\n",
      "Iter 7/80 - Loss hyperparam: 0.587\n",
      "Iter 8/80 - Loss hyperparam: 0.544\n",
      "Iter 9/80 - Loss hyperparam: 0.501\n",
      "Iter 10/80 - Loss hyperparam: 0.457\n",
      "Iter 11/80 - Loss hyperparam: 0.414\n",
      "Iter 12/80 - Loss hyperparam: 0.369\n",
      "Iter 13/80 - Loss hyperparam: 0.325\n",
      "Iter 14/80 - Loss hyperparam: 0.280\n",
      "Iter 15/80 - Loss hyperparam: 0.235\n",
      "Iter 16/80 - Loss hyperparam: 0.190\n",
      "Iter 17/80 - Loss hyperparam: 0.144\n",
      "Iter 18/80 - Loss hyperparam: 0.098\n",
      "Iter 19/80 - Loss hyperparam: 0.051\n",
      "Iter 20/80 - Loss hyperparam: 0.004\n",
      "Iter 21/80 - Loss hyperparam: -0.044\n",
      "Iter 22/80 - Loss hyperparam: -0.092\n",
      "Iter 23/80 - Loss hyperparam: -0.140\n",
      "Iter 24/80 - Loss hyperparam: -0.189\n",
      "Iter 25/80 - Loss hyperparam: -0.238\n",
      "Iter 26/80 - Loss hyperparam: -0.287\n",
      "Iter 27/80 - Loss hyperparam: -0.336\n",
      "Iter 28/80 - Loss hyperparam: -0.386\n",
      "Iter 29/80 - Loss hyperparam: -0.435\n",
      "Iter 30/80 - Loss hyperparam: -0.484\n",
      "Iter 31/80 - Loss hyperparam: -0.534\n",
      "Iter 32/80 - Loss hyperparam: -0.583\n",
      "Iter 33/80 - Loss hyperparam: -0.633\n",
      "Iter 34/80 - Loss hyperparam: -0.683\n",
      "Iter 35/80 - Loss hyperparam: -0.732\n",
      "Iter 36/80 - Loss hyperparam: -0.782\n",
      "Iter 37/80 - Loss hyperparam: -0.832\n",
      "Iter 38/80 - Loss hyperparam: -0.882\n",
      "Iter 39/80 - Loss hyperparam: -0.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40/80 - Loss hyperparam: -0.981\n",
      "Iter 41/80 - Loss hyperparam: -1.030\n",
      "Iter 42/80 - Loss hyperparam: -1.080\n",
      "Iter 43/80 - Loss hyperparam: -1.129\n",
      "Iter 44/80 - Loss hyperparam: -1.178\n",
      "Iter 45/80 - Loss hyperparam: -1.227\n",
      "Iter 46/80 - Loss hyperparam: -1.277\n",
      "Iter 47/80 - Loss hyperparam: -1.326\n",
      "Iter 48/80 - Loss hyperparam: -1.374\n",
      "Iter 49/80 - Loss hyperparam: -1.423\n",
      "Iter 50/80 - Loss hyperparam: -1.472\n",
      "Iter 51/80 - Loss hyperparam: -1.521\n",
      "Iter 52/80 - Loss hyperparam: -1.569\n",
      "Iter 53/80 - Loss hyperparam: -1.617\n",
      "Iter 54/80 - Loss hyperparam: -1.665\n",
      "Iter 55/80 - Loss hyperparam: -1.713\n",
      "Iter 56/80 - Loss hyperparam: -1.761\n",
      "Iter 57/80 - Loss hyperparam: -1.808\n",
      "Iter 58/80 - Loss hyperparam: -1.855\n",
      "Iter 59/80 - Loss hyperparam: -1.901\n",
      "Iter 60/80 - Loss hyperparam: -1.948\n",
      "Iter 61/80 - Loss hyperparam: -1.994\n",
      "Iter 62/80 - Loss hyperparam: -2.040\n",
      "Iter 63/80 - Loss hyperparam: -2.086\n",
      "Iter 64/80 - Loss hyperparam: -2.130\n",
      "Iter 65/80 - Loss hyperparam: -2.174\n",
      "Iter 66/80 - Loss hyperparam: -2.218\n",
      "Iter 67/80 - Loss hyperparam: -2.261\n",
      "Iter 68/80 - Loss hyperparam: -2.304\n",
      "Iter 69/80 - Loss hyperparam: -2.346\n",
      "Iter 70/80 - Loss hyperparam: -2.388\n",
      "Iter 71/80 - Loss hyperparam: -2.429\n",
      "Iter 72/80 - Loss hyperparam: -2.468\n",
      "Iter 73/80 - Loss hyperparam: -2.508\n",
      "Iter 74/80 - Loss hyperparam: -2.546\n",
      "Iter 75/80 - Loss hyperparam: -2.584\n",
      "Iter 76/80 - Loss hyperparam: -2.621\n",
      "Iter 77/80 - Loss hyperparam: -2.658\n",
      "Iter 78/80 - Loss hyperparam: -2.689\n",
      "Iter 79/80 - Loss hyperparam: -2.724\n",
      "Iter 80/80 - Loss hyperparam: -2.758\n",
      "END HYPERPARAMETERS optimization\n",
      "5\n",
      "Loss theta: -0.320\n",
      "Loss theta: -0.242\n",
      "Loss theta: -0.352\n",
      "Loss theta: -0.320\n",
      "Loss theta: -0.323\n",
      "Loss theta: -0.324\n",
      "Loss theta: -0.352\n",
      "Loss theta: -0.352\n",
      "Loss design: -0.352\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.5964, -1.5319]], requires_grad=True): {'func_evals': 7, 'n_iter': 2, 'al': [tensor(0.4303), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([0.0058, 0.0197]), 't': 0, 'old_dirs': [tensor([0.0444, 0.1510])], 'old_stps': [tensor([0.0135, 0.0457])], 'ro': [tensor(133.2734)], 'H_diag': tensor(0.3031), 'prev_flat_grad': tensor([-0.0192, -0.0649]), 'prev_loss': -0.35233235359191895}})\n",
      "tensor(0.7124, grad_fn=<CopyBackwards>)\n",
      "tensor([[-2.0456],\n",
      "        [-1.3030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.0744],\n",
      "        [0.7090]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.5964, -1.5319]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.844\n",
      "Iter 2/80 - Loss hyperparam: 0.801\n",
      "Iter 3/80 - Loss hyperparam: 0.758\n",
      "Iter 4/80 - Loss hyperparam: 0.715\n",
      "Iter 5/80 - Loss hyperparam: 0.673\n",
      "Iter 6/80 - Loss hyperparam: 0.630\n",
      "Iter 7/80 - Loss hyperparam: 0.587\n",
      "Iter 8/80 - Loss hyperparam: 0.544\n",
      "Iter 9/80 - Loss hyperparam: 0.500\n",
      "Iter 10/80 - Loss hyperparam: 0.457\n",
      "Iter 11/80 - Loss hyperparam: 0.413\n",
      "Iter 12/80 - Loss hyperparam: 0.369\n",
      "Iter 13/80 - Loss hyperparam: 0.324\n",
      "Iter 14/80 - Loss hyperparam: 0.280\n",
      "Iter 15/80 - Loss hyperparam: 0.235\n",
      "Iter 16/80 - Loss hyperparam: 0.189\n",
      "Iter 17/80 - Loss hyperparam: 0.143\n",
      "Iter 18/80 - Loss hyperparam: 0.097\n",
      "Iter 19/80 - Loss hyperparam: 0.050\n",
      "Iter 20/80 - Loss hyperparam: 0.002\n",
      "Iter 21/80 - Loss hyperparam: -0.046\n",
      "Iter 22/80 - Loss hyperparam: -0.094\n",
      "Iter 23/80 - Loss hyperparam: -0.143\n",
      "Iter 24/80 - Loss hyperparam: -0.191\n",
      "Iter 25/80 - Loss hyperparam: -0.240\n",
      "Iter 26/80 - Loss hyperparam: -0.289\n",
      "Iter 27/80 - Loss hyperparam: -0.339\n",
      "Iter 28/80 - Loss hyperparam: -0.388\n",
      "Iter 29/80 - Loss hyperparam: -0.438\n",
      "Iter 30/80 - Loss hyperparam: -0.487\n",
      "Iter 31/80 - Loss hyperparam: -0.537\n",
      "Iter 32/80 - Loss hyperparam: -0.587\n",
      "Iter 33/80 - Loss hyperparam: -0.637\n",
      "Iter 34/80 - Loss hyperparam: -0.687\n",
      "Iter 35/80 - Loss hyperparam: -0.737\n",
      "Iter 36/80 - Loss hyperparam: -0.787\n",
      "Iter 37/80 - Loss hyperparam: -0.837\n",
      "Iter 38/80 - Loss hyperparam: -0.886\n",
      "Iter 39/80 - Loss hyperparam: -0.936\n",
      "Iter 40/80 - Loss hyperparam: -0.986\n",
      "Iter 41/80 - Loss hyperparam: -1.035\n",
      "Iter 42/80 - Loss hyperparam: -1.085\n",
      "Iter 43/80 - Loss hyperparam: -1.134\n",
      "Iter 44/80 - Loss hyperparam: -1.184\n",
      "Iter 45/80 - Loss hyperparam: -1.233\n",
      "Iter 46/80 - Loss hyperparam: -1.282\n",
      "Iter 47/80 - Loss hyperparam: -1.331\n",
      "Iter 48/80 - Loss hyperparam: -1.381\n",
      "Iter 49/80 - Loss hyperparam: -1.430\n",
      "Iter 50/80 - Loss hyperparam: -1.479\n",
      "Iter 51/80 - Loss hyperparam: -1.527\n",
      "Iter 52/80 - Loss hyperparam: -1.576\n",
      "Iter 53/80 - Loss hyperparam: -1.624\n",
      "Iter 54/80 - Loss hyperparam: -1.672\n",
      "Iter 55/80 - Loss hyperparam: -1.720\n",
      "Iter 56/80 - Loss hyperparam: -1.768\n",
      "Iter 57/80 - Loss hyperparam: -1.816\n",
      "Iter 58/80 - Loss hyperparam: -1.863\n",
      "Iter 59/80 - Loss hyperparam: -1.910\n",
      "Iter 60/80 - Loss hyperparam: -1.956\n",
      "Iter 61/80 - Loss hyperparam: -2.002\n",
      "Iter 62/80 - Loss hyperparam: -2.048\n",
      "Iter 63/80 - Loss hyperparam: -2.094\n",
      "Iter 64/80 - Loss hyperparam: -2.139\n",
      "Iter 65/80 - Loss hyperparam: -2.183\n",
      "Iter 66/80 - Loss hyperparam: -2.227\n",
      "Iter 67/80 - Loss hyperparam: -2.270\n",
      "Iter 68/80 - Loss hyperparam: -2.313\n",
      "Iter 69/80 - Loss hyperparam: -2.355\n",
      "Iter 70/80 - Loss hyperparam: -2.396\n",
      "Iter 71/80 - Loss hyperparam: -2.438\n",
      "Iter 72/80 - Loss hyperparam: -2.478\n",
      "Iter 73/80 - Loss hyperparam: -2.516\n",
      "Iter 74/80 - Loss hyperparam: -2.556\n",
      "Iter 75/80 - Loss hyperparam: -2.593\n",
      "Iter 76/80 - Loss hyperparam: -2.629\n",
      "Iter 77/80 - Loss hyperparam: -2.667\n",
      "Iter 78/80 - Loss hyperparam: -2.703\n",
      "Iter 79/80 - Loss hyperparam: -2.734\n",
      "Iter 80/80 - Loss hyperparam: -2.768\n",
      "END HYPERPARAMETERS optimization\n",
      "6\n",
      "Loss theta: -0.313\n",
      "Loss theta: -0.292\n",
      "Loss theta: -0.279\n",
      "Loss theta: -0.277\n",
      "Loss theta: -0.281\n",
      "Loss theta: -0.313\n",
      "Loss theta: -0.313\n",
      "Loss theta: -0.313\n",
      "Loss theta: -0.313\n",
      "Loss design: -0.313\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.5964, -1.5319]], requires_grad=True): {'func_evals': 8, 'n_iter': 1, 'd': tensor([-0.0895,  0.0078]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([ 0.0895, -0.0078]), 'prev_loss': -0.31340306997299194}})\n",
      "tensor(0.6973, grad_fn=<CopyBackwards>)\n",
      "tensor([[-2.0099],\n",
      "        [-1.2719]], grad_fn=<CopySlices>)\n",
      "tensor([[0.0520],\n",
      "        [0.6959]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.5964, -1.5319]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.844\n",
      "Iter 2/80 - Loss hyperparam: 0.801\n",
      "Iter 3/80 - Loss hyperparam: 0.758\n",
      "Iter 4/80 - Loss hyperparam: 0.716\n",
      "Iter 5/80 - Loss hyperparam: 0.673\n",
      "Iter 6/80 - Loss hyperparam: 0.630\n",
      "Iter 7/80 - Loss hyperparam: 0.587\n",
      "Iter 8/80 - Loss hyperparam: 0.543\n",
      "Iter 9/80 - Loss hyperparam: 0.499\n",
      "Iter 10/80 - Loss hyperparam: 0.455\n",
      "Iter 11/80 - Loss hyperparam: 0.411\n",
      "Iter 12/80 - Loss hyperparam: 0.367\n",
      "Iter 13/80 - Loss hyperparam: 0.322\n",
      "Iter 14/80 - Loss hyperparam: 0.276\n",
      "Iter 15/80 - Loss hyperparam: 0.230\n",
      "Iter 16/80 - Loss hyperparam: 0.184\n",
      "Iter 17/80 - Loss hyperparam: 0.138\n",
      "Iter 18/80 - Loss hyperparam: 0.091\n",
      "Iter 19/80 - Loss hyperparam: 0.044\n",
      "Iter 20/80 - Loss hyperparam: -0.003\n",
      "Iter 21/80 - Loss hyperparam: -0.050\n",
      "Iter 22/80 - Loss hyperparam: -0.098\n",
      "Iter 23/80 - Loss hyperparam: -0.146\n",
      "Iter 24/80 - Loss hyperparam: -0.195\n",
      "Iter 25/80 - Loss hyperparam: -0.244\n",
      "Iter 26/80 - Loss hyperparam: -0.294\n",
      "Iter 27/80 - Loss hyperparam: -0.343\n",
      "Iter 28/80 - Loss hyperparam: -0.393\n",
      "Iter 29/80 - Loss hyperparam: -0.443\n",
      "Iter 30/80 - Loss hyperparam: -0.493\n",
      "Iter 31/80 - Loss hyperparam: -0.542\n",
      "Iter 32/80 - Loss hyperparam: -0.592\n",
      "Iter 33/80 - Loss hyperparam: -0.642\n",
      "Iter 34/80 - Loss hyperparam: -0.692\n",
      "Iter 35/80 - Loss hyperparam: -0.742\n",
      "Iter 36/80 - Loss hyperparam: -0.792\n",
      "Iter 37/80 - Loss hyperparam: -0.842\n",
      "Iter 38/80 - Loss hyperparam: -0.892\n",
      "Iter 39/80 - Loss hyperparam: -0.942\n",
      "Iter 40/80 - Loss hyperparam: -0.992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 41/80 - Loss hyperparam: -1.041\n",
      "Iter 42/80 - Loss hyperparam: -1.091\n",
      "Iter 43/80 - Loss hyperparam: -1.141\n",
      "Iter 44/80 - Loss hyperparam: -1.190\n",
      "Iter 45/80 - Loss hyperparam: -1.240\n",
      "Iter 46/80 - Loss hyperparam: -1.289\n",
      "Iter 47/80 - Loss hyperparam: -1.338\n",
      "Iter 48/80 - Loss hyperparam: -1.388\n",
      "Iter 49/80 - Loss hyperparam: -1.437\n",
      "Iter 50/80 - Loss hyperparam: -1.486\n",
      "Iter 51/80 - Loss hyperparam: -1.534\n",
      "Iter 52/80 - Loss hyperparam: -1.583\n",
      "Iter 53/80 - Loss hyperparam: -1.632\n",
      "Iter 54/80 - Loss hyperparam: -1.680\n",
      "Iter 55/80 - Loss hyperparam: -1.728\n",
      "Iter 56/80 - Loss hyperparam: -1.776\n",
      "Iter 57/80 - Loss hyperparam: -1.823\n",
      "Iter 58/80 - Loss hyperparam: -1.871\n",
      "Iter 59/80 - Loss hyperparam: -1.918\n",
      "Iter 60/80 - Loss hyperparam: -1.965\n",
      "Iter 61/80 - Loss hyperparam: -2.010\n",
      "Iter 62/80 - Loss hyperparam: -2.057\n",
      "Iter 63/80 - Loss hyperparam: -2.102\n",
      "Iter 64/80 - Loss hyperparam: -2.148\n",
      "Iter 65/80 - Loss hyperparam: -2.193\n",
      "Iter 66/80 - Loss hyperparam: -2.237\n",
      "Iter 67/80 - Loss hyperparam: -2.280\n",
      "Iter 68/80 - Loss hyperparam: -2.324\n",
      "Iter 69/80 - Loss hyperparam: -2.366\n",
      "Iter 70/80 - Loss hyperparam: -2.408\n",
      "Iter 71/80 - Loss hyperparam: -2.449\n",
      "Iter 72/80 - Loss hyperparam: -2.489\n",
      "Iter 73/80 - Loss hyperparam: -2.529\n",
      "Iter 74/80 - Loss hyperparam: -2.566\n",
      "Iter 75/80 - Loss hyperparam: -2.605\n",
      "Iter 76/80 - Loss hyperparam: -2.641\n",
      "Iter 77/80 - Loss hyperparam: -2.679\n",
      "Iter 78/80 - Loss hyperparam: -2.711\n",
      "Iter 79/80 - Loss hyperparam: -2.747\n",
      "Iter 80/80 - Loss hyperparam: -2.780\n",
      "END HYPERPARAMETERS optimization\n",
      "7\n",
      "Loss theta: -0.521\n",
      "Loss theta: -0.475\n",
      "Loss theta: -0.536\n",
      "Loss theta: -0.527\n",
      "Loss theta: -0.532\n",
      "Loss theta: -0.536\n",
      "Loss theta: -0.536\n",
      "Loss theta: -0.526\n",
      "Loss theta: -0.528\n",
      "Loss theta: -0.531\n",
      "Loss theta: -0.536\n",
      "Loss theta: -0.536\n",
      "Loss design: -0.536\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.6242, -1.4726]], requires_grad=True): {'func_evals': 11, 'n_iter': 3, 'al': [tensor(-0.0367), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0036, -0.0041]), 't': 0, 'old_dirs': [tensor([-0.0898,  0.2132])], 'old_stps': [tensor([-0.0278,  0.0594])], 'ro': [tensor(66.0095)], 'H_diag': tensor(0.2832), 'prev_flat_grad': tensor([0.0129, 0.0154]), 'prev_loss': -0.5361145734786987}})\n",
      "tensor(0.7685, grad_fn=<CopyBackwards>)\n",
      "tensor([[-1.7816],\n",
      "        [-1.0935]], grad_fn=<CopySlices>)\n",
      "tensor([[0.1471],\n",
      "        [0.7549]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.6242, -1.4726]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.831\n",
      "Iter 2/80 - Loss hyperparam: 0.789\n",
      "Iter 3/80 - Loss hyperparam: 0.746\n",
      "Iter 4/80 - Loss hyperparam: 0.704\n",
      "Iter 5/80 - Loss hyperparam: 0.662\n",
      "Iter 6/80 - Loss hyperparam: 0.619\n",
      "Iter 7/80 - Loss hyperparam: 0.576\n",
      "Iter 8/80 - Loss hyperparam: 0.534\n",
      "Iter 9/80 - Loss hyperparam: 0.490\n",
      "Iter 10/80 - Loss hyperparam: 0.447\n",
      "Iter 11/80 - Loss hyperparam: 0.404\n",
      "Iter 12/80 - Loss hyperparam: 0.361\n",
      "Iter 13/80 - Loss hyperparam: 0.317\n",
      "Iter 14/80 - Loss hyperparam: 0.272\n",
      "Iter 15/80 - Loss hyperparam: 0.227\n",
      "Iter 16/80 - Loss hyperparam: 0.182\n",
      "Iter 17/80 - Loss hyperparam: 0.136\n",
      "Iter 18/80 - Loss hyperparam: 0.089\n",
      "Iter 19/80 - Loss hyperparam: 0.041\n",
      "Iter 20/80 - Loss hyperparam: -0.007\n",
      "Iter 21/80 - Loss hyperparam: -0.055\n",
      "Iter 22/80 - Loss hyperparam: -0.103\n",
      "Iter 23/80 - Loss hyperparam: -0.152\n",
      "Iter 24/80 - Loss hyperparam: -0.200\n",
      "Iter 25/80 - Loss hyperparam: -0.249\n",
      "Iter 26/80 - Loss hyperparam: -0.298\n",
      "Iter 27/80 - Loss hyperparam: -0.347\n",
      "Iter 28/80 - Loss hyperparam: -0.397\n",
      "Iter 29/80 - Loss hyperparam: -0.446\n",
      "Iter 30/80 - Loss hyperparam: -0.496\n",
      "Iter 31/80 - Loss hyperparam: -0.546\n",
      "Iter 32/80 - Loss hyperparam: -0.596\n",
      "Iter 33/80 - Loss hyperparam: -0.646\n",
      "Iter 34/80 - Loss hyperparam: -0.696\n",
      "Iter 35/80 - Loss hyperparam: -0.746\n",
      "Iter 36/80 - Loss hyperparam: -0.796\n",
      "Iter 37/80 - Loss hyperparam: -0.846\n",
      "Iter 38/80 - Loss hyperparam: -0.896\n",
      "Iter 39/80 - Loss hyperparam: -0.946\n",
      "Iter 40/80 - Loss hyperparam: -0.996\n",
      "Iter 41/80 - Loss hyperparam: -1.046\n",
      "Iter 42/80 - Loss hyperparam: -1.095\n",
      "Iter 43/80 - Loss hyperparam: -1.145\n",
      "Iter 44/80 - Loss hyperparam: -1.195\n",
      "Iter 45/80 - Loss hyperparam: -1.244\n",
      "Iter 46/80 - Loss hyperparam: -1.294\n",
      "Iter 47/80 - Loss hyperparam: -1.343\n",
      "Iter 48/80 - Loss hyperparam: -1.393\n",
      "Iter 49/80 - Loss hyperparam: -1.442\n",
      "Iter 50/80 - Loss hyperparam: -1.491\n",
      "Iter 51/80 - Loss hyperparam: -1.540\n",
      "Iter 52/80 - Loss hyperparam: -1.589\n",
      "Iter 53/80 - Loss hyperparam: -1.638\n",
      "Iter 54/80 - Loss hyperparam: -1.686\n",
      "Iter 55/80 - Loss hyperparam: -1.734\n",
      "Iter 56/80 - Loss hyperparam: -1.783\n",
      "Iter 57/80 - Loss hyperparam: -1.830\n",
      "Iter 58/80 - Loss hyperparam: -1.878\n",
      "Iter 59/80 - Loss hyperparam: -1.925\n",
      "Iter 60/80 - Loss hyperparam: -1.972\n",
      "Iter 61/80 - Loss hyperparam: -2.019\n",
      "Iter 62/80 - Loss hyperparam: -2.065\n",
      "Iter 63/80 - Loss hyperparam: -2.110\n",
      "Iter 64/80 - Loss hyperparam: -2.156\n",
      "Iter 65/80 - Loss hyperparam: -2.201\n",
      "Iter 66/80 - Loss hyperparam: -2.245\n",
      "Iter 67/80 - Loss hyperparam: -2.289\n",
      "Iter 68/80 - Loss hyperparam: -2.332\n",
      "Iter 69/80 - Loss hyperparam: -2.375\n",
      "Iter 70/80 - Loss hyperparam: -2.416\n",
      "Iter 71/80 - Loss hyperparam: -2.457\n",
      "Iter 72/80 - Loss hyperparam: -2.497\n",
      "Iter 73/80 - Loss hyperparam: -2.538\n",
      "Iter 74/80 - Loss hyperparam: -2.575\n",
      "Iter 75/80 - Loss hyperparam: -2.614\n",
      "Iter 76/80 - Loss hyperparam: -2.651\n",
      "Iter 77/80 - Loss hyperparam: -2.688\n",
      "Iter 78/80 - Loss hyperparam: -2.721\n",
      "Iter 79/80 - Loss hyperparam: -2.757\n",
      "Iter 80/80 - Loss hyperparam: -2.789\n",
      "END HYPERPARAMETERS optimization\n",
      "8\n",
      "Loss theta: -0.514\n",
      "Loss theta: -0.490\n",
      "Loss theta: -0.519\n",
      "Loss theta: -0.513\n",
      "Loss theta: -0.512\n",
      "Loss theta: -0.510\n",
      "Loss theta: -0.570\n",
      "Loss theta: -0.573\n",
      "Loss theta: -0.510\n",
      "Loss theta: -0.573\n",
      "Loss theta: -0.573\n",
      "Loss theta: -0.506\n",
      "Loss theta: -0.525\n",
      "Loss theta: -0.526\n",
      "Loss theta: -0.509\n",
      "Loss theta: -0.573\n",
      "Loss theta: -0.573\n",
      "Loss theta: -0.573\n",
      "Loss theta: -0.573\n",
      "Loss design: -0.573\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.6358, -1.4509]], requires_grad=True): {'func_evals': 18, 'n_iter': 3, 'al': [tensor(2.6949), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0727,  0.0578]), 't': 0, 'old_dirs': [tensor([-0.0011,  0.0899])], 'old_stps': [tensor([-0.0116,  0.0216])], 'ro': [tensor(511.4100)], 'H_diag': tensor(0.2420), 'prev_flat_grad': tensor([ 0.1364, -0.1705]), 'prev_loss': -0.5732163786888123}})\n",
      "tensor(0.7455, grad_fn=<CopyBackwards>)\n",
      "tensor([[-1.8215],\n",
      "        [-1.1269]], grad_fn=<CopySlices>)\n",
      "tensor([[0.1284],\n",
      "        [0.7349]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.6358, -1.4509]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.832\n",
      "Iter 2/80 - Loss hyperparam: 0.790\n",
      "Iter 3/80 - Loss hyperparam: 0.747\n",
      "Iter 4/80 - Loss hyperparam: 0.705\n",
      "Iter 5/80 - Loss hyperparam: 0.662\n",
      "Iter 6/80 - Loss hyperparam: 0.619\n",
      "Iter 7/80 - Loss hyperparam: 0.576\n",
      "Iter 8/80 - Loss hyperparam: 0.533\n",
      "Iter 9/80 - Loss hyperparam: 0.490\n",
      "Iter 10/80 - Loss hyperparam: 0.446\n",
      "Iter 11/80 - Loss hyperparam: 0.402\n",
      "Iter 12/80 - Loss hyperparam: 0.359\n",
      "Iter 13/80 - Loss hyperparam: 0.315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14/80 - Loss hyperparam: 0.270\n",
      "Iter 15/80 - Loss hyperparam: 0.226\n",
      "Iter 16/80 - Loss hyperparam: 0.180\n",
      "Iter 17/80 - Loss hyperparam: 0.134\n",
      "Iter 18/80 - Loss hyperparam: 0.087\n",
      "Iter 19/80 - Loss hyperparam: 0.040\n",
      "Iter 20/80 - Loss hyperparam: -0.008\n",
      "Iter 21/80 - Loss hyperparam: -0.056\n",
      "Iter 22/80 - Loss hyperparam: -0.105\n",
      "Iter 23/80 - Loss hyperparam: -0.153\n",
      "Iter 24/80 - Loss hyperparam: -0.202\n",
      "Iter 25/80 - Loss hyperparam: -0.251\n",
      "Iter 26/80 - Loss hyperparam: -0.300\n",
      "Iter 27/80 - Loss hyperparam: -0.350\n",
      "Iter 28/80 - Loss hyperparam: -0.399\n",
      "Iter 29/80 - Loss hyperparam: -0.449\n",
      "Iter 30/80 - Loss hyperparam: -0.499\n",
      "Iter 31/80 - Loss hyperparam: -0.549\n",
      "Iter 32/80 - Loss hyperparam: -0.599\n",
      "Iter 33/80 - Loss hyperparam: -0.649\n",
      "Iter 34/80 - Loss hyperparam: -0.700\n",
      "Iter 35/80 - Loss hyperparam: -0.750\n",
      "Iter 36/80 - Loss hyperparam: -0.800\n",
      "Iter 37/80 - Loss hyperparam: -0.850\n",
      "Iter 38/80 - Loss hyperparam: -0.900\n",
      "Iter 39/80 - Loss hyperparam: -0.950\n",
      "Iter 40/80 - Loss hyperparam: -1.000\n",
      "Iter 41/80 - Loss hyperparam: -1.050\n",
      "Iter 42/80 - Loss hyperparam: -1.100\n",
      "Iter 43/80 - Loss hyperparam: -1.150\n",
      "Iter 44/80 - Loss hyperparam: -1.200\n",
      "Iter 45/80 - Loss hyperparam: -1.249\n",
      "Iter 46/80 - Loss hyperparam: -1.299\n",
      "Iter 47/80 - Loss hyperparam: -1.349\n",
      "Iter 48/80 - Loss hyperparam: -1.398\n",
      "Iter 49/80 - Loss hyperparam: -1.448\n",
      "Iter 50/80 - Loss hyperparam: -1.497\n",
      "Iter 51/80 - Loss hyperparam: -1.546\n",
      "Iter 52/80 - Loss hyperparam: -1.595\n",
      "Iter 53/80 - Loss hyperparam: -1.644\n",
      "Iter 54/80 - Loss hyperparam: -1.692\n",
      "Iter 55/80 - Loss hyperparam: -1.741\n",
      "Iter 56/80 - Loss hyperparam: -1.789\n",
      "Iter 57/80 - Loss hyperparam: -1.837\n",
      "Iter 58/80 - Loss hyperparam: -1.884\n",
      "Iter 59/80 - Loss hyperparam: -1.932\n",
      "Iter 60/80 - Loss hyperparam: -1.979\n",
      "Iter 61/80 - Loss hyperparam: -2.025\n",
      "Iter 62/80 - Loss hyperparam: -2.071\n",
      "Iter 63/80 - Loss hyperparam: -2.117\n",
      "Iter 64/80 - Loss hyperparam: -2.163\n",
      "Iter 65/80 - Loss hyperparam: -2.209\n",
      "Iter 66/80 - Loss hyperparam: -2.253\n",
      "Iter 67/80 - Loss hyperparam: -2.296\n",
      "Iter 68/80 - Loss hyperparam: -2.340\n",
      "Iter 69/80 - Loss hyperparam: -2.382\n",
      "Iter 70/80 - Loss hyperparam: -2.425\n",
      "Iter 71/80 - Loss hyperparam: -2.466\n",
      "Iter 72/80 - Loss hyperparam: -2.506\n",
      "Iter 73/80 - Loss hyperparam: -2.546\n",
      "Iter 74/80 - Loss hyperparam: -2.585\n",
      "Iter 75/80 - Loss hyperparam: -2.623\n",
      "Iter 76/80 - Loss hyperparam: -2.659\n",
      "Iter 77/80 - Loss hyperparam: -2.697\n",
      "Iter 78/80 - Loss hyperparam: -2.731\n",
      "Iter 79/80 - Loss hyperparam: -2.768\n",
      "Iter 80/80 - Loss hyperparam: -2.799\n",
      "END HYPERPARAMETERS optimization\n",
      "9\n",
      "Loss theta: -0.088\n",
      "Loss theta: 0.308\n",
      "Loss theta: -0.163\n",
      "Loss theta: -0.192\n",
      "Loss theta: -0.082\n",
      "Loss theta: -0.144\n",
      "Loss theta: -0.150\n",
      "Loss theta: -0.163\n",
      "Loss theta: -0.192\n",
      "Loss theta: -0.192\n",
      "Loss theta: -0.192\n",
      "Loss theta: -0.192\n",
      "Loss design: -0.192\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[-0.6722, -1.6055]], requires_grad=True): {'func_evals': 11, 'n_iter': 3, 'al': [tensor(0.0099), tensor(3.7644), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0188,  0.1765]), 't': 0, 'old_dirs': [tensor([-0.1957, -0.8850]), tensor([0.0441, 0.0170])], 'old_stps': [tensor([-0.0406, -0.1777]), tensor([0.0042, 0.0231])], 'ro': [tensor(6.0543), tensor(1739.5891)], 'H_diag': tensor(0.2571), 'prev_flat_grad': tensor([ 0.0243, -0.0981]), 'prev_loss': -0.19152046740055084}})\n",
      "tensor(0.6846, grad_fn=<CopyBackwards>)\n",
      "tensor([[-2.2228],\n",
      "        [-1.4640]], grad_fn=<CopySlices>)\n",
      "tensor([[0.0387],\n",
      "        [0.6841]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4995],\n",
      "        [0.9995]])\n",
      "tensor([[0.5005],\n",
      "        [1.0005]])\n",
      "Parameter containing:\n",
      "tensor([[-0.6722, -1.6055]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/80 - Loss hyperparam: 0.831\n",
      "Iter 2/80 - Loss hyperparam: 0.789\n",
      "Iter 3/80 - Loss hyperparam: 0.747\n",
      "Iter 4/80 - Loss hyperparam: 0.705\n",
      "Iter 5/80 - Loss hyperparam: 0.662\n",
      "Iter 6/80 - Loss hyperparam: 0.620\n",
      "Iter 7/80 - Loss hyperparam: 0.577\n",
      "Iter 8/80 - Loss hyperparam: 0.533\n",
      "Iter 9/80 - Loss hyperparam: 0.490\n",
      "Iter 10/80 - Loss hyperparam: 0.446\n",
      "Iter 11/80 - Loss hyperparam: 0.402\n",
      "Iter 12/80 - Loss hyperparam: 0.358\n",
      "Iter 13/80 - Loss hyperparam: 0.313\n",
      "Iter 14/80 - Loss hyperparam: 0.269\n",
      "Iter 15/80 - Loss hyperparam: 0.224\n",
      "Iter 16/80 - Loss hyperparam: 0.178\n",
      "Iter 17/80 - Loss hyperparam: 0.132\n",
      "Iter 18/80 - Loss hyperparam: 0.086\n",
      "Iter 19/80 - Loss hyperparam: 0.039\n",
      "Iter 20/80 - Loss hyperparam: -0.009\n",
      "Iter 21/80 - Loss hyperparam: -0.057\n",
      "Iter 22/80 - Loss hyperparam: -0.105\n",
      "Iter 23/80 - Loss hyperparam: -0.154\n",
      "Iter 24/80 - Loss hyperparam: -0.203\n",
      "Iter 25/80 - Loss hyperparam: -0.252\n",
      "Iter 26/80 - Loss hyperparam: -0.302\n",
      "Iter 27/80 - Loss hyperparam: -0.352\n",
      "Iter 28/80 - Loss hyperparam: -0.402\n",
      "Iter 29/80 - Loss hyperparam: -0.452\n",
      "Iter 30/80 - Loss hyperparam: -0.502\n",
      "Iter 31/80 - Loss hyperparam: -0.552\n",
      "Iter 32/80 - Loss hyperparam: -0.602\n",
      "Iter 33/80 - Loss hyperparam: -0.652\n",
      "Iter 34/80 - Loss hyperparam: -0.702\n",
      "Iter 35/80 - Loss hyperparam: -0.753\n",
      "Iter 36/80 - Loss hyperparam: -0.803\n",
      "Iter 37/80 - Loss hyperparam: -0.854\n",
      "Iter 38/80 - Loss hyperparam: -0.904\n",
      "Iter 39/80 - Loss hyperparam: -0.954\n",
      "Iter 40/80 - Loss hyperparam: -1.005\n",
      "Iter 41/80 - Loss hyperparam: -1.055\n",
      "Iter 42/80 - Loss hyperparam: -1.105\n",
      "Iter 43/80 - Loss hyperparam: -1.155\n",
      "Iter 44/80 - Loss hyperparam: -1.205\n",
      "Iter 45/80 - Loss hyperparam: -1.255\n",
      "Iter 46/80 - Loss hyperparam: -1.305\n",
      "Iter 47/80 - Loss hyperparam: -1.354\n",
      "Iter 48/80 - Loss hyperparam: -1.404\n",
      "Iter 49/80 - Loss hyperparam: -1.454\n",
      "Iter 50/80 - Loss hyperparam: -1.503\n",
      "Iter 51/80 - Loss hyperparam: -1.552\n",
      "Iter 52/80 - Loss hyperparam: -1.601\n",
      "Iter 53/80 - Loss hyperparam: -1.650\n",
      "Iter 54/80 - Loss hyperparam: -1.699\n",
      "Iter 55/80 - Loss hyperparam: -1.747\n",
      "Iter 56/80 - Loss hyperparam: -1.796\n",
      "Iter 57/80 - Loss hyperparam: -1.844\n",
      "Iter 58/80 - Loss hyperparam: -1.892\n",
      "Iter 59/80 - Loss hyperparam: -1.939\n",
      "Iter 60/80 - Loss hyperparam: -1.986\n",
      "Iter 61/80 - Loss hyperparam: -2.033\n",
      "Iter 62/80 - Loss hyperparam: -2.080\n",
      "Iter 63/80 - Loss hyperparam: -2.126\n",
      "Iter 64/80 - Loss hyperparam: -2.172\n",
      "Iter 65/80 - Loss hyperparam: -2.216\n",
      "Iter 66/80 - Loss hyperparam: -2.261\n",
      "Iter 67/80 - Loss hyperparam: -2.306\n",
      "Iter 68/80 - Loss hyperparam: -2.349\n",
      "Iter 69/80 - Loss hyperparam: -2.392\n",
      "Iter 70/80 - Loss hyperparam: -2.434\n",
      "Iter 71/80 - Loss hyperparam: -2.476\n",
      "Iter 72/80 - Loss hyperparam: -2.517\n",
      "Iter 73/80 - Loss hyperparam: -2.556\n",
      "Iter 74/80 - Loss hyperparam: -2.596\n",
      "Iter 75/80 - Loss hyperparam: -2.633\n",
      "Iter 76/80 - Loss hyperparam: -2.671\n",
      "Iter 77/80 - Loss hyperparam: -2.709\n",
      "Iter 78/80 - Loss hyperparam: -2.743\n",
      "Iter 79/80 - Loss hyperparam: -2.777\n",
      "Iter 80/80 - Loss hyperparam: -2.810\n",
      "END HYPERPARAMETERS optimization\n",
      "10\n",
      "Loss theta: -0.828\n",
      "Loss theta: 0.316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5d2f03a7401b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mx0_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_theta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconduct_design_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_design\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mg_theta2_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg_theta2_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper_bound\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtol_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fbe6c5ef8c42>\u001b[0m in \u001b[0;36mconduct_design_opti\u001b[0;34m(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBFGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_search_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"strong_wolfe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fbe6c5ef8c42>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta2_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconduct_param_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_theta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_param_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#         print(x_d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         print(lower_bound)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter_hp = 80\n",
    "iter_design = 40 \n",
    "iter_param = 50\n",
    "\n",
    "\n",
    "f_target = Tensor(vf.tgt_vec) \n",
    "f_target = f_target.reshape(f_target.shape[0],1) \n",
    "tol_vector = 0.0005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size = 10\n",
    "loc_sample = np.random.random_sample((loc_size,2))\n",
    "g_theta2_vec = (Tensor(loc_sample)).flatten()\n",
    "\n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([0.5,0.7])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x = x00 \n",
    "\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "iter = 0 \n",
    "tol = 0.009 \n",
    "while(SUCCESS == False and iter < 50):\n",
    "    print('START HYPERPARAMETERS optimization')\n",
    "    model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp)\n",
    "\n",
    "    print('END HYPERPARAMETERS optimization')\n",
    "    print(iter)\n",
    "    \n",
    "    x0_new,g_theta2,lower_bound, upper_bound = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new)\n",
    "    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.flatten()], 0)\n",
    "    print(torch.norm(upper_bound - tol_vector))\n",
    "#     if (torch.norm(upper_bound - tol_vector) <= 0.1 ):\n",
    "#         print('bkhjghf')\n",
    "#         lr_new = lr_new * 0.1\n",
    "    print(lower_bound)\n",
    "    print(upper_bound)\n",
    "    print(f_target-tol_vector)\n",
    "    print(f_target+tol_vector)\n",
    "    loc_sample = np.random.random_sample((loc_size,2))\n",
    "    x0 = x0_new #Tensor(np.random.random_sample((1,2))) #x0_new\n",
    "    vec_x = torch.cat([vec_x, x0_new])\n",
    "    g_theta2_detach = g_theta2.detach()\n",
    "    new_data = vfield_(g_theta2_detach)\n",
    "    agg_data = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "    g_theta1= torch.cat([g_theta1, g_theta2], 0)\n",
    "\n",
    "    SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    iter = iter + 1\n",
    "    print(x0_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "\n",
    "x0 = Tensor(np.array([0.2016, 0.0966]))\n",
    "print(vf(x0))\n",
    "x0 = x0.reshape(1,2)\n",
    "print(x0)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "pr = likelihood(model(x0))\n",
    "print(pr.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "res = res + torch.ones(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta2_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(70,2)\n",
    "print(vec_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v2)\n",
    "np.savetxt('g_theta2_005.txt', v2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt('vec_x_005.txt', vec_x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
