{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import sep_vvkernels as svvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "from LBFGS import FullBatchLBFGS\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = VField()\n",
    "f_target = vf.tgt_vec\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "def g_theta(sample_size, D):\n",
    "    loc = np.random.random_sample((sample_size,D))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "train_y = torch.zeros([sample_size, N])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    train_y[i] = Tensor(vf(train_x[i])) #+ torch.randn(Tensor(vf(train_x[i])).size()) * 0.02\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = Tensor(vf(x[i])) #+ torch.randn(Tensor(vf(x[i])).size()) * 0.02\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criteria(tol_vector, f_target, lower_bound, upper_bound):\n",
    "    lower_tol_vector = f_target - tol_vector\n",
    "    upper_tol_vector = f_target + tol_vector\n",
    "    SUCCESS = True\n",
    "    FAILURE = False\n",
    "    for i in range(f_target.shape[0]):\n",
    "            if (lower_bound[i] < lower_tol_vector[i]) or  (upper_bound[i] > upper_tol_vector[i]):\n",
    "                SUCCESS = False  \n",
    "            if ((lower_bound[i] > upper_tol_vector[i]) or  (upper_bound[i] < lower_tol_vector[i])):\n",
    "                FAILURE = True\n",
    "    return SUCCESS, FAILURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(vvk_rbf.vvkRBFKernel())\n",
    "\n",
    "\n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "#\\         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "       # self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "# def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "#     noises = torch.ones(agg_data.shape[0]) * 0.0001\n",
    "#     likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)\n",
    "\n",
    "#     model = MultitaskGPModel(g_theta1, agg_data, likelihood,3)\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),  lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "#     mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "#     for i in range(training_iter):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(g_theta1)\n",
    "#         #output_ll = likelihood(output)\n",
    "\n",
    "#         #loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "#         loss = -mll(output, agg_data)\n",
    "#         loss.backward(retain_graph=True)\n",
    "\n",
    "#         print('Iter %d/%d - Loss hyperparam: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "#         optimizer.step()\n",
    "\n",
    "#     return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels):\n",
    "    noises = torch.ones(agg_data.shape[0]) * 0.001\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = FullBatchLBFGS(model.parameters(), lr=.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    def closure():\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "        #output_ll = likelihood(output)\n",
    "\n",
    "        #loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss = -mll(output, agg_data)\n",
    "        print('Loss gp: %.3f' % ( loss))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    loss = closure()\n",
    "    loss.backward()\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        options = {'closure': closure, 'current_loss': loss, 'max_ls': 10}\n",
    "        loss, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "\n",
    "        if fail:\n",
    "            print('Convergence reached!')\n",
    "            break\n",
    "        #print(i)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class design_opti(nn.Module):\n",
    "    def __init__(self, sample, x):\n",
    "        super(design_opti, self).__init__()\n",
    "        #loc = np.random.random_sample((loc_size,2))\n",
    "        self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "        self.x_design = nn.Parameter(Tensor(x))\n",
    "    def forward(self):\n",
    "       \n",
    "        g_theta2_new = self.g_theta2 #filter_sample(self.g_theta2, 0.009)\n",
    "        \n",
    "        return (g_theta2_new), self.x_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new):\n",
    "    design = design_opti(loc_sample,x0)\n",
    "    loc_sample0 = loc_sample\n",
    "    \n",
    "    g_theta2, x_d = design.forward()\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss2,lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, g_theta2, model, likelihood)\n",
    "        loss2 = -1. * loss2\n",
    "        loss2.backward(retain_graph=True)\n",
    "#         print(x_d)\n",
    "#         print(lower_bound)\n",
    "#         print(upper_bound)\n",
    "       \n",
    "        return loss2\n",
    "        \n",
    "        \n",
    "        \n",
    "    optimizer = torch.optim.LBFGS(design.parameters(), lr=lr_new, history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    g_theta2, x_d = design.forward()\n",
    "    loss2,lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, g_theta2, model, likelihood)\n",
    "    loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    #print(x_d)\n",
    "    return x_d, g_theta2, lower_bound, upper_bound\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.869\n",
      "Loss gp: 0.858\n",
      "Loss gp: 0.846\n",
      "Loss gp: 0.823\n",
      "Loss gp: 0.776\n",
      "Loss gp: 0.679\n",
      "Loss gp: 0.471\n",
      "Loss gp: 0.022\n",
      "Loss gp: -0.924\n",
      "Loss gp: -2.560\n",
      "Loss gp: -2.999\n",
      "Loss gp: -3.016\n",
      "Loss gp: -3.025\n",
      "Loss gp: -3.035\n",
      "Loss gp: -3.043\n",
      "Loss gp: -3.051\n",
      "Loss gp: -3.057\n",
      "Loss gp: -3.063\n",
      "Loss gp: -3.069\n",
      "Loss gp: -3.073\n",
      "Loss gp: -3.078\n",
      "Loss gp: -3.082\n",
      "Loss gp: -3.086\n",
      "Loss gp: -3.089\n",
      "Loss gp: -3.093\n",
      "Loss gp: -3.097\n",
      "Loss gp: -3.102\n",
      "Loss gp: -3.107\n",
      "Loss gp: -3.112\n",
      "Loss gp: -3.117\n",
      "Loss gp: -3.125\n",
      "Loss gp: -3.135\n",
      "Loss gp: -3.144\n",
      "Loss gp: -3.154\n",
      "Loss gp: -3.163\n",
      "Loss gp: -3.171\n",
      "Loss gp: -3.179\n",
      "Loss gp: -3.185\n",
      "Loss gp: -3.188\n",
      "Loss gp: -3.192\n",
      "Loss gp: -3.194\n",
      "Loss gp: -3.196\n",
      "Loss gp: -3.197\n",
      "Loss gp: -3.198\n",
      "Loss gp: -3.199\n",
      "Loss gp: -3.200\n",
      "Loss gp: -3.200\n",
      "Loss gp: -3.201\n",
      "Loss gp: -3.201\n",
      "Loss gp: -3.202\n",
      "Loss gp: -3.202\n",
      "Loss gp: -3.203\n",
      "Loss gp: -3.203\n",
      "Loss gp: -3.203\n",
      "Loss gp: -3.203\n",
      "Loss gp: -3.203\n",
      "Loss gp: -3.204\n",
      "Loss gp: -3.204\n",
      "Loss gp: -3.204\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.205\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/utils/cholesky.py:83: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: 36.055\n",
      "tensor(1.1199, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4805],\n",
      "        [1.0018]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4883],\n",
      "        [1.0093]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.0258, 0.2134]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.862\n",
      "Loss gp: 0.850\n",
      "Loss gp: 0.839\n",
      "Loss gp: 0.816\n",
      "Loss gp: 0.768\n",
      "Loss gp: 0.670\n",
      "Loss gp: 0.461\n",
      "Loss gp: 0.009\n",
      "Loss gp: -0.943\n",
      "Loss gp: -2.592\n",
      "Loss gp: -2.994\n",
      "Loss gp: -3.015\n",
      "Loss gp: -3.035\n",
      "Loss gp: -3.047\n",
      "Loss gp: -3.055\n",
      "Loss gp: -3.063\n",
      "Loss gp: -3.070\n",
      "Loss gp: -3.075\n",
      "Loss gp: -3.081\n",
      "Loss gp: -3.086\n",
      "Loss gp: -3.090\n",
      "Loss gp: -3.096\n",
      "Loss gp: -3.101\n",
      "Loss gp: -3.105\n",
      "Loss gp: -3.108\n",
      "Loss gp: -3.110\n",
      "Loss gp: -3.113\n",
      "Loss gp: -3.115\n",
      "Loss gp: -3.118\n",
      "Loss gp: -3.121\n",
      "Loss gp: -3.124\n",
      "Loss gp: -3.128\n",
      "Loss gp: -3.132\n",
      "Loss gp: -3.140\n",
      "Loss gp: -3.145\n",
      "Loss gp: -3.150\n",
      "Loss gp: -3.158\n",
      "Loss gp: -3.165\n",
      "Loss gp: -3.174\n",
      "Loss gp: -3.183\n",
      "Loss gp: -3.190\n",
      "Loss gp: -3.197\n",
      "Loss gp: -3.202\n",
      "Loss gp: -3.206\n",
      "Loss gp: -3.210\n",
      "Loss gp: -3.214\n",
      "Loss gp: -3.217\n",
      "Loss gp: -3.219\n",
      "Loss gp: -3.222\n",
      "Loss gp: -3.223\n",
      "Loss gp: -3.225\n",
      "Loss gp: -3.227\n",
      "Loss gp: -3.229\n",
      "Loss gp: -3.232\n",
      "Loss gp: -3.233\n",
      "Loss gp: -3.235\n",
      "Loss gp: -3.236\n",
      "Loss gp: -3.237\n",
      "Loss gp: -3.239\n",
      "Loss gp: -3.240\n",
      "Loss gp: -3.241\n",
      "Loss gp: -3.242\n",
      "Loss gp: -3.243\n",
      "Loss gp: -3.244\n",
      "Loss gp: -3.244\n",
      "Loss gp: -3.245\n",
      "Loss gp: -3.246\n",
      "Loss gp: -3.246\n",
      "Loss gp: -3.246\n",
      "Loss gp: -3.246\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.247\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/utils/cholesky.py:83: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: -12.341\n",
      "tensor(1.1203, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4957],\n",
      "        [0.9964]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5024],\n",
      "        [1.0029]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1896, 0.1075]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.845\n",
      "Loss gp: 0.833\n",
      "Loss gp: 0.822\n",
      "Loss gp: 0.798\n",
      "Loss gp: 0.750\n",
      "Loss gp: 0.650\n",
      "Loss gp: 0.437\n",
      "Loss gp: -0.026\n",
      "Loss gp: -0.999\n",
      "Loss gp: -2.682\n",
      "Loss gp: -3.111\n",
      "Loss gp: -3.114\n",
      "Loss gp: -3.117\n",
      "Loss gp: -3.119\n",
      "Loss gp: -3.123\n",
      "Loss gp: -3.126\n",
      "Loss gp: -3.131\n",
      "Loss gp: -3.134\n",
      "Loss gp: -3.139\n",
      "Loss gp: -3.143\n",
      "Loss gp: -3.149\n",
      "Loss gp: -3.155\n",
      "Loss gp: -3.165\n",
      "Loss gp: -3.173\n",
      "Loss gp: -3.185\n",
      "Loss gp: -3.194\n",
      "Loss gp: -3.207\n",
      "Loss gp: -3.220\n",
      "Loss gp: -3.229\n",
      "Loss gp: -3.238\n",
      "Loss gp: -3.244\n",
      "Loss gp: -3.248\n",
      "Loss gp: -3.252\n",
      "Loss gp: -3.256\n",
      "Loss gp: -3.259\n",
      "Loss gp: -3.260\n",
      "Loss gp: -3.262\n",
      "Loss gp: -3.263\n",
      "Loss gp: -3.264\n",
      "Loss gp: -3.265\n",
      "Loss gp: -3.265\n",
      "Loss gp: -3.266\n",
      "Loss gp: -3.266\n",
      "Loss gp: -3.267\n",
      "Loss gp: -3.267\n",
      "Loss gp: -3.267\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.270\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "2\n",
      "Loss design: -12.458\n",
      "tensor(1.1217, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4968],\n",
      "        [0.9975]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5036],\n",
      "        [1.0039]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1908, 0.1084]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.851\n",
      "Loss gp: 0.840\n",
      "Loss gp: 0.828\n",
      "Loss gp: 0.805\n",
      "Loss gp: 0.756\n",
      "Loss gp: 0.656\n",
      "Loss gp: 0.444\n",
      "Loss gp: -0.018\n",
      "Loss gp: -0.992\n",
      "Loss gp: -2.664\n",
      "Loss gp: -3.057\n",
      "Loss gp: -3.067\n",
      "Loss gp: -3.077\n",
      "Loss gp: -3.105\n",
      "Loss gp: -3.122\n",
      "Loss gp: -3.132\n",
      "Loss gp: -3.142\n",
      "Loss gp: -3.151\n",
      "Loss gp: -3.160\n",
      "Loss gp: -3.167\n",
      "Loss gp: -3.175\n",
      "Loss gp: -3.182\n",
      "Loss gp: -3.191\n",
      "Loss gp: -3.198\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.211\n",
      "Loss gp: -3.219\n",
      "Loss gp: -3.227\n",
      "Loss gp: -3.236\n",
      "Loss gp: -3.245\n",
      "Loss gp: -3.254\n",
      "Loss gp: -3.261\n",
      "Loss gp: -3.267\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.275\n",
      "Loss gp: -3.278\n",
      "Loss gp: -3.280\n",
      "Loss gp: -3.282\n",
      "Loss gp: -3.283\n",
      "Loss gp: -3.284\n",
      "Loss gp: -3.286\n",
      "Loss gp: -3.287\n",
      "Loss gp: -3.288\n",
      "Loss gp: -3.289\n",
      "Loss gp: -3.289\n",
      "Loss gp: -3.290\n",
      "Loss gp: -3.290\n",
      "Loss gp: -3.290\n",
      "Loss gp: -3.291\n",
      "Loss gp: -3.291\n",
      "Loss gp: -3.291\n",
      "Loss gp: -3.292\n",
      "Loss gp: -3.292\n",
      "Loss gp: -3.292\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.295\n",
      "Loss gp: -3.295\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "3\n",
      "Loss design: -12.590\n",
      "tensor(1.1217, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4972],\n",
      "        [0.9977]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5035],\n",
      "        [1.0039]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1908, 0.1084]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.838\n",
      "Loss gp: 0.826\n",
      "Loss gp: 0.814\n",
      "Loss gp: 0.790\n",
      "Loss gp: 0.742\n",
      "Loss gp: 0.641\n",
      "Loss gp: 0.425\n",
      "Loss gp: -0.042\n",
      "Loss gp: -1.029\n",
      "Loss gp: -2.725\n",
      "Loss gp: -3.129\n",
      "Loss gp: -3.132\n",
      "Loss gp: -3.135\n",
      "Loss gp: -3.141\n",
      "Loss gp: -3.155\n",
      "Loss gp: -3.164\n",
      "Loss gp: -3.171\n",
      "Loss gp: -3.178\n",
      "Loss gp: -3.190\n",
      "Loss gp: -3.204\n",
      "Loss gp: -3.218\n",
      "Loss gp: -3.231\n",
      "Loss gp: -3.244\n",
      "Loss gp: -3.255\n",
      "Loss gp: -3.266\n",
      "Loss gp: -3.275\n",
      "Loss gp: -3.282\n",
      "Loss gp: -3.287\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.297\n",
      "Loss gp: -3.300\n",
      "Loss gp: -3.303\n",
      "Loss gp: -3.305\n",
      "Loss gp: -3.307\n",
      "Loss gp: -3.308\n",
      "Loss gp: -3.309\n",
      "Loss gp: -3.310\n",
      "Loss gp: -3.310\n",
      "Loss gp: -3.311\n",
      "Loss gp: -3.311\n",
      "Loss gp: -3.312\n",
      "Loss gp: -3.312\n",
      "Loss gp: -3.312\n",
      "Loss gp: -3.313\n",
      "Loss gp: -3.313\n",
      "Loss gp: -3.313\n",
      "Loss gp: -3.313\n",
      "Loss gp: -3.314\n",
      "Loss gp: -3.314\n",
      "Loss gp: -3.314\n",
      "Loss gp: -3.314\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.316\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "4\n",
      "Loss design: -12.417\n",
      "tensor(1.1218, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4974],\n",
      "        [0.9979]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5036],\n",
      "        [1.0039]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1908, 0.1084]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.820\n",
      "Loss gp: 0.808\n",
      "Loss gp: 0.796\n",
      "Loss gp: 0.772\n",
      "Loss gp: 0.723\n",
      "Loss gp: 0.621\n",
      "Loss gp: 0.403\n",
      "Loss gp: -0.071\n",
      "Loss gp: -1.068\n",
      "Loss gp: -2.808\n",
      "Loss gp: -3.178\n",
      "Loss gp: -3.198\n",
      "Loss gp: -3.205\n",
      "Loss gp: -3.211\n",
      "Loss gp: -3.217\n",
      "Loss gp: -3.220\n",
      "Loss gp: -3.223\n",
      "Loss gp: -3.225\n",
      "Loss gp: -3.227\n",
      "Loss gp: -3.229\n",
      "Loss gp: -3.231\n",
      "Loss gp: -3.234\n",
      "Loss gp: -3.236\n",
      "Loss gp: -3.238\n",
      "Loss gp: -3.239\n",
      "Loss gp: -3.242\n",
      "Loss gp: -3.245\n",
      "Loss gp: -3.248\n",
      "Loss gp: -3.250\n",
      "Loss gp: -3.254\n",
      "Loss gp: -3.256\n",
      "Loss gp: -3.261\n",
      "Loss gp: -3.264\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.281\n",
      "Loss gp: -3.288\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.299\n",
      "Loss gp: -3.304\n",
      "Loss gp: -3.308\n",
      "Loss gp: -3.311\n",
      "Loss gp: -3.314\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.318\n",
      "Loss gp: -3.319\n",
      "Loss gp: -3.320\n",
      "Loss gp: -3.321\n",
      "Loss gp: -3.322\n",
      "Loss gp: -3.323\n",
      "Loss gp: -3.323\n",
      "Loss gp: -3.324\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.326\n",
      "Loss gp: -3.326\n",
      "Loss gp: -3.327\n",
      "Loss gp: -3.327\n",
      "Loss gp: -3.327\n",
      "Loss gp: -3.328\n",
      "Loss gp: -3.328\n",
      "Loss gp: -3.328\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.330\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.330\n",
      "Loss gp: -3.330\n",
      "Loss gp: -3.329\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "5\n",
      "Loss design: -12.413\n",
      "tensor(1.1213, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4968],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5031],\n",
      "        [1.0036]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1909, 0.1078]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.826\n",
      "Loss gp: 0.814\n",
      "Loss gp: 0.803\n",
      "Loss gp: 0.778\n",
      "Loss gp: 0.729\n",
      "Loss gp: 0.627\n",
      "Loss gp: 0.408\n",
      "Loss gp: -0.066\n",
      "Loss gp: -1.067\n",
      "Loss gp: -2.790\n",
      "Loss gp: -3.194\n",
      "Loss gp: -3.199\n",
      "Loss gp: -3.203\n",
      "Loss gp: -3.212\n",
      "Loss gp: -3.221\n",
      "Loss gp: -3.225\n",
      "Loss gp: -3.231\n",
      "Loss gp: -3.239\n",
      "Loss gp: -3.245\n",
      "Loss gp: -3.252\n",
      "Loss gp: -3.261\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.277\n",
      "Loss gp: -3.286\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.303\n",
      "Loss gp: -3.310\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.321\n",
      "Loss gp: -3.326\n",
      "Loss gp: -3.328\n",
      "Loss gp: -3.332\n",
      "Loss gp: -3.335\n",
      "Loss gp: -3.336\n",
      "Loss gp: -3.337\n",
      "Loss gp: -3.338\n",
      "Loss gp: -3.339\n",
      "Loss gp: -3.340\n",
      "Loss gp: -3.340\n",
      "Loss gp: -3.341\n",
      "Loss gp: -3.341\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.344\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "6\n",
      "Loss design: -12.247\n",
      "tensor(1.1215, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4970],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5032],\n",
      "        [1.0037]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1911, 0.1078]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.832\n",
      "Loss gp: 0.821\n",
      "Loss gp: 0.809\n",
      "Loss gp: 0.785\n",
      "Loss gp: 0.735\n",
      "Loss gp: 0.633\n",
      "Loss gp: 0.415\n",
      "Loss gp: -0.058\n",
      "Loss gp: -1.057\n",
      "Loss gp: -2.755\n",
      "Loss gp: -3.114\n",
      "Loss gp: -3.123\n",
      "Loss gp: -3.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -3.146\n",
      "Loss gp: -3.179\n",
      "Loss gp: -3.197\n",
      "Loss gp: -3.204\n",
      "Loss gp: -3.211\n",
      "Loss gp: -3.216\n",
      "Loss gp: -3.223\n",
      "Loss gp: -3.229\n",
      "Loss gp: -3.237\n",
      "Loss gp: -3.244\n",
      "Loss gp: -3.255\n",
      "Loss gp: -3.264\n",
      "Loss gp: -3.277\n",
      "Loss gp: -3.289\n",
      "Loss gp: -3.300\n",
      "Loss gp: -3.310\n",
      "Loss gp: -3.318\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.330\n",
      "Loss gp: -3.336\n",
      "Loss gp: -3.339\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.345\n",
      "Loss gp: -3.348\n",
      "Loss gp: -3.349\n",
      "Loss gp: -3.350\n",
      "Loss gp: -3.351\n",
      "Loss gp: -3.352\n",
      "Loss gp: -3.353\n",
      "Loss gp: -3.353\n",
      "Loss gp: -3.354\n",
      "Loss gp: -3.354\n",
      "Loss gp: -3.355\n",
      "Loss gp: -3.355\n",
      "Loss gp: -3.355\n",
      "Loss gp: -3.356\n",
      "Loss gp: -3.356\n",
      "Loss gp: -3.356\n",
      "Loss gp: -3.356\n",
      "Loss gp: -3.357\n",
      "Loss gp: -3.357\n",
      "Loss gp: -3.357\n",
      "Loss gp: -3.357\n",
      "Loss gp: -3.357\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.359\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "7\n",
      "Loss design: -12.823\n",
      "tensor(1.1201, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4963],\n",
      "        [0.9968]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5024],\n",
      "        [1.0026]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1950, 0.1038]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.823\n",
      "Loss gp: 0.811\n",
      "Loss gp: 0.799\n",
      "Loss gp: 0.774\n",
      "Loss gp: 0.725\n",
      "Loss gp: 0.622\n",
      "Loss gp: 0.402\n",
      "Loss gp: -0.075\n",
      "Loss gp: -1.082\n",
      "Loss gp: -2.830\n",
      "Loss gp: -3.229\n",
      "Loss gp: -3.243\n",
      "Loss gp: -3.252\n",
      "Loss gp: -3.260\n",
      "Loss gp: -3.267\n",
      "Loss gp: -3.273\n",
      "Loss gp: -3.278\n",
      "Loss gp: -3.283\n",
      "Loss gp: -3.286\n",
      "Loss gp: -3.290\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.296\n",
      "Loss gp: -3.299\n",
      "Loss gp: -3.303\n",
      "Loss gp: -3.306\n",
      "Loss gp: -3.310\n",
      "Loss gp: -3.314\n",
      "Loss gp: -3.317\n",
      "Loss gp: -3.321\n",
      "Loss gp: -3.324\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.333\n",
      "Loss gp: -3.337\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.346\n",
      "Loss gp: -3.350\n",
      "Loss gp: -3.353\n",
      "Loss gp: -3.355\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.360\n",
      "Loss gp: -3.361\n",
      "Loss gp: -3.363\n",
      "Loss gp: -3.364\n",
      "Loss gp: -3.365\n",
      "Loss gp: -3.366\n",
      "Loss gp: -3.366\n",
      "Loss gp: -3.367\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.370\n",
      "Loss gp: -3.370\n",
      "Loss gp: -3.370\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.373\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "8\n",
      "Loss design: -12.694\n",
      "tensor(1.1207, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4968],\n",
      "        [0.9972]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5028],\n",
      "        [1.0030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1954, 0.1041]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.815\n",
      "Loss gp: 0.803\n",
      "Loss gp: 0.791\n",
      "Loss gp: 0.767\n",
      "Loss gp: 0.717\n",
      "Loss gp: 0.613\n",
      "Loss gp: 0.392\n",
      "Loss gp: -0.088\n",
      "Loss gp: -1.103\n",
      "Loss gp: -2.835\n",
      "Loss gp: -3.183\n",
      "Loss gp: -3.186\n",
      "Loss gp: -3.190\n",
      "Loss gp: -3.197\n",
      "Loss gp: -3.210\n",
      "Loss gp: -3.222\n",
      "Loss gp: -3.234\n",
      "Loss gp: -3.246\n",
      "Loss gp: -3.261\n",
      "Loss gp: -3.276\n",
      "Loss gp: -3.288\n",
      "Loss gp: -3.298\n",
      "Loss gp: -3.312\n",
      "Loss gp: -3.321\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.337\n",
      "Loss gp: -3.345\n",
      "Loss gp: -3.352\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.362\n",
      "Loss gp: -3.367\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.376\n",
      "Loss gp: -3.377\n",
      "Loss gp: -3.378\n",
      "Loss gp: -3.379\n",
      "Loss gp: -3.380\n",
      "Loss gp: -3.381\n",
      "Loss gp: -3.381\n",
      "Loss gp: -3.382\n",
      "Loss gp: -3.382\n",
      "Loss gp: -3.382\n",
      "Loss gp: -3.383\n",
      "Loss gp: -3.383\n",
      "Loss gp: -3.383\n",
      "Loss gp: -3.383\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.386\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "9\n",
      "Loss design: -12.634\n",
      "tensor(1.1207, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4970],\n",
      "        [0.9974]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5029],\n",
      "        [1.0030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1954, 0.1042]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.822\n",
      "Loss gp: 0.810\n",
      "Loss gp: 0.798\n",
      "Loss gp: 0.774\n",
      "Loss gp: 0.724\n",
      "Loss gp: 0.620\n",
      "Loss gp: 0.399\n",
      "Loss gp: -0.082\n",
      "Loss gp: -1.098\n",
      "Loss gp: -2.798\n",
      "Loss gp: -3.141\n",
      "Loss gp: -3.157\n",
      "Loss gp: -3.172\n",
      "Loss gp: -3.197\n",
      "Loss gp: -3.217\n",
      "Loss gp: -3.235\n",
      "Loss gp: -3.250\n",
      "Loss gp: -3.263\n",
      "Loss gp: -3.275\n",
      "Loss gp: -3.287\n",
      "Loss gp: -3.297\n",
      "Loss gp: -3.307\n",
      "Loss gp: -3.316\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.335\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.350\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.365\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.378\n",
      "Loss gp: -3.381\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.387\n",
      "Loss gp: -3.388\n",
      "Loss gp: -3.389\n",
      "Loss gp: -3.390\n",
      "Loss gp: -3.391\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.393\n",
      "Loss gp: -3.393\n",
      "Loss gp: -3.393\n",
      "Loss gp: -3.393\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.394\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "10\n",
      "Loss design: -12.356\n",
      "tensor(1.1207, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4968],\n",
      "        [0.9971]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5029],\n",
      "        [1.0030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1955, 0.1042]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.811\n",
      "Loss gp: 0.799\n",
      "Loss gp: 0.787\n",
      "Loss gp: 0.763\n",
      "Loss gp: 0.712\n",
      "Loss gp: 0.608\n",
      "Loss gp: 0.385\n",
      "Loss gp: -0.098\n",
      "Loss gp: -1.121\n",
      "Loss gp: -2.859\n",
      "Loss gp: -3.211\n",
      "Loss gp: -3.216\n",
      "Loss gp: -3.221\n",
      "Loss gp: -3.230\n",
      "Loss gp: -3.239\n",
      "Loss gp: -3.248\n",
      "Loss gp: -3.257\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.279\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.306\n",
      "Loss gp: -3.319\n",
      "Loss gp: -3.330\n",
      "Loss gp: -3.341\n",
      "Loss gp: -3.353\n",
      "Loss gp: -3.361\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.379\n",
      "Loss gp: -3.383\n",
      "Loss gp: -3.389\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.395\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.400\n",
      "Loss gp: -3.401\n",
      "Loss gp: -3.402\n",
      "Loss gp: -3.402\n",
      "Loss gp: -3.403\n",
      "Loss gp: -3.403\n",
      "Loss gp: -3.404\n",
      "Loss gp: -3.404\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.407\n",
      "Loss gp: -3.407\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.406\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "11\n",
      "Loss design: -12.549\n",
      "tensor(1.1207, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4971],\n",
      "        [0.9973]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5029],\n",
      "        [1.0030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1955, 0.1042]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.806\n",
      "Loss gp: 0.794\n",
      "Loss gp: 0.782\n",
      "Loss gp: 0.757\n",
      "Loss gp: 0.707\n",
      "Loss gp: 0.602\n",
      "Loss gp: 0.378\n",
      "Loss gp: -0.109\n",
      "Loss gp: -1.137\n",
      "Loss gp: -2.890\n",
      "Loss gp: -3.239\n",
      "Loss gp: -3.244\n",
      "Loss gp: -3.247\n",
      "Loss gp: -3.257\n",
      "Loss gp: -3.265\n",
      "Loss gp: -3.274\n",
      "Loss gp: -3.282\n",
      "Loss gp: -3.292\n",
      "Loss gp: -3.305\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.336\n",
      "Loss gp: -3.346\n",
      "Loss gp: -3.356\n",
      "Loss gp: -3.364\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.381\n",
      "Loss gp: -3.387\n",
      "Loss gp: -3.391\n",
      "Loss gp: -3.397\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -3.404\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.407\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.409\n",
      "Loss gp: -3.410\n",
      "Loss gp: -3.410\n",
      "Loss gp: -3.411\n",
      "Loss gp: -3.412\n",
      "Loss gp: -3.412\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.416\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "12\n",
      "Loss design: -12.773\n",
      "tensor(1.1205, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4972],\n",
      "        [0.9975]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5027],\n",
      "        [1.0029]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1955, 0.1042]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.808\n",
      "Loss gp: 0.796\n",
      "Loss gp: 0.784\n",
      "Loss gp: 0.759\n",
      "Loss gp: 0.709\n",
      "Loss gp: 0.604\n",
      "Loss gp: 0.380\n",
      "Loss gp: -0.106\n",
      "Loss gp: -1.134\n",
      "Loss gp: -2.881\n",
      "Loss gp: -3.228\n",
      "Loss gp: -3.235\n",
      "Loss gp: -3.242\n",
      "Loss gp: -3.255\n",
      "Loss gp: -3.268\n",
      "Loss gp: -3.284\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.302\n",
      "Loss gp: -3.312\n",
      "Loss gp: -3.322\n",
      "Loss gp: -3.333\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.352\n",
      "Loss gp: -3.361\n",
      "Loss gp: -3.370\n",
      "Loss gp: -3.378\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.390\n",
      "Loss gp: -3.397\n",
      "Loss gp: -3.401\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.411\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.418\n",
      "Loss gp: -3.419\n",
      "Loss gp: -3.420\n",
      "Loss gp: -3.420\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.422\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "13\n",
      "Loss design: -12.500\n",
      "tensor(1.1207, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4971],\n",
      "        [0.9974]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5029],\n",
      "        [1.0031]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1955, 0.1042]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.804\n",
      "Loss gp: 0.791\n",
      "Loss gp: 0.779\n",
      "Loss gp: 0.754\n",
      "Loss gp: 0.704\n",
      "Loss gp: 0.598\n",
      "Loss gp: 0.374\n",
      "Loss gp: -0.115\n",
      "Loss gp: -1.147\n",
      "Loss gp: -2.905\n",
      "Loss gp: -3.261\n",
      "Loss gp: -3.270\n",
      "Loss gp: -3.278\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.308\n",
      "Loss gp: -3.318\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.333\n",
      "Loss gp: -3.341\n",
      "Loss gp: -3.348\n",
      "Loss gp: -3.354\n",
      "Loss gp: -3.360\n",
      "Loss gp: -3.367\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.381\n",
      "Loss gp: -3.388\n",
      "Loss gp: -3.395\n",
      "Loss gp: -3.401\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.410\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.419\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.427\n",
      "Loss gp: -3.428\n",
      "Loss gp: -3.429\n",
      "Loss gp: -3.429\n",
      "Loss gp: -3.429\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.432\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "14\n",
      "Loss design: -12.658\n",
      "tensor(1.1208, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4973],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5030],\n",
      "        [1.0031]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1957, 0.1041]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.811\n",
      "Loss gp: 0.799\n",
      "Loss gp: 0.787\n",
      "Loss gp: 0.762\n",
      "Loss gp: 0.711\n",
      "Loss gp: 0.606\n",
      "Loss gp: 0.382\n",
      "Loss gp: -0.105\n",
      "Loss gp: -1.136\n",
      "Loss gp: -2.838\n",
      "Loss gp: -3.135\n",
      "Loss gp: -3.159\n",
      "Loss gp: -3.181\n",
      "Loss gp: -3.225\n",
      "Loss gp: -3.262\n",
      "Loss gp: -3.288\n",
      "Loss gp: -3.303\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.335\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.351\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.366\n",
      "Loss gp: -3.376\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.391\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.411\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.419\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.428\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.435\n",
      "Loss gp: -3.436\n",
      "Loss gp: -3.436\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.437\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "15\n",
      "Loss design: -12.491\n",
      "tensor(1.1206, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4971],\n",
      "        [0.9973]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5028],\n",
      "        [1.0029]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1957, 0.1040]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.801\n",
      "Loss gp: 0.789\n",
      "Loss gp: 0.777\n",
      "Loss gp: 0.752\n",
      "Loss gp: 0.701\n",
      "Loss gp: 0.595\n",
      "Loss gp: 0.370\n",
      "Loss gp: -0.121\n",
      "Loss gp: -1.159\n",
      "Loss gp: -2.912\n",
      "Loss gp: -3.242\n",
      "Loss gp: -3.251\n",
      "Loss gp: -3.259\n",
      "Loss gp: -3.271\n",
      "Loss gp: -3.283\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.303\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.325\n",
      "Loss gp: -3.337\n",
      "Loss gp: -3.347\n",
      "Loss gp: -3.357\n",
      "Loss gp: -3.366\n",
      "Loss gp: -3.375\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.401\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.411\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.429\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.435\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.439\n",
      "Loss gp: -3.440\n",
      "Loss gp: -3.441\n",
      "Loss gp: -3.442\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.445\n",
      "Loss gp: -3.445\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.447\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "16\n",
      "Loss design: -12.766\n",
      "tensor(1.1207, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4975],\n",
      "        [0.9978]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5029],\n",
      "        [1.0030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1957, 0.1040]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.798\n",
      "Loss gp: 0.786\n",
      "Loss gp: 0.774\n",
      "Loss gp: 0.749\n",
      "Loss gp: 0.698\n",
      "Loss gp: 0.592\n",
      "Loss gp: 0.366\n",
      "Loss gp: -0.126\n",
      "Loss gp: -1.167\n",
      "Loss gp: -2.935\n",
      "Loss gp: -3.257\n",
      "Loss gp: -3.263\n",
      "Loss gp: -3.269\n",
      "Loss gp: -3.279\n",
      "Loss gp: -3.289\n",
      "Loss gp: -3.299\n",
      "Loss gp: -3.308\n",
      "Loss gp: -3.319\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.341\n",
      "Loss gp: -3.352\n",
      "Loss gp: -3.362\n",
      "Loss gp: -3.377\n",
      "Loss gp: -3.388\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.418\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.438\n",
      "Loss gp: -3.441\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.445\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.448\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.449\n",
      "Loss gp: -3.450\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.455\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.454\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "17\n",
      "Loss design: -12.811\n",
      "tensor(1.1206, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4976],\n",
      "        [0.9979]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5028],\n",
      "        [1.0030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1957, 0.1040]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.801\n",
      "Loss gp: 0.788\n",
      "Loss gp: 0.776\n",
      "Loss gp: 0.751\n",
      "Loss gp: 0.700\n",
      "Loss gp: 0.594\n",
      "Loss gp: 0.368\n",
      "Loss gp: -0.125\n",
      "Loss gp: -1.166\n",
      "Loss gp: -2.904\n",
      "Loss gp: -3.195\n",
      "Loss gp: -3.208\n",
      "Loss gp: -3.222\n",
      "Loss gp: -3.240\n",
      "Loss gp: -3.257\n",
      "Loss gp: -3.272\n",
      "Loss gp: -3.284\n",
      "Loss gp: -3.296\n",
      "Loss gp: -3.308\n",
      "Loss gp: -3.322\n",
      "Loss gp: -3.333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -3.345\n",
      "Loss gp: -3.354\n",
      "Loss gp: -3.366\n",
      "Loss gp: -3.378\n",
      "Loss gp: -3.389\n",
      "Loss gp: -3.400\n",
      "Loss gp: -3.410\n",
      "Loss gp: -3.418\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.436\n",
      "Loss gp: -3.439\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.449\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.455\n",
      "Loss gp: -3.456\n",
      "Loss gp: -3.456\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.459\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "18\n",
      "Loss design: -12.612\n",
      "tensor(1.1206, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4974],\n",
      "        [0.9977]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5028],\n",
      "        [1.0030]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1957, 0.1040]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.793\n",
      "Loss gp: 0.781\n",
      "Loss gp: 0.768\n",
      "Loss gp: 0.743\n",
      "Loss gp: 0.692\n",
      "Loss gp: 0.585\n",
      "Loss gp: 0.358\n",
      "Loss gp: -0.137\n",
      "Loss gp: -1.184\n",
      "Loss gp: -2.959\n",
      "Loss gp: -3.279\n",
      "Loss gp: -3.287\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.304\n",
      "Loss gp: -3.313\n",
      "Loss gp: -3.324\n",
      "Loss gp: -3.337\n",
      "Loss gp: -3.349\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.382\n",
      "Loss gp: -3.393\n",
      "Loss gp: -3.403\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.428\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.439\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.450\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.455\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.462\n",
      "Loss gp: -3.463\n",
      "Loss gp: -3.463\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.464\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "19\n",
      "Loss design: -12.587\n",
      "tensor(1.1204, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4973],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5026],\n",
      "        [1.0028]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1957, 0.1039]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.797\n",
      "Loss gp: 0.785\n",
      "Loss gp: 0.772\n",
      "Loss gp: 0.747\n",
      "Loss gp: 0.696\n",
      "Loss gp: 0.589\n",
      "Loss gp: 0.362\n",
      "Loss gp: -0.132\n",
      "Loss gp: -1.178\n",
      "Loss gp: -2.916\n",
      "Loss gp: -3.207\n",
      "Loss gp: -3.226\n",
      "Loss gp: -3.243\n",
      "Loss gp: -3.267\n",
      "Loss gp: -3.287\n",
      "Loss gp: -3.303\n",
      "Loss gp: -3.317\n",
      "Loss gp: -3.331\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.356\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.380\n",
      "Loss gp: -3.390\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.407\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.463\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.466\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.470\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "20\n",
      "Loss design: -12.675\n",
      "tensor(1.1203, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4974],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5026],\n",
      "        [1.0027]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1958, 0.1037]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.796\n",
      "Loss gp: 0.784\n",
      "Loss gp: 0.771\n",
      "Loss gp: 0.746\n",
      "Loss gp: 0.695\n",
      "Loss gp: 0.588\n",
      "Loss gp: 0.360\n",
      "Loss gp: -0.136\n",
      "Loss gp: -1.184\n",
      "Loss gp: -2.914\n",
      "Loss gp: -3.195\n",
      "Loss gp: -3.216\n",
      "Loss gp: -3.237\n",
      "Loss gp: -3.267\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.333\n",
      "Loss gp: -3.348\n",
      "Loss gp: -3.361\n",
      "Loss gp: -3.372\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.396\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.424\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.449\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.456\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.463\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.471\n",
      "Loss gp: -3.471\n",
      "Loss gp: -3.472\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.475\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "21\n",
      "Loss design: -12.815\n",
      "tensor(1.1213, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4983],\n",
      "        [0.9984]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5035],\n",
      "        [1.0034]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1968, 0.1039]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.790\n",
      "Loss gp: 0.778\n",
      "Loss gp: 0.766\n",
      "Loss gp: 0.740\n",
      "Loss gp: 0.689\n",
      "Loss gp: 0.582\n",
      "Loss gp: 0.354\n",
      "Loss gp: -0.143\n",
      "Loss gp: -1.195\n",
      "Loss gp: -2.990\n",
      "Loss gp: -3.319\n",
      "Loss gp: -3.327\n",
      "Loss gp: -3.333\n",
      "Loss gp: -3.344\n",
      "Loss gp: -3.355\n",
      "Loss gp: -3.365\n",
      "Loss gp: -3.375\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.412\n",
      "Loss gp: -3.419\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.438\n",
      "Loss gp: -3.442\n",
      "Loss gp: -3.449\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.472\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.480\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "22\n",
      "Loss design: -12.602\n",
      "tensor(1.1203, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4974],\n",
      "        [0.9977]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5026],\n",
      "        [1.0028]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1964, 0.1033]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.791\n",
      "Loss gp: 0.778\n",
      "Loss gp: 0.766\n",
      "Loss gp: 0.741\n",
      "Loss gp: 0.689\n",
      "Loss gp: 0.582\n",
      "Loss gp: 0.354\n",
      "Loss gp: -0.143\n",
      "Loss gp: -1.195\n",
      "Loss gp: -2.970\n",
      "Loss gp: -3.277\n",
      "Loss gp: -3.290\n",
      "Loss gp: -3.302\n",
      "Loss gp: -3.320\n",
      "Loss gp: -3.334\n",
      "Loss gp: -3.347\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.378\n",
      "Loss gp: -3.388\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.424\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.438\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.450\n",
      "Loss gp: -3.455\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.472\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.481\n",
      "Loss gp: -3.481\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.483\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.486\n",
      "Loss gp: -3.486\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "23\n",
      "Loss design: -13.461\n",
      "tensor(1.1198, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4974],\n",
      "        [0.9977]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5022],\n",
      "        [1.0023]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1967, 0.1028]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.787\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.762\n",
      "Loss gp: 0.737\n",
      "Loss gp: 0.685\n",
      "Loss gp: 0.578\n",
      "Loss gp: 0.348\n",
      "Loss gp: -0.151\n",
      "Loss gp: -1.207\n",
      "Loss gp: -2.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -3.290\n",
      "Loss gp: -3.299\n",
      "Loss gp: -3.309\n",
      "Loss gp: -3.320\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.340\n",
      "Loss gp: -3.349\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.379\n",
      "Loss gp: -3.388\n",
      "Loss gp: -3.401\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.439\n",
      "Loss gp: -3.445\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.471\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.483\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.486\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.489\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "24\n",
      "Loss design: -12.786\n",
      "tensor(1.1201, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4975],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5025],\n",
      "        [1.0025]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1969, 0.1030]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.791\n",
      "Loss gp: 0.778\n",
      "Loss gp: 0.766\n",
      "Loss gp: 0.740\n",
      "Loss gp: 0.689\n",
      "Loss gp: 0.582\n",
      "Loss gp: 0.353\n",
      "Loss gp: -0.146\n",
      "Loss gp: -1.200\n",
      "Loss gp: -2.967\n",
      "Loss gp: -3.264\n",
      "Loss gp: -3.281\n",
      "Loss gp: -3.297\n",
      "Loss gp: -3.323\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.357\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.382\n",
      "Loss gp: -3.391\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.412\n",
      "Loss gp: -3.418\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.438\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.450\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.463\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.481\n",
      "Loss gp: -3.483\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.491\n",
      "Loss gp: -3.492\n",
      "Loss gp: -3.492\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.495\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "25\n",
      "Loss design: -12.917\n",
      "tensor(1.1201, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4976],\n",
      "        [0.9979]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5024],\n",
      "        [1.0026]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1969, 0.1028]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.788\n",
      "Loss gp: 0.776\n",
      "Loss gp: 0.764\n",
      "Loss gp: 0.738\n",
      "Loss gp: 0.687\n",
      "Loss gp: 0.579\n",
      "Loss gp: 0.350\n",
      "Loss gp: -0.150\n",
      "Loss gp: -1.206\n",
      "Loss gp: -2.957\n",
      "Loss gp: -3.233\n",
      "Loss gp: -3.254\n",
      "Loss gp: -3.273\n",
      "Loss gp: -3.298\n",
      "Loss gp: -3.320\n",
      "Loss gp: -3.337\n",
      "Loss gp: -3.351\n",
      "Loss gp: -3.365\n",
      "Loss gp: -3.376\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.402\n",
      "Loss gp: -3.410\n",
      "Loss gp: -3.418\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.433\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.491\n",
      "Loss gp: -3.492\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "26\n",
      "Loss design: -12.989\n",
      "tensor(1.1199, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4975],\n",
      "        [0.9978]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5023],\n",
      "        [1.0024]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1969, 0.1028]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.784\n",
      "Loss gp: 0.772\n",
      "Loss gp: 0.759\n",
      "Loss gp: 0.734\n",
      "Loss gp: 0.682\n",
      "Loss gp: 0.575\n",
      "Loss gp: 0.344\n",
      "Loss gp: -0.157\n",
      "Loss gp: -1.217\n",
      "Loss gp: -3.001\n",
      "Loss gp: -3.297\n",
      "Loss gp: -3.310\n",
      "Loss gp: -3.322\n",
      "Loss gp: -3.334\n",
      "Loss gp: -3.345\n",
      "Loss gp: -3.355\n",
      "Loss gp: -3.365\n",
      "Loss gp: -3.376\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.395\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.442\n",
      "Loss gp: -3.449\n",
      "Loss gp: -3.455\n",
      "Loss gp: -3.462\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.492\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.500\n",
      "Loss gp: -3.500\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.504\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "27\n",
      "Loss design: -13.142\n",
      "tensor(1.1200, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4977],\n",
      "        [0.9979]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5024],\n",
      "        [1.0024]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1969, 0.1028]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.793\n",
      "Loss gp: 0.780\n",
      "Loss gp: 0.768\n",
      "Loss gp: 0.742\n",
      "Loss gp: 0.691\n",
      "Loss gp: 0.583\n",
      "Loss gp: 0.354\n",
      "Loss gp: -0.145\n",
      "Loss gp: -1.200\n",
      "Loss gp: -2.878\n",
      "Loss gp: -3.095\n",
      "Loss gp: -3.136\n",
      "Loss gp: -3.173\n",
      "Loss gp: -3.244\n",
      "Loss gp: -3.304\n",
      "Loss gp: -3.342\n",
      "Loss gp: -3.364\n",
      "Loss gp: -3.379\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.404\n",
      "Loss gp: -3.414\n",
      "Loss gp: -3.423\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.483\n",
      "Loss gp: -3.486\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.491\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.500\n",
      "Loss gp: -3.500\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "28\n",
      "Loss design: -12.693\n",
      "tensor(1.1199, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4973],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5024],\n",
      "        [1.0024]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1969, 0.1028]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.784\n",
      "Loss gp: 0.772\n",
      "Loss gp: 0.759\n",
      "Loss gp: 0.734\n",
      "Loss gp: 0.682\n",
      "Loss gp: 0.574\n",
      "Loss gp: 0.343\n",
      "Loss gp: -0.159\n",
      "Loss gp: -1.221\n",
      "Loss gp: -2.991\n",
      "Loss gp: -3.274\n",
      "Loss gp: -3.291\n",
      "Loss gp: -3.307\n",
      "Loss gp: -3.324\n",
      "Loss gp: -3.338\n",
      "Loss gp: -3.352\n",
      "Loss gp: -3.364\n",
      "Loss gp: -3.375\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.403\n",
      "Loss gp: -3.413\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.466\n",
      "Loss gp: -3.472\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.491\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.500\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.505\n",
      "Loss gp: -3.505\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "29\n",
      "Loss design: -12.860\n",
      "tensor(1.1201, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4976],\n",
      "        [0.9978]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5024],\n",
      "        [1.0026]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1971, 0.1029]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.781\n",
      "Loss gp: 0.769\n",
      "Loss gp: 0.756\n",
      "Loss gp: 0.731\n",
      "Loss gp: 0.679\n",
      "Loss gp: 0.571\n",
      "Loss gp: 0.340\n",
      "Loss gp: -0.163\n",
      "Loss gp: -1.228\n",
      "Loss gp: -3.016\n",
      "Loss gp: -3.304\n",
      "Loss gp: -3.315\n",
      "Loss gp: -3.327\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.355\n",
      "Loss gp: -3.366\n",
      "Loss gp: -3.380\n",
      "Loss gp: -3.390\n",
      "Loss gp: -3.400\n",
      "Loss gp: -3.409\n",
      "Loss gp: -3.420\n",
      "Loss gp: -3.429\n",
      "Loss gp: -3.436\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.472\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.483\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.491\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.505\n",
      "Loss gp: -3.505\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "30\n",
      "Loss design: -13.121\n",
      "tensor(1.1201, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4979],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5025],\n",
      "        [1.0025]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1971, 0.1029]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.780\n",
      "Loss gp: 0.768\n",
      "Loss gp: 0.755\n",
      "Loss gp: 0.730\n",
      "Loss gp: 0.678\n",
      "Loss gp: 0.569\n",
      "Loss gp: 0.338\n",
      "Loss gp: -0.165\n",
      "Loss gp: -1.231\n",
      "Loss gp: -3.036\n",
      "Loss gp: -3.349\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.378\n",
      "Loss gp: -3.388\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.424\n",
      "Loss gp: -3.433\n",
      "Loss gp: -3.441\n",
      "Loss gp: -3.448\n",
      "Loss gp: -3.455\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.475\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "31\n",
      "Loss design: -13.249\n",
      "tensor(1.1198, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4978],\n",
      "        [0.9979]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5023],\n",
      "        [1.0023]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1971, 0.1026]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.763\n",
      "Loss gp: 0.750\n",
      "Loss gp: 0.724\n",
      "Loss gp: 0.672\n",
      "Loss gp: 0.564\n",
      "Loss gp: 0.332\n",
      "Loss gp: -0.174\n",
      "Loss gp: -1.244\n",
      "Loss gp: -3.078\n",
      "Loss gp: -3.397\n",
      "Loss gp: -3.401\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.412\n",
      "Loss gp: -3.419\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.439\n",
      "Loss gp: -3.445\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.492\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "32\n",
      "Loss design: -12.826\n",
      "tensor(1.1198, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4976],\n",
      "        [0.9978]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5023],\n",
      "        [1.0023]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1971, 0.1026]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.780\n",
      "Loss gp: 0.767\n",
      "Loss gp: 0.755\n",
      "Loss gp: 0.729\n",
      "Loss gp: 0.677\n",
      "Loss gp: 0.569\n",
      "Loss gp: 0.337\n",
      "Loss gp: -0.167\n",
      "Loss gp: -1.234\n",
      "Loss gp: -3.031\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.387\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.418\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.433\n",
      "Loss gp: -3.440\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.463\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.505\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "33\n",
      "Loss design: -12.626\n",
      "tensor(1.1199, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4974],\n",
      "        [0.9977]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5023],\n",
      "        [1.0025]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1972, 0.1027]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.779\n",
      "Loss gp: 0.766\n",
      "Loss gp: 0.754\n",
      "Loss gp: 0.728\n",
      "Loss gp: 0.676\n",
      "Loss gp: 0.568\n",
      "Loss gp: 0.336\n",
      "Loss gp: -0.169\n",
      "Loss gp: -1.237\n",
      "Loss gp: -3.041\n",
      "Loss gp: -3.346\n",
      "Loss gp: -3.360\n",
      "Loss gp: -3.373\n",
      "Loss gp: -3.388\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.409\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.425\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.448\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.471\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.491\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.518\n",
      "Loss gp: -3.519\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "34\n",
      "Loss design: -12.858\n",
      "tensor(1.1199, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4976],\n",
      "        [0.9978]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5023],\n",
      "        [1.0024]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1972, 0.1026]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.779\n",
      "Loss gp: 0.767\n",
      "Loss gp: 0.754\n",
      "Loss gp: 0.728\n",
      "Loss gp: 0.676\n",
      "Loss gp: 0.568\n",
      "Loss gp: 0.336\n",
      "Loss gp: -0.169\n",
      "Loss gp: -1.237\n",
      "Loss gp: -3.037\n",
      "Loss gp: -3.330\n",
      "Loss gp: -3.345\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.387\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.410\n",
      "Loss gp: -3.419\n",
      "Loss gp: -3.427\n",
      "Loss gp: -3.434\n",
      "Loss gp: -3.440\n",
      "Loss gp: -3.447\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.486\n",
      "Loss gp: -3.491\n",
      "Loss gp: -3.496\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -3.514\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.518\n",
      "Loss gp: -3.519\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.523\n",
      "Loss gp: -3.523\n",
      "Loss gp: -3.523\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "35\n",
      "Loss design: -12.952\n",
      "tensor(1.1201, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4980],\n",
      "        [0.9982]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5025],\n",
      "        [1.0026]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1972, 0.1026]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.785\n",
      "Loss gp: 0.773\n",
      "Loss gp: 0.760\n",
      "Loss gp: 0.735\n",
      "Loss gp: 0.683\n",
      "Loss gp: 0.574\n",
      "Loss gp: 0.343\n",
      "Loss gp: -0.160\n",
      "Loss gp: -1.223\n",
      "Loss gp: -2.938\n",
      "Loss gp: -3.162\n",
      "Loss gp: -3.193\n",
      "Loss gp: -3.221\n",
      "Loss gp: -3.286\n",
      "Loss gp: -3.343\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.402\n",
      "Loss gp: -3.415\n",
      "Loss gp: -3.424\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.440\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.472\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.488\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.505\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.518\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.523\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "36\n",
      "Loss design: -13.122\n",
      "tensor(1.1193, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4975],\n",
      "        [0.9977]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5019],\n",
      "        [1.0020]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1970, 0.1022]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.781\n",
      "Loss gp: 0.768\n",
      "Loss gp: 0.756\n",
      "Loss gp: 0.730\n",
      "Loss gp: 0.678\n",
      "Loss gp: 0.569\n",
      "Loss gp: 0.337\n",
      "Loss gp: -0.168\n",
      "Loss gp: -1.236\n",
      "Loss gp: -2.990\n",
      "Loss gp: -3.237\n",
      "Loss gp: -3.266\n",
      "Loss gp: -3.290\n",
      "Loss gp: -3.321\n",
      "Loss gp: -3.347\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.403\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.427\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.463\n",
      "Loss gp: -3.471\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.486\n",
      "Loss gp: -3.492\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.518\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.523\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "37\n",
      "Loss design: -13.013\n",
      "tensor(1.1193, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4972],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5018],\n",
      "        [1.0020]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1970, 0.1022]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.776\n",
      "Loss gp: 0.764\n",
      "Loss gp: 0.751\n",
      "Loss gp: 0.726\n",
      "Loss gp: 0.673\n",
      "Loss gp: 0.565\n",
      "Loss gp: 0.332\n",
      "Loss gp: -0.174\n",
      "Loss gp: -1.247\n",
      "Loss gp: -3.054\n",
      "Loss gp: -3.327\n",
      "Loss gp: -3.338\n",
      "Loss gp: -3.348\n",
      "Loss gp: -3.362\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.386\n",
      "Loss gp: -3.397\n",
      "Loss gp: -3.407\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.436\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.518\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.534\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "38\n",
      "Loss design: -13.032\n",
      "tensor(1.1193, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4972],\n",
      "        [0.9976]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5019],\n",
      "        [1.0020]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1970, 0.1022]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.762\n",
      "Loss gp: 0.750\n",
      "Loss gp: 0.724\n",
      "Loss gp: 0.672\n",
      "Loss gp: 0.563\n",
      "Loss gp: 0.330\n",
      "Loss gp: -0.177\n",
      "Loss gp: -1.251\n",
      "Loss gp: -3.045\n",
      "Loss gp: -3.318\n",
      "Loss gp: -3.333\n",
      "Loss gp: -3.351\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.396\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.418\n",
      "Loss gp: -3.427\n",
      "Loss gp: -3.436\n",
      "Loss gp: -3.445\n",
      "Loss gp: -3.454\n",
      "Loss gp: -3.461\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.483\n",
      "Loss gp: -3.489\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.501\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.519\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "39\n",
      "Loss design: -13.137\n",
      "tensor(1.1198, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4977],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5022],\n",
      "        [1.0024]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1973, 0.1024]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.762\n",
      "Loss gp: 0.750\n",
      "Loss gp: 0.724\n",
      "Loss gp: 0.672\n",
      "Loss gp: 0.563\n",
      "Loss gp: 0.330\n",
      "Loss gp: -0.177\n",
      "Loss gp: -1.251\n",
      "Loss gp: -3.059\n",
      "Loss gp: -3.335\n",
      "Loss gp: -3.348\n",
      "Loss gp: -3.360\n",
      "Loss gp: -3.374\n",
      "Loss gp: -3.387\n",
      "Loss gp: -3.399\n",
      "Loss gp: -3.410\n",
      "Loss gp: -3.420\n",
      "Loss gp: -3.428\n",
      "Loss gp: -3.436\n",
      "Loss gp: -3.444\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.515\n",
      "Loss gp: -3.518\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.526\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -3.539\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "40\n",
      "Loss design: -13.226\n",
      "tensor(1.1197, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4979],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5022],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1973, 0.1024]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.778\n",
      "Loss gp: 0.765\n",
      "Loss gp: 0.753\n",
      "Loss gp: 0.727\n",
      "Loss gp: 0.675\n",
      "Loss gp: 0.566\n",
      "Loss gp: 0.333\n",
      "Loss gp: -0.174\n",
      "Loss gp: -1.246\n",
      "Loss gp: -2.979\n",
      "Loss gp: -3.212\n",
      "Loss gp: -3.248\n",
      "Loss gp: -3.279\n",
      "Loss gp: -3.335\n",
      "Loss gp: -3.378\n",
      "Loss gp: -3.401\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.430\n",
      "Loss gp: -3.441\n",
      "Loss gp: -3.450\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.471\n",
      "Loss gp: -3.477\n",
      "Loss gp: -3.482\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.492\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.523\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "41\n",
      "Loss design: -13.219\n",
      "tensor(1.1196, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4978],\n",
      "        [0.9980]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5021],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1973, 0.1023]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.763\n",
      "Loss gp: 0.750\n",
      "Loss gp: 0.725\n",
      "Loss gp: 0.672\n",
      "Loss gp: 0.563\n",
      "Loss gp: 0.330\n",
      "Loss gp: -0.177\n",
      "Loss gp: -1.252\n",
      "Loss gp: -3.038\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.312\n",
      "Loss gp: -3.329\n",
      "Loss gp: -3.350\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.411\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.441\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.459\n",
      "Loss gp: -3.466\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.484\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.497\n",
      "Loss gp: -3.503\n",
      "Loss gp: -3.510\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.519\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "42\n",
      "Loss design: -13.169\n",
      "tensor(1.1196, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4978],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5020],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1973, 0.1021]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.763\n",
      "Loss gp: 0.750\n",
      "Loss gp: 0.725\n",
      "Loss gp: 0.672\n",
      "Loss gp: 0.563\n",
      "Loss gp: 0.330\n",
      "Loss gp: -0.177\n",
      "Loss gp: -1.252\n",
      "Loss gp: -3.037\n",
      "Loss gp: -3.293\n",
      "Loss gp: -3.311\n",
      "Loss gp: -3.328\n",
      "Loss gp: -3.350\n",
      "Loss gp: -3.369\n",
      "Loss gp: -3.384\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.411\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.433\n",
      "Loss gp: -3.442\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.468\n",
      "Loss gp: -3.476\n",
      "Loss gp: -3.483\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.504\n",
      "Loss gp: -3.509\n",
      "Loss gp: -3.514\n",
      "Loss gp: -3.519\n",
      "Loss gp: -3.522\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "43\n",
      "Loss design: -13.257\n",
      "tensor(1.1198, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4980],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5023],\n",
      "        [1.0023]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1975, 0.1023]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.772\n",
      "Loss gp: 0.760\n",
      "Loss gp: 0.747\n",
      "Loss gp: 0.721\n",
      "Loss gp: 0.669\n",
      "Loss gp: 0.559\n",
      "Loss gp: 0.326\n",
      "Loss gp: -0.183\n",
      "Loss gp: -1.261\n",
      "Loss gp: -3.075\n",
      "Loss gp: -3.346\n",
      "Loss gp: -3.359\n",
      "Loss gp: -3.368\n",
      "Loss gp: -3.383\n",
      "Loss gp: -3.394\n",
      "Loss gp: -3.406\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.428\n",
      "Loss gp: -3.437\n",
      "Loss gp: -3.445\n",
      "Loss gp: -3.455\n",
      "Loss gp: -3.462\n",
      "Loss gp: -3.469\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.500\n",
      "Loss gp: -3.507\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "44\n",
      "Loss design: -13.308\n",
      "tensor(1.1196, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4979],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5021],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1975, 0.1021]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.771\n",
      "Loss gp: 0.758\n",
      "Loss gp: 0.746\n",
      "Loss gp: 0.720\n",
      "Loss gp: 0.667\n",
      "Loss gp: 0.558\n",
      "Loss gp: 0.324\n",
      "Loss gp: -0.185\n",
      "Loss gp: -1.264\n",
      "Loss gp: -3.087\n",
      "Loss gp: -3.371\n",
      "Loss gp: -3.382\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.405\n",
      "Loss gp: -3.416\n",
      "Loss gp: -3.426\n",
      "Loss gp: -3.435\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.451\n",
      "Loss gp: -3.458\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.471\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.499\n",
      "Loss gp: -3.505\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.517\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.537\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "45\n",
      "Loss design: -13.200\n",
      "tensor(1.1197, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4979],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5022],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1977, 0.1021]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.762\n",
      "Loss gp: 0.749\n",
      "Loss gp: 0.724\n",
      "Loss gp: 0.671\n",
      "Loss gp: 0.562\n",
      "Loss gp: 0.329\n",
      "Loss gp: -0.180\n",
      "Loss gp: -1.256\n",
      "Loss gp: -3.019\n",
      "Loss gp: -3.264\n",
      "Loss gp: -3.294\n",
      "Loss gp: -3.321\n",
      "Loss gp: -3.362\n",
      "Loss gp: -3.389\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.435\n",
      "Loss gp: -3.446\n",
      "Loss gp: -3.456\n",
      "Loss gp: -3.465\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.494\n",
      "Loss gp: -3.500\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.511\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.527\n",
      "Loss gp: -3.530\n",
      "Loss gp: -3.533\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.539\n",
      "Loss gp: -3.541\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "46\n",
      "Loss design: -13.104\n",
      "tensor(1.1196, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4978],\n",
      "        [0.9980]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5021],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1977, 0.1021]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.773\n",
      "Loss gp: 0.760\n",
      "Loss gp: 0.748\n",
      "Loss gp: 0.722\n",
      "Loss gp: 0.670\n",
      "Loss gp: 0.560\n",
      "Loss gp: 0.327\n",
      "Loss gp: -0.183\n",
      "Loss gp: -1.261\n",
      "Loss gp: -3.049\n",
      "Loss gp: -3.313\n",
      "Loss gp: -3.336\n",
      "Loss gp: -3.358\n",
      "Loss gp: -3.381\n",
      "Loss gp: -3.400\n",
      "Loss gp: -3.417\n",
      "Loss gp: -3.431\n",
      "Loss gp: -3.442\n",
      "Loss gp: -3.452\n",
      "Loss gp: -3.460\n",
      "Loss gp: -3.467\n",
      "Loss gp: -3.474\n",
      "Loss gp: -3.479\n",
      "Loss gp: -3.485\n",
      "Loss gp: -3.490\n",
      "Loss gp: -3.498\n",
      "Loss gp: -3.506\n",
      "Loss gp: -3.512\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.524\n",
      "Loss gp: -3.528\n",
      "Loss gp: -3.531\n",
      "Loss gp: -3.534\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "47\n",
      "Loss design: -13.154\n",
      "tensor(1.1196, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4978],\n",
      "        [0.9980]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5021],\n",
      "        [1.0021]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1977, 0.1021]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.770\n",
      "Loss gp: 0.758\n",
      "Loss gp: 0.745\n",
      "Loss gp: 0.719\n",
      "Loss gp: 0.667\n",
      "Loss gp: 0.557\n",
      "Loss gp: 0.323\n",
      "Loss gp: -0.187\n",
      "Loss gp: -1.267\n",
      "Loss gp: -3.089\n",
      "Loss gp: -3.363\n",
      "Loss gp: -3.375\n",
      "Loss gp: -3.385\n",
      "Loss gp: -3.398\n",
      "Loss gp: -3.411\n",
      "Loss gp: -3.422\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.441\n",
      "Loss gp: -3.450\n",
      "Loss gp: -3.457\n",
      "Loss gp: -3.464\n",
      "Loss gp: -3.473\n",
      "Loss gp: -3.480\n",
      "Loss gp: -3.487\n",
      "Loss gp: -3.495\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.513\n",
      "Loss gp: -3.520\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.535\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.546\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "48\n",
      "Loss design: -13.262\n",
      "tensor(1.1195, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4979],\n",
      "        [0.9982]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5020],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1976, 0.1020]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.772\n",
      "Loss gp: 0.759\n",
      "Loss gp: 0.746\n",
      "Loss gp: 0.721\n",
      "Loss gp: 0.668\n",
      "Loss gp: 0.559\n",
      "Loss gp: 0.325\n",
      "Loss gp: -0.185\n",
      "Loss gp: -1.265\n",
      "Loss gp: -3.053\n",
      "Loss gp: -3.299\n",
      "Loss gp: -3.317\n",
      "Loss gp: -3.334\n",
      "Loss gp: -3.356\n",
      "Loss gp: -3.375\n",
      "Loss gp: -3.392\n",
      "Loss gp: -3.408\n",
      "Loss gp: -3.421\n",
      "Loss gp: -3.432\n",
      "Loss gp: -3.443\n",
      "Loss gp: -3.453\n",
      "Loss gp: -3.462\n",
      "Loss gp: -3.470\n",
      "Loss gp: -3.478\n",
      "Loss gp: -3.486\n",
      "Loss gp: -3.493\n",
      "Loss gp: -3.502\n",
      "Loss gp: -3.508\n",
      "Loss gp: -3.516\n",
      "Loss gp: -3.521\n",
      "Loss gp: -3.525\n",
      "Loss gp: -3.529\n",
      "Loss gp: -3.532\n",
      "Loss gp: -3.536\n",
      "Loss gp: -3.538\n",
      "Loss gp: -3.540\n",
      "Loss gp: -3.542\n",
      "Loss gp: -3.543\n",
      "Loss gp: -3.544\n",
      "Loss gp: -3.545\n",
      "Loss gp: -3.547\n",
      "Loss gp: -3.548\n",
      "Loss gp: -3.549\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.550\n",
      "Loss gp: -3.551\n",
      "Loss gp: -3.552\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.553\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.554\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.555\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Loss gp: -3.556\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "49\n",
      "Loss design: -13.290\n",
      "tensor(1.1196, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4979],\n",
      "        [0.9981]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5022],\n",
      "        [1.0022]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4990],\n",
      "        [0.9990]])\n",
      "tensor([[0.5010],\n",
      "        [1.0010]])\n",
      "Parameter containing:\n",
      "tensor([[0.1977, 0.1019]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "iter_hp = 100\n",
    "iter_design = 40 \n",
    "iter_param = 50\n",
    "num_base_kernels = 3\n",
    "\n",
    "\n",
    "f_target = Tensor(vf.tgt_vec) \n",
    "f_target = f_target.reshape(f_target.shape[0],1) \n",
    "tol_vector = 0.001 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size = 10\n",
    "loc_sample = np.random.random_sample((loc_size,2))\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten()\n",
    "\n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([0.5,0.7])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x = Tensor(np.array([0.5,0.7])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "iter = 0 \n",
    "tol = 0.009 \n",
    "while(SUCCESS == False and iter < 50):\n",
    "    print('START HYPERPARAMETERS optimization')\n",
    "    model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels)\n",
    "\n",
    "    print('END HYPERPARAMETERS optimization')\n",
    "    print(iter)\n",
    "    \n",
    "    x0_new,g_theta2,lower_bound, upper_bound = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new)\n",
    "    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.flatten()], 0)\n",
    "    print(torch.norm(upper_bound - tol_vector))\n",
    "#     if (torch.norm(upper_bound - tol_vector) <= 0.1 ):\n",
    "#         print('bkhjghf')\n",
    "#         lr_new = lr_new * 0.1\n",
    "    print(lower_bound)\n",
    "    print(upper_bound)\n",
    "    print(f_target-tol_vector)\n",
    "    print(f_target+tol_vector)\n",
    "    loc_sample = np.random.random_sample((loc_size,2))\n",
    "    x0 = x0_new #Tensor(np.random.random_sample((1,2))) #x0_new\n",
    "    vec_x = torch.cat([vec_x, x0_new])\n",
    "    g_theta2_detach =  g_theta2.detach()\n",
    "    new_data = vfield_(g_theta2_detach)\n",
    "    agg_data = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "    g_theta1= torch.cat([g_theta1, g_theta2], 0)\n",
    "\n",
    "    SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    iter = iter + 1\n",
    "    print(x0_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50018048 1.00016152]\n",
      "tensor([[0.2000, 0.1002]])\n",
      "tensor([[0.4986, 0.9984]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "\n",
    "x0 = Tensor(np.array([0.2000, 0.1002]))\n",
    "print(vf(x0))\n",
    "x0 = x0.reshape(1,2)\n",
    "print(x0)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "pr = likelihood(model(x0))\n",
    "print(pr.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = Tensor(np.random.random_sample((50,2)))\n",
    "    predictions = likelihood(model(test_x))\n",
    "    mean = predictions.mean\n",
    "    lower, upper = predictions.confidence_region()\n",
    "    \n",
    "plt.plot(mean, 'bo')\n",
    "plt.plot(vfield_(test_x), 'r+')\n",
    "plt.plot(upper, 'gx')\n",
    "plt.plot(lower, 'mx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200])\n"
     ]
    }
   ],
   "source": [
    "iter\n",
    "print(agg_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "res = res + torch.ones(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1020])\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(g_theta2_vec.shape)\n",
    "print(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 = g_theta2_vec.reshape(19,2)\n",
    "# print(vec_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(v2)\n",
    "# np.savetxt('g_theta2_sameopti_keeping.txt', v2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.savetxt('vec_x_sameopti_keeping.txt', vec_x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class design_opti_pll(nn.Module):\n",
    "#     def __init__(self, x):\n",
    "#         super(design_opti_pll, self).__init__()\n",
    "#         #loc = np.random.random_sample((loc_size,2))\n",
    "#         #self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "#         self.x_design = nn.Parameter(Tensor(x))\n",
    "#     def forward(self):\n",
    "       \n",
    "#         #g_theta2_new = self.g_theta2 #filter_sample(self.g_theta2, 0.009)\n",
    "        \n",
    "#         return self.x_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conduct_design_pll(x0,f_target, g_theta, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new):\n",
    "#     design = design_opti_pll(x0)\n",
    "#     loc_sample0 = loc_sample\n",
    "#     x_d = design.forward()\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta, agg_data, model, likelihood)\n",
    "#         #loss2 = -1. * loss2\n",
    "#         loss2.backward(retain_graph=True)\n",
    "# #         print(x_d)\n",
    "# #         print(lower_bound)\n",
    "# #         print(upper_bound)\n",
    "       \n",
    "#         return loss2\n",
    "        \n",
    "        \n",
    "        \n",
    "#     optimizer = torch.optim.LBFGS(design.parameters(), lr=lr_new, history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "#     optimizer.step(closure)\n",
    "\n",
    "#     x_d = design.forward()\n",
    "#     loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta, agg_data, model, likelihood)\n",
    "#     #loss2 = -1. * loss2\n",
    "#     print('Loss design: %.3f' % ( loss2))\n",
    "#    # print(optimizer.state_dict())\n",
    "#     print(x_d)\n",
    "#     return x_d, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_hp = 100\n",
    "# iter_design = 40 \n",
    "# iter_param = 50\n",
    "# num_base_kernels = 5\n",
    "\n",
    "# f_target = Tensor(vf.tgt_vec) \n",
    "# f_target = f_target.reshape(f_target.shape[0],1) \n",
    "# tol_vector = 0.001 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "# loc_size = 200\n",
    "# loc_sample = np.random.random_sample((loc_size,2))\n",
    "# g_theta_ = (Tensor(loc_sample).clone())\n",
    "# agg_data1 = vfield_(g_theta_)\n",
    "# agg_data1 = agg_data1.flatten()\n",
    "\n",
    "\n",
    "# x0 = Tensor(np.array([0.5,0.7])) \n",
    "# x0 = x0.reshape(1,2)\n",
    "# x00 = x0 \n",
    "# vec_x = Tensor(np.array([0.5,0.7])) \n",
    "# vec_x = vec_x.reshape(1,2)\n",
    "\n",
    "# lr_new = 1.\n",
    "\n",
    "# SUCCESS = False \n",
    "# FAILURE = False \n",
    " \n",
    "# tol = 0.009 \n",
    "# print('START HYPERPARAMETERS optimization')\n",
    "# model, likelihood = hyper_opti(g_theta_,agg_data1,iter_hp,num_base_kernels)\n",
    "\n",
    "# print('END HYPERPARAMETERS optimization')\n",
    "\n",
    "# x0_new,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_, agg_data1, model, likelihood, iter_design, iter_param, lr_new)\n",
    "# print(lower_bound)\n",
    "# print(upper_bound)\n",
    "# print(f_target-tol_vector)\n",
    "# print(f_target+tol_vector)\n",
    "# loc_sample = np.random.random_sample((loc_size,2))\n",
    "\n",
    "\n",
    "# SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "# print(x0_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
