{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = VField()\n",
    "f_target = vf.tgt_vec\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "def g_theta(sample_size, D):\n",
    "    loc = np.random.random_sample((sample_size,D))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "train_y = torch.zeros([sample_size, N])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    train_y[i] = Tensor(vf(train_x[i])) + torch.randn(Tensor(vf(train_x[i])).size()) * 0.005\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = Tensor(vf(x[i])) + torch.randn(Tensor(vf(x[i])).size()) * 0.005\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_y)\n",
    "f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.plot(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criteria(tol_vector, f_target, lower_bound, upper_bound):\n",
    "    lower_tol_vector = f_target - tol_vector\n",
    "    upper_tol_vector = f_target + tol_vector\n",
    "    SUCCESS = True\n",
    "    FAILURE = False\n",
    "    for i in range(f_target.shape[0]):\n",
    "            if (lower_bound[i] < lower_tol_vector[i]) or  (upper_bound[i] > upper_tol_vector[i]):\n",
    "                SUCCESS = False  \n",
    "            if ((lower_bound[i] > upper_tol_vector[i]) or  (upper_bound[i] < lower_tol_vector[i])):\n",
    "                FAILURE = True\n",
    "    return SUCCESS, FAILURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        \n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "#         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "        self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    for i in range(training_iter):\n",
    "#         for param_name, param in model.named_parameters():\n",
    "#             print(param_name)\n",
    "#             print(param)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "   #     output_ll = likelihood(output)\n",
    "        #print(torch.flatten(model.mean_module.forward(g_theta1)))\n",
    "        loss = -mll(output, agg_data)\n",
    "        #loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        print('Iter %d/%d - Loss hyperparam: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sample(x,tol):\n",
    "    index = range(x.shape[0])\n",
    "       \n",
    "    index_del = []\n",
    "    check = False\n",
    "    for ii in range (x.shape[0]):\n",
    "#         if (x[ii,0] < 0 or x[ii,1] <0):\n",
    "#             check = True\n",
    "#             index_del.append(ii)\n",
    "        for jj in range(ii+1, x.shape[0]):\n",
    "            if (torch.norm(x[ii] - x[jj])) <= tol:\n",
    "                check = True\n",
    "                index_del.append(jj)\n",
    "    if check == True :\n",
    "        index_del = (np.unique(index_del,))\n",
    "        index_del = np.array((index_del), dtype = int)\n",
    "                \n",
    "        index_ = np.delete(index, index_del)\n",
    "        index_ = np.array(index_)\n",
    "        x_new = torch.zeros(index_.shape[0], x.shape[1])\n",
    "        jj = 0\n",
    "        for ii in index_:\n",
    "                    \n",
    "            x_new[jj] = x[ii]\n",
    "            jj = jj + 1\n",
    "    else:\n",
    "        index_ = index\n",
    "        x_new = x\n",
    "    return x_new, check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dist(y1, y2,tol):\n",
    "    index = range(y2.shape[0])\n",
    "       \n",
    "    index_del = []\n",
    "    check = False\n",
    "    for ii in range (y1.shape[0]):\n",
    "#         if (x[ii,0] < 0 or x[ii,1] <0):\n",
    "#             check = True\n",
    "#             index_del.append(ii)\n",
    "        for jj in range(y2.shape[0]):\n",
    "            if (torch.norm(y1[ii] - y2[jj])) <= tol:\n",
    "                check = True\n",
    "                index_del.append(jj)\n",
    "    if check == True :\n",
    "        index_del = (np.unique(index_del,))\n",
    "        index_del = np.array((index_del), dtype = int)\n",
    "                \n",
    "        index_ = np.delete(index, index_del)\n",
    "        index_ = np.array(index_)\n",
    "        y2_new = torch.zeros(index_.shape[0], y2.shape[1])\n",
    "        jj = 0\n",
    "        for ii in index_:\n",
    "                    \n",
    "            y2_new[jj] = y2[ii]\n",
    "            jj = jj + 1\n",
    "    else:\n",
    "        index_ = index\n",
    "        y2_new = y2\n",
    "    return y2_new, check\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class param_opti(nn.Module):\n",
    "    def __init__(self, sample, x):\n",
    "        super(param_opti, self).__init__()\n",
    "        #loc = np.random.random_sample((loc_size,2))\n",
    "        self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "        self.x_design = nn.Parameter(Tensor(x))\n",
    "    def forward(self):\n",
    "       \n",
    "        g_theta2_new = self.g_theta2 #filter_sample(self.g_theta2, 0.009)\n",
    "        \n",
    "        return (g_theta2_new), self.x_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conduct_param_opti(x0,loc_sample0, f_target,g_theta1, agg_data, model, likelihood, training_iter, tol_value):\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "#     _par = param_opti(loc_sample0,x0)\n",
    "#     _lr = 0.1\n",
    "#     optimizer = torch.optim.Adam(_par.parameters(), _lr)\n",
    "#     tol_vector = tol_value * torch.ones(f_target.shape)\n",
    "#     j  = 0\n",
    "#     SUCCESS = False\n",
    "#     while (j <= (training_iter) and SUCCESS == False):\n",
    "#         optimizer.zero_grad()\n",
    "#         g_theta2,x = _par.forward()\n",
    "#         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "#         loss1 = -1 * loss1\n",
    "#         loss1.backward(retain_graph=True)\n",
    "#         print('Iter %d/%d - Loss theta2: %.3f' % (j + 1, training_iter, loss1.item()))\n",
    "#         optimizer.step()\n",
    "#         j = j+1\n",
    "#         SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "# #         g_theta_new, check = filter_sample(g_theta2, 0.009)\n",
    "# #         if (check == True):\n",
    "# #             print('blaaa')\n",
    "# #             _lr = _lr * 0.7\n",
    "# #             optimizer = torch.optim.Adam(_par.parameters(), _lr)\n",
    "            \n",
    "#     return g_theta2, x, lower_bound, upper_bound, SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conduct_param_opti(x0,loc_sample0, f_target,g_theta1, agg_data, model, likelihood, training_iter, tol_value):\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "#     _par = param_opti(loc_sample0,x0)\n",
    "#     def closure():\n",
    "#         g_theta2,x = _par.forward()\n",
    "#         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "#         loss1 = -1 * loss1\n",
    "#         loss1.backward(retain_graph=True)\n",
    "        \n",
    "#         return loss1\n",
    "#     optimizer = torch.optim.LBFGS(_par.parameters(), lr=0.0009, history_size=10, max_iter=10, line_search_fn=\"strong_wolfe\")\n",
    "#     j  = 0\n",
    "#     SUCCESS = False\n",
    "#     tol_vector = tol_value * torch.ones(f_target.shape)\n",
    "#     while (j <= (training_iter) and SUCCESS == False):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         g_theta2,x = _par.forward()\n",
    "#         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "#         loss1 = -1 * loss1\n",
    "        \n",
    "#         print('Iter %d/%d - Loss theta2: %.3f' % (j + 1, training_iter, loss1.item()))\n",
    "#         optimizer.step(closure)\n",
    "#         j = j+1\n",
    "#         SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "#     return g_theta2, x, lower_bound, upper_bound, SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_param_opti(x0,loc_sample0, f_target,g_theta1, agg_data, model, likelihood, training_iter, tol_value):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    _par = param_opti(loc_sample0,x0)\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        g_theta2,x = _par.forward()\n",
    "        loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "        loss1 = -1 * loss1\n",
    "        loss1.backward(retain_graph=True)\n",
    "        \n",
    "        return loss1\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(_par.parameters(), lr=0.0009, history_size=10, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    tol_vector = tol_value * torch.ones(f_target.shape)\n",
    "    g_theta2,x = _par.forward()\n",
    "    loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "    SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    \n",
    "    print('Loss design: %.3f' % ( -1. * loss1))\n",
    "    return g_theta2, x, lower_bound, upper_bound, SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_hp = 50\n",
    "iter_design = 50\n",
    "iter_param = 10\n",
    "\n",
    "\n",
    "f_target = Tensor(vf.tgt_vec)\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "tol_value = 0.005 # * torch.ones(f_target.shape)\n",
    "tol_vector = tol_value * torch.ones(f_target.shape)\n",
    "\n",
    "loc_size = 10\n",
    "loc_sample = Tensor(np.random.random_sample((loc_size,D)))\n",
    "\n",
    "\n",
    "\n",
    "g_theta1 = x_train\n",
    "g_theta1_tilde = x_train\n",
    "agg_data = y_train.flatten()\n",
    "agg_data_tilde = agg_data\n",
    "# agg_data.append(y_train.flatten())\n",
    "# agg_data = torch.cat(agg_data)\n",
    "\n",
    "x0 = Tensor(np.array([0.25, 0.17]))\n",
    "x0 = x0.reshape(1, D)\n",
    "x00 = x0\n",
    "vec_x = x00\n",
    "SUCCESS = False\n",
    "FAILURE = False\n",
    "iter = 0\n",
    "tol = 0.009\n",
    "\n",
    "while(SUCCESS == False and iter < 100):\n",
    "    \n",
    "    \n",
    "    print('START HYPERPARAMETERS optimization')\n",
    "    model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    g_theta2,x0_new,lower_bound, upper_bound, SUCCESS = conduct_param_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,tol_value)\n",
    "    print(lower_bound)\n",
    "    print(upper_bound)\n",
    "    print(f_target-tol_vector)\n",
    "    print(f_target+tol_vector)\n",
    "    loc_sample = np.random.random_sample((loc_size,D)) #g_theta2 #\n",
    "\n",
    "    x0 = x0_new #Tensor(np.random.random_sample((1,D)))\n",
    "    vec_x = torch.cat([vec_x, x0_new])\n",
    "    print(x0_new)\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "#     pred = likelihood(model(g_theta2))\n",
    "    #1y_train_new = (pred.mean)\n",
    "    #agg_data.append(y_train_new.flatten())\n",
    "    g_theta2_detach = g_theta2.detach()\n",
    "    new_data = vfield_(g_theta2_detach)\n",
    "    agg_data = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "    g_theta1= torch.cat([g_theta1, g_theta2_detach], 0)\n",
    "#     g_theta1, check = filter_sample(g_theta1,tol)\n",
    "\n",
    "    \n",
    "#    # g_theta1_tilde= torch.cat([g_theta1, g_theta2_detach], 0) #filter_sample(g_theta1,tol)\n",
    "    \n",
    "# #     g_theta1, check = check_dist(x0, g_theta1, 0.1 * tol)\n",
    "# # #     if (check == True):\n",
    "# # #         print('kjhg')\n",
    "# # #         g_theta1, check2 = filter_sample(g_theta1,tol)\n",
    "  \n",
    "\n",
    "\n",
    "#     y_train_new = vfield_(g_theta1)  ##3improve this\n",
    "#     agg_data = y_train_new.flatten()\n",
    " #   loc_sample, check = check_dist(g_theta1, Tensor(loc_sample), tol)\n",
    "# # #    print(loc_sample)\n",
    "#     loc_sample, check = check_dist(x0, Tensor(loc_sample), tol)\n",
    "    SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    iter = iter + 1\n",
    "\n",
    "print(x0_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x0 = Tensor(np.array([0.1998, 0.1004]))\n",
    "#\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([0.2026, 0.0928]))\n",
    "print(vf(x0))\n",
    "x0 = x0.reshape(1,2)\n",
    "print(x0)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "pr = likelihood(model(x0))\n",
    "print(pr.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conduct_param_opti(x0,loc_sample0, f_target,g_theta1, agg_data, model, likelihood, training_iter):\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "#     _par = param_opti(loc_sample0,x0)\n",
    "#     def closure():\n",
    "#         g_theta2,x = _par.forward()\n",
    "#         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "#         loss1 = -1 * loss1\n",
    "#         loss1.backward(retain_graph=True)\n",
    "        \n",
    "#         return loss1\n",
    "    \n",
    "#     #optimizer = torch.optim.Adam(_par.parameters(), lr=0.0006)\n",
    "#     optimizer = torch.optim.LBFGS(_par.parameters(), lr=0.06, history_size=10, max_iter=4, line_search_fn=\"strong_wolfe\")\n",
    "    \n",
    "#     for j in range(training_iter):\n",
    "#         optimizer.zero_grad()\n",
    "#         g_theta2,x = _par.forward()\n",
    "#         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "#         loss1 = -1 * loss1\n",
    "        \n",
    "#         print('Iter %d/%d - Loss theta2: %.3f' % (j + 1, training_iter, loss1.item()))\n",
    "#         optimizer.step(closure)\n",
    "          \n",
    "#     return g_theta2, x, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range (train_x.shape[0]):\n",
    "    for jj in range(ii+1, train_x.shape[0]):\n",
    "        if (torch.norm(train_x[ii] - train_x[jj])) <= 0.009:\n",
    "            print(ii)\n",
    "            print(jj)\n",
    "            print('blaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvk_rbf.vvkRBFKernel().forward(g_theta2, g_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sample(g_theta1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_theta1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
