{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = VField()\n",
    "f_target = vf.tgt_vec\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "def g_theta(sample_size, D):\n",
    "    loc = np.random.random_sample((sample_size,D))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "\n",
    "\n",
    "\n",
    "sample_x = torch.linspace(0, 1, 300)\n",
    "k = 100\n",
    "N = sample_x.shape[0]\n",
    "indices = torch.randperm(N)[:k]\n",
    "train_x = sample_x[indices]\n",
    "\n",
    "\n",
    "train_y = torch.stack([\n",
    "    torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.0001,\n",
    "    torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.0001,\n",
    "], -1)   #torch.zeros([sample_size, N])\n",
    "\n",
    "train_y = train_y.flatten()\n",
    "\n",
    "\n",
    "# for i in range(sample_size):\n",
    "#     train_y[i] = Tensor(vf(train_x[i])) + torch.randn(Tensor(vf(train_x[i])).size()) * 0.005\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfield_(x):\n",
    "    out = torch.stack([\n",
    "    torch.sin(x * (2 * math.pi)) + torch.randn(x.size()) * 0.0001,\n",
    "    torch.cos(x * (2 * math.pi)) + torch.randn(x.size()) * 0.0001,\n",
    "], -1)\n",
    "    \n",
    "    return out.flatten()\n",
    "#     x = x.reshape(x.shape[0],D)\n",
    "#     out = torch.zeros(x.shape[0], N)\n",
    "#     for i in range(x.shape[0]):\n",
    "#         out[i] = Tensor(vf(x[i]))+ torch.randn(Tensor(vf(x[i])).size()) * 0.005\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.2900e-01, -5.5947e-01, -1.1545e-01, -9.9345e-01, -8.9731e-01,\n",
      "         4.4134e-01,  1.7779e-01, -9.8419e-01,  5.4618e-01, -8.3758e-01,\n",
      "        -3.3015e-01,  9.4407e-01, -9.8116e-01, -1.9314e-01, -9.9695e-01,\n",
      "         7.8665e-02,  9.9816e-01,  5.7773e-02,  9.1479e-05,  1.0001e+00,\n",
      "        -5.1053e-01, -8.5975e-01, -9.6146e-01, -2.7475e-01,  7.3792e-01,\n",
      "        -6.7492e-01, -6.3926e-01,  7.6899e-01, -9.9921e-01,  3.6828e-02,\n",
      "        -6.0629e-01,  7.9512e-01, -5.1963e-01,  8.5450e-01, -1.5278e-05,\n",
      "         1.0001e+00,  6.9395e-01, -7.1992e-01,  6.8655e-01,  7.2728e-01,\n",
      "         3.2993e-01,  9.4380e-01,  4.2720e-01,  9.0415e-01,  9.2345e-01,\n",
      "         3.8389e-01, -9.0652e-01,  4.2236e-01, -4.8305e-01,  8.7540e-01,\n",
      "        -7.2355e-01, -6.9018e-01, -5.7254e-01,  8.2012e-01,  3.6937e-01,\n",
      "         9.2928e-01, -9.5225e-01,  3.0488e-01, -3.5939e-01, -9.3316e-01,\n",
      "        -7.7905e-01, -6.2702e-01,  5.9794e-01, -8.0173e-01,  8.7805e-01,\n",
      "         4.7852e-01,  4.5538e-01, -8.9024e-01, -9.7445e-01,  2.2394e-01,\n",
      "        -6.1453e-01, -7.8889e-01, -1.6715e-01,  9.8591e-01,  1.0506e-01,\n",
      "         9.9450e-01,  1.6726e-01,  9.8615e-01, -6.8647e-01,  7.2722e-01,\n",
      "        -4.1761e-01, -9.0874e-01, -9.2324e-01,  3.8400e-01,  9.7443e-01,\n",
      "         2.2393e-01, -6.6303e-01, -7.4827e-01, -3.1491e-02, -9.9947e-01,\n",
      "         9.5237e-01,  3.0484e-01,  8.8285e-01, -4.6937e-01,  1.0437e-02,\n",
      "        -1.0000e+00, -2.2906e-01,  9.7327e-01, -9.9995e-01,  1.5916e-02,\n",
      "         7.3081e-01,  6.8260e-01,  7.1655e-01,  6.9778e-01, -1.0462e-02,\n",
      "        -9.9995e-01, -9.4584e-01,  3.2487e-01, -9.9965e-01, -2.6096e-02,\n",
      "         9.7217e-01, -2.3436e-01,  2.3928e-01, -9.7096e-01,  1.4653e-01,\n",
      "         9.8928e-01, -5.0138e-01,  8.6523e-01,  8.1079e-01,  5.8522e-01,\n",
      "         7.7912e-01, -6.2699e-01,  9.8865e-01, -1.5181e-01,  3.2006e-01,\n",
      "        -9.4726e-01,  9.4898e-01, -3.1500e-01, -4.0805e-01,  9.1295e-01,\n",
      "         9.6707e-01, -2.5453e-01,  2.2913e-01,  9.7324e-01,  6.1478e-01,\n",
      "        -7.8868e-01, -9.9766e-01, -6.8206e-02,  5.5505e-01,  8.3162e-01,\n",
      "         6.3111e-01, -7.7584e-01,  6.7873e-01, -7.3445e-01,  9.1917e-01,\n",
      "        -3.9343e-01, -5.8099e-01, -8.1401e-01, -9.6442e-01,  2.6471e-01,\n",
      "        -9.8678e-01,  1.6235e-01,  8.3720e-02,  9.9662e-01, -8.6239e-01,\n",
      "        -5.0602e-01,  4.8328e-01,  8.7542e-01, -9.9390e-01, -1.1025e-01,\n",
      "        -2.3925e-01, -9.7116e-01,  3.5968e-01, -9.3312e-01,  1.5682e-01,\n",
      "        -9.8762e-01, -9.0178e-01, -4.3171e-01,  9.4228e-01, -3.3489e-01,\n",
      "         6.4725e-01, -7.6239e-01,  5.1071e-01, -8.5966e-01,  6.2294e-01,\n",
      "         7.8233e-01, -6.3125e-01, -7.7566e-01,  8.2298e-01,  5.6813e-01,\n",
      "        -3.4972e-01,  9.3671e-01, -6.2989e-02,  9.9806e-01,  9.9613e-01,\n",
      "        -8.9393e-02, -4.5536e-01, -8.9034e-01, -2.0847e-01,  9.7775e-01,\n",
      "         8.3455e-01,  5.5059e-01, -9.8993e-01,  1.4128e-01,  7.5191e-01,\n",
      "        -6.5932e-01,  3.4959e-01,  9.3669e-01,  5.0166e-01,  8.6512e-01])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe572233610>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_y)\n",
    "f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.plot(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criteria(tol_vector, f_target, lower_bound, upper_bound):\n",
    "    lower_tol_vector = f_target - tol_vector\n",
    "    upper_tol_vector = f_target + tol_vector\n",
    "    SUCCESS = True\n",
    "    FAILURE = False\n",
    "    for i in range(f_target.shape[0]):\n",
    "            if (lower_bound[i] < lower_tol_vector[i]) or  (upper_bound[i] > upper_tol_vector[i]):\n",
    "                SUCCESS = False  \n",
    "            if ((lower_bound[i] > upper_tol_vector[i]) or  (upper_bound[i] < lower_tol_vector[i])):\n",
    "                FAILURE = True\n",
    "    return SUCCESS, FAILURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "#         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "        self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "        output_ll = likelihood(output)\n",
    "        #print(torch.flatten(model.mean_module.forward(g_theta1)))\n",
    "        loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        print('Iter %d/%d - Loss hyperparam: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class param_opti(nn.Module):\n",
    "    def __init__(self, sample, x):\n",
    "        super(param_opti, self).__init__()\n",
    "        #loc = np.random.random_sample((loc_size,2))\n",
    "        self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "        self.x_design = nn.Parameter(Tensor(x))\n",
    "    def forward(self):\n",
    "       \n",
    "        index = range(self.g_theta2.shape[0])\n",
    "       \n",
    "        index_del = []\n",
    "        check = False\n",
    "        for ii in range (self.g_theta2.shape[0]):\n",
    "            for jj in range(ii+1, self.g_theta2.shape[0]):\n",
    "                if (torch.norm(self.g_theta2[ii] - self.g_theta2[jj])) <= 0.009:\n",
    "                    check = True\n",
    "                    index_del.append(jj)\n",
    "        if check == True :\n",
    "            index_del = (np.unique(index_del,))\n",
    "            index_del = np.array((index_del), dtype = int)\n",
    "                \n",
    "            index_ = np.delete(index, index_del)\n",
    "            index_ = np.array(index_)\n",
    "            g_theta2_new = torch.zeros(index_.shape[0], self.g_theta2.shape[1])\n",
    "            jj = 0\n",
    "            for ii in index_:\n",
    "                    \n",
    "                g_theta2_new[jj] = self.g_theta2[ii]\n",
    "                jj = jj + 1\n",
    "        else:\n",
    "            g_theta2_new = self.g_theta2\n",
    "            \n",
    "        return (g_theta2_new), self.x_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_param_opti(x0,loc_sample0, f_target,g_theta1, agg_data, model, likelihood, training_iter):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    _par = param_opti(loc_sample0,x0)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(_par.parameters(), lr=0.0006)\n",
    "    \n",
    "    for j in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        g_theta2,x = _par.forward()\n",
    "        loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "        loss1 = -1 * loss1\n",
    "        loss1.backward(retain_graph=True)\n",
    "        print('Iter %d/%d - Loss theta2: %.3f' % (j + 1, training_iter, loss1.item()))\n",
    "        optimizer.step()\n",
    "          \n",
    "    return g_theta2, x, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 72.566\n",
      "Iter 2/50 - Loss hyperparam: 62.048\n",
      "Iter 3/50 - Loss hyperparam: 50.914\n",
      "Iter 4/50 - Loss hyperparam: 38.996\n",
      "Iter 5/50 - Loss hyperparam: 26.435\n",
      "Iter 6/50 - Loss hyperparam: 13.648\n",
      "Iter 7/50 - Loss hyperparam: 0.924\n",
      "Iter 8/50 - Loss hyperparam: -11.883\n",
      "Iter 9/50 - Loss hyperparam: -25.093\n",
      "Iter 10/50 - Loss hyperparam: -38.958\n",
      "Iter 11/50 - Loss hyperparam: -53.622\n",
      "Iter 12/50 - Loss hyperparam: -69.042\n",
      "Iter 13/50 - Loss hyperparam: -84.916\n",
      "Iter 14/50 - Loss hyperparam: -100.943\n",
      "Iter 15/50 - Loss hyperparam: -117.287\n",
      "Iter 16/50 - Loss hyperparam: -133.225\n",
      "Iter 17/50 - Loss hyperparam: -146.091\n",
      "Iter 18/50 - Loss hyperparam: -153.387\n",
      "Iter 19/50 - Loss hyperparam: -157.011\n",
      "Iter 20/50 - Loss hyperparam: -161.199\n",
      "Iter 21/50 - Loss hyperparam: -167.787\n",
      "Iter 22/50 - Loss hyperparam: -176.859\n",
      "Iter 23/50 - Loss hyperparam: -188.085\n",
      "Iter 24/50 - Loss hyperparam: -201.513\n",
      "Iter 25/50 - Loss hyperparam: -217.310\n",
      "Iter 26/50 - Loss hyperparam: -233.885\n",
      "Iter 27/50 - Loss hyperparam: -247.114\n",
      "Iter 28/50 - Loss hyperparam: -255.266\n",
      "Iter 29/50 - Loss hyperparam: -260.850\n",
      "Iter 30/50 - Loss hyperparam: -267.044\n",
      "Iter 31/50 - Loss hyperparam: -276.907\n",
      "Iter 32/50 - Loss hyperparam: -291.431\n",
      "Iter 33/50 - Loss hyperparam: -307.476\n",
      "Iter 34/50 - Loss hyperparam: -320.845\n",
      "Iter 35/50 - Loss hyperparam: -328.132\n",
      "Iter 36/50 - Loss hyperparam: -333.089\n",
      "Iter 37/50 - Loss hyperparam: -343.475\n",
      "Iter 38/50 - Loss hyperparam: -358.181\n",
      "Iter 39/50 - Loss hyperparam: -373.637\n",
      "Iter 40/50 - Loss hyperparam: -380.776\n",
      "Iter 41/50 - Loss hyperparam: -387.271\n",
      "Iter 42/50 - Loss hyperparam: -399.188\n",
      "Iter 43/50 - Loss hyperparam: -415.083\n",
      "Iter 44/50 - Loss hyperparam: -423.659\n",
      "Iter 45/50 - Loss hyperparam: -429.599\n",
      "Iter 46/50 - Loss hyperparam: -443.826\n",
      "Iter 47/50 - Loss hyperparam: -457.073\n",
      "Iter 48/50 - Loss hyperparam: -462.373\n",
      "Iter 49/50 - Loss hyperparam: -474.489\n",
      "Iter 50/50 - Loss hyperparam: -485.724\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.035\n",
      "Iter 2/100 - Loss theta2: -7.048\n",
      "Iter 3/100 - Loss theta2: -7.039\n",
      "Iter 4/100 - Loss theta2: -7.042\n",
      "Iter 5/100 - Loss theta2: -7.048\n",
      "Iter 6/100 - Loss theta2: -7.048\n",
      "Iter 7/100 - Loss theta2: -7.045\n",
      "Iter 8/100 - Loss theta2: -7.044\n",
      "Iter 9/100 - Loss theta2: -7.047\n",
      "Iter 10/100 - Loss theta2: -7.049\n",
      "Iter 11/100 - Loss theta2: -7.048\n",
      "Iter 12/100 - Loss theta2: -7.047\n",
      "Iter 13/100 - Loss theta2: -7.046\n",
      "Iter 14/100 - Loss theta2: -7.047\n",
      "Iter 15/100 - Loss theta2: -7.048\n",
      "Iter 16/100 - Loss theta2: -7.049\n",
      "Iter 17/100 - Loss theta2: -7.048\n",
      "Iter 18/100 - Loss theta2: -7.047\n",
      "Iter 19/100 - Loss theta2: -7.048\n",
      "Iter 20/100 - Loss theta2: -7.049\n",
      "Iter 21/100 - Loss theta2: -7.048\n",
      "Iter 22/100 - Loss theta2: -7.048\n",
      "Iter 23/100 - Loss theta2: -7.048\n",
      "Iter 24/100 - Loss theta2: -7.049\n",
      "Iter 25/100 - Loss theta2: -7.048\n",
      "Iter 26/100 - Loss theta2: -7.048\n",
      "Iter 27/100 - Loss theta2: -7.049\n",
      "Iter 28/100 - Loss theta2: -7.048\n",
      "Iter 29/100 - Loss theta2: -7.048\n",
      "Iter 30/100 - Loss theta2: -7.048\n",
      "Iter 31/100 - Loss theta2: -7.049\n",
      "Iter 32/100 - Loss theta2: -7.049\n",
      "Iter 33/100 - Loss theta2: -7.048\n",
      "Iter 34/100 - Loss theta2: -7.048\n",
      "Iter 35/100 - Loss theta2: -7.048\n",
      "Iter 36/100 - Loss theta2: -7.048\n",
      "Iter 37/100 - Loss theta2: -7.049\n",
      "Iter 38/100 - Loss theta2: -7.049\n",
      "Iter 39/100 - Loss theta2: -7.048\n",
      "Iter 40/100 - Loss theta2: -7.048\n",
      "Iter 41/100 - Loss theta2: -7.049\n",
      "Iter 42/100 - Loss theta2: -7.049\n",
      "Iter 43/100 - Loss theta2: -7.049\n",
      "Iter 44/100 - Loss theta2: -7.048\n",
      "Iter 45/100 - Loss theta2: -7.049\n",
      "Iter 46/100 - Loss theta2: -7.048\n",
      "Iter 47/100 - Loss theta2: -7.048\n",
      "Iter 48/100 - Loss theta2: -7.049\n",
      "Iter 49/100 - Loss theta2: -7.049\n",
      "Iter 50/100 - Loss theta2: -7.049\n",
      "Iter 51/100 - Loss theta2: -7.049\n",
      "Iter 52/100 - Loss theta2: -7.048\n",
      "Iter 53/100 - Loss theta2: -7.049\n",
      "Iter 54/100 - Loss theta2: -7.049\n",
      "Iter 55/100 - Loss theta2: -7.049\n",
      "Iter 56/100 - Loss theta2: -7.049\n",
      "Iter 57/100 - Loss theta2: -7.048\n",
      "Iter 58/100 - Loss theta2: -7.048\n",
      "Iter 59/100 - Loss theta2: -7.049\n",
      "Iter 60/100 - Loss theta2: -7.049\n",
      "Iter 61/100 - Loss theta2: -7.049\n",
      "Iter 62/100 - Loss theta2: -7.050\n",
      "Iter 63/100 - Loss theta2: -7.049\n",
      "Iter 64/100 - Loss theta2: -7.048\n",
      "Iter 65/100 - Loss theta2: -7.049\n",
      "Iter 66/100 - Loss theta2: -7.049\n",
      "Iter 67/100 - Loss theta2: -7.048\n",
      "Iter 68/100 - Loss theta2: -7.049\n",
      "Iter 69/100 - Loss theta2: -7.049\n",
      "Iter 70/100 - Loss theta2: -7.049\n",
      "Iter 71/100 - Loss theta2: -7.048\n",
      "Iter 72/100 - Loss theta2: -7.049\n",
      "Iter 73/100 - Loss theta2: -7.049\n",
      "Iter 74/100 - Loss theta2: -7.049\n",
      "Iter 75/100 - Loss theta2: -7.049\n",
      "Iter 76/100 - Loss theta2: -7.049\n",
      "Iter 77/100 - Loss theta2: -7.049\n",
      "Iter 78/100 - Loss theta2: -7.049\n",
      "Iter 79/100 - Loss theta2: -7.049\n",
      "Iter 80/100 - Loss theta2: -7.048\n",
      "Iter 81/100 - Loss theta2: -7.048\n",
      "Iter 82/100 - Loss theta2: -7.049\n",
      "Iter 83/100 - Loss theta2: -7.049\n",
      "Iter 84/100 - Loss theta2: -7.048\n",
      "Iter 85/100 - Loss theta2: -7.048\n",
      "Iter 86/100 - Loss theta2: -7.048\n",
      "Iter 87/100 - Loss theta2: -7.048\n",
      "Iter 88/100 - Loss theta2: -7.049\n",
      "Iter 89/100 - Loss theta2: -7.049\n",
      "Iter 90/100 - Loss theta2: -7.049\n",
      "Iter 91/100 - Loss theta2: -7.049\n",
      "Iter 92/100 - Loss theta2: -7.049\n",
      "Iter 93/100 - Loss theta2: -7.049\n",
      "Iter 94/100 - Loss theta2: -7.048\n",
      "Iter 95/100 - Loss theta2: -7.049\n",
      "Iter 96/100 - Loss theta2: -7.048\n",
      "Iter 97/100 - Loss theta2: -7.048\n",
      "Iter 98/100 - Loss theta2: -7.049\n",
      "Iter 99/100 - Loss theta2: -7.049\n",
      "Iter 100/100 - Loss theta2: -7.049\n",
      "tensor([[0.6855],\n",
      "        [0.6855]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7201],\n",
      "        [0.7202]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1245]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 16.456\n",
      "Iter 2/50 - Loss hyperparam: 2.376\n",
      "Iter 3/50 - Loss hyperparam: -11.507\n",
      "Iter 4/50 - Loss hyperparam: -25.292\n",
      "Iter 5/50 - Loss hyperparam: -38.907\n",
      "Iter 6/50 - Loss hyperparam: -51.460\n",
      "Iter 7/50 - Loss hyperparam: -62.222\n",
      "Iter 8/50 - Loss hyperparam: -71.285\n",
      "Iter 9/50 - Loss hyperparam: -79.132\n",
      "Iter 10/50 - Loss hyperparam: -86.661\n",
      "Iter 11/50 - Loss hyperparam: -94.859\n",
      "Iter 12/50 - Loss hyperparam: -104.088\n",
      "Iter 13/50 - Loss hyperparam: -114.016\n",
      "Iter 14/50 - Loss hyperparam: -124.244\n",
      "Iter 15/50 - Loss hyperparam: -134.418\n",
      "Iter 16/50 - Loss hyperparam: -143.832\n",
      "Iter 17/50 - Loss hyperparam: -152.428\n",
      "Iter 18/50 - Loss hyperparam: -161.149\n",
      "Iter 19/50 - Loss hyperparam: -170.669\n",
      "Iter 20/50 - Loss hyperparam: -181.418\n",
      "Iter 21/50 - Loss hyperparam: -193.168\n",
      "Iter 22/50 - Loss hyperparam: -204.772\n",
      "Iter 23/50 - Loss hyperparam: -215.549\n",
      "Iter 24/50 - Loss hyperparam: -225.211\n",
      "Iter 25/50 - Loss hyperparam: -235.155\n",
      "Iter 26/50 - Loss hyperparam: -246.086\n",
      "Iter 27/50 - Loss hyperparam: -257.727\n",
      "Iter 28/50 - Loss hyperparam: -268.389\n",
      "Iter 29/50 - Loss hyperparam: -277.766\n",
      "Iter 30/50 - Loss hyperparam: -287.329\n",
      "Iter 31/50 - Loss hyperparam: -298.541\n",
      "Iter 32/50 - Loss hyperparam: -310.287\n",
      "Iter 33/50 - Loss hyperparam: -320.567\n",
      "Iter 34/50 - Loss hyperparam: -330.720\n",
      "Iter 35/50 - Loss hyperparam: -342.331\n",
      "Iter 36/50 - Loss hyperparam: -353.345\n",
      "Iter 37/50 - Loss hyperparam: -363.059\n",
      "Iter 38/50 - Loss hyperparam: -374.239\n",
      "Iter 39/50 - Loss hyperparam: -385.163\n",
      "Iter 40/50 - Loss hyperparam: -394.441\n",
      "Iter 41/50 - Loss hyperparam: -405.560\n",
      "Iter 42/50 - Loss hyperparam: -416.475\n",
      "Iter 43/50 - Loss hyperparam: -427.180\n",
      "Iter 44/50 - Loss hyperparam: -437.687\n",
      "Iter 45/50 - Loss hyperparam: -447.621\n",
      "Iter 46/50 - Loss hyperparam: -459.659\n",
      "Iter 47/50 - Loss hyperparam: -468.819\n",
      "Iter 48/50 - Loss hyperparam: -479.132\n",
      "Iter 49/50 - Loss hyperparam: -489.628\n",
      "Iter 50/50 - Loss hyperparam: -500.601\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -6.915\n",
      "Iter 2/100 - Loss theta2: -6.902\n",
      "Iter 3/100 - Loss theta2: -6.898\n",
      "Iter 4/100 - Loss theta2: -6.897\n",
      "Iter 5/100 - Loss theta2: -6.893\n",
      "Iter 6/100 - Loss theta2: -6.897\n",
      "Iter 7/100 - Loss theta2: -6.901\n",
      "Iter 8/100 - Loss theta2: -6.900\n",
      "Iter 9/100 - Loss theta2: -6.897\n",
      "Iter 10/100 - Loss theta2: -6.897\n",
      "Iter 11/100 - Loss theta2: -6.900\n",
      "Iter 12/100 - Loss theta2: -6.903\n",
      "Iter 13/100 - Loss theta2: -6.901\n",
      "Iter 14/100 - Loss theta2: -6.900\n",
      "Iter 15/100 - Loss theta2: -6.901\n",
      "Iter 16/100 - Loss theta2: -6.902\n",
      "Iter 17/100 - Loss theta2: -6.904\n",
      "Iter 18/100 - Loss theta2: -6.903\n",
      "Iter 19/100 - Loss theta2: -6.903\n",
      "Iter 20/100 - Loss theta2: -6.903\n",
      "Iter 21/100 - Loss theta2: -6.904\n",
      "Iter 22/100 - Loss theta2: -6.904\n",
      "Iter 23/100 - Loss theta2: -6.905\n",
      "Iter 24/100 - Loss theta2: -6.904\n",
      "Iter 25/100 - Loss theta2: -6.905\n",
      "Iter 26/100 - Loss theta2: -6.905\n",
      "Iter 27/100 - Loss theta2: -6.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28/100 - Loss theta2: -6.906\n",
      "Iter 29/100 - Loss theta2: -6.906\n",
      "Iter 30/100 - Loss theta2: -6.907\n",
      "Iter 31/100 - Loss theta2: -6.907\n",
      "Iter 32/100 - Loss theta2: -6.908\n",
      "Iter 33/100 - Loss theta2: -6.908\n",
      "Iter 34/100 - Loss theta2: -6.908\n",
      "Iter 35/100 - Loss theta2: -6.909\n",
      "Iter 36/100 - Loss theta2: -6.909\n",
      "Iter 37/100 - Loss theta2: -6.910\n",
      "Iter 38/100 - Loss theta2: -6.910\n",
      "Iter 39/100 - Loss theta2: -6.910\n",
      "Iter 40/100 - Loss theta2: -6.910\n",
      "Iter 41/100 - Loss theta2: -6.911\n",
      "Iter 42/100 - Loss theta2: -6.911\n",
      "Iter 43/100 - Loss theta2: -6.912\n",
      "Iter 44/100 - Loss theta2: -6.912\n",
      "Iter 45/100 - Loss theta2: -6.912\n",
      "Iter 46/100 - Loss theta2: -6.913\n",
      "Iter 47/100 - Loss theta2: -6.913\n",
      "Iter 48/100 - Loss theta2: -6.913\n",
      "Iter 49/100 - Loss theta2: -6.914\n",
      "Iter 50/100 - Loss theta2: -6.914\n",
      "Iter 51/100 - Loss theta2: -6.914\n",
      "Iter 52/100 - Loss theta2: -6.915\n",
      "Iter 53/100 - Loss theta2: -6.915\n",
      "Iter 54/100 - Loss theta2: -6.916\n",
      "Iter 55/100 - Loss theta2: -6.916\n",
      "Iter 56/100 - Loss theta2: -6.917\n",
      "Iter 57/100 - Loss theta2: -6.917\n",
      "Iter 58/100 - Loss theta2: -6.917\n",
      "Iter 59/100 - Loss theta2: -6.918\n",
      "Iter 60/100 - Loss theta2: -6.918\n",
      "Iter 61/100 - Loss theta2: -6.919\n",
      "Iter 62/100 - Loss theta2: -6.919\n",
      "Iter 63/100 - Loss theta2: -6.919\n",
      "Iter 64/100 - Loss theta2: -6.920\n",
      "Iter 65/100 - Loss theta2: -6.920\n",
      "Iter 66/100 - Loss theta2: -6.921\n",
      "Iter 67/100 - Loss theta2: -6.922\n",
      "Iter 68/100 - Loss theta2: -6.922\n",
      "Iter 69/100 - Loss theta2: -6.922\n",
      "Iter 70/100 - Loss theta2: -6.922\n",
      "Iter 71/100 - Loss theta2: -6.923\n",
      "Iter 72/100 - Loss theta2: -6.933\n",
      "Iter 73/100 - Loss theta2: -6.933\n",
      "Iter 74/100 - Loss theta2: -6.933\n",
      "Iter 75/100 - Loss theta2: -6.934\n",
      "Iter 76/100 - Loss theta2: -6.925\n",
      "Iter 77/100 - Loss theta2: -6.925\n",
      "Iter 78/100 - Loss theta2: -6.926\n",
      "Iter 79/100 - Loss theta2: -6.926\n",
      "Iter 80/100 - Loss theta2: -6.927\n",
      "Iter 81/100 - Loss theta2: -6.927\n",
      "Iter 82/100 - Loss theta2: -6.927\n",
      "Iter 83/100 - Loss theta2: -6.927\n",
      "Iter 84/100 - Loss theta2: -6.928\n",
      "Iter 85/100 - Loss theta2: -6.928\n",
      "Iter 86/100 - Loss theta2: -6.928\n",
      "Iter 87/100 - Loss theta2: -6.929\n",
      "Iter 88/100 - Loss theta2: -6.929\n",
      "Iter 89/100 - Loss theta2: -6.930\n",
      "Iter 90/100 - Loss theta2: -6.930\n",
      "Iter 91/100 - Loss theta2: -6.930\n",
      "Iter 92/100 - Loss theta2: -6.938\n",
      "Iter 93/100 - Loss theta2: -6.938\n",
      "Iter 94/100 - Loss theta2: -6.938\n",
      "Iter 95/100 - Loss theta2: -6.932\n",
      "Iter 96/100 - Loss theta2: -6.931\n",
      "Iter 97/100 - Loss theta2: -6.932\n",
      "Iter 98/100 - Loss theta2: -6.932\n",
      "Iter 99/100 - Loss theta2: -6.933\n",
      "Iter 100/100 - Loss theta2: -6.933\n",
      "tensor([[0.6832],\n",
      "        [0.6839]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7193],\n",
      "        [0.7192]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 57.316\n",
      "Iter 2/50 - Loss hyperparam: 42.723\n",
      "Iter 3/50 - Loss hyperparam: 27.183\n",
      "Iter 4/50 - Loss hyperparam: 10.716\n",
      "Iter 5/50 - Loss hyperparam: -6.261\n",
      "Iter 6/50 - Loss hyperparam: -23.029\n",
      "Iter 7/50 - Loss hyperparam: -39.307\n",
      "Iter 8/50 - Loss hyperparam: -55.638\n",
      "Iter 9/50 - Loss hyperparam: -72.388\n",
      "Iter 10/50 - Loss hyperparam: -89.229\n",
      "Iter 11/50 - Loss hyperparam: -105.089\n",
      "Iter 12/50 - Loss hyperparam: -118.201\n",
      "Iter 13/50 - Loss hyperparam: -127.284\n",
      "Iter 14/50 - Loss hyperparam: -133.490\n",
      "Iter 15/50 - Loss hyperparam: -139.236\n",
      "Iter 16/50 - Loss hyperparam: -146.142\n",
      "Iter 17/50 - Loss hyperparam: -154.838\n",
      "Iter 18/50 - Loss hyperparam: -165.344\n",
      "Iter 19/50 - Loss hyperparam: -177.613\n",
      "Iter 20/50 - Loss hyperparam: -191.842\n",
      "Iter 21/50 - Loss hyperparam: -207.829\n",
      "Iter 22/50 - Loss hyperparam: -223.892\n",
      "Iter 23/50 - Loss hyperparam: -237.024\n",
      "Iter 24/50 - Loss hyperparam: -246.116\n",
      "Iter 25/50 - Loss hyperparam: -254.003\n",
      "Iter 26/50 - Loss hyperparam: -263.308\n",
      "Iter 27/50 - Loss hyperparam: -274.691\n",
      "Iter 28/50 - Loss hyperparam: -288.254\n",
      "Iter 29/50 - Loss hyperparam: -303.803\n",
      "Iter 30/50 - Loss hyperparam: -317.902\n",
      "Iter 31/50 - Loss hyperparam: -327.161\n",
      "Iter 32/50 - Loss hyperparam: -335.801\n",
      "Iter 33/50 - Loss hyperparam: -347.362\n",
      "Iter 34/50 - Loss hyperparam: -361.563\n",
      "Iter 35/50 - Loss hyperparam: -377.017\n",
      "Iter 36/50 - Loss hyperparam: -387.613\n",
      "Iter 37/50 - Loss hyperparam: -395.820\n",
      "Iter 38/50 - Loss hyperparam: -407.898\n",
      "Iter 39/50 - Loss hyperparam: -422.813\n",
      "Iter 40/50 - Loss hyperparam: -435.362\n",
      "Iter 41/50 - Loss hyperparam: -443.561\n",
      "Iter 42/50 - Loss hyperparam: -456.287\n",
      "Iter 43/50 - Loss hyperparam: -471.350\n",
      "Iter 44/50 - Loss hyperparam: -480.247\n",
      "Iter 45/50 - Loss hyperparam: -491.375\n",
      "Iter 46/50 - Loss hyperparam: -506.238\n",
      "Iter 47/50 - Loss hyperparam: -514.609\n",
      "Iter 48/50 - Loss hyperparam: -526.203\n",
      "Iter 49/50 - Loss hyperparam: -540.898\n",
      "Iter 50/50 - Loss hyperparam: -549.423\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.107\n",
      "Iter 2/100 - Loss theta2: -7.090\n",
      "Iter 3/100 - Loss theta2: -7.107\n",
      "Iter 4/100 - Loss theta2: -7.105\n",
      "Iter 5/100 - Loss theta2: -7.099\n",
      "Iter 6/100 - Loss theta2: -7.102\n",
      "Iter 7/100 - Loss theta2: -7.108\n",
      "Iter 8/100 - Loss theta2: -7.109\n",
      "Iter 9/100 - Loss theta2: -7.105\n",
      "Iter 10/100 - Loss theta2: -7.105\n",
      "Iter 11/100 - Loss theta2: -7.107\n",
      "Iter 12/100 - Loss theta2: -7.109\n",
      "Iter 13/100 - Loss theta2: -7.110\n",
      "Iter 14/100 - Loss theta2: -7.108\n",
      "Iter 15/100 - Loss theta2: -7.107\n",
      "Iter 16/100 - Loss theta2: -7.108\n",
      "Iter 17/100 - Loss theta2: -7.109\n",
      "Iter 18/100 - Loss theta2: -7.110\n",
      "Iter 19/100 - Loss theta2: -7.110\n",
      "Iter 20/100 - Loss theta2: -7.108\n",
      "Iter 21/100 - Loss theta2: -7.109\n",
      "Iter 22/100 - Loss theta2: -7.109\n",
      "Iter 23/100 - Loss theta2: -7.110\n",
      "Iter 24/100 - Loss theta2: -7.110\n",
      "Iter 25/100 - Loss theta2: -7.110\n",
      "Iter 26/100 - Loss theta2: -7.110\n",
      "Iter 27/100 - Loss theta2: -7.111\n",
      "Iter 28/100 - Loss theta2: -7.111\n",
      "Iter 29/100 - Loss theta2: -7.110\n",
      "Iter 30/100 - Loss theta2: -7.111\n",
      "Iter 31/100 - Loss theta2: -7.110\n",
      "Iter 32/100 - Loss theta2: -7.111\n",
      "Iter 33/100 - Loss theta2: -7.111\n",
      "Iter 34/100 - Loss theta2: -7.111\n",
      "Iter 35/100 - Loss theta2: -7.111\n",
      "Iter 36/100 - Loss theta2: -7.111\n",
      "Iter 37/100 - Loss theta2: -7.111\n",
      "Iter 38/100 - Loss theta2: -7.111\n",
      "Iter 39/100 - Loss theta2: -7.111\n",
      "Iter 40/100 - Loss theta2: -7.111\n",
      "Iter 41/100 - Loss theta2: -7.112\n",
      "Iter 42/100 - Loss theta2: -7.111\n",
      "Iter 43/100 - Loss theta2: -7.111\n",
      "Iter 44/100 - Loss theta2: -7.112\n",
      "Iter 45/100 - Loss theta2: -7.111\n",
      "Iter 46/100 - Loss theta2: -7.111\n",
      "Iter 47/100 - Loss theta2: -7.112\n",
      "Iter 48/100 - Loss theta2: -7.111\n",
      "Iter 49/100 - Loss theta2: -7.111\n",
      "Iter 50/100 - Loss theta2: -7.111\n",
      "Iter 51/100 - Loss theta2: -7.111\n",
      "Iter 52/100 - Loss theta2: -7.111\n",
      "Iter 53/100 - Loss theta2: -7.112\n",
      "Iter 54/100 - Loss theta2: -7.112\n",
      "Iter 55/100 - Loss theta2: -7.112\n",
      "Iter 56/100 - Loss theta2: -7.112\n",
      "Iter 57/100 - Loss theta2: -7.112\n",
      "Iter 58/100 - Loss theta2: -7.111\n",
      "Iter 59/100 - Loss theta2: -7.112\n",
      "Iter 60/100 - Loss theta2: -7.112\n",
      "Iter 61/100 - Loss theta2: -7.111\n",
      "Iter 62/100 - Loss theta2: -7.112\n",
      "Iter 63/100 - Loss theta2: -7.112\n",
      "Iter 64/100 - Loss theta2: -7.112\n",
      "Iter 65/100 - Loss theta2: -7.112\n",
      "Iter 66/100 - Loss theta2: -7.112\n",
      "Iter 67/100 - Loss theta2: -7.112\n",
      "Iter 68/100 - Loss theta2: -7.112\n",
      "Iter 69/100 - Loss theta2: -7.112\n",
      "Iter 70/100 - Loss theta2: -7.112\n",
      "Iter 71/100 - Loss theta2: -7.112\n",
      "Iter 72/100 - Loss theta2: -7.113\n",
      "Iter 73/100 - Loss theta2: -7.112\n",
      "Iter 74/100 - Loss theta2: -7.112\n",
      "Iter 75/100 - Loss theta2: -7.112\n",
      "Iter 76/100 - Loss theta2: -7.112\n",
      "Iter 77/100 - Loss theta2: -7.112\n",
      "Iter 78/100 - Loss theta2: -7.112\n",
      "Iter 79/100 - Loss theta2: -7.112\n",
      "Iter 80/100 - Loss theta2: -7.113\n",
      "Iter 81/100 - Loss theta2: -7.113\n",
      "Iter 82/100 - Loss theta2: -7.113\n",
      "Iter 83/100 - Loss theta2: -7.113\n",
      "Iter 84/100 - Loss theta2: -7.112\n",
      "Iter 85/100 - Loss theta2: -7.113\n",
      "Iter 86/100 - Loss theta2: -7.112\n",
      "Iter 87/100 - Loss theta2: -7.113\n",
      "Iter 88/100 - Loss theta2: -7.113\n",
      "Iter 89/100 - Loss theta2: -7.113\n",
      "Iter 90/100 - Loss theta2: -7.113\n",
      "Iter 91/100 - Loss theta2: -7.113\n",
      "Iter 92/100 - Loss theta2: -7.112\n",
      "Iter 93/100 - Loss theta2: -7.113\n",
      "Iter 94/100 - Loss theta2: -7.113\n",
      "Iter 95/100 - Loss theta2: -7.113\n",
      "Iter 96/100 - Loss theta2: -7.112\n",
      "Iter 97/100 - Loss theta2: -7.112\n",
      "Iter 98/100 - Loss theta2: -7.112\n",
      "Iter 99/100 - Loss theta2: -7.113\n",
      "Iter 100/100 - Loss theta2: -7.112\n",
      "tensor([[0.6852],\n",
      "        [0.6856]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7183],\n",
      "        [0.7182]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1245]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 20.334\n",
      "Iter 2/50 - Loss hyperparam: 1.999\n",
      "Iter 3/50 - Loss hyperparam: -15.248\n",
      "Iter 4/50 - Loss hyperparam: -31.174\n",
      "Iter 5/50 - Loss hyperparam: -45.313\n",
      "Iter 6/50 - Loss hyperparam: -57.549\n",
      "Iter 7/50 - Loss hyperparam: -68.636\n",
      "Iter 8/50 - Loss hyperparam: -79.296\n",
      "Iter 9/50 - Loss hyperparam: -89.967\n",
      "Iter 10/50 - Loss hyperparam: -101.021\n",
      "Iter 11/50 - Loss hyperparam: -112.486\n",
      "Iter 12/50 - Loss hyperparam: -123.979\n",
      "Iter 13/50 - Loss hyperparam: -134.981\n",
      "Iter 14/50 - Loss hyperparam: -145.304\n",
      "Iter 15/50 - Loss hyperparam: -155.216\n",
      "Iter 16/50 - Loss hyperparam: -165.128\n",
      "Iter 17/50 - Loss hyperparam: -175.534\n",
      "Iter 18/50 - Loss hyperparam: -186.762\n",
      "Iter 19/50 - Loss hyperparam: -198.642\n",
      "Iter 20/50 - Loss hyperparam: -210.873\n",
      "Iter 21/50 - Loss hyperparam: -223.315\n",
      "Iter 22/50 - Loss hyperparam: -235.557\n",
      "Iter 23/50 - Loss hyperparam: -247.317\n",
      "Iter 24/50 - Loss hyperparam: -259.095\n",
      "Iter 25/50 - Loss hyperparam: -271.675\n",
      "Iter 26/50 - Loss hyperparam: -285.185\n",
      "Iter 27/50 - Loss hyperparam: -298.093\n",
      "Iter 28/50 - Loss hyperparam: -309.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29/50 - Loss hyperparam: -320.612\n",
      "Iter 30/50 - Loss hyperparam: -333.780\n",
      "Iter 31/50 - Loss hyperparam: -346.321\n",
      "Iter 32/50 - Loss hyperparam: -356.711\n",
      "Iter 33/50 - Loss hyperparam: -369.068\n",
      "Iter 34/50 - Loss hyperparam: -383.009\n",
      "Iter 35/50 - Loss hyperparam: -394.418\n",
      "Iter 36/50 - Loss hyperparam: -406.922\n",
      "Iter 37/50 - Loss hyperparam: -420.429\n",
      "Iter 38/50 - Loss hyperparam: -431.773\n",
      "Iter 39/50 - Loss hyperparam: -444.426\n",
      "Iter 40/50 - Loss hyperparam: -456.717\n",
      "Iter 41/50 - Loss hyperparam: -468.512\n",
      "Iter 42/50 - Loss hyperparam: -481.041\n",
      "Iter 43/50 - Loss hyperparam: -492.942\n",
      "Iter 44/50 - Loss hyperparam: -505.566\n",
      "Iter 45/50 - Loss hyperparam: -517.056\n",
      "Iter 46/50 - Loss hyperparam: -530.539\n",
      "Iter 47/50 - Loss hyperparam: -541.139\n",
      "Iter 48/50 - Loss hyperparam: -554.142\n",
      "Iter 49/50 - Loss hyperparam: -566.073\n",
      "Iter 50/50 - Loss hyperparam: -577.825\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.057\n",
      "Iter 2/100 - Loss theta2: -7.055\n",
      "Iter 3/100 - Loss theta2: -7.059\n",
      "Iter 4/100 - Loss theta2: -7.063\n",
      "Iter 5/100 - Loss theta2: -7.060\n",
      "Iter 6/100 - Loss theta2: -7.059\n",
      "Iter 7/100 - Loss theta2: -7.063\n",
      "Iter 8/100 - Loss theta2: -7.064\n",
      "Iter 9/100 - Loss theta2: -7.062\n",
      "Iter 10/100 - Loss theta2: -7.061\n",
      "Iter 11/100 - Loss theta2: -7.063\n",
      "Iter 12/100 - Loss theta2: -7.064\n",
      "Iter 13/100 - Loss theta2: -7.063\n",
      "Iter 14/100 - Loss theta2: -7.063\n",
      "Iter 15/100 - Loss theta2: -7.063\n",
      "Iter 16/100 - Loss theta2: -7.064\n",
      "Iter 17/100 - Loss theta2: -7.064\n",
      "Iter 18/100 - Loss theta2: -7.064\n",
      "Iter 19/100 - Loss theta2: -7.063\n",
      "Iter 20/100 - Loss theta2: -7.064\n",
      "Iter 21/100 - Loss theta2: -7.065\n",
      "Iter 22/100 - Loss theta2: -7.064\n",
      "Iter 23/100 - Loss theta2: -7.064\n",
      "Iter 24/100 - Loss theta2: -7.064\n",
      "Iter 25/100 - Loss theta2: -7.064\n",
      "Iter 26/100 - Loss theta2: -7.065\n",
      "Iter 27/100 - Loss theta2: -7.064\n",
      "Iter 28/100 - Loss theta2: -7.065\n",
      "Iter 29/100 - Loss theta2: -7.065\n",
      "Iter 30/100 - Loss theta2: -7.065\n",
      "Iter 31/100 - Loss theta2: -7.065\n",
      "Iter 32/100 - Loss theta2: -7.065\n",
      "Iter 33/100 - Loss theta2: -7.065\n",
      "Iter 34/100 - Loss theta2: -7.065\n",
      "Iter 35/100 - Loss theta2: -7.064\n",
      "Iter 36/100 - Loss theta2: -7.065\n",
      "Iter 37/100 - Loss theta2: -7.065\n",
      "Iter 38/100 - Loss theta2: -7.065\n",
      "Iter 39/100 - Loss theta2: -7.065\n",
      "Iter 40/100 - Loss theta2: -7.065\n",
      "Iter 41/100 - Loss theta2: -7.065\n",
      "Iter 42/100 - Loss theta2: -7.065\n",
      "Iter 43/100 - Loss theta2: -7.066\n",
      "Iter 44/100 - Loss theta2: -7.065\n",
      "Iter 45/100 - Loss theta2: -7.065\n",
      "Iter 46/100 - Loss theta2: -7.065\n",
      "Iter 47/100 - Loss theta2: -7.066\n",
      "Iter 48/100 - Loss theta2: -7.066\n",
      "Iter 49/100 - Loss theta2: -7.066\n",
      "Iter 50/100 - Loss theta2: -7.066\n",
      "Iter 51/100 - Loss theta2: -7.066\n",
      "Iter 52/100 - Loss theta2: -7.066\n",
      "Iter 53/100 - Loss theta2: -7.067\n",
      "Iter 54/100 - Loss theta2: -7.066\n",
      "Iter 55/100 - Loss theta2: -7.067\n",
      "Iter 56/100 - Loss theta2: -7.067\n",
      "Iter 57/100 - Loss theta2: -7.066\n",
      "Iter 58/100 - Loss theta2: -7.066\n",
      "Iter 59/100 - Loss theta2: -7.066\n",
      "Iter 60/100 - Loss theta2: -7.067\n",
      "Iter 61/100 - Loss theta2: -7.067\n",
      "Iter 62/100 - Loss theta2: -7.067\n",
      "Iter 63/100 - Loss theta2: -7.067\n",
      "Iter 64/100 - Loss theta2: -7.067\n",
      "Iter 65/100 - Loss theta2: -7.067\n",
      "Iter 66/100 - Loss theta2: -7.067\n",
      "Iter 67/100 - Loss theta2: -7.068\n",
      "Iter 68/100 - Loss theta2: -7.068\n",
      "Iter 69/100 - Loss theta2: -7.068\n",
      "Iter 70/100 - Loss theta2: -7.068\n",
      "Iter 71/100 - Loss theta2: -7.068\n",
      "Iter 72/100 - Loss theta2: -7.067\n",
      "Iter 73/100 - Loss theta2: -7.069\n",
      "Iter 74/100 - Loss theta2: -7.068\n",
      "Iter 75/100 - Loss theta2: -7.068\n",
      "Iter 76/100 - Loss theta2: -7.068\n",
      "Iter 77/100 - Loss theta2: -7.069\n",
      "Iter 78/100 - Loss theta2: -7.068\n",
      "Iter 79/100 - Loss theta2: -7.068\n",
      "Iter 80/100 - Loss theta2: -7.069\n",
      "Iter 81/100 - Loss theta2: -7.069\n",
      "Iter 82/100 - Loss theta2: -7.069\n",
      "Iter 83/100 - Loss theta2: -7.069\n",
      "Iter 84/100 - Loss theta2: -7.069\n",
      "Iter 85/100 - Loss theta2: -7.069\n",
      "Iter 86/100 - Loss theta2: -7.070\n",
      "Iter 87/100 - Loss theta2: -7.069\n",
      "Iter 88/100 - Loss theta2: -7.069\n",
      "Iter 89/100 - Loss theta2: -7.069\n",
      "Iter 90/100 - Loss theta2: -7.070\n",
      "Iter 91/100 - Loss theta2: -7.070\n",
      "Iter 92/100 - Loss theta2: -7.070\n",
      "Iter 93/100 - Loss theta2: -7.070\n",
      "Iter 94/100 - Loss theta2: -7.070\n",
      "Iter 95/100 - Loss theta2: -7.070\n",
      "Iter 96/100 - Loss theta2: -7.070\n",
      "Iter 97/100 - Loss theta2: -7.070\n",
      "Iter 98/100 - Loss theta2: -7.071\n",
      "Iter 99/100 - Loss theta2: -7.071\n",
      "Iter 100/100 - Loss theta2: -7.071\n",
      "tensor([[0.6846],\n",
      "        [0.6850]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7182],\n",
      "        [0.7180]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1242]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 56.180\n",
      "Iter 2/50 - Loss hyperparam: 38.015\n",
      "Iter 3/50 - Loss hyperparam: 18.647\n",
      "Iter 4/50 - Loss hyperparam: -1.865\n",
      "Iter 5/50 - Loss hyperparam: -22.991\n",
      "Iter 6/50 - Loss hyperparam: -43.571\n",
      "Iter 7/50 - Loss hyperparam: -62.486\n",
      "Iter 8/50 - Loss hyperparam: -79.823\n",
      "Iter 9/50 - Loss hyperparam: -96.425\n",
      "Iter 10/50 - Loss hyperparam: -112.147\n",
      "Iter 11/50 - Loss hyperparam: -125.824\n",
      "Iter 12/50 - Loss hyperparam: -136.288\n",
      "Iter 13/50 - Loss hyperparam: -143.714\n",
      "Iter 14/50 - Loss hyperparam: -150.006\n",
      "Iter 15/50 - Loss hyperparam: -157.530\n",
      "Iter 16/50 - Loss hyperparam: -167.775\n",
      "Iter 17/50 - Loss hyperparam: -181.087\n",
      "Iter 18/50 - Loss hyperparam: -196.930\n",
      "Iter 19/50 - Loss hyperparam: -213.968\n",
      "Iter 20/50 - Loss hyperparam: -230.220\n",
      "Iter 21/50 - Loss hyperparam: -244.260\n",
      "Iter 22/50 - Loss hyperparam: -256.297\n",
      "Iter 23/50 - Loss hyperparam: -267.229\n",
      "Iter 24/50 - Loss hyperparam: -278.208\n",
      "Iter 25/50 - Loss hyperparam: -290.361\n",
      "Iter 26/50 - Loss hyperparam: -304.769\n",
      "Iter 27/50 - Loss hyperparam: -321.662\n",
      "Iter 28/50 - Loss hyperparam: -338.240\n",
      "Iter 29/50 - Loss hyperparam: -350.228\n",
      "Iter 30/50 - Loss hyperparam: -359.405\n",
      "Iter 31/50 - Loss hyperparam: -371.401\n",
      "Iter 32/50 - Loss hyperparam: -386.871\n",
      "Iter 33/50 - Loss hyperparam: -403.004\n",
      "Iter 34/50 - Loss hyperparam: -417.758\n",
      "Iter 35/50 - Loss hyperparam: -428.788\n",
      "Iter 36/50 - Loss hyperparam: -440.235\n",
      "Iter 37/50 - Loss hyperparam: -456.925\n",
      "Iter 38/50 - Loss hyperparam: -471.952\n",
      "Iter 39/50 - Loss hyperparam: -482.760\n",
      "Iter 40/50 - Loss hyperparam: -495.190\n",
      "Iter 41/50 - Loss hyperparam: -511.157\n",
      "Iter 42/50 - Loss hyperparam: -524.824\n",
      "Iter 43/50 - Loss hyperparam: -535.422\n",
      "Iter 44/50 - Loss hyperparam: -551.416\n",
      "Iter 45/50 - Loss hyperparam: -565.161\n",
      "Iter 46/50 - Loss hyperparam: -575.800\n",
      "Iter 47/50 - Loss hyperparam: -592.538\n",
      "Iter 48/50 - Loss hyperparam: -603.593\n",
      "Iter 49/50 - Loss hyperparam: -617.014\n",
      "Iter 50/50 - Loss hyperparam: -629.898\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.102\n",
      "Iter 2/100 - Loss theta2: -7.088\n",
      "Iter 3/100 - Loss theta2: -7.101\n",
      "Iter 4/100 - Loss theta2: -7.094\n",
      "Iter 5/100 - Loss theta2: -7.089\n",
      "Iter 6/100 - Loss theta2: -7.093\n",
      "Iter 7/100 - Loss theta2: -7.097\n",
      "Iter 8/100 - Loss theta2: -7.096\n",
      "Iter 9/100 - Loss theta2: -7.094\n",
      "Iter 10/100 - Loss theta2: -7.094\n",
      "Iter 11/100 - Loss theta2: -7.097\n",
      "Iter 12/100 - Loss theta2: -7.099\n",
      "Iter 13/100 - Loss theta2: -7.098\n",
      "Iter 14/100 - Loss theta2: -7.098\n",
      "Iter 15/100 - Loss theta2: -7.097\n",
      "Iter 16/100 - Loss theta2: -7.099\n",
      "Iter 17/100 - Loss theta2: -7.100\n",
      "Iter 18/100 - Loss theta2: -7.100\n",
      "Iter 19/100 - Loss theta2: -7.100\n",
      "Iter 20/100 - Loss theta2: -7.100\n",
      "Iter 21/100 - Loss theta2: -7.101\n",
      "Iter 22/100 - Loss theta2: -7.102\n",
      "Iter 23/100 - Loss theta2: -7.102\n",
      "Iter 24/100 - Loss theta2: -7.101\n",
      "Iter 25/100 - Loss theta2: -7.102\n",
      "Iter 26/100 - Loss theta2: -7.102\n",
      "Iter 27/100 - Loss theta2: -7.103\n",
      "Iter 28/100 - Loss theta2: -7.102\n",
      "Iter 29/100 - Loss theta2: -7.103\n",
      "Iter 30/100 - Loss theta2: -7.103\n",
      "Iter 31/100 - Loss theta2: -7.104\n",
      "Iter 32/100 - Loss theta2: -7.103\n",
      "Iter 33/100 - Loss theta2: -7.104\n",
      "Iter 34/100 - Loss theta2: -7.104\n",
      "Iter 35/100 - Loss theta2: -7.105\n",
      "Iter 36/100 - Loss theta2: -7.105\n",
      "Iter 37/100 - Loss theta2: -7.104\n",
      "Iter 38/100 - Loss theta2: -7.105\n",
      "Iter 39/100 - Loss theta2: -7.105\n",
      "Iter 40/100 - Loss theta2: -7.105\n",
      "Iter 41/100 - Loss theta2: -7.105\n",
      "Iter 42/100 - Loss theta2: -7.105\n",
      "Iter 43/100 - Loss theta2: -7.106\n",
      "Iter 44/100 - Loss theta2: -7.106\n",
      "Iter 45/100 - Loss theta2: -7.106\n",
      "Iter 46/100 - Loss theta2: -7.107\n",
      "Iter 47/100 - Loss theta2: -7.107\n",
      "Iter 48/100 - Loss theta2: -7.107\n",
      "Iter 49/100 - Loss theta2: -7.107\n",
      "Iter 50/100 - Loss theta2: -7.108\n",
      "Iter 51/100 - Loss theta2: -7.108\n",
      "Iter 52/100 - Loss theta2: -7.110\n",
      "Iter 53/100 - Loss theta2: -7.110\n",
      "Iter 54/100 - Loss theta2: -7.110\n",
      "Iter 55/100 - Loss theta2: -7.108\n",
      "Iter 56/100 - Loss theta2: -7.108\n",
      "Iter 57/100 - Loss theta2: -7.108\n",
      "Iter 58/100 - Loss theta2: -7.109\n",
      "Iter 59/100 - Loss theta2: -7.108\n",
      "Iter 60/100 - Loss theta2: -7.108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 61/100 - Loss theta2: -7.108\n",
      "Iter 62/100 - Loss theta2: -7.109\n",
      "Iter 63/100 - Loss theta2: -7.109\n",
      "Iter 64/100 - Loss theta2: -7.109\n",
      "Iter 65/100 - Loss theta2: -7.111\n",
      "Iter 66/100 - Loss theta2: -7.111\n",
      "Iter 67/100 - Loss theta2: -7.110\n",
      "Iter 68/100 - Loss theta2: -7.110\n",
      "Iter 69/100 - Loss theta2: -7.109\n",
      "Iter 70/100 - Loss theta2: -7.109\n",
      "Iter 71/100 - Loss theta2: -7.109\n",
      "Iter 72/100 - Loss theta2: -7.109\n",
      "Iter 73/100 - Loss theta2: -7.109\n",
      "Iter 74/100 - Loss theta2: -7.110\n",
      "Iter 75/100 - Loss theta2: -7.110\n",
      "Iter 76/100 - Loss theta2: -7.110\n",
      "Iter 77/100 - Loss theta2: -7.111\n",
      "Iter 78/100 - Loss theta2: -7.111\n",
      "Iter 79/100 - Loss theta2: -7.111\n",
      "Iter 80/100 - Loss theta2: -7.111\n",
      "Iter 81/100 - Loss theta2: -7.111\n",
      "Iter 82/100 - Loss theta2: -7.111\n",
      "Iter 83/100 - Loss theta2: -7.111\n",
      "Iter 84/100 - Loss theta2: -7.111\n",
      "Iter 85/100 - Loss theta2: -7.111\n",
      "Iter 86/100 - Loss theta2: -7.111\n",
      "Iter 87/100 - Loss theta2: -7.111\n",
      "Iter 88/100 - Loss theta2: -7.111\n",
      "Iter 89/100 - Loss theta2: -7.111\n",
      "Iter 90/100 - Loss theta2: -7.111\n",
      "Iter 91/100 - Loss theta2: -7.110\n",
      "Iter 92/100 - Loss theta2: -7.111\n",
      "Iter 93/100 - Loss theta2: -7.111\n",
      "Iter 94/100 - Loss theta2: -7.111\n",
      "Iter 95/100 - Loss theta2: -7.111\n",
      "Iter 96/100 - Loss theta2: -7.111\n",
      "Iter 97/100 - Loss theta2: -7.111\n",
      "Iter 98/100 - Loss theta2: -7.111\n",
      "Iter 99/100 - Loss theta2: -7.111\n",
      "Iter 100/100 - Loss theta2: -7.111\n",
      "tensor([[0.6850],\n",
      "        [0.6851]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7175],\n",
      "        [0.7175]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 75.619\n",
      "Iter 2/50 - Loss hyperparam: 57.227\n",
      "Iter 3/50 - Loss hyperparam: 37.608\n",
      "Iter 4/50 - Loss hyperparam: 16.577\n",
      "Iter 5/50 - Loss hyperparam: -5.801\n",
      "Iter 6/50 - Loss hyperparam: -28.842\n",
      "Iter 7/50 - Loss hyperparam: -51.488\n",
      "Iter 8/50 - Loss hyperparam: -72.743\n",
      "Iter 9/50 - Loss hyperparam: -91.855\n",
      "Iter 10/50 - Loss hyperparam: -108.847\n",
      "Iter 11/50 - Loss hyperparam: -124.673\n",
      "Iter 12/50 - Loss hyperparam: -139.572\n",
      "Iter 13/50 - Loss hyperparam: -152.579\n",
      "Iter 14/50 - Loss hyperparam: -162.885\n",
      "Iter 15/50 - Loss hyperparam: -171.121\n",
      "Iter 16/50 - Loss hyperparam: -178.940\n",
      "Iter 17/50 - Loss hyperparam: -187.941\n",
      "Iter 18/50 - Loss hyperparam: -199.400\n",
      "Iter 19/50 - Loss hyperparam: -214.144\n",
      "Iter 20/50 - Loss hyperparam: -232.191\n",
      "Iter 21/50 - Loss hyperparam: -252.339\n",
      "Iter 22/50 - Loss hyperparam: -271.627\n",
      "Iter 23/50 - Loss hyperparam: -286.216\n",
      "Iter 24/50 - Loss hyperparam: -295.886\n",
      "Iter 25/50 - Loss hyperparam: -305.162\n",
      "Iter 26/50 - Loss hyperparam: -317.532\n",
      "Iter 27/50 - Loss hyperparam: -333.350\n",
      "Iter 28/50 - Loss hyperparam: -350.721\n",
      "Iter 29/50 - Loss hyperparam: -367.489\n",
      "Iter 30/50 - Loss hyperparam: -382.691\n",
      "Iter 31/50 - Loss hyperparam: -394.880\n",
      "Iter 32/50 - Loss hyperparam: -405.246\n",
      "Iter 33/50 - Loss hyperparam: -418.589\n",
      "Iter 34/50 - Loss hyperparam: -437.504\n",
      "Iter 35/50 - Loss hyperparam: -456.069\n",
      "Iter 36/50 - Loss hyperparam: -466.666\n",
      "Iter 37/50 - Loss hyperparam: -477.621\n",
      "Iter 38/50 - Loss hyperparam: -494.409\n",
      "Iter 39/50 - Loss hyperparam: -511.244\n",
      "Iter 40/50 - Loss hyperparam: -524.828\n",
      "Iter 41/50 - Loss hyperparam: -535.639\n",
      "Iter 42/50 - Loss hyperparam: -551.760\n",
      "Iter 43/50 - Loss hyperparam: -569.426\n",
      "Iter 44/50 - Loss hyperparam: -578.713\n",
      "Iter 45/50 - Loss hyperparam: -595.111\n",
      "Iter 46/50 - Loss hyperparam: -610.746\n",
      "Iter 47/50 - Loss hyperparam: -622.066\n",
      "Iter 48/50 - Loss hyperparam: -637.911\n",
      "Iter 49/50 - Loss hyperparam: -651.524\n",
      "Iter 50/50 - Loss hyperparam: -664.866\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.165\n",
      "Iter 2/100 - Loss theta2: -7.137\n",
      "Iter 3/100 - Loss theta2: -7.163\n",
      "Iter 4/100 - Loss theta2: -7.159\n",
      "Iter 5/100 - Loss theta2: -7.149\n",
      "Iter 6/100 - Loss theta2: -7.153\n",
      "Iter 7/100 - Loss theta2: -7.162\n",
      "Iter 8/100 - Loss theta2: -7.166\n",
      "Iter 9/100 - Loss theta2: -7.161\n",
      "Iter 10/100 - Loss theta2: -7.158\n",
      "Iter 11/100 - Loss theta2: -7.158\n",
      "Iter 12/100 - Loss theta2: -7.163\n",
      "Iter 13/100 - Loss theta2: -7.165\n",
      "Iter 14/100 - Loss theta2: -7.166\n",
      "Iter 15/100 - Loss theta2: -7.163\n",
      "Iter 16/100 - Loss theta2: -7.162\n",
      "Iter 17/100 - Loss theta2: -7.163\n",
      "Iter 18/100 - Loss theta2: -7.165\n",
      "Iter 19/100 - Loss theta2: -7.166\n",
      "Iter 20/100 - Loss theta2: -7.166\n",
      "Iter 21/100 - Loss theta2: -7.165\n",
      "Iter 22/100 - Loss theta2: -7.164\n",
      "Iter 23/100 - Loss theta2: -7.165\n",
      "Iter 24/100 - Loss theta2: -7.166\n",
      "Iter 25/100 - Loss theta2: -7.167\n",
      "Iter 26/100 - Loss theta2: -7.167\n",
      "Iter 27/100 - Loss theta2: -7.166\n",
      "Iter 28/100 - Loss theta2: -7.166\n",
      "Iter 29/100 - Loss theta2: -7.167\n",
      "Iter 30/100 - Loss theta2: -7.167\n",
      "Iter 31/100 - Loss theta2: -7.168\n",
      "Iter 32/100 - Loss theta2: -7.167\n",
      "Iter 33/100 - Loss theta2: -7.166\n",
      "Iter 34/100 - Loss theta2: -7.167\n",
      "Iter 35/100 - Loss theta2: -7.167\n",
      "Iter 36/100 - Loss theta2: -7.168\n",
      "Iter 37/100 - Loss theta2: -7.168\n",
      "Iter 38/100 - Loss theta2: -7.167\n",
      "Iter 39/100 - Loss theta2: -7.167\n",
      "Iter 40/100 - Loss theta2: -7.168\n",
      "Iter 41/100 - Loss theta2: -7.168\n",
      "Iter 42/100 - Loss theta2: -7.168\n",
      "Iter 43/100 - Loss theta2: -7.168\n",
      "Iter 44/100 - Loss theta2: -7.168\n",
      "Iter 45/100 - Loss theta2: -7.168\n",
      "Iter 46/100 - Loss theta2: -7.169\n",
      "Iter 47/100 - Loss theta2: -7.168\n",
      "Iter 48/100 - Loss theta2: -7.168\n",
      "Iter 49/100 - Loss theta2: -7.169\n",
      "Iter 50/100 - Loss theta2: -7.169\n",
      "Iter 51/100 - Loss theta2: -7.169\n",
      "Iter 52/100 - Loss theta2: -7.169\n",
      "Iter 53/100 - Loss theta2: -7.169\n",
      "Iter 54/100 - Loss theta2: -7.169\n",
      "Iter 55/100 - Loss theta2: -7.169\n",
      "Iter 56/100 - Loss theta2: -7.169\n",
      "Iter 57/100 - Loss theta2: -7.169\n",
      "Iter 58/100 - Loss theta2: -7.169\n",
      "Iter 59/100 - Loss theta2: -7.170\n",
      "Iter 60/100 - Loss theta2: -7.170\n",
      "Iter 61/100 - Loss theta2: -7.170\n",
      "Iter 62/100 - Loss theta2: -7.170\n",
      "Iter 63/100 - Loss theta2: -7.170\n",
      "Iter 64/100 - Loss theta2: -7.170\n",
      "Iter 65/100 - Loss theta2: -7.170\n",
      "Iter 66/100 - Loss theta2: -7.170\n",
      "Iter 67/100 - Loss theta2: -7.170\n",
      "Iter 68/100 - Loss theta2: -7.170\n",
      "Iter 69/100 - Loss theta2: -7.170\n",
      "Iter 70/100 - Loss theta2: -7.171\n",
      "Iter 71/100 - Loss theta2: -7.171\n",
      "Iter 72/100 - Loss theta2: -7.171\n",
      "Iter 73/100 - Loss theta2: -7.170\n",
      "Iter 74/100 - Loss theta2: -7.171\n",
      "Iter 75/100 - Loss theta2: -7.171\n",
      "Iter 76/100 - Loss theta2: -7.171\n",
      "Iter 77/100 - Loss theta2: -7.172\n",
      "Iter 78/100 - Loss theta2: -7.171\n",
      "Iter 79/100 - Loss theta2: -7.171\n",
      "Iter 80/100 - Loss theta2: -7.171\n",
      "Iter 81/100 - Loss theta2: -7.171\n",
      "Iter 82/100 - Loss theta2: -7.171\n",
      "Iter 83/100 - Loss theta2: -7.171\n",
      "Iter 84/100 - Loss theta2: -7.171\n",
      "Iter 85/100 - Loss theta2: -7.172\n",
      "Iter 86/100 - Loss theta2: -7.171\n",
      "Iter 87/100 - Loss theta2: -7.172\n",
      "Iter 88/100 - Loss theta2: -7.172\n",
      "Iter 89/100 - Loss theta2: -7.172\n",
      "Iter 90/100 - Loss theta2: -7.172\n",
      "Iter 91/100 - Loss theta2: -7.172\n",
      "Iter 92/100 - Loss theta2: -7.172\n",
      "Iter 93/100 - Loss theta2: -7.171\n",
      "Iter 94/100 - Loss theta2: -7.172\n",
      "Iter 95/100 - Loss theta2: -7.172\n",
      "Iter 96/100 - Loss theta2: -7.172\n",
      "Iter 97/100 - Loss theta2: -7.172\n",
      "Iter 98/100 - Loss theta2: -7.172\n",
      "Iter 99/100 - Loss theta2: -7.172\n",
      "Iter 100/100 - Loss theta2: -7.172\n",
      "tensor([[0.6856],\n",
      "        [0.6862]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7176],\n",
      "        [0.7176]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 37.940\n",
      "Iter 2/50 - Loss hyperparam: 19.143\n",
      "Iter 3/50 - Loss hyperparam: -0.310\n",
      "Iter 4/50 - Loss hyperparam: -20.651\n",
      "Iter 5/50 - Loss hyperparam: -42.136\n",
      "Iter 6/50 - Loss hyperparam: -63.741\n",
      "Iter 7/50 - Loss hyperparam: -84.133\n",
      "Iter 8/50 - Loss hyperparam: -102.489\n",
      "Iter 9/50 - Loss hyperparam: -117.699\n",
      "Iter 10/50 - Loss hyperparam: -129.144\n",
      "Iter 11/50 - Loss hyperparam: -138.063\n",
      "Iter 12/50 - Loss hyperparam: -146.231\n",
      "Iter 13/50 - Loss hyperparam: -155.151\n",
      "Iter 14/50 - Loss hyperparam: -166.039\n",
      "Iter 15/50 - Loss hyperparam: -179.217\n",
      "Iter 16/50 - Loss hyperparam: -194.199\n",
      "Iter 17/50 - Loss hyperparam: -210.541\n",
      "Iter 18/50 - Loss hyperparam: -227.285\n",
      "Iter 19/50 - Loss hyperparam: -242.763\n",
      "Iter 20/50 - Loss hyperparam: -256.268\n",
      "Iter 21/50 - Loss hyperparam: -267.857\n",
      "Iter 22/50 - Loss hyperparam: -279.562\n",
      "Iter 23/50 - Loss hyperparam: -293.132\n",
      "Iter 24/50 - Loss hyperparam: -309.093\n",
      "Iter 25/50 - Loss hyperparam: -326.699\n",
      "Iter 26/50 - Loss hyperparam: -343.805\n",
      "Iter 27/50 - Loss hyperparam: -358.217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28/50 - Loss hyperparam: -370.237\n",
      "Iter 29/50 - Loss hyperparam: -382.651\n",
      "Iter 30/50 - Loss hyperparam: -397.877\n",
      "Iter 31/50 - Loss hyperparam: -414.953\n",
      "Iter 32/50 - Loss hyperparam: -430.443\n",
      "Iter 33/50 - Loss hyperparam: -442.788\n",
      "Iter 34/50 - Loss hyperparam: -455.895\n",
      "Iter 35/50 - Loss hyperparam: -472.784\n",
      "Iter 36/50 - Loss hyperparam: -490.066\n",
      "Iter 37/50 - Loss hyperparam: -503.148\n",
      "Iter 38/50 - Loss hyperparam: -515.702\n",
      "Iter 39/50 - Loss hyperparam: -532.586\n",
      "Iter 40/50 - Loss hyperparam: -548.640\n",
      "Iter 41/50 - Loss hyperparam: -560.499\n",
      "Iter 42/50 - Loss hyperparam: -575.695\n",
      "Iter 43/50 - Loss hyperparam: -591.888\n",
      "Iter 44/50 - Loss hyperparam: -604.768\n",
      "Iter 45/50 - Loss hyperparam: -621.186\n",
      "Iter 46/50 - Loss hyperparam: -634.517\n",
      "Iter 47/50 - Loss hyperparam: -647.642\n",
      "Iter 48/50 - Loss hyperparam: -665.083\n",
      "Iter 49/50 - Loss hyperparam: -677.944\n",
      "Iter 50/50 - Loss hyperparam: -693.270\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.106\n",
      "Iter 2/100 - Loss theta2: -7.084\n",
      "Iter 3/100 - Loss theta2: -7.105\n",
      "Iter 4/100 - Loss theta2: -7.102\n",
      "Iter 5/100 - Loss theta2: -7.094\n",
      "Iter 6/100 - Loss theta2: -7.098\n",
      "Iter 7/100 - Loss theta2: -7.105\n",
      "Iter 8/100 - Loss theta2: -7.106\n",
      "Iter 9/100 - Loss theta2: -7.102\n",
      "Iter 10/100 - Loss theta2: -7.101\n",
      "Iter 11/100 - Loss theta2: -7.102\n",
      "Iter 12/100 - Loss theta2: -7.105\n",
      "Iter 13/100 - Loss theta2: -7.107\n",
      "Iter 14/100 - Loss theta2: -7.105\n",
      "Iter 15/100 - Loss theta2: -7.104\n",
      "Iter 16/100 - Loss theta2: -7.104\n",
      "Iter 17/100 - Loss theta2: -7.105\n",
      "Iter 18/100 - Loss theta2: -7.107\n",
      "Iter 19/100 - Loss theta2: -7.107\n",
      "Iter 20/100 - Loss theta2: -7.106\n",
      "Iter 21/100 - Loss theta2: -7.105\n",
      "Iter 22/100 - Loss theta2: -7.106\n",
      "Iter 23/100 - Loss theta2: -7.107\n",
      "Iter 24/100 - Loss theta2: -7.108\n",
      "Iter 25/100 - Loss theta2: -7.107\n",
      "Iter 26/100 - Loss theta2: -7.106\n",
      "Iter 27/100 - Loss theta2: -7.106\n",
      "Iter 28/100 - Loss theta2: -7.107\n",
      "Iter 29/100 - Loss theta2: -7.108\n",
      "Iter 30/100 - Loss theta2: -7.108\n",
      "Iter 31/100 - Loss theta2: -7.108\n",
      "Iter 32/100 - Loss theta2: -7.108\n",
      "Iter 33/100 - Loss theta2: -7.108\n",
      "Iter 34/100 - Loss theta2: -7.109\n",
      "Iter 35/100 - Loss theta2: -7.108\n",
      "Iter 36/100 - Loss theta2: -7.108\n",
      "Iter 37/100 - Loss theta2: -7.108\n",
      "Iter 38/100 - Loss theta2: -7.108\n",
      "Iter 39/100 - Loss theta2: -7.108\n",
      "Iter 40/100 - Loss theta2: -7.109\n",
      "Iter 41/100 - Loss theta2: -7.110\n",
      "Iter 42/100 - Loss theta2: -7.108\n",
      "Iter 43/100 - Loss theta2: -7.109\n",
      "Iter 44/100 - Loss theta2: -7.109\n",
      "Iter 45/100 - Loss theta2: -7.109\n",
      "Iter 46/100 - Loss theta2: -7.109\n",
      "Iter 47/100 - Loss theta2: -7.109\n",
      "Iter 48/100 - Loss theta2: -7.109\n",
      "Iter 49/100 - Loss theta2: -7.109\n",
      "Iter 50/100 - Loss theta2: -7.110\n",
      "Iter 51/100 - Loss theta2: -7.110\n",
      "Iter 52/100 - Loss theta2: -7.110\n",
      "Iter 53/100 - Loss theta2: -7.110\n",
      "Iter 54/100 - Loss theta2: -7.110\n",
      "Iter 55/100 - Loss theta2: -7.111\n",
      "Iter 56/100 - Loss theta2: -7.110\n",
      "Iter 57/100 - Loss theta2: -7.111\n",
      "Iter 58/100 - Loss theta2: -7.110\n",
      "Iter 59/100 - Loss theta2: -7.110\n",
      "Iter 60/100 - Loss theta2: -7.110\n",
      "Iter 61/100 - Loss theta2: -7.111\n",
      "Iter 62/100 - Loss theta2: -7.111\n",
      "Iter 63/100 - Loss theta2: -7.111\n",
      "Iter 64/100 - Loss theta2: -7.111\n",
      "Iter 65/100 - Loss theta2: -7.111\n",
      "Iter 66/100 - Loss theta2: -7.111\n",
      "Iter 67/100 - Loss theta2: -7.111\n",
      "Iter 68/100 - Loss theta2: -7.111\n",
      "Iter 69/100 - Loss theta2: -7.112\n",
      "Iter 70/100 - Loss theta2: -7.112\n",
      "Iter 71/100 - Loss theta2: -7.112\n",
      "Iter 72/100 - Loss theta2: -7.111\n",
      "Iter 73/100 - Loss theta2: -7.112\n",
      "Iter 74/100 - Loss theta2: -7.112\n",
      "Iter 75/100 - Loss theta2: -7.112\n",
      "Iter 76/100 - Loss theta2: -7.113\n",
      "Iter 77/100 - Loss theta2: -7.112\n",
      "Iter 78/100 - Loss theta2: -7.113\n",
      "Iter 79/100 - Loss theta2: -7.112\n",
      "Iter 80/100 - Loss theta2: -7.112\n",
      "Iter 81/100 - Loss theta2: -7.113\n",
      "Iter 82/100 - Loss theta2: -7.113\n",
      "Iter 83/100 - Loss theta2: -7.113\n",
      "Iter 84/100 - Loss theta2: -7.113\n",
      "Iter 85/100 - Loss theta2: -7.113\n",
      "Iter 86/100 - Loss theta2: -7.114\n",
      "Iter 87/100 - Loss theta2: -7.113\n",
      "Iter 88/100 - Loss theta2: -7.113\n",
      "Iter 89/100 - Loss theta2: -7.113\n",
      "Iter 90/100 - Loss theta2: -7.114\n",
      "Iter 91/100 - Loss theta2: -7.113\n",
      "Iter 92/100 - Loss theta2: -7.113\n",
      "Iter 93/100 - Loss theta2: -7.114\n",
      "Iter 94/100 - Loss theta2: -7.114\n",
      "Iter 95/100 - Loss theta2: -7.114\n",
      "Iter 96/100 - Loss theta2: -7.114\n",
      "Iter 97/100 - Loss theta2: -7.114\n",
      "Iter 98/100 - Loss theta2: -7.115\n",
      "Iter 99/100 - Loss theta2: -7.114\n",
      "Iter 100/100 - Loss theta2: -7.115\n",
      "tensor([[0.6851],\n",
      "        [0.6855]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7180],\n",
      "        [0.7179]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: -6.346\n",
      "Iter 2/50 - Loss hyperparam: -21.161\n",
      "Iter 3/50 - Loss hyperparam: -35.108\n",
      "Iter 4/50 - Loss hyperparam: -50.448\n",
      "Iter 5/50 - Loss hyperparam: -65.920\n",
      "Iter 6/50 - Loss hyperparam: -81.163\n",
      "Iter 7/50 - Loss hyperparam: -96.494\n",
      "Iter 8/50 - Loss hyperparam: -111.015\n",
      "Iter 9/50 - Loss hyperparam: -124.124\n",
      "Iter 10/50 - Loss hyperparam: -136.674\n",
      "Iter 11/50 - Loss hyperparam: -149.297\n",
      "Iter 12/50 - Loss hyperparam: -161.615\n",
      "Iter 13/50 - Loss hyperparam: -173.813\n",
      "Iter 14/50 - Loss hyperparam: -186.604\n",
      "Iter 15/50 - Loss hyperparam: -199.602\n",
      "Iter 16/50 - Loss hyperparam: -212.579\n",
      "Iter 17/50 - Loss hyperparam: -226.204\n",
      "Iter 18/50 - Loss hyperparam: -240.042\n",
      "Iter 19/50 - Loss hyperparam: -253.941\n",
      "Iter 20/50 - Loss hyperparam: -268.467\n",
      "Iter 21/50 - Loss hyperparam: -282.907\n",
      "Iter 22/50 - Loss hyperparam: -297.748\n",
      "Iter 23/50 - Loss hyperparam: -312.728\n",
      "Iter 24/50 - Loss hyperparam: -327.779\n",
      "Iter 25/50 - Loss hyperparam: -343.056\n",
      "Iter 26/50 - Loss hyperparam: -358.262\n",
      "Iter 27/50 - Loss hyperparam: -373.546\n",
      "Iter 28/50 - Loss hyperparam: -388.811\n",
      "Iter 29/50 - Loss hyperparam: -403.773\n",
      "Iter 30/50 - Loss hyperparam: -418.879\n",
      "Iter 31/50 - Loss hyperparam: -433.661\n",
      "Iter 32/50 - Loss hyperparam: -448.249\n",
      "Iter 33/50 - Loss hyperparam: -463.118\n",
      "Iter 34/50 - Loss hyperparam: -478.337\n",
      "Iter 35/50 - Loss hyperparam: -493.694\n",
      "Iter 36/50 - Loss hyperparam: -509.016\n",
      "Iter 37/50 - Loss hyperparam: -523.919\n",
      "Iter 38/50 - Loss hyperparam: -538.265\n",
      "Iter 39/50 - Loss hyperparam: -554.035\n",
      "Iter 40/50 - Loss hyperparam: -570.090\n",
      "Iter 41/50 - Loss hyperparam: -583.885\n",
      "Iter 42/50 - Loss hyperparam: -599.346\n",
      "Iter 43/50 - Loss hyperparam: -615.302\n",
      "Iter 44/50 - Loss hyperparam: -628.859\n",
      "Iter 45/50 - Loss hyperparam: -644.083\n",
      "Iter 46/50 - Loss hyperparam: -660.565\n",
      "Iter 47/50 - Loss hyperparam: -674.527\n",
      "Iter 48/50 - Loss hyperparam: -688.022\n",
      "Iter 49/50 - Loss hyperparam: -704.229\n",
      "Iter 50/50 - Loss hyperparam: -720.321\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.091\n",
      "Iter 2/100 - Loss theta2: -7.071\n",
      "Iter 3/100 - Loss theta2: -7.090\n",
      "Iter 4/100 - Loss theta2: -7.087\n",
      "Iter 5/100 - Loss theta2: -7.080\n",
      "Iter 6/100 - Loss theta2: -7.085\n",
      "Iter 7/100 - Loss theta2: -7.091\n",
      "Iter 8/100 - Loss theta2: -7.092\n",
      "Iter 9/100 - Loss theta2: -7.088\n",
      "Iter 10/100 - Loss theta2: -7.087\n",
      "Iter 11/100 - Loss theta2: -7.088\n",
      "Iter 12/100 - Loss theta2: -7.091\n",
      "Iter 13/100 - Loss theta2: -7.092\n",
      "Iter 14/100 - Loss theta2: -7.091\n",
      "Iter 15/100 - Loss theta2: -7.090\n",
      "Iter 16/100 - Loss theta2: -7.090\n",
      "Iter 17/100 - Loss theta2: -7.092\n",
      "Iter 18/100 - Loss theta2: -7.093\n",
      "Iter 19/100 - Loss theta2: -7.092\n",
      "Iter 20/100 - Loss theta2: -7.092\n",
      "Iter 21/100 - Loss theta2: -7.091\n",
      "Iter 22/100 - Loss theta2: -7.092\n",
      "Iter 23/100 - Loss theta2: -7.092\n",
      "Iter 24/100 - Loss theta2: -7.092\n",
      "Iter 25/100 - Loss theta2: -7.093\n",
      "Iter 26/100 - Loss theta2: -7.092\n",
      "Iter 27/100 - Loss theta2: -7.092\n",
      "Iter 28/100 - Loss theta2: -7.093\n",
      "Iter 29/100 - Loss theta2: -7.093\n",
      "Iter 30/100 - Loss theta2: -7.093\n",
      "Iter 31/100 - Loss theta2: -7.093\n",
      "Iter 32/100 - Loss theta2: -7.093\n",
      "Iter 33/100 - Loss theta2: -7.094\n",
      "Iter 34/100 - Loss theta2: -7.094\n",
      "Iter 35/100 - Loss theta2: -7.093\n",
      "Iter 36/100 - Loss theta2: -7.093\n",
      "Iter 37/100 - Loss theta2: -7.093\n",
      "Iter 38/100 - Loss theta2: -7.092\n",
      "Iter 39/100 - Loss theta2: -7.094\n",
      "Iter 40/100 - Loss theta2: -7.094\n",
      "Iter 41/100 - Loss theta2: -7.093\n",
      "Iter 42/100 - Loss theta2: -7.094\n",
      "Iter 43/100 - Loss theta2: -7.093\n",
      "Iter 44/100 - Loss theta2: -7.093\n",
      "Iter 45/100 - Loss theta2: -7.094\n",
      "Iter 46/100 - Loss theta2: -7.094\n",
      "Iter 47/100 - Loss theta2: -7.093\n",
      "Iter 48/100 - Loss theta2: -7.094\n",
      "Iter 49/100 - Loss theta2: -7.093\n",
      "Iter 50/100 - Loss theta2: -7.095\n",
      "Iter 51/100 - Loss theta2: -7.094\n",
      "Iter 52/100 - Loss theta2: -7.094\n",
      "Iter 53/100 - Loss theta2: -7.094\n",
      "Iter 54/100 - Loss theta2: -7.094\n",
      "Iter 55/100 - Loss theta2: -7.094\n",
      "Iter 56/100 - Loss theta2: -7.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 57/100 - Loss theta2: -7.094\n",
      "Iter 58/100 - Loss theta2: -7.095\n",
      "Iter 59/100 - Loss theta2: -7.095\n",
      "Iter 60/100 - Loss theta2: -7.095\n",
      "Iter 61/100 - Loss theta2: -7.094\n",
      "Iter 62/100 - Loss theta2: -7.094\n",
      "Iter 63/100 - Loss theta2: -7.095\n",
      "Iter 64/100 - Loss theta2: -7.094\n",
      "Iter 65/100 - Loss theta2: -7.094\n",
      "Iter 66/100 - Loss theta2: -7.094\n",
      "Iter 67/100 - Loss theta2: -7.094\n",
      "Iter 68/100 - Loss theta2: -7.095\n",
      "Iter 69/100 - Loss theta2: -7.093\n",
      "Iter 70/100 - Loss theta2: -7.094\n",
      "Iter 71/100 - Loss theta2: -7.095\n",
      "Iter 72/100 - Loss theta2: -7.094\n",
      "Iter 73/100 - Loss theta2: -7.094\n",
      "Iter 74/100 - Loss theta2: -7.094\n",
      "Iter 75/100 - Loss theta2: -7.095\n",
      "Iter 76/100 - Loss theta2: -7.094\n",
      "Iter 77/100 - Loss theta2: -7.094\n",
      "Iter 78/100 - Loss theta2: -7.095\n",
      "Iter 79/100 - Loss theta2: -7.095\n",
      "Iter 80/100 - Loss theta2: -7.094\n",
      "Iter 81/100 - Loss theta2: -7.095\n",
      "Iter 82/100 - Loss theta2: -7.094\n",
      "Iter 83/100 - Loss theta2: -7.095\n",
      "Iter 84/100 - Loss theta2: -7.095\n",
      "Iter 85/100 - Loss theta2: -7.094\n",
      "Iter 86/100 - Loss theta2: -7.095\n",
      "Iter 87/100 - Loss theta2: -7.095\n",
      "Iter 88/100 - Loss theta2: -7.095\n",
      "Iter 89/100 - Loss theta2: -7.094\n",
      "Iter 90/100 - Loss theta2: -7.095\n",
      "Iter 91/100 - Loss theta2: -7.095\n",
      "Iter 92/100 - Loss theta2: -7.094\n",
      "Iter 93/100 - Loss theta2: -7.095\n",
      "Iter 94/100 - Loss theta2: -7.095\n",
      "Iter 95/100 - Loss theta2: -7.095\n",
      "Iter 96/100 - Loss theta2: -7.094\n",
      "Iter 97/100 - Loss theta2: -7.094\n",
      "Iter 98/100 - Loss theta2: -7.095\n",
      "Iter 99/100 - Loss theta2: -7.094\n",
      "Iter 100/100 - Loss theta2: -7.095\n",
      "tensor([[0.6854],\n",
      "        [0.6853]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7186],\n",
      "        [0.7187]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 68.996\n",
      "Iter 2/50 - Loss hyperparam: 48.042\n",
      "Iter 3/50 - Loss hyperparam: 25.776\n",
      "Iter 4/50 - Loss hyperparam: 2.639\n",
      "Iter 5/50 - Loss hyperparam: -20.539\n",
      "Iter 6/50 - Loss hyperparam: -43.710\n",
      "Iter 7/50 - Loss hyperparam: -67.494\n",
      "Iter 8/50 - Loss hyperparam: -91.786\n",
      "Iter 9/50 - Loss hyperparam: -115.852\n",
      "Iter 10/50 - Loss hyperparam: -138.264\n",
      "Iter 11/50 - Loss hyperparam: -157.233\n",
      "Iter 12/50 - Loss hyperparam: -172.330\n",
      "Iter 13/50 - Loss hyperparam: -184.271\n",
      "Iter 14/50 - Loss hyperparam: -193.825\n",
      "Iter 15/50 - Loss hyperparam: -202.529\n",
      "Iter 16/50 - Loss hyperparam: -212.187\n",
      "Iter 17/50 - Loss hyperparam: -224.386\n",
      "Iter 18/50 - Loss hyperparam: -240.155\n",
      "Iter 19/50 - Loss hyperparam: -259.395\n",
      "Iter 20/50 - Loss hyperparam: -280.745\n",
      "Iter 21/50 - Loss hyperparam: -301.933\n",
      "Iter 22/50 - Loss hyperparam: -321.222\n",
      "Iter 23/50 - Loss hyperparam: -337.618\n",
      "Iter 24/50 - Loss hyperparam: -350.354\n",
      "Iter 25/50 - Loss hyperparam: -361.633\n",
      "Iter 26/50 - Loss hyperparam: -375.835\n",
      "Iter 27/50 - Loss hyperparam: -394.654\n",
      "Iter 28/50 - Loss hyperparam: -415.786\n",
      "Iter 29/50 - Loss hyperparam: -436.070\n",
      "Iter 30/50 - Loss hyperparam: -453.041\n",
      "Iter 31/50 - Loss hyperparam: -464.748\n",
      "Iter 32/50 - Loss hyperparam: -477.403\n",
      "Iter 33/50 - Loss hyperparam: -495.821\n",
      "Iter 34/50 - Loss hyperparam: -516.855\n",
      "Iter 35/50 - Loss hyperparam: -536.572\n",
      "Iter 36/50 - Loss hyperparam: -549.326\n",
      "Iter 37/50 - Loss hyperparam: -563.294\n",
      "Iter 38/50 - Loss hyperparam: -582.455\n",
      "Iter 39/50 - Loss hyperparam: -603.421\n",
      "Iter 40/50 - Loss hyperparam: -616.666\n",
      "Iter 41/50 - Loss hyperparam: -630.770\n",
      "Iter 42/50 - Loss hyperparam: -650.625\n",
      "Iter 43/50 - Loss hyperparam: -668.543\n",
      "Iter 44/50 - Loss hyperparam: -681.412\n",
      "Iter 45/50 - Loss hyperparam: -700.137\n",
      "Iter 46/50 - Loss hyperparam: -717.904\n",
      "Iter 47/50 - Loss hyperparam: -730.125\n",
      "Iter 48/50 - Loss hyperparam: -751.197\n",
      "Iter 49/50 - Loss hyperparam: -766.426\n",
      "Iter 50/50 - Loss hyperparam: -780.800\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.175\n",
      "Iter 2/100 - Loss theta2: -7.145\n",
      "Iter 3/100 - Loss theta2: -7.174\n",
      "Iter 4/100 - Loss theta2: -7.169\n",
      "Iter 5/100 - Loss theta2: -7.158\n",
      "Iter 6/100 - Loss theta2: -7.162\n",
      "Iter 7/100 - Loss theta2: -7.172\n",
      "Iter 8/100 - Loss theta2: -7.175\n",
      "Iter 9/100 - Loss theta2: -7.172\n",
      "Iter 10/100 - Loss theta2: -7.167\n",
      "Iter 11/100 - Loss theta2: -7.168\n",
      "Iter 12/100 - Loss theta2: -7.172\n",
      "Iter 13/100 - Loss theta2: -7.174\n",
      "Iter 14/100 - Loss theta2: -7.175\n",
      "Iter 15/100 - Loss theta2: -7.173\n",
      "Iter 16/100 - Loss theta2: -7.172\n",
      "Iter 17/100 - Loss theta2: -7.172\n",
      "Iter 18/100 - Loss theta2: -7.174\n",
      "Iter 19/100 - Loss theta2: -7.175\n",
      "Iter 20/100 - Loss theta2: -7.176\n",
      "Iter 21/100 - Loss theta2: -7.175\n",
      "Iter 22/100 - Loss theta2: -7.173\n",
      "Iter 23/100 - Loss theta2: -7.174\n",
      "Iter 24/100 - Loss theta2: -7.175\n",
      "Iter 25/100 - Loss theta2: -7.177\n",
      "Iter 26/100 - Loss theta2: -7.175\n",
      "Iter 27/100 - Loss theta2: -7.175\n",
      "Iter 28/100 - Loss theta2: -7.175\n",
      "Iter 29/100 - Loss theta2: -7.176\n",
      "Iter 30/100 - Loss theta2: -7.176\n",
      "Iter 31/100 - Loss theta2: -7.177\n",
      "Iter 32/100 - Loss theta2: -7.176\n",
      "Iter 33/100 - Loss theta2: -7.176\n",
      "Iter 34/100 - Loss theta2: -7.176\n",
      "Iter 35/100 - Loss theta2: -7.177\n",
      "Iter 36/100 - Loss theta2: -7.177\n",
      "Iter 37/100 - Loss theta2: -7.178\n",
      "Iter 38/100 - Loss theta2: -7.177\n",
      "Iter 39/100 - Loss theta2: -7.176\n",
      "Iter 40/100 - Loss theta2: -7.177\n",
      "Iter 41/100 - Loss theta2: -7.176\n",
      "Iter 42/100 - Loss theta2: -7.177\n",
      "Iter 43/100 - Loss theta2: -7.177\n",
      "Iter 44/100 - Loss theta2: -7.177\n",
      "Iter 45/100 - Loss theta2: -7.178\n",
      "Iter 46/100 - Loss theta2: -7.178\n",
      "Iter 47/100 - Loss theta2: -7.178\n",
      "Iter 48/100 - Loss theta2: -7.178\n",
      "Iter 49/100 - Loss theta2: -7.177\n",
      "Iter 50/100 - Loss theta2: -7.178\n",
      "Iter 51/100 - Loss theta2: -7.178\n",
      "Iter 52/100 - Loss theta2: -7.178\n",
      "Iter 53/100 - Loss theta2: -7.177\n",
      "Iter 54/100 - Loss theta2: -7.178\n",
      "Iter 55/100 - Loss theta2: -7.177\n",
      "Iter 56/100 - Loss theta2: -7.178\n",
      "Iter 57/100 - Loss theta2: -7.178\n",
      "Iter 58/100 - Loss theta2: -7.178\n",
      "Iter 59/100 - Loss theta2: -7.178\n",
      "Iter 60/100 - Loss theta2: -7.179\n",
      "Iter 61/100 - Loss theta2: -7.178\n",
      "Iter 62/100 - Loss theta2: -7.178\n",
      "Iter 63/100 - Loss theta2: -7.178\n",
      "Iter 64/100 - Loss theta2: -7.179\n",
      "Iter 65/100 - Loss theta2: -7.178\n",
      "Iter 66/100 - Loss theta2: -7.178\n",
      "Iter 67/100 - Loss theta2: -7.177\n",
      "Iter 68/100 - Loss theta2: -7.178\n",
      "Iter 69/100 - Loss theta2: -7.178\n",
      "Iter 70/100 - Loss theta2: -7.179\n",
      "Iter 71/100 - Loss theta2: -7.179\n",
      "Iter 72/100 - Loss theta2: -7.179\n",
      "Iter 73/100 - Loss theta2: -7.179\n",
      "Iter 74/100 - Loss theta2: -7.179\n",
      "Iter 75/100 - Loss theta2: -7.179\n",
      "Iter 76/100 - Loss theta2: -7.178\n",
      "Iter 77/100 - Loss theta2: -7.178\n",
      "Iter 78/100 - Loss theta2: -7.179\n",
      "Iter 79/100 - Loss theta2: -7.179\n",
      "Iter 80/100 - Loss theta2: -7.178\n",
      "Iter 81/100 - Loss theta2: -7.179\n",
      "Iter 82/100 - Loss theta2: -7.179\n",
      "Iter 83/100 - Loss theta2: -7.179\n",
      "Iter 84/100 - Loss theta2: -7.179\n",
      "Iter 85/100 - Loss theta2: -7.178\n",
      "Iter 86/100 - Loss theta2: -7.179\n",
      "Iter 87/100 - Loss theta2: -7.179\n",
      "Iter 88/100 - Loss theta2: -7.179\n",
      "Iter 89/100 - Loss theta2: -7.179\n",
      "Iter 90/100 - Loss theta2: -7.179\n",
      "Iter 91/100 - Loss theta2: -7.180\n",
      "Iter 92/100 - Loss theta2: -7.179\n",
      "Iter 93/100 - Loss theta2: -7.179\n",
      "Iter 94/100 - Loss theta2: -7.179\n",
      "Iter 95/100 - Loss theta2: -7.180\n",
      "Iter 96/100 - Loss theta2: -7.180\n",
      "Iter 97/100 - Loss theta2: -7.179\n",
      "Iter 98/100 - Loss theta2: -7.180\n",
      "Iter 99/100 - Loss theta2: -7.179\n",
      "Iter 100/100 - Loss theta2: -7.180\n",
      "tensor([[0.6855],\n",
      "        [0.6861]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7172]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 56.430\n",
      "Iter 2/50 - Loss hyperparam: 35.553\n",
      "Iter 3/50 - Loss hyperparam: 15.545\n",
      "Iter 4/50 - Loss hyperparam: -4.408\n",
      "Iter 5/50 - Loss hyperparam: -25.703\n",
      "Iter 6/50 - Loss hyperparam: -48.731\n",
      "Iter 7/50 - Loss hyperparam: -73.304\n",
      "Iter 8/50 - Loss hyperparam: -98.829\n",
      "Iter 9/50 - Loss hyperparam: -124.422\n",
      "Iter 10/50 - Loss hyperparam: -148.134\n",
      "Iter 11/50 - Loss hyperparam: -167.780\n",
      "Iter 12/50 - Loss hyperparam: -182.728\n",
      "Iter 13/50 - Loss hyperparam: -193.779\n",
      "Iter 14/50 - Loss hyperparam: -202.440\n",
      "Iter 15/50 - Loss hyperparam: -211.364\n",
      "Iter 16/50 - Loss hyperparam: -223.001\n",
      "Iter 17/50 - Loss hyperparam: -238.032\n",
      "Iter 18/50 - Loss hyperparam: -255.952\n",
      "Iter 19/50 - Loss hyperparam: -276.354\n",
      "Iter 20/50 - Loss hyperparam: -298.544\n",
      "Iter 21/50 - Loss hyperparam: -319.986\n",
      "Iter 22/50 - Loss hyperparam: -338.303\n",
      "Iter 23/50 - Loss hyperparam: -352.935\n",
      "Iter 24/50 - Loss hyperparam: -365.210\n",
      "Iter 25/50 - Loss hyperparam: -379.347\n",
      "Iter 26/50 - Loss hyperparam: -397.056\n",
      "Iter 27/50 - Loss hyperparam: -418.063\n",
      "Iter 28/50 - Loss hyperparam: -440.409\n",
      "Iter 29/50 - Loss hyperparam: -459.491\n",
      "Iter 30/50 - Loss hyperparam: -473.597\n",
      "Iter 31/50 - Loss hyperparam: -486.026\n",
      "Iter 32/50 - Loss hyperparam: -502.471\n",
      "Iter 33/50 - Loss hyperparam: -523.855\n",
      "Iter 34/50 - Loss hyperparam: -545.328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35/50 - Loss hyperparam: -561.605\n",
      "Iter 36/50 - Loss hyperparam: -575.182\n",
      "Iter 37/50 - Loss hyperparam: -593.116\n",
      "Iter 38/50 - Loss hyperparam: -614.807\n",
      "Iter 39/50 - Loss hyperparam: -632.187\n",
      "Iter 40/50 - Loss hyperparam: -645.479\n",
      "Iter 41/50 - Loss hyperparam: -664.327\n",
      "Iter 42/50 - Loss hyperparam: -685.399\n",
      "Iter 43/50 - Loss hyperparam: -698.695\n",
      "Iter 44/50 - Loss hyperparam: -714.301\n",
      "Iter 45/50 - Loss hyperparam: -737.303\n",
      "Iter 46/50 - Loss hyperparam: -751.845\n",
      "Iter 47/50 - Loss hyperparam: -767.688\n",
      "Iter 48/50 - Loss hyperparam: -787.750\n",
      "Iter 49/50 - Loss hyperparam: -802.477\n",
      "Iter 50/50 - Loss hyperparam: -823.196\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.157\n",
      "Iter 2/100 - Loss theta2: -7.151\n",
      "Iter 3/100 - Loss theta2: -7.158\n",
      "Iter 4/100 - Loss theta2: -7.162\n",
      "Iter 5/100 - Loss theta2: -7.157\n",
      "Iter 6/100 - Loss theta2: -7.157\n",
      "Iter 7/100 - Loss theta2: -7.161\n",
      "Iter 8/100 - Loss theta2: -7.162\n",
      "Iter 9/100 - Loss theta2: -7.159\n",
      "Iter 10/100 - Loss theta2: -7.159\n",
      "Iter 11/100 - Loss theta2: -7.161\n",
      "Iter 12/100 - Loss theta2: -7.163\n",
      "Iter 13/100 - Loss theta2: -7.162\n",
      "Iter 14/100 - Loss theta2: -7.161\n",
      "Iter 15/100 - Loss theta2: -7.161\n",
      "Iter 16/100 - Loss theta2: -7.162\n",
      "Iter 17/100 - Loss theta2: -7.162\n",
      "Iter 18/100 - Loss theta2: -7.161\n",
      "Iter 19/100 - Loss theta2: -7.161\n",
      "Iter 20/100 - Loss theta2: -7.162\n",
      "Iter 21/100 - Loss theta2: -7.163\n",
      "Iter 22/100 - Loss theta2: -7.163\n",
      "Iter 23/100 - Loss theta2: -7.162\n",
      "Iter 24/100 - Loss theta2: -7.162\n",
      "Iter 25/100 - Loss theta2: -7.164\n",
      "Iter 26/100 - Loss theta2: -7.163\n",
      "Iter 27/100 - Loss theta2: -7.162\n",
      "Iter 28/100 - Loss theta2: -7.163\n",
      "Iter 29/100 - Loss theta2: -7.163\n",
      "Iter 30/100 - Loss theta2: -7.163\n",
      "Iter 31/100 - Loss theta2: -7.163\n",
      "Iter 32/100 - Loss theta2: -7.163\n",
      "Iter 33/100 - Loss theta2: -7.163\n",
      "Iter 34/100 - Loss theta2: -7.163\n",
      "Iter 35/100 - Loss theta2: -7.162\n",
      "Iter 36/100 - Loss theta2: -7.163\n",
      "Iter 37/100 - Loss theta2: -7.163\n",
      "Iter 38/100 - Loss theta2: -7.164\n",
      "Iter 39/100 - Loss theta2: -7.163\n",
      "Iter 40/100 - Loss theta2: -7.163\n",
      "Iter 41/100 - Loss theta2: -7.163\n",
      "Iter 42/100 - Loss theta2: -7.163\n",
      "Iter 43/100 - Loss theta2: -7.163\n",
      "Iter 44/100 - Loss theta2: -7.164\n",
      "Iter 45/100 - Loss theta2: -7.162\n",
      "Iter 46/100 - Loss theta2: -7.163\n",
      "Iter 47/100 - Loss theta2: -7.162\n",
      "Iter 48/100 - Loss theta2: -7.163\n",
      "Iter 49/100 - Loss theta2: -7.163\n",
      "Iter 50/100 - Loss theta2: -7.163\n",
      "Iter 51/100 - Loss theta2: -7.164\n",
      "Iter 52/100 - Loss theta2: -7.163\n",
      "Iter 53/100 - Loss theta2: -7.163\n",
      "Iter 54/100 - Loss theta2: -7.163\n",
      "Iter 55/100 - Loss theta2: -7.163\n",
      "Iter 56/100 - Loss theta2: -7.163\n",
      "Iter 57/100 - Loss theta2: -7.163\n",
      "Iter 58/100 - Loss theta2: -7.163\n",
      "Iter 59/100 - Loss theta2: -7.163\n",
      "Iter 60/100 - Loss theta2: -7.163\n",
      "Iter 61/100 - Loss theta2: -7.163\n",
      "Iter 62/100 - Loss theta2: -7.164\n",
      "Iter 63/100 - Loss theta2: -7.163\n",
      "Iter 64/100 - Loss theta2: -7.163\n",
      "Iter 65/100 - Loss theta2: -7.163\n",
      "Iter 66/100 - Loss theta2: -7.163\n",
      "Iter 67/100 - Loss theta2: -7.163\n",
      "Iter 68/100 - Loss theta2: -7.163\n",
      "Iter 69/100 - Loss theta2: -7.163\n",
      "Iter 70/100 - Loss theta2: -7.163\n",
      "Iter 71/100 - Loss theta2: -7.163\n",
      "Iter 72/100 - Loss theta2: -7.163\n",
      "Iter 73/100 - Loss theta2: -7.164\n",
      "Iter 74/100 - Loss theta2: -7.162\n",
      "Iter 75/100 - Loss theta2: -7.163\n",
      "Iter 76/100 - Loss theta2: -7.164\n",
      "Iter 77/100 - Loss theta2: -7.163\n",
      "Iter 78/100 - Loss theta2: -7.164\n",
      "Iter 79/100 - Loss theta2: -7.163\n",
      "Iter 80/100 - Loss theta2: -7.164\n",
      "Iter 81/100 - Loss theta2: -7.164\n",
      "Iter 82/100 - Loss theta2: -7.163\n",
      "Iter 83/100 - Loss theta2: -7.164\n",
      "Iter 84/100 - Loss theta2: -7.163\n",
      "Iter 85/100 - Loss theta2: -7.164\n",
      "Iter 86/100 - Loss theta2: -7.163\n",
      "Iter 87/100 - Loss theta2: -7.164\n",
      "Iter 88/100 - Loss theta2: -7.163\n",
      "Iter 89/100 - Loss theta2: -7.164\n",
      "Iter 90/100 - Loss theta2: -7.163\n",
      "Iter 91/100 - Loss theta2: -7.164\n",
      "Iter 92/100 - Loss theta2: -7.163\n",
      "Iter 93/100 - Loss theta2: -7.164\n",
      "Iter 94/100 - Loss theta2: -7.164\n",
      "Iter 95/100 - Loss theta2: -7.163\n",
      "Iter 96/100 - Loss theta2: -7.164\n",
      "Iter 97/100 - Loss theta2: -7.163\n",
      "Iter 98/100 - Loss theta2: -7.163\n",
      "Iter 99/100 - Loss theta2: -7.163\n",
      "Iter 100/100 - Loss theta2: -7.163\n",
      "tensor([[0.6851],\n",
      "        [0.6860]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7171],\n",
      "        [0.7170]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1241]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 139.149\n",
      "Iter 2/50 - Loss hyperparam: 117.584\n",
      "Iter 3/50 - Loss hyperparam: 96.133\n",
      "Iter 4/50 - Loss hyperparam: 74.754\n",
      "Iter 5/50 - Loss hyperparam: 53.012\n",
      "Iter 6/50 - Loss hyperparam: 30.250\n",
      "Iter 7/50 - Loss hyperparam: 5.975\n",
      "Iter 8/50 - Loss hyperparam: -20.257\n",
      "Iter 9/50 - Loss hyperparam: -48.620\n",
      "Iter 10/50 - Loss hyperparam: -78.710\n",
      "Iter 11/50 - Loss hyperparam: -109.788\n",
      "Iter 12/50 - Loss hyperparam: -141.059\n",
      "Iter 13/50 - Loss hyperparam: -171.224\n",
      "Iter 14/50 - Loss hyperparam: -198.083\n",
      "Iter 15/50 - Loss hyperparam: -220.083\n",
      "Iter 16/50 - Loss hyperparam: -238.068\n",
      "Iter 17/50 - Loss hyperparam: -252.772\n",
      "Iter 18/50 - Loss hyperparam: -264.563\n",
      "Iter 19/50 - Loss hyperparam: -275.540\n",
      "Iter 20/50 - Loss hyperparam: -288.227\n",
      "Iter 21/50 - Loss hyperparam: -303.686\n",
      "Iter 22/50 - Loss hyperparam: -321.833\n",
      "Iter 23/50 - Loss hyperparam: -342.619\n",
      "Iter 24/50 - Loss hyperparam: -366.677\n",
      "Iter 25/50 - Loss hyperparam: -394.205\n",
      "Iter 26/50 - Loss hyperparam: -422.233\n",
      "Iter 27/50 - Loss hyperparam: -443.496\n",
      "Iter 28/50 - Loss hyperparam: -454.225\n",
      "Iter 29/50 - Loss hyperparam: -461.983\n",
      "Iter 30/50 - Loss hyperparam: -475.949\n",
      "Iter 31/50 - Loss hyperparam: -499.744\n",
      "Iter 32/50 - Loss hyperparam: -530.851\n",
      "Iter 33/50 - Loss hyperparam: -557.277\n",
      "Iter 34/50 - Loss hyperparam: -567.935\n",
      "Iter 35/50 - Loss hyperparam: -577.217\n",
      "Iter 36/50 - Loss hyperparam: -597.596\n",
      "Iter 37/50 - Loss hyperparam: -625.881\n",
      "Iter 38/50 - Loss hyperparam: -648.571\n",
      "Iter 39/50 - Loss hyperparam: -662.597\n",
      "Iter 40/50 - Loss hyperparam: -677.394\n",
      "Iter 41/50 - Loss hyperparam: -697.614\n",
      "Iter 42/50 - Loss hyperparam: -722.666\n",
      "Iter 43/50 - Loss hyperparam: -741.670\n",
      "Iter 44/50 - Loss hyperparam: -751.718\n",
      "Iter 45/50 - Loss hyperparam: -776.734\n",
      "Iter 46/50 - Loss hyperparam: -800.869\n",
      "Iter 47/50 - Loss hyperparam: -808.999\n",
      "Iter 48/50 - Loss hyperparam: -835.213\n",
      "Iter 49/50 - Loss hyperparam: -852.812\n",
      "Iter 50/50 - Loss hyperparam: -867.893\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.330\n",
      "Iter 2/100 - Loss theta2: -7.297\n",
      "Iter 3/100 - Loss theta2: -7.330\n",
      "Iter 4/100 - Loss theta2: -7.324\n",
      "Iter 5/100 - Loss theta2: -7.313\n",
      "Iter 6/100 - Loss theta2: -7.318\n",
      "Iter 7/100 - Loss theta2: -7.326\n",
      "Iter 8/100 - Loss theta2: -7.333\n",
      "Iter 9/100 - Loss theta2: -7.327\n",
      "Iter 10/100 - Loss theta2: -7.323\n",
      "Iter 11/100 - Loss theta2: -7.324\n",
      "Iter 12/100 - Loss theta2: -7.328\n",
      "Iter 13/100 - Loss theta2: -7.333\n",
      "Iter 14/100 - Loss theta2: -7.332\n",
      "Iter 15/100 - Loss theta2: -7.329\n",
      "Iter 16/100 - Loss theta2: -7.329\n",
      "Iter 17/100 - Loss theta2: -7.331\n",
      "Iter 18/100 - Loss theta2: -7.332\n",
      "Iter 19/100 - Loss theta2: -7.334\n",
      "Iter 20/100 - Loss theta2: -7.334\n",
      "Iter 21/100 - Loss theta2: -7.334\n",
      "Iter 22/100 - Loss theta2: -7.334\n",
      "Iter 23/100 - Loss theta2: -7.334\n",
      "Iter 24/100 - Loss theta2: -7.335\n",
      "Iter 25/100 - Loss theta2: -7.336\n",
      "Iter 26/100 - Loss theta2: -7.335\n",
      "Iter 27/100 - Loss theta2: -7.336\n",
      "Iter 28/100 - Loss theta2: -7.334\n",
      "Iter 29/100 - Loss theta2: -7.335\n",
      "Iter 30/100 - Loss theta2: -7.336\n",
      "Iter 31/100 - Loss theta2: -7.335\n",
      "Iter 32/100 - Loss theta2: -7.337\n",
      "Iter 33/100 - Loss theta2: -7.337\n",
      "Iter 34/100 - Loss theta2: -7.337\n",
      "Iter 35/100 - Loss theta2: -7.335\n",
      "Iter 36/100 - Loss theta2: -7.337\n",
      "Iter 37/100 - Loss theta2: -7.337\n",
      "Iter 38/100 - Loss theta2: -7.336\n",
      "Iter 39/100 - Loss theta2: -7.336\n",
      "Iter 40/100 - Loss theta2: -7.338\n",
      "Iter 41/100 - Loss theta2: -7.338\n",
      "Iter 42/100 - Loss theta2: -7.337\n",
      "Iter 43/100 - Loss theta2: -7.338\n",
      "Iter 44/100 - Loss theta2: -7.339\n",
      "Iter 45/100 - Loss theta2: -7.338\n",
      "Iter 46/100 - Loss theta2: -7.339\n",
      "Iter 47/100 - Loss theta2: -7.338\n",
      "Iter 48/100 - Loss theta2: -7.339\n",
      "Iter 49/100 - Loss theta2: -7.339\n",
      "Iter 50/100 - Loss theta2: -7.339\n",
      "Iter 51/100 - Loss theta2: -7.339\n",
      "Iter 52/100 - Loss theta2: -7.339\n",
      "Iter 53/100 - Loss theta2: -7.339\n",
      "Iter 54/100 - Loss theta2: -7.339\n",
      "Iter 55/100 - Loss theta2: -7.340\n",
      "Iter 56/100 - Loss theta2: -7.338\n",
      "Iter 57/100 - Loss theta2: -7.339\n",
      "Iter 58/100 - Loss theta2: -7.341\n",
      "Iter 59/100 - Loss theta2: -7.339\n",
      "Iter 60/100 - Loss theta2: -7.340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 61/100 - Loss theta2: -7.341\n",
      "Iter 62/100 - Loss theta2: -7.340\n",
      "Iter 63/100 - Loss theta2: -7.341\n",
      "Iter 64/100 - Loss theta2: -7.341\n",
      "Iter 65/100 - Loss theta2: -7.342\n",
      "Iter 66/100 - Loss theta2: -7.340\n",
      "Iter 67/100 - Loss theta2: -7.341\n",
      "Iter 68/100 - Loss theta2: -7.341\n",
      "Iter 69/100 - Loss theta2: -7.339\n",
      "Iter 70/100 - Loss theta2: -7.340\n",
      "Iter 71/100 - Loss theta2: -7.339\n",
      "Iter 72/100 - Loss theta2: -7.341\n",
      "Iter 73/100 - Loss theta2: -7.342\n",
      "Iter 74/100 - Loss theta2: -7.341\n",
      "Iter 75/100 - Loss theta2: -7.341\n",
      "Iter 76/100 - Loss theta2: -7.341\n",
      "Iter 77/100 - Loss theta2: -7.341\n",
      "Iter 78/100 - Loss theta2: -7.342\n",
      "Iter 79/100 - Loss theta2: -7.342\n",
      "Iter 80/100 - Loss theta2: -7.341\n",
      "Iter 81/100 - Loss theta2: -7.341\n",
      "Iter 82/100 - Loss theta2: -7.341\n",
      "Iter 83/100 - Loss theta2: -7.342\n",
      "Iter 84/100 - Loss theta2: -7.343\n",
      "Iter 85/100 - Loss theta2: -7.343\n",
      "Iter 86/100 - Loss theta2: -7.342\n",
      "Iter 87/100 - Loss theta2: -7.342\n",
      "Iter 88/100 - Loss theta2: -7.342\n",
      "Iter 89/100 - Loss theta2: -7.343\n",
      "Iter 90/100 - Loss theta2: -7.341\n",
      "Iter 91/100 - Loss theta2: -7.341\n",
      "Iter 92/100 - Loss theta2: -7.342\n",
      "Iter 93/100 - Loss theta2: -7.342\n",
      "Iter 94/100 - Loss theta2: -7.341\n",
      "Iter 95/100 - Loss theta2: -7.343\n",
      "Iter 96/100 - Loss theta2: -7.342\n",
      "Iter 97/100 - Loss theta2: -7.342\n",
      "Iter 98/100 - Loss theta2: -7.341\n",
      "Iter 99/100 - Loss theta2: -7.343\n",
      "Iter 100/100 - Loss theta2: -7.341\n",
      "tensor([[0.6877],\n",
      "        [0.6880]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7171],\n",
      "        [0.7171]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1241]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 109.731\n",
      "Iter 2/50 - Loss hyperparam: 88.439\n",
      "Iter 3/50 - Loss hyperparam: 65.523\n",
      "Iter 4/50 - Loss hyperparam: 41.298\n",
      "Iter 5/50 - Loss hyperparam: 16.166\n",
      "Iter 6/50 - Loss hyperparam: -9.700\n",
      "Iter 7/50 - Loss hyperparam: -35.923\n",
      "Iter 8/50 - Loss hyperparam: -61.758\n",
      "Iter 9/50 - Loss hyperparam: -86.927\n",
      "Iter 10/50 - Loss hyperparam: -112.396\n",
      "Iter 11/50 - Loss hyperparam: -139.501\n",
      "Iter 12/50 - Loss hyperparam: -168.552\n",
      "Iter 13/50 - Loss hyperparam: -198.405\n",
      "Iter 14/50 - Loss hyperparam: -225.993\n",
      "Iter 15/50 - Loss hyperparam: -246.840\n",
      "Iter 16/50 - Loss hyperparam: -259.671\n",
      "Iter 17/50 - Loss hyperparam: -268.526\n",
      "Iter 18/50 - Loss hyperparam: -277.585\n",
      "Iter 19/50 - Loss hyperparam: -289.159\n",
      "Iter 20/50 - Loss hyperparam: -304.004\n",
      "Iter 21/50 - Loss hyperparam: -322.496\n",
      "Iter 22/50 - Loss hyperparam: -345.404\n",
      "Iter 23/50 - Loss hyperparam: -372.902\n",
      "Iter 24/50 - Loss hyperparam: -402.544\n",
      "Iter 25/50 - Loss hyperparam: -428.346\n",
      "Iter 26/50 - Loss hyperparam: -445.872\n",
      "Iter 27/50 - Loss hyperparam: -458.174\n",
      "Iter 28/50 - Loss hyperparam: -470.114\n",
      "Iter 29/50 - Loss hyperparam: -485.289\n",
      "Iter 30/50 - Loss hyperparam: -506.803\n",
      "Iter 31/50 - Loss hyperparam: -534.705\n",
      "Iter 32/50 - Loss hyperparam: -561.140\n",
      "Iter 33/50 - Loss hyperparam: -578.688\n",
      "Iter 34/50 - Loss hyperparam: -591.389\n",
      "Iter 35/50 - Loss hyperparam: -605.950\n",
      "Iter 36/50 - Loss hyperparam: -629.294\n",
      "Iter 37/50 - Loss hyperparam: -657.693\n",
      "Iter 38/50 - Loss hyperparam: -677.397\n",
      "Iter 39/50 - Loss hyperparam: -689.899\n",
      "Iter 40/50 - Loss hyperparam: -705.674\n",
      "Iter 41/50 - Loss hyperparam: -732.965\n",
      "Iter 42/50 - Loss hyperparam: -755.198\n",
      "Iter 43/50 - Loss hyperparam: -768.081\n",
      "Iter 44/50 - Loss hyperparam: -786.146\n",
      "Iter 45/50 - Loss hyperparam: -812.935\n",
      "Iter 46/50 - Loss hyperparam: -830.165\n",
      "Iter 47/50 - Loss hyperparam: -844.464\n",
      "Iter 48/50 - Loss hyperparam: -870.129\n",
      "Iter 49/50 - Loss hyperparam: -885.964\n",
      "Iter 50/50 - Loss hyperparam: -903.673\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.334\n",
      "Iter 2/100 - Loss theta2: -7.317\n",
      "Iter 3/100 - Loss theta2: -7.334\n",
      "Iter 4/100 - Loss theta2: -7.334\n",
      "Iter 5/100 - Loss theta2: -7.328\n",
      "Iter 6/100 - Loss theta2: -7.331\n",
      "Iter 7/100 - Loss theta2: -7.336\n",
      "Iter 8/100 - Loss theta2: -7.336\n",
      "Iter 9/100 - Loss theta2: -7.331\n",
      "Iter 10/100 - Loss theta2: -7.332\n",
      "Iter 11/100 - Loss theta2: -7.335\n",
      "Iter 12/100 - Loss theta2: -7.337\n",
      "Iter 13/100 - Loss theta2: -7.335\n",
      "Iter 14/100 - Loss theta2: -7.334\n",
      "Iter 15/100 - Loss theta2: -7.335\n",
      "Iter 16/100 - Loss theta2: -7.337\n",
      "Iter 17/100 - Loss theta2: -7.338\n",
      "Iter 18/100 - Loss theta2: -7.338\n",
      "Iter 19/100 - Loss theta2: -7.336\n",
      "Iter 20/100 - Loss theta2: -7.337\n",
      "Iter 21/100 - Loss theta2: -7.338\n",
      "Iter 22/100 - Loss theta2: -7.338\n",
      "Iter 23/100 - Loss theta2: -7.338\n",
      "Iter 24/100 - Loss theta2: -7.338\n",
      "Iter 25/100 - Loss theta2: -7.338\n",
      "Iter 26/100 - Loss theta2: -7.338\n",
      "Iter 27/100 - Loss theta2: -7.339\n",
      "Iter 28/100 - Loss theta2: -7.339\n",
      "Iter 29/100 - Loss theta2: -7.338\n",
      "Iter 30/100 - Loss theta2: -7.338\n",
      "Iter 31/100 - Loss theta2: -7.338\n",
      "Iter 32/100 - Loss theta2: -7.340\n",
      "Iter 33/100 - Loss theta2: -7.338\n",
      "Iter 34/100 - Loss theta2: -7.339\n",
      "Iter 35/100 - Loss theta2: -7.339\n",
      "Iter 36/100 - Loss theta2: -7.339\n",
      "Iter 37/100 - Loss theta2: -7.338\n",
      "Iter 38/100 - Loss theta2: -7.339\n",
      "Iter 39/100 - Loss theta2: -7.339\n",
      "Iter 40/100 - Loss theta2: -7.339\n",
      "Iter 41/100 - Loss theta2: -7.340\n",
      "Iter 42/100 - Loss theta2: -7.339\n",
      "Iter 43/100 - Loss theta2: -7.340\n",
      "Iter 44/100 - Loss theta2: -7.338\n",
      "Iter 45/100 - Loss theta2: -7.340\n",
      "Iter 46/100 - Loss theta2: -7.339\n",
      "Iter 47/100 - Loss theta2: -7.339\n",
      "Iter 48/100 - Loss theta2: -7.338\n",
      "Iter 49/100 - Loss theta2: -7.339\n",
      "Iter 50/100 - Loss theta2: -7.339\n",
      "Iter 51/100 - Loss theta2: -7.339\n",
      "Iter 52/100 - Loss theta2: -7.340\n",
      "Iter 53/100 - Loss theta2: -7.340\n",
      "Iter 54/100 - Loss theta2: -7.339\n",
      "Iter 55/100 - Loss theta2: -7.339\n",
      "Iter 56/100 - Loss theta2: -7.338\n",
      "Iter 57/100 - Loss theta2: -7.339\n",
      "Iter 58/100 - Loss theta2: -7.339\n",
      "Iter 59/100 - Loss theta2: -7.341\n",
      "Iter 60/100 - Loss theta2: -7.340\n",
      "Iter 61/100 - Loss theta2: -7.339\n",
      "Iter 62/100 - Loss theta2: -7.340\n",
      "Iter 63/100 - Loss theta2: -7.340\n",
      "Iter 64/100 - Loss theta2: -7.339\n",
      "Iter 65/100 - Loss theta2: -7.340\n",
      "Iter 66/100 - Loss theta2: -7.339\n",
      "Iter 67/100 - Loss theta2: -7.341\n",
      "Iter 68/100 - Loss theta2: -7.340\n",
      "Iter 69/100 - Loss theta2: -7.340\n",
      "Iter 70/100 - Loss theta2: -7.340\n",
      "Iter 71/100 - Loss theta2: -7.340\n",
      "Iter 72/100 - Loss theta2: -7.341\n",
      "Iter 73/100 - Loss theta2: -7.340\n",
      "Iter 74/100 - Loss theta2: -7.340\n",
      "Iter 75/100 - Loss theta2: -7.339\n",
      "Iter 76/100 - Loss theta2: -7.340\n",
      "Iter 77/100 - Loss theta2: -7.339\n",
      "Iter 78/100 - Loss theta2: -7.339\n",
      "Iter 79/100 - Loss theta2: -7.340\n",
      "Iter 80/100 - Loss theta2: -7.340\n",
      "Iter 81/100 - Loss theta2: -7.340\n",
      "Iter 82/100 - Loss theta2: -7.340\n",
      "Iter 83/100 - Loss theta2: -7.340\n",
      "Iter 84/100 - Loss theta2: -7.340\n",
      "Iter 85/100 - Loss theta2: -7.340\n",
      "Iter 86/100 - Loss theta2: -7.340\n",
      "Iter 87/100 - Loss theta2: -7.340\n",
      "Iter 88/100 - Loss theta2: -7.340\n",
      "Iter 89/100 - Loss theta2: -7.340\n",
      "Iter 90/100 - Loss theta2: -7.340\n",
      "Iter 91/100 - Loss theta2: -7.340\n",
      "Iter 92/100 - Loss theta2: -7.340\n",
      "Iter 93/100 - Loss theta2: -7.341\n",
      "Iter 94/100 - Loss theta2: -7.341\n",
      "Iter 95/100 - Loss theta2: -7.340\n",
      "Iter 96/100 - Loss theta2: -7.340\n",
      "Iter 97/100 - Loss theta2: -7.340\n",
      "Iter 98/100 - Loss theta2: -7.341\n",
      "Iter 99/100 - Loss theta2: -7.340\n",
      "Iter 100/100 - Loss theta2: -7.340\n",
      "tensor([[0.6875],\n",
      "        [0.6875]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7166],\n",
      "        [0.7165]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 137.586\n",
      "Iter 2/50 - Loss hyperparam: 118.930\n",
      "Iter 3/50 - Loss hyperparam: 100.882\n",
      "Iter 4/50 - Loss hyperparam: 82.928\n",
      "Iter 5/50 - Loss hyperparam: 64.648\n",
      "Iter 6/50 - Loss hyperparam: 45.696\n",
      "Iter 7/50 - Loss hyperparam: 25.858\n",
      "Iter 8/50 - Loss hyperparam: 5.056\n",
      "Iter 9/50 - Loss hyperparam: -16.840\n",
      "Iter 10/50 - Loss hyperparam: -40.018\n",
      "Iter 11/50 - Loss hyperparam: -64.454\n",
      "Iter 12/50 - Loss hyperparam: -90.163\n",
      "Iter 13/50 - Loss hyperparam: -117.322\n",
      "Iter 14/50 - Loss hyperparam: -146.305\n",
      "Iter 15/50 - Loss hyperparam: -177.566\n",
      "Iter 16/50 - Loss hyperparam: -211.401\n",
      "Iter 17/50 - Loss hyperparam: -247.584\n",
      "Iter 18/50 - Loss hyperparam: -284.794\n",
      "Iter 19/50 - Loss hyperparam: -319.440\n",
      "Iter 20/50 - Loss hyperparam: -344.811\n",
      "Iter 21/50 - Loss hyperparam: -356.369\n",
      "Iter 22/50 - Loss hyperparam: -360.395\n",
      "Iter 23/50 - Loss hyperparam: -365.962\n",
      "Iter 24/50 - Loss hyperparam: -376.955\n",
      "Iter 25/50 - Loss hyperparam: -394.481\n",
      "Iter 26/50 - Loss hyperparam: -418.264\n",
      "Iter 27/50 - Loss hyperparam: -448.291\n",
      "Iter 28/50 - Loss hyperparam: -483.110\n",
      "Iter 29/50 - Loss hyperparam: -517.851\n",
      "Iter 30/50 - Loss hyperparam: -542.377\n",
      "Iter 31/50 - Loss hyperparam: -552.867\n",
      "Iter 32/50 - Loss hyperparam: -559.304\n",
      "Iter 33/50 - Loss hyperparam: -573.009\n",
      "Iter 34/50 - Loss hyperparam: -597.316\n",
      "Iter 35/50 - Loss hyperparam: -630.195\n",
      "Iter 36/50 - Loss hyperparam: -664.702\n",
      "Iter 37/50 - Loss hyperparam: -683.707\n",
      "Iter 38/50 - Loss hyperparam: -689.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39/50 - Loss hyperparam: -704.015\n",
      "Iter 40/50 - Loss hyperparam: -731.677\n",
      "Iter 41/50 - Loss hyperparam: -765.578\n",
      "Iter 42/50 - Loss hyperparam: -785.605\n",
      "Iter 43/50 - Loss hyperparam: -793.472\n",
      "Iter 44/50 - Loss hyperparam: -812.938\n",
      "Iter 45/50 - Loss hyperparam: -846.235\n",
      "Iter 46/50 - Loss hyperparam: -867.904\n",
      "Iter 47/50 - Loss hyperparam: -876.434\n",
      "Iter 48/50 - Loss hyperparam: -902.274\n",
      "Iter 49/50 - Loss hyperparam: -931.609\n",
      "Iter 50/50 - Loss hyperparam: -935.647\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.266\n",
      "Iter 2/100 - Loss theta2: -7.254\n",
      "Iter 3/100 - Loss theta2: -7.267\n",
      "Iter 4/100 - Loss theta2: -7.269\n",
      "Iter 5/100 - Loss theta2: -7.264\n",
      "Iter 6/100 - Loss theta2: -7.266\n",
      "Iter 7/100 - Loss theta2: -7.271\n",
      "Iter 8/100 - Loss theta2: -7.271\n",
      "Iter 9/100 - Loss theta2: -7.268\n",
      "Iter 10/100 - Loss theta2: -7.268\n",
      "Iter 11/100 - Loss theta2: -7.271\n",
      "Iter 12/100 - Loss theta2: -7.273\n",
      "Iter 13/100 - Loss theta2: -7.272\n",
      "Iter 14/100 - Loss theta2: -7.271\n",
      "Iter 15/100 - Loss theta2: -7.271\n",
      "Iter 16/100 - Loss theta2: -7.273\n",
      "Iter 17/100 - Loss theta2: -7.274\n",
      "Iter 18/100 - Loss theta2: -7.273\n",
      "Iter 19/100 - Loss theta2: -7.272\n",
      "Iter 20/100 - Loss theta2: -7.274\n",
      "Iter 21/100 - Loss theta2: -7.275\n",
      "Iter 22/100 - Loss theta2: -7.275\n",
      "Iter 23/100 - Loss theta2: -7.276\n",
      "Iter 24/100 - Loss theta2: -7.275\n",
      "Iter 25/100 - Loss theta2: -7.276\n",
      "Iter 26/100 - Loss theta2: -7.277\n",
      "Iter 27/100 - Loss theta2: -7.276\n",
      "Iter 28/100 - Loss theta2: -7.276\n",
      "Iter 29/100 - Loss theta2: -7.276\n",
      "Iter 30/100 - Loss theta2: -7.278\n",
      "Iter 31/100 - Loss theta2: -7.277\n",
      "Iter 32/100 - Loss theta2: -7.278\n",
      "Iter 33/100 - Loss theta2: -7.277\n",
      "Iter 34/100 - Loss theta2: -7.279\n",
      "Iter 35/100 - Loss theta2: -7.279\n",
      "Iter 36/100 - Loss theta2: -7.279\n",
      "Iter 37/100 - Loss theta2: -7.280\n",
      "Iter 38/100 - Loss theta2: -7.278\n",
      "Iter 39/100 - Loss theta2: -7.280\n",
      "Iter 40/100 - Loss theta2: -7.280\n",
      "Iter 41/100 - Loss theta2: -7.280\n",
      "Iter 42/100 - Loss theta2: -7.281\n",
      "Iter 43/100 - Loss theta2: -7.281\n",
      "Iter 44/100 - Loss theta2: -7.281\n",
      "Iter 45/100 - Loss theta2: -7.282\n",
      "Iter 46/100 - Loss theta2: -7.282\n",
      "Iter 47/100 - Loss theta2: -7.282\n",
      "Iter 48/100 - Loss theta2: -7.281\n",
      "Iter 49/100 - Loss theta2: -7.283\n",
      "Iter 50/100 - Loss theta2: -7.283\n",
      "Iter 51/100 - Loss theta2: -7.282\n",
      "Iter 52/100 - Loss theta2: -7.283\n",
      "Iter 53/100 - Loss theta2: -7.283\n",
      "Iter 54/100 - Loss theta2: -7.283\n",
      "Iter 55/100 - Loss theta2: -7.284\n",
      "Iter 56/100 - Loss theta2: -7.285\n",
      "Iter 57/100 - Loss theta2: -7.285\n",
      "Iter 58/100 - Loss theta2: -7.285\n",
      "Iter 59/100 - Loss theta2: -7.285\n",
      "Iter 60/100 - Loss theta2: -7.285\n",
      "Iter 61/100 - Loss theta2: -7.285\n",
      "Iter 62/100 - Loss theta2: -7.286\n",
      "Iter 63/100 - Loss theta2: -7.286\n",
      "Iter 64/100 - Loss theta2: -7.286\n",
      "Iter 65/100 - Loss theta2: -7.287\n",
      "Iter 66/100 - Loss theta2: -7.287\n",
      "Iter 67/100 - Loss theta2: -7.286\n",
      "Iter 68/100 - Loss theta2: -7.287\n",
      "Iter 69/100 - Loss theta2: -7.286\n",
      "Iter 70/100 - Loss theta2: -7.288\n",
      "Iter 71/100 - Loss theta2: -7.287\n",
      "Iter 72/100 - Loss theta2: -7.288\n",
      "Iter 73/100 - Loss theta2: -7.287\n",
      "Iter 74/100 - Loss theta2: -7.287\n",
      "Iter 75/100 - Loss theta2: -7.288\n",
      "Iter 76/100 - Loss theta2: -7.287\n",
      "Iter 77/100 - Loss theta2: -7.289\n",
      "Iter 78/100 - Loss theta2: -7.288\n",
      "Iter 79/100 - Loss theta2: -7.288\n",
      "Iter 80/100 - Loss theta2: -7.289\n",
      "Iter 81/100 - Loss theta2: -7.290\n",
      "Iter 82/100 - Loss theta2: -7.289\n",
      "Iter 83/100 - Loss theta2: -7.289\n",
      "Iter 84/100 - Loss theta2: -7.289\n",
      "Iter 85/100 - Loss theta2: -7.289\n",
      "Iter 86/100 - Loss theta2: -7.289\n",
      "Iter 87/100 - Loss theta2: -7.289\n",
      "Iter 88/100 - Loss theta2: -7.290\n",
      "Iter 89/100 - Loss theta2: -7.290\n",
      "Iter 90/100 - Loss theta2: -7.290\n",
      "Iter 91/100 - Loss theta2: -7.290\n",
      "Iter 92/100 - Loss theta2: -7.290\n",
      "Iter 93/100 - Loss theta2: -7.290\n",
      "Iter 94/100 - Loss theta2: -7.292\n",
      "Iter 95/100 - Loss theta2: -7.291\n",
      "Iter 96/100 - Loss theta2: -7.292\n",
      "Iter 97/100 - Loss theta2: -7.291\n",
      "Iter 98/100 - Loss theta2: -7.291\n",
      "Iter 99/100 - Loss theta2: -7.292\n",
      "Iter 100/100 - Loss theta2: -7.291\n",
      "tensor([[0.6866],\n",
      "        [0.6878]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7175],\n",
      "        [0.7171]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1245]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 51.878\n",
      "Iter 2/50 - Loss hyperparam: 23.542\n",
      "Iter 3/50 - Loss hyperparam: -3.962\n",
      "Iter 4/50 - Loss hyperparam: -31.216\n",
      "Iter 5/50 - Loss hyperparam: -58.339\n",
      "Iter 6/50 - Loss hyperparam: -85.204\n",
      "Iter 7/50 - Loss hyperparam: -111.962\n",
      "Iter 8/50 - Loss hyperparam: -138.307\n",
      "Iter 9/50 - Loss hyperparam: -162.528\n",
      "Iter 10/50 - Loss hyperparam: -182.624\n",
      "Iter 11/50 - Loss hyperparam: -197.712\n",
      "Iter 12/50 - Loss hyperparam: -209.194\n",
      "Iter 13/50 - Loss hyperparam: -220.146\n",
      "Iter 14/50 - Loss hyperparam: -233.449\n",
      "Iter 15/50 - Loss hyperparam: -250.628\n",
      "Iter 16/50 - Loss hyperparam: -271.689\n",
      "Iter 17/50 - Loss hyperparam: -295.320\n",
      "Iter 18/50 - Loss hyperparam: -318.994\n",
      "Iter 19/50 - Loss hyperparam: -339.799\n",
      "Iter 20/50 - Loss hyperparam: -356.972\n",
      "Iter 21/50 - Loss hyperparam: -372.701\n",
      "Iter 22/50 - Loss hyperparam: -389.554\n",
      "Iter 23/50 - Loss hyperparam: -408.738\n",
      "Iter 24/50 - Loss hyperparam: -430.049\n",
      "Iter 25/50 - Loss hyperparam: -453.198\n",
      "Iter 26/50 - Loss hyperparam: -477.162\n",
      "Iter 27/50 - Loss hyperparam: -498.173\n",
      "Iter 28/50 - Loss hyperparam: -514.449\n",
      "Iter 29/50 - Loss hyperparam: -531.529\n",
      "Iter 30/50 - Loss hyperparam: -554.113\n",
      "Iter 31/50 - Loss hyperparam: -578.614\n",
      "Iter 32/50 - Loss hyperparam: -598.566\n",
      "Iter 33/50 - Loss hyperparam: -615.593\n",
      "Iter 34/50 - Loss hyperparam: -634.143\n",
      "Iter 35/50 - Loss hyperparam: -656.088\n",
      "Iter 36/50 - Loss hyperparam: -680.081\n",
      "Iter 37/50 - Loss hyperparam: -697.726\n",
      "Iter 38/50 - Loss hyperparam: -716.440\n",
      "Iter 39/50 - Loss hyperparam: -740.914\n",
      "Iter 40/50 - Loss hyperparam: -760.960\n",
      "Iter 41/50 - Loss hyperparam: -778.817\n",
      "Iter 42/50 - Loss hyperparam: -800.754\n",
      "Iter 43/50 - Loss hyperparam: -822.303\n",
      "Iter 44/50 - Loss hyperparam: -839.418\n",
      "Iter 45/50 - Loss hyperparam: -862.390\n",
      "Iter 46/50 - Loss hyperparam: -881.480\n",
      "Iter 47/50 - Loss hyperparam: -901.573\n",
      "Iter 48/50 - Loss hyperparam: -921.896\n",
      "Iter 49/50 - Loss hyperparam: -941.048\n",
      "Iter 50/50 - Loss hyperparam: -963.788\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.243\n",
      "Iter 2/100 - Loss theta2: -7.227\n",
      "Iter 3/100 - Loss theta2: -7.242\n",
      "Iter 4/100 - Loss theta2: -7.243\n",
      "Iter 5/100 - Loss theta2: -7.237\n",
      "Iter 6/100 - Loss theta2: -7.240\n",
      "Iter 7/100 - Loss theta2: -7.246\n",
      "Iter 8/100 - Loss theta2: -7.245\n",
      "Iter 9/100 - Loss theta2: -7.242\n",
      "Iter 10/100 - Loss theta2: -7.242\n",
      "Iter 11/100 - Loss theta2: -7.244\n",
      "Iter 12/100 - Loss theta2: -7.247\n",
      "Iter 13/100 - Loss theta2: -7.246\n",
      "Iter 14/100 - Loss theta2: -7.245\n",
      "Iter 15/100 - Loss theta2: -7.245\n",
      "Iter 16/100 - Loss theta2: -7.246\n",
      "Iter 17/100 - Loss theta2: -7.247\n",
      "Iter 18/100 - Loss theta2: -7.247\n",
      "Iter 19/100 - Loss theta2: -7.246\n",
      "Iter 20/100 - Loss theta2: -7.245\n",
      "Iter 21/100 - Loss theta2: -7.247\n",
      "Iter 22/100 - Loss theta2: -7.248\n",
      "Iter 23/100 - Loss theta2: -7.249\n",
      "Iter 24/100 - Loss theta2: -7.247\n",
      "Iter 25/100 - Loss theta2: -7.249\n",
      "Iter 26/100 - Loss theta2: -7.249\n",
      "Iter 27/100 - Loss theta2: -7.250\n",
      "Iter 28/100 - Loss theta2: -7.249\n",
      "Iter 29/100 - Loss theta2: -7.250\n",
      "Iter 30/100 - Loss theta2: -7.250\n",
      "Iter 31/100 - Loss theta2: -7.251\n",
      "Iter 32/100 - Loss theta2: -7.251\n",
      "Iter 33/100 - Loss theta2: -7.250\n",
      "Iter 34/100 - Loss theta2: -7.250\n",
      "Iter 35/100 - Loss theta2: -7.250\n",
      "Iter 36/100 - Loss theta2: -7.251\n",
      "Iter 37/100 - Loss theta2: -7.251\n",
      "Iter 38/100 - Loss theta2: -7.251\n",
      "Iter 39/100 - Loss theta2: -7.252\n",
      "Iter 40/100 - Loss theta2: -7.252\n",
      "Iter 41/100 - Loss theta2: -7.252\n",
      "Iter 42/100 - Loss theta2: -7.252\n",
      "Iter 43/100 - Loss theta2: -7.252\n",
      "Iter 44/100 - Loss theta2: -7.253\n",
      "Iter 45/100 - Loss theta2: -7.253\n",
      "Iter 46/100 - Loss theta2: -7.253\n",
      "Iter 47/100 - Loss theta2: -7.253\n",
      "Iter 48/100 - Loss theta2: -7.252\n",
      "Iter 49/100 - Loss theta2: -7.253\n",
      "Iter 50/100 - Loss theta2: -7.254\n",
      "Iter 51/100 - Loss theta2: -7.253\n",
      "Iter 52/100 - Loss theta2: -7.253\n",
      "Iter 53/100 - Loss theta2: -7.253\n",
      "Iter 54/100 - Loss theta2: -7.254\n",
      "Iter 55/100 - Loss theta2: -7.253\n",
      "Iter 56/100 - Loss theta2: -7.254\n",
      "Iter 57/100 - Loss theta2: -7.254\n",
      "Iter 58/100 - Loss theta2: -7.254\n",
      "Iter 59/100 - Loss theta2: -7.254\n",
      "Iter 60/100 - Loss theta2: -7.254\n",
      "Iter 61/100 - Loss theta2: -7.254\n",
      "Iter 62/100 - Loss theta2: -7.254\n",
      "Iter 63/100 - Loss theta2: -7.255\n",
      "Iter 64/100 - Loss theta2: -7.255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 65/100 - Loss theta2: -7.254\n",
      "Iter 66/100 - Loss theta2: -7.254\n",
      "Iter 67/100 - Loss theta2: -7.255\n",
      "Iter 68/100 - Loss theta2: -7.255\n",
      "Iter 69/100 - Loss theta2: -7.255\n",
      "Iter 70/100 - Loss theta2: -7.255\n",
      "Iter 71/100 - Loss theta2: -7.256\n",
      "Iter 72/100 - Loss theta2: -7.256\n",
      "Iter 73/100 - Loss theta2: -7.255\n",
      "Iter 74/100 - Loss theta2: -7.256\n",
      "Iter 75/100 - Loss theta2: -7.255\n",
      "Iter 76/100 - Loss theta2: -7.255\n",
      "Iter 77/100 - Loss theta2: -7.256\n",
      "Iter 78/100 - Loss theta2: -7.256\n",
      "Iter 79/100 - Loss theta2: -7.256\n",
      "Iter 80/100 - Loss theta2: -7.256\n",
      "Iter 81/100 - Loss theta2: -7.256\n",
      "Iter 82/100 - Loss theta2: -7.256\n",
      "Iter 83/100 - Loss theta2: -7.256\n",
      "Iter 84/100 - Loss theta2: -7.256\n",
      "Iter 85/100 - Loss theta2: -7.257\n",
      "Iter 86/100 - Loss theta2: -7.256\n",
      "Iter 87/100 - Loss theta2: -7.256\n",
      "Iter 88/100 - Loss theta2: -7.255\n",
      "Iter 89/100 - Loss theta2: -7.254\n",
      "Iter 90/100 - Loss theta2: -7.256\n",
      "Iter 91/100 - Loss theta2: -7.257\n",
      "Iter 92/100 - Loss theta2: -7.257\n",
      "Iter 93/100 - Loss theta2: -7.256\n",
      "Iter 94/100 - Loss theta2: -7.257\n",
      "Iter 95/100 - Loss theta2: -7.256\n",
      "Iter 96/100 - Loss theta2: -7.257\n",
      "Iter 97/100 - Loss theta2: -7.257\n",
      "Iter 98/100 - Loss theta2: -7.257\n",
      "Iter 99/100 - Loss theta2: -7.256\n",
      "Iter 100/100 - Loss theta2: -7.257\n",
      "tensor([[0.6869],\n",
      "        [0.6874]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7181],\n",
      "        [0.7179]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 36.143\n",
      "Iter 2/50 - Loss hyperparam: 4.949\n",
      "Iter 3/50 - Loss hyperparam: -26.968\n",
      "Iter 4/50 - Loss hyperparam: -58.073\n",
      "Iter 5/50 - Loss hyperparam: -86.526\n",
      "Iter 6/50 - Loss hyperparam: -112.456\n",
      "Iter 7/50 - Loss hyperparam: -136.305\n",
      "Iter 8/50 - Loss hyperparam: -156.922\n",
      "Iter 9/50 - Loss hyperparam: -173.467\n",
      "Iter 10/50 - Loss hyperparam: -186.838\n",
      "Iter 11/50 - Loss hyperparam: -199.484\n",
      "Iter 12/50 - Loss hyperparam: -213.995\n",
      "Iter 13/50 - Loss hyperparam: -231.739\n",
      "Iter 14/50 - Loss hyperparam: -252.502\n",
      "Iter 15/50 - Loss hyperparam: -274.848\n",
      "Iter 16/50 - Loss hyperparam: -296.623\n",
      "Iter 17/50 - Loss hyperparam: -316.198\n",
      "Iter 18/50 - Loss hyperparam: -333.829\n",
      "Iter 19/50 - Loss hyperparam: -350.797\n",
      "Iter 20/50 - Loss hyperparam: -368.319\n",
      "Iter 21/50 - Loss hyperparam: -387.270\n",
      "Iter 22/50 - Loss hyperparam: -408.380\n",
      "Iter 23/50 - Loss hyperparam: -431.849\n",
      "Iter 24/50 - Loss hyperparam: -455.300\n",
      "Iter 25/50 - Loss hyperparam: -475.134\n",
      "Iter 26/50 - Loss hyperparam: -492.503\n",
      "Iter 27/50 - Loss hyperparam: -512.357\n",
      "Iter 28/50 - Loss hyperparam: -535.367\n",
      "Iter 29/50 - Loss hyperparam: -558.557\n",
      "Iter 30/50 - Loss hyperparam: -580.223\n",
      "Iter 31/50 - Loss hyperparam: -598.968\n",
      "Iter 32/50 - Loss hyperparam: -617.886\n",
      "Iter 33/50 - Loss hyperparam: -641.390\n",
      "Iter 34/50 - Loss hyperparam: -663.607\n",
      "Iter 35/50 - Loss hyperparam: -682.053\n",
      "Iter 36/50 - Loss hyperparam: -702.277\n",
      "Iter 37/50 - Loss hyperparam: -725.553\n",
      "Iter 38/50 - Loss hyperparam: -747.244\n",
      "Iter 39/50 - Loss hyperparam: -765.845\n",
      "Iter 40/50 - Loss hyperparam: -789.379\n",
      "Iter 41/50 - Loss hyperparam: -810.772\n",
      "Iter 42/50 - Loss hyperparam: -829.317\n",
      "Iter 43/50 - Loss hyperparam: -853.305\n",
      "Iter 44/50 - Loss hyperparam: -872.256\n",
      "Iter 45/50 - Loss hyperparam: -893.119\n",
      "Iter 46/50 - Loss hyperparam: -914.192\n",
      "Iter 47/50 - Loss hyperparam: -934.825\n",
      "Iter 48/50 - Loss hyperparam: -957.040\n",
      "Iter 49/50 - Loss hyperparam: -976.750\n",
      "Iter 50/50 - Loss hyperparam: -996.404\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.226\n",
      "Iter 2/100 - Loss theta2: -7.201\n",
      "Iter 3/100 - Loss theta2: -7.225\n",
      "Iter 4/100 - Loss theta2: -7.221\n",
      "Iter 5/100 - Loss theta2: -7.212\n",
      "Iter 6/100 - Loss theta2: -7.216\n",
      "Iter 7/100 - Loss theta2: -7.225\n",
      "Iter 8/100 - Loss theta2: -7.225\n",
      "Iter 9/100 - Loss theta2: -7.222\n",
      "Iter 10/100 - Loss theta2: -7.219\n",
      "Iter 11/100 - Loss theta2: -7.221\n",
      "Iter 12/100 - Loss theta2: -7.225\n",
      "Iter 13/100 - Loss theta2: -7.227\n",
      "Iter 14/100 - Loss theta2: -7.225\n",
      "Iter 15/100 - Loss theta2: -7.223\n",
      "Iter 16/100 - Loss theta2: -7.223\n",
      "Iter 17/100 - Loss theta2: -7.225\n",
      "Iter 18/100 - Loss theta2: -7.226\n",
      "Iter 19/100 - Loss theta2: -7.226\n",
      "Iter 20/100 - Loss theta2: -7.225\n",
      "Iter 21/100 - Loss theta2: -7.224\n",
      "Iter 22/100 - Loss theta2: -7.224\n",
      "Iter 23/100 - Loss theta2: -7.227\n",
      "Iter 24/100 - Loss theta2: -7.227\n",
      "Iter 25/100 - Loss theta2: -7.227\n",
      "Iter 26/100 - Loss theta2: -7.226\n",
      "Iter 27/100 - Loss theta2: -7.226\n",
      "Iter 28/100 - Loss theta2: -7.226\n",
      "Iter 29/100 - Loss theta2: -7.227\n",
      "Iter 30/100 - Loss theta2: -7.226\n",
      "Iter 31/100 - Loss theta2: -7.227\n",
      "Iter 32/100 - Loss theta2: -7.226\n",
      "Iter 33/100 - Loss theta2: -7.227\n",
      "Iter 34/100 - Loss theta2: -7.227\n",
      "Iter 35/100 - Loss theta2: -7.227\n",
      "Iter 36/100 - Loss theta2: -7.227\n",
      "Iter 37/100 - Loss theta2: -7.227\n",
      "Iter 38/100 - Loss theta2: -7.227\n",
      "Iter 39/100 - Loss theta2: -7.228\n",
      "Iter 40/100 - Loss theta2: -7.228\n",
      "Iter 41/100 - Loss theta2: -7.227\n",
      "Iter 42/100 - Loss theta2: -7.227\n",
      "Iter 43/100 - Loss theta2: -7.227\n",
      "Iter 44/100 - Loss theta2: -7.228\n",
      "Iter 45/100 - Loss theta2: -7.227\n",
      "Iter 46/100 - Loss theta2: -7.226\n",
      "Iter 47/100 - Loss theta2: -7.227\n",
      "Iter 48/100 - Loss theta2: -7.228\n",
      "Iter 49/100 - Loss theta2: -7.228\n",
      "Iter 50/100 - Loss theta2: -7.228\n",
      "Iter 51/100 - Loss theta2: -7.228\n",
      "Iter 52/100 - Loss theta2: -7.227\n",
      "Iter 53/100 - Loss theta2: -7.228\n",
      "Iter 54/100 - Loss theta2: -7.228\n",
      "Iter 55/100 - Loss theta2: -7.228\n",
      "Iter 56/100 - Loss theta2: -7.229\n",
      "Iter 57/100 - Loss theta2: -7.229\n",
      "Iter 58/100 - Loss theta2: -7.228\n",
      "Iter 59/100 - Loss theta2: -7.229\n",
      "Iter 60/100 - Loss theta2: -7.228\n",
      "Iter 61/100 - Loss theta2: -7.228\n",
      "Iter 62/100 - Loss theta2: -7.228\n",
      "Iter 63/100 - Loss theta2: -7.229\n",
      "Iter 64/100 - Loss theta2: -7.229\n",
      "Iter 65/100 - Loss theta2: -7.227\n",
      "Iter 66/100 - Loss theta2: -7.228\n",
      "Iter 67/100 - Loss theta2: -7.228\n",
      "Iter 68/100 - Loss theta2: -7.229\n",
      "Iter 69/100 - Loss theta2: -7.228\n",
      "Iter 70/100 - Loss theta2: -7.229\n",
      "Iter 71/100 - Loss theta2: -7.229\n",
      "Iter 72/100 - Loss theta2: -7.228\n",
      "Iter 73/100 - Loss theta2: -7.228\n",
      "Iter 74/100 - Loss theta2: -7.228\n",
      "Iter 75/100 - Loss theta2: -7.229\n",
      "Iter 76/100 - Loss theta2: -7.228\n",
      "Iter 77/100 - Loss theta2: -7.229\n",
      "Iter 78/100 - Loss theta2: -7.229\n",
      "Iter 79/100 - Loss theta2: -7.229\n",
      "Iter 80/100 - Loss theta2: -7.229\n",
      "Iter 81/100 - Loss theta2: -7.229\n",
      "Iter 82/100 - Loss theta2: -7.228\n",
      "Iter 83/100 - Loss theta2: -7.229\n",
      "Iter 84/100 - Loss theta2: -7.230\n",
      "Iter 85/100 - Loss theta2: -7.229\n",
      "Iter 86/100 - Loss theta2: -7.230\n",
      "Iter 87/100 - Loss theta2: -7.230\n",
      "Iter 88/100 - Loss theta2: -7.230\n",
      "Iter 89/100 - Loss theta2: -7.230\n",
      "Iter 90/100 - Loss theta2: -7.230\n",
      "Iter 91/100 - Loss theta2: -7.230\n",
      "Iter 92/100 - Loss theta2: -7.229\n",
      "Iter 93/100 - Loss theta2: -7.230\n",
      "Iter 94/100 - Loss theta2: -7.230\n",
      "Iter 95/100 - Loss theta2: -7.230\n",
      "Iter 96/100 - Loss theta2: -7.230\n",
      "Iter 97/100 - Loss theta2: -7.230\n",
      "Iter 98/100 - Loss theta2: -7.230\n",
      "Iter 99/100 - Loss theta2: -7.230\n",
      "Iter 100/100 - Loss theta2: -7.231\n",
      "tensor([[0.6869],\n",
      "        [0.6877]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7191],\n",
      "        [0.7188]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 131.881\n",
      "Iter 2/50 - Loss hyperparam: 106.377\n",
      "Iter 3/50 - Loss hyperparam: 79.717\n",
      "Iter 4/50 - Loss hyperparam: 51.931\n",
      "Iter 5/50 - Loss hyperparam: 22.922\n",
      "Iter 6/50 - Loss hyperparam: -7.621\n",
      "Iter 7/50 - Loss hyperparam: -39.983\n",
      "Iter 8/50 - Loss hyperparam: -74.067\n",
      "Iter 9/50 - Loss hyperparam: -109.301\n",
      "Iter 10/50 - Loss hyperparam: -144.742\n",
      "Iter 11/50 - Loss hyperparam: -178.850\n",
      "Iter 12/50 - Loss hyperparam: -209.588\n",
      "Iter 13/50 - Loss hyperparam: -236.173\n",
      "Iter 14/50 - Loss hyperparam: -259.873\n",
      "Iter 15/50 - Loss hyperparam: -280.878\n",
      "Iter 16/50 - Loss hyperparam: -298.238\n",
      "Iter 17/50 - Loss hyperparam: -312.790\n",
      "Iter 18/50 - Loss hyperparam: -327.141\n",
      "Iter 19/50 - Loss hyperparam: -343.269\n",
      "Iter 20/50 - Loss hyperparam: -362.070\n",
      "Iter 21/50 - Loss hyperparam: -384.455\n",
      "Iter 22/50 - Loss hyperparam: -411.618\n",
      "Iter 23/50 - Loss hyperparam: -443.764\n",
      "Iter 24/50 - Loss hyperparam: -478.005\n",
      "Iter 25/50 - Loss hyperparam: -506.812\n",
      "Iter 26/50 - Loss hyperparam: -523.184\n",
      "Iter 27/50 - Loss hyperparam: -532.650\n",
      "Iter 28/50 - Loss hyperparam: -547.092\n",
      "Iter 29/50 - Loss hyperparam: -571.873\n",
      "Iter 30/50 - Loss hyperparam: -605.326\n",
      "Iter 31/50 - Loss hyperparam: -638.453\n",
      "Iter 32/50 - Loss hyperparam: -659.997\n",
      "Iter 33/50 - Loss hyperparam: -674.483\n",
      "Iter 34/50 - Loss hyperparam: -693.358\n",
      "Iter 35/50 - Loss hyperparam: -719.220\n",
      "Iter 36/50 - Loss hyperparam: -747.514\n",
      "Iter 37/50 - Loss hyperparam: -774.623\n",
      "Iter 38/50 - Loss hyperparam: -793.971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39/50 - Loss hyperparam: -809.462\n",
      "Iter 40/50 - Loss hyperparam: -836.260\n",
      "Iter 41/50 - Loss hyperparam: -869.535\n",
      "Iter 42/50 - Loss hyperparam: -884.033\n",
      "Iter 43/50 - Loss hyperparam: -902.683\n",
      "Iter 44/50 - Loss hyperparam: -935.155\n",
      "Iter 45/50 - Loss hyperparam: -955.346\n",
      "Iter 46/50 - Loss hyperparam: -973.859\n",
      "Iter 47/50 - Loss hyperparam: -999.691\n",
      "Iter 48/50 - Loss hyperparam: -1025.891\n",
      "Iter 49/50 - Loss hyperparam: -1039.754\n",
      "Iter 50/50 - Loss hyperparam: -1073.156\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.283\n",
      "Iter 2/100 - Loss theta2: -7.253\n",
      "Iter 3/100 - Loss theta2: -7.282\n",
      "Iter 4/100 - Loss theta2: -7.279\n",
      "Iter 5/100 - Loss theta2: -7.267\n",
      "Iter 6/100 - Loss theta2: -7.274\n",
      "Iter 7/100 - Loss theta2: -7.283\n",
      "Iter 8/100 - Loss theta2: -7.285\n",
      "Iter 9/100 - Loss theta2: -7.281\n",
      "Iter 10/100 - Loss theta2: -7.277\n",
      "Iter 11/100 - Loss theta2: -7.279\n",
      "Iter 12/100 - Loss theta2: -7.285\n",
      "Iter 13/100 - Loss theta2: -7.287\n",
      "Iter 14/100 - Loss theta2: -7.286\n",
      "Iter 15/100 - Loss theta2: -7.283\n",
      "Iter 16/100 - Loss theta2: -7.283\n",
      "Iter 17/100 - Loss theta2: -7.284\n",
      "Iter 18/100 - Loss theta2: -7.286\n",
      "Iter 19/100 - Loss theta2: -7.288\n",
      "Iter 20/100 - Loss theta2: -7.287\n",
      "Iter 21/100 - Loss theta2: -7.286\n",
      "Iter 22/100 - Loss theta2: -7.287\n",
      "Iter 23/100 - Loss theta2: -7.286\n",
      "Iter 24/100 - Loss theta2: -7.288\n",
      "Iter 25/100 - Loss theta2: -7.289\n",
      "Iter 26/100 - Loss theta2: -7.288\n",
      "Iter 27/100 - Loss theta2: -7.288\n",
      "Iter 28/100 - Loss theta2: -7.288\n",
      "Iter 29/100 - Loss theta2: -7.289\n",
      "Iter 30/100 - Loss theta2: -7.290\n",
      "Iter 31/100 - Loss theta2: -7.290\n",
      "Iter 32/100 - Loss theta2: -7.289\n",
      "Iter 33/100 - Loss theta2: -7.289\n",
      "Iter 34/100 - Loss theta2: -7.289\n",
      "Iter 35/100 - Loss theta2: -7.290\n",
      "Iter 36/100 - Loss theta2: -7.291\n",
      "Iter 37/100 - Loss theta2: -7.290\n",
      "Iter 38/100 - Loss theta2: -7.291\n",
      "Iter 39/100 - Loss theta2: -7.290\n",
      "Iter 40/100 - Loss theta2: -7.291\n",
      "Iter 41/100 - Loss theta2: -7.291\n",
      "Iter 42/100 - Loss theta2: -7.291\n",
      "Iter 43/100 - Loss theta2: -7.291\n",
      "Iter 44/100 - Loss theta2: -7.290\n",
      "Iter 45/100 - Loss theta2: -7.292\n",
      "Iter 46/100 - Loss theta2: -7.291\n",
      "Iter 47/100 - Loss theta2: -7.292\n",
      "Iter 48/100 - Loss theta2: -7.293\n",
      "Iter 49/100 - Loss theta2: -7.293\n",
      "Iter 50/100 - Loss theta2: -7.292\n",
      "Iter 51/100 - Loss theta2: -7.293\n",
      "Iter 52/100 - Loss theta2: -7.292\n",
      "Iter 53/100 - Loss theta2: -7.292\n",
      "Iter 54/100 - Loss theta2: -7.292\n",
      "Iter 55/100 - Loss theta2: -7.293\n",
      "Iter 56/100 - Loss theta2: -7.293\n",
      "Iter 57/100 - Loss theta2: -7.293\n",
      "Iter 58/100 - Loss theta2: -7.293\n",
      "Iter 59/100 - Loss theta2: -7.292\n",
      "Iter 60/100 - Loss theta2: -7.293\n",
      "Iter 61/100 - Loss theta2: -7.293\n",
      "Iter 62/100 - Loss theta2: -7.294\n",
      "Iter 63/100 - Loss theta2: -7.294\n",
      "Iter 64/100 - Loss theta2: -7.294\n",
      "Iter 65/100 - Loss theta2: -7.294\n",
      "Iter 66/100 - Loss theta2: -7.294\n",
      "Iter 67/100 - Loss theta2: -7.294\n",
      "Iter 68/100 - Loss theta2: -7.295\n",
      "Iter 69/100 - Loss theta2: -7.295\n",
      "Iter 70/100 - Loss theta2: -7.296\n",
      "Iter 71/100 - Loss theta2: -7.295\n",
      "Iter 72/100 - Loss theta2: -7.295\n",
      "Iter 73/100 - Loss theta2: -7.295\n",
      "Iter 74/100 - Loss theta2: -7.296\n",
      "Iter 75/100 - Loss theta2: -7.295\n",
      "Iter 76/100 - Loss theta2: -7.295\n",
      "Iter 77/100 - Loss theta2: -7.296\n",
      "Iter 78/100 - Loss theta2: -7.296\n",
      "Iter 79/100 - Loss theta2: -7.296\n",
      "Iter 80/100 - Loss theta2: -7.295\n",
      "Iter 81/100 - Loss theta2: -7.295\n",
      "Iter 82/100 - Loss theta2: -7.296\n",
      "Iter 83/100 - Loss theta2: -7.297\n",
      "Iter 84/100 - Loss theta2: -7.297\n",
      "Iter 85/100 - Loss theta2: -7.297\n",
      "Iter 86/100 - Loss theta2: -7.296\n",
      "Iter 87/100 - Loss theta2: -7.297\n",
      "Iter 88/100 - Loss theta2: -7.297\n",
      "Iter 89/100 - Loss theta2: -7.297\n",
      "Iter 90/100 - Loss theta2: -7.297\n",
      "Iter 91/100 - Loss theta2: -7.297\n",
      "Iter 92/100 - Loss theta2: -7.297\n",
      "Iter 93/100 - Loss theta2: -7.297\n",
      "Iter 94/100 - Loss theta2: -7.297\n",
      "Iter 95/100 - Loss theta2: -7.298\n",
      "Iter 96/100 - Loss theta2: -7.297\n",
      "Iter 97/100 - Loss theta2: -7.297\n",
      "Iter 98/100 - Loss theta2: -7.297\n",
      "Iter 99/100 - Loss theta2: -7.298\n",
      "Iter 100/100 - Loss theta2: -7.298\n",
      "tensor([[0.6867],\n",
      "        [0.6872]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7167],\n",
      "        [0.7165]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 134.955\n",
      "Iter 2/50 - Loss hyperparam: 108.120\n",
      "Iter 3/50 - Loss hyperparam: 79.268\n",
      "Iter 4/50 - Loss hyperparam: 48.845\n",
      "Iter 5/50 - Loss hyperparam: 17.208\n",
      "Iter 6/50 - Loss hyperparam: -15.667\n",
      "Iter 7/50 - Loss hyperparam: -49.671\n",
      "Iter 8/50 - Loss hyperparam: -84.135\n",
      "Iter 9/50 - Loss hyperparam: -118.126\n",
      "Iter 10/50 - Loss hyperparam: -150.749\n",
      "Iter 11/50 - Loss hyperparam: -181.827\n",
      "Iter 12/50 - Loss hyperparam: -212.877\n",
      "Iter 13/50 - Loss hyperparam: -245.124\n",
      "Iter 14/50 - Loss hyperparam: -277.008\n",
      "Iter 15/50 - Loss hyperparam: -304.298\n",
      "Iter 16/50 - Loss hyperparam: -322.772\n",
      "Iter 17/50 - Loss hyperparam: -333.226\n",
      "Iter 18/50 - Loss hyperparam: -341.729\n",
      "Iter 19/50 - Loss hyperparam: -354.166\n",
      "Iter 20/50 - Loss hyperparam: -373.374\n",
      "Iter 21/50 - Loss hyperparam: -399.807\n",
      "Iter 22/50 - Loss hyperparam: -432.352\n",
      "Iter 23/50 - Loss hyperparam: -467.852\n",
      "Iter 24/50 - Loss hyperparam: -500.556\n",
      "Iter 25/50 - Loss hyperparam: -525.858\n",
      "Iter 26/50 - Loss hyperparam: -544.901\n",
      "Iter 27/50 - Loss hyperparam: -561.275\n",
      "Iter 28/50 - Loss hyperparam: -578.890\n",
      "Iter 29/50 - Loss hyperparam: -600.442\n",
      "Iter 30/50 - Loss hyperparam: -628.405\n",
      "Iter 31/50 - Loss hyperparam: -662.692\n",
      "Iter 32/50 - Loss hyperparam: -693.430\n",
      "Iter 33/50 - Loss hyperparam: -709.322\n",
      "Iter 34/50 - Loss hyperparam: -723.098\n",
      "Iter 35/50 - Loss hyperparam: -748.246\n",
      "Iter 36/50 - Loss hyperparam: -780.691\n",
      "Iter 37/50 - Loss hyperparam: -810.026\n",
      "Iter 38/50 - Loss hyperparam: -831.624\n",
      "Iter 39/50 - Loss hyperparam: -846.890\n",
      "Iter 40/50 - Loss hyperparam: -871.827\n",
      "Iter 41/50 - Loss hyperparam: -907.026\n",
      "Iter 42/50 - Loss hyperparam: -925.893\n",
      "Iter 43/50 - Loss hyperparam: -943.758\n",
      "Iter 44/50 - Loss hyperparam: -971.828\n",
      "Iter 45/50 - Loss hyperparam: -1002.105\n",
      "Iter 46/50 - Loss hyperparam: -1016.165\n",
      "Iter 47/50 - Loss hyperparam: -1043.665\n",
      "Iter 48/50 - Loss hyperparam: -1070.993\n",
      "Iter 49/50 - Loss hyperparam: -1088.579\n",
      "Iter 50/50 - Loss hyperparam: -1115.856\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.374\n",
      "Iter 2/100 - Loss theta2: -7.352\n",
      "Iter 3/100 - Loss theta2: -7.373\n",
      "Iter 4/100 - Loss theta2: -7.370\n",
      "Iter 5/100 - Loss theta2: -7.362\n",
      "Iter 6/100 - Loss theta2: -7.368\n",
      "Iter 7/100 - Loss theta2: -7.375\n",
      "Iter 8/100 - Loss theta2: -7.374\n",
      "Iter 9/100 - Loss theta2: -7.371\n",
      "Iter 10/100 - Loss theta2: -7.370\n",
      "Iter 11/100 - Loss theta2: -7.372\n",
      "Iter 12/100 - Loss theta2: -7.375\n",
      "Iter 13/100 - Loss theta2: -7.374\n",
      "Iter 14/100 - Loss theta2: -7.372\n",
      "Iter 15/100 - Loss theta2: -7.371\n",
      "Iter 16/100 - Loss theta2: -7.374\n",
      "Iter 17/100 - Loss theta2: -7.376\n",
      "Iter 18/100 - Loss theta2: -7.375\n",
      "Iter 19/100 - Loss theta2: -7.375\n",
      "Iter 20/100 - Loss theta2: -7.374\n",
      "Iter 21/100 - Loss theta2: -7.374\n",
      "Iter 22/100 - Loss theta2: -7.375\n",
      "Iter 23/100 - Loss theta2: -7.377\n",
      "Iter 24/100 - Loss theta2: -7.375\n",
      "Iter 25/100 - Loss theta2: -7.375\n",
      "Iter 26/100 - Loss theta2: -7.375\n",
      "Iter 27/100 - Loss theta2: -7.377\n",
      "Iter 28/100 - Loss theta2: -7.376\n",
      "Iter 29/100 - Loss theta2: -7.376\n",
      "Iter 30/100 - Loss theta2: -7.375\n",
      "Iter 31/100 - Loss theta2: -7.376\n",
      "Iter 32/100 - Loss theta2: -7.376\n",
      "Iter 33/100 - Loss theta2: -7.376\n",
      "Iter 34/100 - Loss theta2: -7.376\n",
      "Iter 35/100 - Loss theta2: -7.376\n",
      "Iter 36/100 - Loss theta2: -7.377\n",
      "Iter 37/100 - Loss theta2: -7.376\n",
      "Iter 38/100 - Loss theta2: -7.377\n",
      "Iter 39/100 - Loss theta2: -7.375\n",
      "Iter 40/100 - Loss theta2: -7.377\n",
      "Iter 41/100 - Loss theta2: -7.376\n",
      "Iter 42/100 - Loss theta2: -7.375\n",
      "Iter 43/100 - Loss theta2: -7.377\n",
      "Iter 44/100 - Loss theta2: -7.377\n",
      "Iter 45/100 - Loss theta2: -7.378\n",
      "Iter 46/100 - Loss theta2: -7.378\n",
      "Iter 47/100 - Loss theta2: -7.377\n",
      "Iter 48/100 - Loss theta2: -7.376\n",
      "Iter 49/100 - Loss theta2: -7.377\n",
      "Iter 50/100 - Loss theta2: -7.378\n",
      "Iter 51/100 - Loss theta2: -7.377\n",
      "Iter 52/100 - Loss theta2: -7.377\n",
      "Iter 53/100 - Loss theta2: -7.378\n",
      "Iter 54/100 - Loss theta2: -7.377\n",
      "Iter 55/100 - Loss theta2: -7.377\n",
      "Iter 56/100 - Loss theta2: -7.377\n",
      "Iter 57/100 - Loss theta2: -7.378\n",
      "Iter 58/100 - Loss theta2: -7.377\n",
      "Iter 59/100 - Loss theta2: -7.377\n",
      "Iter 60/100 - Loss theta2: -7.376\n",
      "Iter 61/100 - Loss theta2: -7.377\n",
      "Iter 62/100 - Loss theta2: -7.377\n",
      "Iter 63/100 - Loss theta2: -7.377\n",
      "Iter 64/100 - Loss theta2: -7.377\n",
      "Iter 65/100 - Loss theta2: -7.377\n",
      "Iter 66/100 - Loss theta2: -7.379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 67/100 - Loss theta2: -7.378\n",
      "Iter 68/100 - Loss theta2: -7.377\n",
      "Iter 69/100 - Loss theta2: -7.378\n",
      "Iter 70/100 - Loss theta2: -7.378\n",
      "Iter 71/100 - Loss theta2: -7.378\n",
      "Iter 72/100 - Loss theta2: -7.377\n",
      "Iter 73/100 - Loss theta2: -7.376\n",
      "Iter 74/100 - Loss theta2: -7.377\n",
      "Iter 75/100 - Loss theta2: -7.378\n",
      "Iter 76/100 - Loss theta2: -7.377\n",
      "Iter 77/100 - Loss theta2: -7.378\n",
      "Iter 78/100 - Loss theta2: -7.378\n",
      "Iter 79/100 - Loss theta2: -7.378\n",
      "Iter 80/100 - Loss theta2: -7.379\n",
      "Iter 81/100 - Loss theta2: -7.377\n",
      "Iter 82/100 - Loss theta2: -7.377\n",
      "Iter 83/100 - Loss theta2: -7.378\n",
      "Iter 84/100 - Loss theta2: -7.378\n",
      "Iter 85/100 - Loss theta2: -7.379\n",
      "Iter 86/100 - Loss theta2: -7.377\n",
      "Iter 87/100 - Loss theta2: -7.377\n",
      "Iter 88/100 - Loss theta2: -7.378\n",
      "Iter 89/100 - Loss theta2: -7.377\n",
      "Iter 90/100 - Loss theta2: -7.378\n",
      "Iter 91/100 - Loss theta2: -7.377\n",
      "Iter 92/100 - Loss theta2: -7.379\n",
      "Iter 93/100 - Loss theta2: -7.377\n",
      "Iter 94/100 - Loss theta2: -7.378\n",
      "Iter 95/100 - Loss theta2: -7.378\n",
      "Iter 96/100 - Loss theta2: -7.377\n",
      "Iter 97/100 - Loss theta2: -7.378\n",
      "Iter 98/100 - Loss theta2: -7.377\n",
      "Iter 99/100 - Loss theta2: -7.378\n",
      "Iter 100/100 - Loss theta2: -7.377\n",
      "tensor([[0.6877],\n",
      "        [0.6877]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7161],\n",
      "        [0.7161]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 164.632\n",
      "Iter 2/50 - Loss hyperparam: 137.703\n",
      "Iter 3/50 - Loss hyperparam: 108.749\n",
      "Iter 4/50 - Loss hyperparam: 78.017\n",
      "Iter 5/50 - Loss hyperparam: 46.704\n",
      "Iter 6/50 - Loss hyperparam: 16.167\n",
      "Iter 7/50 - Loss hyperparam: -13.047\n",
      "Iter 8/50 - Loss hyperparam: -41.414\n",
      "Iter 9/50 - Loss hyperparam: -70.129\n",
      "Iter 10/50 - Loss hyperparam: -100.531\n",
      "Iter 11/50 - Loss hyperparam: -133.535\n",
      "Iter 12/50 - Loss hyperparam: -169.518\n",
      "Iter 13/50 - Loss hyperparam: -208.298\n",
      "Iter 14/50 - Loss hyperparam: -248.795\n",
      "Iter 15/50 - Loss hyperparam: -288.712\n",
      "Iter 16/50 - Loss hyperparam: -325.995\n",
      "Iter 17/50 - Loss hyperparam: -359.207\n",
      "Iter 18/50 - Loss hyperparam: -383.415\n",
      "Iter 19/50 - Loss hyperparam: -395.745\n",
      "Iter 20/50 - Loss hyperparam: -402.248\n",
      "Iter 21/50 - Loss hyperparam: -411.886\n",
      "Iter 22/50 - Loss hyperparam: -429.007\n",
      "Iter 23/50 - Loss hyperparam: -453.878\n",
      "Iter 24/50 - Loss hyperparam: -485.049\n",
      "Iter 25/50 - Loss hyperparam: -520.573\n",
      "Iter 26/50 - Loss hyperparam: -559.394\n",
      "Iter 27/50 - Loss hyperparam: -597.968\n",
      "Iter 28/50 - Loss hyperparam: -624.685\n",
      "Iter 29/50 - Loss hyperparam: -636.439\n",
      "Iter 30/50 - Loss hyperparam: -648.474\n",
      "Iter 31/50 - Loss hyperparam: -668.546\n",
      "Iter 32/50 - Loss hyperparam: -697.036\n",
      "Iter 33/50 - Loss hyperparam: -734.616\n",
      "Iter 34/50 - Loss hyperparam: -772.862\n",
      "Iter 35/50 - Loss hyperparam: -794.131\n",
      "Iter 36/50 - Loss hyperparam: -807.116\n",
      "Iter 37/50 - Loss hyperparam: -825.224\n",
      "Iter 38/50 - Loss hyperparam: -857.868\n",
      "Iter 39/50 - Loss hyperparam: -897.144\n",
      "Iter 40/50 - Loss hyperparam: -921.910\n",
      "Iter 41/50 - Loss hyperparam: -933.822\n",
      "Iter 42/50 - Loss hyperparam: -957.435\n",
      "Iter 43/50 - Loss hyperparam: -994.593\n",
      "Iter 44/50 - Loss hyperparam: -1023.486\n",
      "Iter 45/50 - Loss hyperparam: -1034.736\n",
      "Iter 46/50 - Loss hyperparam: -1062.598\n",
      "Iter 47/50 - Loss hyperparam: -1100.154\n",
      "Iter 48/50 - Loss hyperparam: -1114.480\n",
      "Iter 49/50 - Loss hyperparam: -1136.334\n",
      "Iter 50/50 - Loss hyperparam: -1170.445\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.398\n",
      "Iter 2/100 - Loss theta2: -7.363\n",
      "Iter 3/100 - Loss theta2: -7.398\n",
      "Iter 4/100 - Loss theta2: -7.391\n",
      "Iter 5/100 - Loss theta2: -7.379\n",
      "Iter 6/100 - Loss theta2: -7.384\n",
      "Iter 7/100 - Loss theta2: -7.395\n",
      "Iter 8/100 - Loss theta2: -7.400\n",
      "Iter 9/100 - Loss theta2: -7.394\n",
      "Iter 10/100 - Loss theta2: -7.390\n",
      "Iter 11/100 - Loss theta2: -7.391\n",
      "Iter 12/100 - Loss theta2: -7.397\n",
      "Iter 13/100 - Loss theta2: -7.401\n",
      "Iter 14/100 - Loss theta2: -7.399\n",
      "Iter 15/100 - Loss theta2: -7.396\n",
      "Iter 16/100 - Loss theta2: -7.395\n",
      "Iter 17/100 - Loss theta2: -7.398\n",
      "Iter 18/100 - Loss theta2: -7.400\n",
      "Iter 19/100 - Loss theta2: -7.401\n",
      "Iter 20/100 - Loss theta2: -7.401\n",
      "Iter 21/100 - Loss theta2: -7.398\n",
      "Iter 22/100 - Loss theta2: -7.398\n",
      "Iter 23/100 - Loss theta2: -7.399\n",
      "Iter 24/100 - Loss theta2: -7.402\n",
      "Iter 25/100 - Loss theta2: -7.403\n",
      "Iter 26/100 - Loss theta2: -7.402\n",
      "Iter 27/100 - Loss theta2: -7.400\n",
      "Iter 28/100 - Loss theta2: -7.401\n",
      "Iter 29/100 - Loss theta2: -7.403\n",
      "Iter 30/100 - Loss theta2: -7.402\n",
      "Iter 31/100 - Loss theta2: -7.403\n",
      "Iter 32/100 - Loss theta2: -7.402\n",
      "Iter 33/100 - Loss theta2: -7.402\n",
      "Iter 34/100 - Loss theta2: -7.403\n",
      "Iter 35/100 - Loss theta2: -7.402\n",
      "Iter 36/100 - Loss theta2: -7.401\n",
      "Iter 37/100 - Loss theta2: -7.403\n",
      "Iter 38/100 - Loss theta2: -7.403\n",
      "Iter 39/100 - Loss theta2: -7.404\n",
      "Iter 40/100 - Loss theta2: -7.402\n",
      "Iter 41/100 - Loss theta2: -7.403\n",
      "Iter 42/100 - Loss theta2: -7.404\n",
      "Iter 43/100 - Loss theta2: -7.403\n",
      "Iter 44/100 - Loss theta2: -7.404\n",
      "Iter 45/100 - Loss theta2: -7.404\n",
      "Iter 46/100 - Loss theta2: -7.403\n",
      "Iter 47/100 - Loss theta2: -7.405\n",
      "Iter 48/100 - Loss theta2: -7.404\n",
      "Iter 49/100 - Loss theta2: -7.404\n",
      "Iter 50/100 - Loss theta2: -7.403\n",
      "Iter 51/100 - Loss theta2: -7.406\n",
      "Iter 52/100 - Loss theta2: -7.405\n",
      "Iter 53/100 - Loss theta2: -7.404\n",
      "Iter 54/100 - Loss theta2: -7.404\n",
      "Iter 55/100 - Loss theta2: -7.405\n",
      "Iter 56/100 - Loss theta2: -7.405\n",
      "Iter 57/100 - Loss theta2: -7.405\n",
      "Iter 58/100 - Loss theta2: -7.405\n",
      "Iter 59/100 - Loss theta2: -7.406\n",
      "Iter 60/100 - Loss theta2: -7.404\n",
      "Iter 61/100 - Loss theta2: -7.405\n",
      "Iter 62/100 - Loss theta2: -7.404\n",
      "Iter 63/100 - Loss theta2: -7.405\n",
      "Iter 64/100 - Loss theta2: -7.408\n",
      "Iter 65/100 - Loss theta2: -7.405\n",
      "Iter 66/100 - Loss theta2: -7.406\n",
      "Iter 67/100 - Loss theta2: -7.405\n",
      "Iter 68/100 - Loss theta2: -7.406\n",
      "Iter 69/100 - Loss theta2: -7.405\n",
      "Iter 70/100 - Loss theta2: -7.407\n",
      "Iter 71/100 - Loss theta2: -7.405\n",
      "Iter 72/100 - Loss theta2: -7.405\n",
      "Iter 73/100 - Loss theta2: -7.405\n",
      "Iter 74/100 - Loss theta2: -7.405\n",
      "Iter 75/100 - Loss theta2: -7.405\n",
      "Iter 76/100 - Loss theta2: -7.407\n",
      "Iter 77/100 - Loss theta2: -7.408\n",
      "Iter 78/100 - Loss theta2: -7.406\n",
      "Iter 79/100 - Loss theta2: -7.407\n",
      "Iter 80/100 - Loss theta2: -7.406\n",
      "Iter 81/100 - Loss theta2: -7.406\n",
      "Iter 82/100 - Loss theta2: -7.408\n",
      "Iter 83/100 - Loss theta2: -7.406\n",
      "Iter 84/100 - Loss theta2: -7.406\n",
      "Iter 85/100 - Loss theta2: -7.407\n",
      "Iter 86/100 - Loss theta2: -7.406\n",
      "Iter 87/100 - Loss theta2: -7.407\n",
      "Iter 88/100 - Loss theta2: -7.406\n",
      "Iter 89/100 - Loss theta2: -7.407\n",
      "Iter 90/100 - Loss theta2: -7.405\n",
      "Iter 91/100 - Loss theta2: -7.407\n",
      "Iter 92/100 - Loss theta2: -7.406\n",
      "Iter 93/100 - Loss theta2: -7.406\n",
      "Iter 94/100 - Loss theta2: -7.406\n",
      "Iter 95/100 - Loss theta2: -7.407\n",
      "Iter 96/100 - Loss theta2: -7.407\n",
      "Iter 97/100 - Loss theta2: -7.406\n",
      "Iter 98/100 - Loss theta2: -7.407\n",
      "Iter 99/100 - Loss theta2: -7.407\n",
      "Iter 100/100 - Loss theta2: -7.409\n",
      "tensor([[0.6887],\n",
      "        [0.6884]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7172],\n",
      "        [0.7172]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 112.879\n",
      "Iter 2/50 - Loss hyperparam: 81.200\n",
      "Iter 3/50 - Loss hyperparam: 47.252\n",
      "Iter 4/50 - Loss hyperparam: 11.416\n",
      "Iter 5/50 - Loss hyperparam: -26.234\n",
      "Iter 6/50 - Loss hyperparam: -65.794\n",
      "Iter 7/50 - Loss hyperparam: -106.494\n",
      "Iter 8/50 - Loss hyperparam: -146.455\n",
      "Iter 9/50 - Loss hyperparam: -183.146\n",
      "Iter 10/50 - Loss hyperparam: -213.975\n",
      "Iter 11/50 - Loss hyperparam: -238.666\n",
      "Iter 12/50 - Loss hyperparam: -259.467\n",
      "Iter 13/50 - Loss hyperparam: -277.604\n",
      "Iter 14/50 - Loss hyperparam: -293.942\n",
      "Iter 15/50 - Loss hyperparam: -310.334\n",
      "Iter 16/50 - Loss hyperparam: -328.523\n",
      "Iter 17/50 - Loss hyperparam: -349.152\n",
      "Iter 18/50 - Loss hyperparam: -372.288\n",
      "Iter 19/50 - Loss hyperparam: -398.408\n",
      "Iter 20/50 - Loss hyperparam: -428.224\n",
      "Iter 21/50 - Loss hyperparam: -460.941\n",
      "Iter 22/50 - Loss hyperparam: -492.664\n",
      "Iter 23/50 - Loss hyperparam: -517.608\n",
      "Iter 24/50 - Loss hyperparam: -534.503\n",
      "Iter 25/50 - Loss hyperparam: -550.276\n",
      "Iter 26/50 - Loss hyperparam: -572.541\n",
      "Iter 27/50 - Loss hyperparam: -603.401\n",
      "Iter 28/50 - Loss hyperparam: -638.799\n",
      "Iter 29/50 - Loss hyperparam: -669.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30/50 - Loss hyperparam: -688.891\n",
      "Iter 31/50 - Loss hyperparam: -706.773\n",
      "Iter 32/50 - Loss hyperparam: -730.956\n",
      "Iter 33/50 - Loss hyperparam: -760.375\n",
      "Iter 34/50 - Loss hyperparam: -789.686\n",
      "Iter 35/50 - Loss hyperparam: -816.234\n",
      "Iter 36/50 - Loss hyperparam: -837.370\n",
      "Iter 37/50 - Loss hyperparam: -859.141\n",
      "Iter 38/50 - Loss hyperparam: -890.665\n",
      "Iter 39/50 - Loss hyperparam: -921.630\n",
      "Iter 40/50 - Loss hyperparam: -938.110\n",
      "Iter 41/50 - Loss hyperparam: -963.683\n",
      "Iter 42/50 - Loss hyperparam: -996.260\n",
      "Iter 43/50 - Loss hyperparam: -1017.243\n",
      "Iter 44/50 - Loss hyperparam: -1040.648\n",
      "Iter 45/50 - Loss hyperparam: -1069.598\n",
      "Iter 46/50 - Loss hyperparam: -1094.975\n",
      "Iter 47/50 - Loss hyperparam: -1115.342\n",
      "Iter 48/50 - Loss hyperparam: -1148.624\n",
      "Iter 49/50 - Loss hyperparam: -1165.803\n",
      "Iter 50/50 - Loss hyperparam: -1196.124\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.360\n",
      "Iter 2/100 - Loss theta2: -7.326\n",
      "Iter 3/100 - Loss theta2: -7.358\n",
      "Iter 4/100 - Loss theta2: -7.352\n",
      "Iter 5/100 - Loss theta2: -7.340\n",
      "Iter 6/100 - Loss theta2: -7.345\n",
      "Iter 7/100 - Loss theta2: -7.356\n",
      "Iter 8/100 - Loss theta2: -7.360\n",
      "Iter 9/100 - Loss theta2: -7.355\n",
      "Iter 10/100 - Loss theta2: -7.352\n",
      "Iter 11/100 - Loss theta2: -7.352\n",
      "Iter 12/100 - Loss theta2: -7.355\n",
      "Iter 13/100 - Loss theta2: -7.359\n",
      "Iter 14/100 - Loss theta2: -7.362\n",
      "Iter 15/100 - Loss theta2: -7.357\n",
      "Iter 16/100 - Loss theta2: -7.355\n",
      "Iter 17/100 - Loss theta2: -7.356\n",
      "Iter 18/100 - Loss theta2: -7.360\n",
      "Iter 19/100 - Loss theta2: -7.360\n",
      "Iter 20/100 - Loss theta2: -7.361\n",
      "Iter 21/100 - Loss theta2: -7.360\n",
      "Iter 22/100 - Loss theta2: -7.357\n",
      "Iter 23/100 - Loss theta2: -7.358\n",
      "Iter 24/100 - Loss theta2: -7.360\n",
      "Iter 25/100 - Loss theta2: -7.362\n",
      "Iter 26/100 - Loss theta2: -7.360\n",
      "Iter 27/100 - Loss theta2: -7.360\n",
      "Iter 28/100 - Loss theta2: -7.360\n",
      "Iter 29/100 - Loss theta2: -7.360\n",
      "Iter 30/100 - Loss theta2: -7.361\n",
      "Iter 31/100 - Loss theta2: -7.361\n",
      "Iter 32/100 - Loss theta2: -7.361\n",
      "Iter 33/100 - Loss theta2: -7.361\n",
      "Iter 34/100 - Loss theta2: -7.360\n",
      "Iter 35/100 - Loss theta2: -7.361\n",
      "Iter 36/100 - Loss theta2: -7.361\n",
      "Iter 37/100 - Loss theta2: -7.362\n",
      "Iter 38/100 - Loss theta2: -7.362\n",
      "Iter 39/100 - Loss theta2: -7.361\n",
      "Iter 40/100 - Loss theta2: -7.362\n",
      "Iter 41/100 - Loss theta2: -7.360\n",
      "Iter 42/100 - Loss theta2: -7.362\n",
      "Iter 43/100 - Loss theta2: -7.362\n",
      "Iter 44/100 - Loss theta2: -7.360\n",
      "Iter 45/100 - Loss theta2: -7.362\n",
      "Iter 46/100 - Loss theta2: -7.362\n",
      "Iter 47/100 - Loss theta2: -7.362\n",
      "Iter 48/100 - Loss theta2: -7.361\n",
      "Iter 49/100 - Loss theta2: -7.362\n",
      "Iter 50/100 - Loss theta2: -7.363\n",
      "Iter 51/100 - Loss theta2: -7.362\n",
      "Iter 52/100 - Loss theta2: -7.363\n",
      "Iter 53/100 - Loss theta2: -7.362\n",
      "Iter 54/100 - Loss theta2: -7.363\n",
      "Iter 55/100 - Loss theta2: -7.362\n",
      "Iter 56/100 - Loss theta2: -7.362\n",
      "Iter 57/100 - Loss theta2: -7.362\n",
      "Iter 58/100 - Loss theta2: -7.362\n",
      "Iter 59/100 - Loss theta2: -7.362\n",
      "Iter 60/100 - Loss theta2: -7.363\n",
      "Iter 61/100 - Loss theta2: -7.362\n",
      "Iter 62/100 - Loss theta2: -7.362\n",
      "Iter 63/100 - Loss theta2: -7.364\n",
      "Iter 64/100 - Loss theta2: -7.363\n",
      "Iter 65/100 - Loss theta2: -7.363\n",
      "Iter 66/100 - Loss theta2: -7.363\n",
      "Iter 67/100 - Loss theta2: -7.363\n",
      "Iter 68/100 - Loss theta2: -7.363\n",
      "Iter 69/100 - Loss theta2: -7.363\n",
      "Iter 70/100 - Loss theta2: -7.364\n",
      "Iter 71/100 - Loss theta2: -7.363\n",
      "Iter 72/100 - Loss theta2: -7.363\n",
      "Iter 73/100 - Loss theta2: -7.365\n",
      "Iter 74/100 - Loss theta2: -7.363\n",
      "Iter 75/100 - Loss theta2: -7.363\n",
      "Iter 76/100 - Loss theta2: -7.363\n",
      "Iter 77/100 - Loss theta2: -7.363\n",
      "Iter 78/100 - Loss theta2: -7.363\n",
      "Iter 79/100 - Loss theta2: -7.364\n",
      "Iter 80/100 - Loss theta2: -7.363\n",
      "Iter 81/100 - Loss theta2: -7.363\n",
      "Iter 82/100 - Loss theta2: -7.364\n",
      "Iter 83/100 - Loss theta2: -7.365\n",
      "Iter 84/100 - Loss theta2: -7.363\n",
      "Iter 85/100 - Loss theta2: -7.363\n",
      "Iter 86/100 - Loss theta2: -7.364\n",
      "Iter 87/100 - Loss theta2: -7.363\n",
      "Iter 88/100 - Loss theta2: -7.363\n",
      "Iter 89/100 - Loss theta2: -7.363\n",
      "Iter 90/100 - Loss theta2: -7.362\n",
      "Iter 91/100 - Loss theta2: -7.362\n",
      "Iter 92/100 - Loss theta2: -7.364\n",
      "Iter 93/100 - Loss theta2: -7.363\n",
      "Iter 94/100 - Loss theta2: -7.364\n",
      "Iter 95/100 - Loss theta2: -7.363\n",
      "Iter 96/100 - Loss theta2: -7.364\n",
      "Iter 97/100 - Loss theta2: -7.362\n",
      "Iter 98/100 - Loss theta2: -7.364\n",
      "Iter 99/100 - Loss theta2: -7.362\n",
      "Iter 100/100 - Loss theta2: -7.363\n",
      "tensor([[0.6879],\n",
      "        [0.6877]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7167],\n",
      "        [0.7166]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 35.157\n",
      "Iter 2/50 - Loss hyperparam: -0.975\n",
      "Iter 3/50 - Loss hyperparam: -36.900\n",
      "Iter 4/50 - Loss hyperparam: -72.502\n",
      "Iter 5/50 - Loss hyperparam: -108.644\n",
      "Iter 6/50 - Loss hyperparam: -142.959\n",
      "Iter 7/50 - Loss hyperparam: -172.267\n",
      "Iter 8/50 - Loss hyperparam: -195.112\n",
      "Iter 9/50 - Loss hyperparam: -213.446\n",
      "Iter 10/50 - Loss hyperparam: -229.894\n",
      "Iter 11/50 - Loss hyperparam: -246.489\n",
      "Iter 12/50 - Loss hyperparam: -264.632\n",
      "Iter 13/50 - Loss hyperparam: -285.406\n",
      "Iter 14/50 - Loss hyperparam: -309.507\n",
      "Iter 15/50 - Loss hyperparam: -336.113\n",
      "Iter 16/50 - Loss hyperparam: -362.919\n",
      "Iter 17/50 - Loss hyperparam: -387.707\n",
      "Iter 18/50 - Loss hyperparam: -410.120\n",
      "Iter 19/50 - Loss hyperparam: -430.629\n",
      "Iter 20/50 - Loss hyperparam: -450.492\n",
      "Iter 21/50 - Loss hyperparam: -472.443\n",
      "Iter 22/50 - Loss hyperparam: -498.492\n",
      "Iter 23/50 - Loss hyperparam: -526.934\n",
      "Iter 24/50 - Loss hyperparam: -554.388\n",
      "Iter 25/50 - Loss hyperparam: -579.427\n",
      "Iter 26/50 - Loss hyperparam: -601.426\n",
      "Iter 27/50 - Loss hyperparam: -623.738\n",
      "Iter 28/50 - Loss hyperparam: -650.627\n",
      "Iter 29/50 - Loss hyperparam: -679.393\n",
      "Iter 30/50 - Loss hyperparam: -706.638\n",
      "Iter 31/50 - Loss hyperparam: -729.373\n",
      "Iter 32/50 - Loss hyperparam: -752.366\n",
      "Iter 33/50 - Loss hyperparam: -779.751\n",
      "Iter 34/50 - Loss hyperparam: -808.004\n",
      "Iter 35/50 - Loss hyperparam: -831.131\n",
      "Iter 36/50 - Loss hyperparam: -854.215\n",
      "Iter 37/50 - Loss hyperparam: -881.745\n",
      "Iter 38/50 - Loss hyperparam: -908.702\n",
      "Iter 39/50 - Loss hyperparam: -931.462\n",
      "Iter 40/50 - Loss hyperparam: -958.354\n",
      "Iter 41/50 - Loss hyperparam: -985.410\n",
      "Iter 42/50 - Loss hyperparam: -1007.420\n",
      "Iter 43/50 - Loss hyperparam: -1035.801\n",
      "Iter 44/50 - Loss hyperparam: -1061.477\n",
      "Iter 45/50 - Loss hyperparam: -1085.328\n",
      "Iter 46/50 - Loss hyperparam: -1111.584\n",
      "Iter 47/50 - Loss hyperparam: -1131.647\n",
      "Iter 48/50 - Loss hyperparam: -1161.317\n",
      "Iter 49/50 - Loss hyperparam: -1185.758\n",
      "Iter 50/50 - Loss hyperparam: -1211.096\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.284\n",
      "Iter 2/100 - Loss theta2: -7.260\n",
      "Iter 3/100 - Loss theta2: -7.283\n",
      "Iter 4/100 - Loss theta2: -7.278\n",
      "Iter 5/100 - Loss theta2: -7.270\n",
      "Iter 6/100 - Loss theta2: -7.275\n",
      "Iter 7/100 - Loss theta2: -7.283\n",
      "Iter 8/100 - Loss theta2: -7.284\n",
      "Iter 9/100 - Loss theta2: -7.279\n",
      "Iter 10/100 - Loss theta2: -7.277\n",
      "Iter 11/100 - Loss theta2: -7.280\n",
      "Iter 12/100 - Loss theta2: -7.283\n",
      "Iter 13/100 - Loss theta2: -7.285\n",
      "Iter 14/100 - Loss theta2: -7.284\n",
      "Iter 15/100 - Loss theta2: -7.280\n",
      "Iter 16/100 - Loss theta2: -7.281\n",
      "Iter 17/100 - Loss theta2: -7.284\n",
      "Iter 18/100 - Loss theta2: -7.284\n",
      "Iter 19/100 - Loss theta2: -7.284\n",
      "Iter 20/100 - Loss theta2: -7.282\n",
      "Iter 21/100 - Loss theta2: -7.282\n",
      "Iter 22/100 - Loss theta2: -7.284\n",
      "Iter 23/100 - Loss theta2: -7.284\n",
      "Iter 24/100 - Loss theta2: -7.285\n",
      "Iter 25/100 - Loss theta2: -7.284\n",
      "Iter 26/100 - Loss theta2: -7.284\n",
      "Iter 27/100 - Loss theta2: -7.284\n",
      "Iter 28/100 - Loss theta2: -7.285\n",
      "Iter 29/100 - Loss theta2: -7.285\n",
      "Iter 30/100 - Loss theta2: -7.285\n",
      "Iter 31/100 - Loss theta2: -7.283\n",
      "Iter 32/100 - Loss theta2: -7.284\n",
      "Iter 33/100 - Loss theta2: -7.284\n",
      "Iter 34/100 - Loss theta2: -7.286\n",
      "Iter 35/100 - Loss theta2: -7.285\n",
      "Iter 36/100 - Loss theta2: -7.285\n",
      "Iter 37/100 - Loss theta2: -7.284\n",
      "Iter 38/100 - Loss theta2: -7.284\n",
      "Iter 39/100 - Loss theta2: -7.284\n",
      "Iter 40/100 - Loss theta2: -7.284\n",
      "Iter 41/100 - Loss theta2: -7.284\n",
      "Iter 42/100 - Loss theta2: -7.287\n",
      "Iter 43/100 - Loss theta2: -7.283\n",
      "Iter 44/100 - Loss theta2: -7.284\n",
      "Iter 45/100 - Loss theta2: -7.286\n",
      "Iter 46/100 - Loss theta2: -7.284\n",
      "Iter 47/100 - Loss theta2: -7.285\n",
      "Iter 48/100 - Loss theta2: -7.284\n",
      "Iter 49/100 - Loss theta2: -7.283\n",
      "Iter 50/100 - Loss theta2: -7.284\n",
      "Iter 51/100 - Loss theta2: -7.285\n",
      "Iter 52/100 - Loss theta2: -7.285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 53/100 - Loss theta2: -7.285\n",
      "Iter 54/100 - Loss theta2: -7.285\n",
      "Iter 55/100 - Loss theta2: -7.285\n",
      "Iter 56/100 - Loss theta2: -7.286\n",
      "Iter 57/100 - Loss theta2: -7.286\n",
      "Iter 58/100 - Loss theta2: -7.286\n",
      "Iter 59/100 - Loss theta2: -7.284\n",
      "Iter 60/100 - Loss theta2: -7.285\n",
      "Iter 61/100 - Loss theta2: -7.285\n",
      "Iter 62/100 - Loss theta2: -7.285\n",
      "Iter 63/100 - Loss theta2: -7.284\n",
      "Iter 64/100 - Loss theta2: -7.284\n",
      "Iter 65/100 - Loss theta2: -7.286\n",
      "Iter 66/100 - Loss theta2: -7.286\n",
      "Iter 67/100 - Loss theta2: -7.285\n",
      "Iter 68/100 - Loss theta2: -7.286\n",
      "Iter 69/100 - Loss theta2: -7.285\n",
      "Iter 70/100 - Loss theta2: -7.286\n",
      "Iter 71/100 - Loss theta2: -7.284\n",
      "Iter 72/100 - Loss theta2: -7.286\n",
      "Iter 73/100 - Loss theta2: -7.285\n",
      "Iter 74/100 - Loss theta2: -7.285\n",
      "Iter 75/100 - Loss theta2: -7.285\n",
      "Iter 76/100 - Loss theta2: -7.286\n",
      "Iter 77/100 - Loss theta2: -7.285\n",
      "Iter 78/100 - Loss theta2: -7.285\n",
      "Iter 79/100 - Loss theta2: -7.286\n",
      "Iter 80/100 - Loss theta2: -7.284\n",
      "Iter 81/100 - Loss theta2: -7.285\n",
      "Iter 82/100 - Loss theta2: -7.285\n",
      "Iter 83/100 - Loss theta2: -7.285\n",
      "Iter 84/100 - Loss theta2: -7.286\n",
      "Iter 85/100 - Loss theta2: -7.286\n",
      "Iter 86/100 - Loss theta2: -7.285\n",
      "Iter 87/100 - Loss theta2: -7.284\n",
      "Iter 88/100 - Loss theta2: -7.285\n",
      "Iter 89/100 - Loss theta2: -7.286\n",
      "Iter 90/100 - Loss theta2: -7.286\n",
      "Iter 91/100 - Loss theta2: -7.285\n",
      "Iter 92/100 - Loss theta2: -7.286\n",
      "Iter 93/100 - Loss theta2: -7.284\n",
      "Iter 94/100 - Loss theta2: -7.285\n",
      "Iter 95/100 - Loss theta2: -7.286\n",
      "Iter 96/100 - Loss theta2: -7.286\n",
      "Iter 97/100 - Loss theta2: -7.285\n",
      "Iter 98/100 - Loss theta2: -7.285\n",
      "Iter 99/100 - Loss theta2: -7.286\n",
      "Iter 100/100 - Loss theta2: -7.284\n",
      "tensor([[0.6878],\n",
      "        [0.6881]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7190],\n",
      "        [0.7188]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 101.824\n",
      "Iter 2/50 - Loss hyperparam: 67.211\n",
      "Iter 3/50 - Loss hyperparam: 30.197\n",
      "Iter 4/50 - Loss hyperparam: -8.770\n",
      "Iter 5/50 - Loss hyperparam: -49.605\n",
      "Iter 6/50 - Loss hyperparam: -91.841\n",
      "Iter 7/50 - Loss hyperparam: -133.531\n",
      "Iter 8/50 - Loss hyperparam: -171.789\n",
      "Iter 9/50 - Loss hyperparam: -204.315\n",
      "Iter 10/50 - Loss hyperparam: -231.546\n",
      "Iter 11/50 - Loss hyperparam: -255.546\n",
      "Iter 12/50 - Loss hyperparam: -276.646\n",
      "Iter 13/50 - Loss hyperparam: -294.873\n",
      "Iter 14/50 - Loss hyperparam: -311.734\n",
      "Iter 15/50 - Loss hyperparam: -329.263\n",
      "Iter 16/50 - Loss hyperparam: -348.784\n",
      "Iter 17/50 - Loss hyperparam: -371.282\n",
      "Iter 18/50 - Loss hyperparam: -397.919\n",
      "Iter 19/50 - Loss hyperparam: -429.199\n",
      "Iter 20/50 - Loss hyperparam: -463.522\n",
      "Iter 21/50 - Loss hyperparam: -496.311\n",
      "Iter 22/50 - Loss hyperparam: -521.707\n",
      "Iter 23/50 - Loss hyperparam: -539.287\n",
      "Iter 24/50 - Loss hyperparam: -556.713\n",
      "Iter 25/50 - Loss hyperparam: -580.739\n",
      "Iter 26/50 - Loss hyperparam: -612.181\n",
      "Iter 27/50 - Loss hyperparam: -646.652\n",
      "Iter 28/50 - Loss hyperparam: -677.185\n",
      "Iter 29/50 - Loss hyperparam: -701.671\n",
      "Iter 30/50 - Loss hyperparam: -723.707\n",
      "Iter 31/50 - Loss hyperparam: -747.197\n",
      "Iter 32/50 - Loss hyperparam: -774.164\n",
      "Iter 33/50 - Loss hyperparam: -805.845\n",
      "Iter 34/50 - Loss hyperparam: -836.740\n",
      "Iter 35/50 - Loss hyperparam: -857.171\n",
      "Iter 36/50 - Loss hyperparam: -878.329\n",
      "Iter 37/50 - Loss hyperparam: -911.665\n",
      "Iter 38/50 - Loss hyperparam: -943.196\n",
      "Iter 39/50 - Loss hyperparam: -964.402\n",
      "Iter 40/50 - Loss hyperparam: -989.853\n",
      "Iter 41/50 - Loss hyperparam: -1020.083\n",
      "Iter 42/50 - Loss hyperparam: -1049.241\n",
      "Iter 43/50 - Loss hyperparam: -1068.821\n",
      "Iter 44/50 - Loss hyperparam: -1100.219\n",
      "Iter 45/50 - Loss hyperparam: -1127.273\n",
      "Iter 46/50 - Loss hyperparam: -1149.881\n",
      "Iter 47/50 - Loss hyperparam: -1180.039\n",
      "Iter 48/50 - Loss hyperparam: -1205.170\n",
      "Iter 49/50 - Loss hyperparam: -1230.667\n",
      "Iter 50/50 - Loss hyperparam: -1258.280\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.344\n",
      "Iter 2/100 - Loss theta2: -7.311\n",
      "Iter 3/100 - Loss theta2: -7.343\n",
      "Iter 4/100 - Loss theta2: -7.338\n",
      "Iter 5/100 - Loss theta2: -7.327\n",
      "Iter 6/100 - Loss theta2: -7.329\n",
      "Iter 7/100 - Loss theta2: -7.341\n",
      "Iter 8/100 - Loss theta2: -7.345\n",
      "Iter 9/100 - Loss theta2: -7.340\n",
      "Iter 10/100 - Loss theta2: -7.336\n",
      "Iter 11/100 - Loss theta2: -7.337\n",
      "Iter 12/100 - Loss theta2: -7.341\n",
      "Iter 13/100 - Loss theta2: -7.344\n",
      "Iter 14/100 - Loss theta2: -7.344\n",
      "Iter 15/100 - Loss theta2: -7.343\n",
      "Iter 16/100 - Loss theta2: -7.340\n",
      "Iter 17/100 - Loss theta2: -7.342\n",
      "Iter 18/100 - Loss theta2: -7.344\n",
      "Iter 19/100 - Loss theta2: -7.346\n",
      "Iter 20/100 - Loss theta2: -7.347\n",
      "Iter 21/100 - Loss theta2: -7.343\n",
      "Iter 22/100 - Loss theta2: -7.344\n",
      "Iter 23/100 - Loss theta2: -7.345\n",
      "Iter 24/100 - Loss theta2: -7.347\n",
      "Iter 25/100 - Loss theta2: -7.346\n",
      "Iter 26/100 - Loss theta2: -7.346\n",
      "Iter 27/100 - Loss theta2: -7.345\n",
      "Iter 28/100 - Loss theta2: -7.346\n",
      "Iter 29/100 - Loss theta2: -7.346\n",
      "Iter 30/100 - Loss theta2: -7.346\n",
      "Iter 31/100 - Loss theta2: -7.347\n",
      "Iter 32/100 - Loss theta2: -7.348\n",
      "Iter 33/100 - Loss theta2: -7.347\n",
      "Iter 34/100 - Loss theta2: -7.346\n",
      "Iter 35/100 - Loss theta2: -7.347\n",
      "Iter 36/100 - Loss theta2: -7.347\n",
      "Iter 37/100 - Loss theta2: -7.347\n",
      "Iter 38/100 - Loss theta2: -7.347\n",
      "Iter 39/100 - Loss theta2: -7.347\n",
      "Iter 40/100 - Loss theta2: -7.347\n",
      "Iter 41/100 - Loss theta2: -7.348\n",
      "Iter 42/100 - Loss theta2: -7.347\n",
      "Iter 43/100 - Loss theta2: -7.348\n",
      "Iter 44/100 - Loss theta2: -7.348\n",
      "Iter 45/100 - Loss theta2: -7.347\n",
      "Iter 46/100 - Loss theta2: -7.349\n",
      "Iter 47/100 - Loss theta2: -7.347\n",
      "Iter 48/100 - Loss theta2: -7.348\n",
      "Iter 49/100 - Loss theta2: -7.349\n",
      "Iter 50/100 - Loss theta2: -7.348\n",
      "Iter 51/100 - Loss theta2: -7.347\n",
      "Iter 52/100 - Loss theta2: -7.347\n",
      "Iter 53/100 - Loss theta2: -7.348\n",
      "Iter 54/100 - Loss theta2: -7.348\n",
      "Iter 55/100 - Loss theta2: -7.348\n",
      "Iter 56/100 - Loss theta2: -7.348\n",
      "Iter 57/100 - Loss theta2: -7.350\n",
      "Iter 58/100 - Loss theta2: -7.348\n",
      "Iter 59/100 - Loss theta2: -7.349\n",
      "Iter 60/100 - Loss theta2: -7.350\n",
      "Iter 61/100 - Loss theta2: -7.350\n",
      "Iter 62/100 - Loss theta2: -7.348\n",
      "Iter 63/100 - Loss theta2: -7.348\n",
      "Iter 64/100 - Loss theta2: -7.349\n",
      "Iter 65/100 - Loss theta2: -7.349\n",
      "Iter 66/100 - Loss theta2: -7.348\n",
      "Iter 67/100 - Loss theta2: -7.350\n",
      "Iter 68/100 - Loss theta2: -7.350\n",
      "Iter 69/100 - Loss theta2: -7.350\n",
      "Iter 70/100 - Loss theta2: -7.349\n",
      "Iter 71/100 - Loss theta2: -7.348\n",
      "Iter 72/100 - Loss theta2: -7.350\n",
      "Iter 73/100 - Loss theta2: -7.350\n",
      "Iter 74/100 - Loss theta2: -7.348\n",
      "Iter 75/100 - Loss theta2: -7.351\n",
      "Iter 76/100 - Loss theta2: -7.350\n",
      "Iter 77/100 - Loss theta2: -7.350\n",
      "Iter 78/100 - Loss theta2: -7.350\n",
      "Iter 79/100 - Loss theta2: -7.351\n",
      "Iter 80/100 - Loss theta2: -7.350\n",
      "Iter 81/100 - Loss theta2: -7.351\n",
      "Iter 82/100 - Loss theta2: -7.349\n",
      "Iter 83/100 - Loss theta2: -7.349\n",
      "Iter 84/100 - Loss theta2: -7.351\n",
      "Iter 85/100 - Loss theta2: -7.351\n",
      "Iter 86/100 - Loss theta2: -7.350\n",
      "Iter 87/100 - Loss theta2: -7.350\n",
      "Iter 88/100 - Loss theta2: -7.351\n",
      "Iter 89/100 - Loss theta2: -7.349\n",
      "Iter 90/100 - Loss theta2: -7.349\n",
      "Iter 91/100 - Loss theta2: -7.351\n",
      "Iter 92/100 - Loss theta2: -7.350\n",
      "Iter 93/100 - Loss theta2: -7.351\n",
      "Iter 94/100 - Loss theta2: -7.351\n",
      "Iter 95/100 - Loss theta2: -7.350\n",
      "Iter 96/100 - Loss theta2: -7.352\n",
      "Iter 97/100 - Loss theta2: -7.352\n",
      "Iter 98/100 - Loss theta2: -7.351\n",
      "Iter 99/100 - Loss theta2: -7.351\n",
      "Iter 100/100 - Loss theta2: -7.351\n",
      "tensor([[0.6881],\n",
      "        [0.6879]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7174]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: -26.322\n",
      "Iter 2/50 - Loss hyperparam: -50.671\n",
      "Iter 3/50 - Loss hyperparam: -79.057\n",
      "Iter 4/50 - Loss hyperparam: -106.541\n",
      "Iter 5/50 - Loss hyperparam: -132.962\n",
      "Iter 6/50 - Loss hyperparam: -159.578\n",
      "Iter 7/50 - Loss hyperparam: -184.859\n",
      "Iter 8/50 - Loss hyperparam: -207.716\n",
      "Iter 9/50 - Loss hyperparam: -229.661\n",
      "Iter 10/50 - Loss hyperparam: -252.125\n",
      "Iter 11/50 - Loss hyperparam: -274.359\n",
      "Iter 12/50 - Loss hyperparam: -295.926\n",
      "Iter 13/50 - Loss hyperparam: -317.914\n",
      "Iter 14/50 - Loss hyperparam: -340.688\n",
      "Iter 15/50 - Loss hyperparam: -363.624\n",
      "Iter 16/50 - Loss hyperparam: -386.646\n",
      "Iter 17/50 - Loss hyperparam: -410.083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18/50 - Loss hyperparam: -434.269\n",
      "Iter 19/50 - Loss hyperparam: -458.571\n",
      "Iter 20/50 - Loss hyperparam: -482.840\n",
      "Iter 21/50 - Loss hyperparam: -508.193\n",
      "Iter 22/50 - Loss hyperparam: -533.243\n",
      "Iter 23/50 - Loss hyperparam: -558.705\n",
      "Iter 24/50 - Loss hyperparam: -584.672\n",
      "Iter 25/50 - Loss hyperparam: -610.490\n",
      "Iter 26/50 - Loss hyperparam: -636.748\n",
      "Iter 27/50 - Loss hyperparam: -663.138\n",
      "Iter 28/50 - Loss hyperparam: -689.571\n",
      "Iter 29/50 - Loss hyperparam: -716.119\n",
      "Iter 30/50 - Loss hyperparam: -742.889\n",
      "Iter 31/50 - Loss hyperparam: -769.476\n",
      "Iter 32/50 - Loss hyperparam: -796.094\n",
      "Iter 33/50 - Loss hyperparam: -822.266\n",
      "Iter 34/50 - Loss hyperparam: -849.067\n",
      "Iter 35/50 - Loss hyperparam: -875.658\n",
      "Iter 36/50 - Loss hyperparam: -901.715\n",
      "Iter 37/50 - Loss hyperparam: -927.820\n",
      "Iter 38/50 - Loss hyperparam: -953.718\n",
      "Iter 39/50 - Loss hyperparam: -979.848\n",
      "Iter 40/50 - Loss hyperparam: -1007.666\n",
      "Iter 41/50 - Loss hyperparam: -1033.357\n",
      "Iter 42/50 - Loss hyperparam: -1057.456\n",
      "Iter 43/50 - Loss hyperparam: -1086.960\n",
      "Iter 44/50 - Loss hyperparam: -1114.715\n",
      "Iter 45/50 - Loss hyperparam: -1137.353\n",
      "Iter 46/50 - Loss hyperparam: -1164.631\n",
      "Iter 47/50 - Loss hyperparam: -1193.917\n",
      "Iter 48/50 - Loss hyperparam: -1217.684\n",
      "Iter 49/50 - Loss hyperparam: -1241.444\n",
      "Iter 50/50 - Loss hyperparam: -1268.950\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.249\n",
      "Iter 2/100 - Loss theta2: -7.218\n",
      "Iter 3/100 - Loss theta2: -7.247\n",
      "Iter 4/100 - Loss theta2: -7.243\n",
      "Iter 5/100 - Loss theta2: -7.231\n",
      "Iter 6/100 - Loss theta2: -7.236\n",
      "Iter 7/100 - Loss theta2: -7.246\n",
      "Iter 8/100 - Loss theta2: -7.251\n",
      "Iter 9/100 - Loss theta2: -7.245\n",
      "Iter 10/100 - Loss theta2: -7.241\n",
      "Iter 11/100 - Loss theta2: -7.242\n",
      "Iter 12/100 - Loss theta2: -7.248\n",
      "Iter 13/100 - Loss theta2: -7.250\n",
      "Iter 14/100 - Loss theta2: -7.252\n",
      "Iter 15/100 - Loss theta2: -7.246\n",
      "Iter 16/100 - Loss theta2: -7.245\n",
      "Iter 17/100 - Loss theta2: -7.246\n",
      "Iter 18/100 - Loss theta2: -7.248\n",
      "Iter 19/100 - Loss theta2: -7.251\n",
      "Iter 20/100 - Loss theta2: -7.251\n",
      "Iter 21/100 - Loss theta2: -7.249\n",
      "Iter 22/100 - Loss theta2: -7.249\n",
      "Iter 23/100 - Loss theta2: -7.249\n",
      "Iter 24/100 - Loss theta2: -7.249\n",
      "Iter 25/100 - Loss theta2: -7.251\n",
      "Iter 26/100 - Loss theta2: -7.251\n",
      "Iter 27/100 - Loss theta2: -7.250\n",
      "Iter 28/100 - Loss theta2: -7.249\n",
      "Iter 29/100 - Loss theta2: -7.250\n",
      "Iter 30/100 - Loss theta2: -7.251\n",
      "Iter 31/100 - Loss theta2: -7.251\n",
      "Iter 32/100 - Loss theta2: -7.251\n",
      "Iter 33/100 - Loss theta2: -7.251\n",
      "Iter 34/100 - Loss theta2: -7.250\n",
      "Iter 35/100 - Loss theta2: -7.251\n",
      "Iter 36/100 - Loss theta2: -7.251\n",
      "Iter 37/100 - Loss theta2: -7.252\n",
      "Iter 38/100 - Loss theta2: -7.251\n",
      "Iter 39/100 - Loss theta2: -7.252\n",
      "Iter 40/100 - Loss theta2: -7.252\n",
      "Iter 41/100 - Loss theta2: -7.252\n",
      "Iter 42/100 - Loss theta2: -7.252\n",
      "Iter 43/100 - Loss theta2: -7.253\n",
      "Iter 44/100 - Loss theta2: -7.251\n",
      "Iter 45/100 - Loss theta2: -7.253\n",
      "Iter 46/100 - Loss theta2: -7.251\n",
      "Iter 47/100 - Loss theta2: -7.252\n",
      "Iter 48/100 - Loss theta2: -7.253\n",
      "Iter 49/100 - Loss theta2: -7.253\n",
      "Iter 50/100 - Loss theta2: -7.253\n",
      "Iter 51/100 - Loss theta2: -7.253\n",
      "Iter 52/100 - Loss theta2: -7.253\n",
      "Iter 53/100 - Loss theta2: -7.253\n",
      "Iter 54/100 - Loss theta2: -7.254\n",
      "Iter 55/100 - Loss theta2: -7.252\n",
      "Iter 56/100 - Loss theta2: -7.255\n",
      "Iter 57/100 - Loss theta2: -7.254\n",
      "Iter 58/100 - Loss theta2: -7.253\n",
      "Iter 59/100 - Loss theta2: -7.255\n",
      "Iter 60/100 - Loss theta2: -7.254\n",
      "Iter 61/100 - Loss theta2: -7.254\n",
      "Iter 62/100 - Loss theta2: -7.254\n",
      "Iter 63/100 - Loss theta2: -7.255\n",
      "Iter 64/100 - Loss theta2: -7.253\n",
      "Iter 65/100 - Loss theta2: -7.255\n",
      "Iter 66/100 - Loss theta2: -7.254\n",
      "Iter 67/100 - Loss theta2: -7.255\n",
      "Iter 68/100 - Loss theta2: -7.254\n",
      "Iter 69/100 - Loss theta2: -7.255\n",
      "Iter 70/100 - Loss theta2: -7.256\n",
      "Iter 71/100 - Loss theta2: -7.253\n",
      "Iter 72/100 - Loss theta2: -7.255\n",
      "Iter 73/100 - Loss theta2: -7.255\n",
      "Iter 74/100 - Loss theta2: -7.254\n",
      "Iter 75/100 - Loss theta2: -7.255\n",
      "Iter 76/100 - Loss theta2: -7.256\n",
      "Iter 77/100 - Loss theta2: -7.254\n",
      "Iter 78/100 - Loss theta2: -7.255\n",
      "Iter 79/100 - Loss theta2: -7.254\n",
      "Iter 80/100 - Loss theta2: -7.254\n",
      "Iter 81/100 - Loss theta2: -7.253\n",
      "Iter 82/100 - Loss theta2: -7.255\n",
      "Iter 83/100 - Loss theta2: -7.255\n",
      "Iter 84/100 - Loss theta2: -7.254\n",
      "Iter 85/100 - Loss theta2: -7.256\n",
      "Iter 86/100 - Loss theta2: -7.255\n",
      "Iter 87/100 - Loss theta2: -7.255\n",
      "Iter 88/100 - Loss theta2: -7.255\n",
      "Iter 89/100 - Loss theta2: -7.256\n",
      "Iter 90/100 - Loss theta2: -7.255\n",
      "Iter 91/100 - Loss theta2: -7.256\n",
      "Iter 92/100 - Loss theta2: -7.256\n",
      "Iter 93/100 - Loss theta2: -7.254\n",
      "Iter 94/100 - Loss theta2: -7.256\n",
      "Iter 95/100 - Loss theta2: -7.255\n",
      "Iter 96/100 - Loss theta2: -7.256\n",
      "Iter 97/100 - Loss theta2: -7.256\n",
      "Iter 98/100 - Loss theta2: -7.257\n",
      "Iter 99/100 - Loss theta2: -7.257\n",
      "Iter 100/100 - Loss theta2: -7.256\n",
      "tensor([[0.6876],\n",
      "        [0.6877]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7190],\n",
      "        [0.7189]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 90.062\n",
      "Iter 2/50 - Loss hyperparam: 52.162\n",
      "Iter 3/50 - Loss hyperparam: 12.089\n",
      "Iter 4/50 - Loss hyperparam: -30.031\n",
      "Iter 5/50 - Loss hyperparam: -74.428\n",
      "Iter 6/50 - Loss hyperparam: -120.594\n",
      "Iter 7/50 - Loss hyperparam: -165.856\n",
      "Iter 8/50 - Loss hyperparam: -205.783\n",
      "Iter 9/50 - Loss hyperparam: -235.759\n",
      "Iter 10/50 - Loss hyperparam: -254.262\n",
      "Iter 11/50 - Loss hyperparam: -266.033\n",
      "Iter 12/50 - Loss hyperparam: -278.192\n",
      "Iter 13/50 - Loss hyperparam: -295.113\n",
      "Iter 14/50 - Loss hyperparam: -318.176\n",
      "Iter 15/50 - Loss hyperparam: -347.058\n",
      "Iter 16/50 - Loss hyperparam: -380.261\n",
      "Iter 17/50 - Loss hyperparam: -414.878\n",
      "Iter 18/50 - Loss hyperparam: -446.597\n",
      "Iter 19/50 - Loss hyperparam: -471.865\n",
      "Iter 20/50 - Loss hyperparam: -491.744\n",
      "Iter 21/50 - Loss hyperparam: -511.388\n",
      "Iter 22/50 - Loss hyperparam: -535.502\n",
      "Iter 23/50 - Loss hyperparam: -565.615\n",
      "Iter 24/50 - Loss hyperparam: -599.540\n",
      "Iter 25/50 - Loss hyperparam: -632.184\n",
      "Iter 26/50 - Loss hyperparam: -659.534\n",
      "Iter 27/50 - Loss hyperparam: -683.172\n",
      "Iter 28/50 - Loss hyperparam: -708.073\n",
      "Iter 29/50 - Loss hyperparam: -737.415\n",
      "Iter 30/50 - Loss hyperparam: -768.980\n",
      "Iter 31/50 - Loss hyperparam: -798.601\n",
      "Iter 32/50 - Loss hyperparam: -825.371\n",
      "Iter 33/50 - Loss hyperparam: -850.731\n",
      "Iter 34/50 - Loss hyperparam: -877.867\n",
      "Iter 35/50 - Loss hyperparam: -907.974\n",
      "Iter 36/50 - Loss hyperparam: -938.922\n",
      "Iter 37/50 - Loss hyperparam: -964.368\n",
      "Iter 38/50 - Loss hyperparam: -990.188\n",
      "Iter 39/50 - Loss hyperparam: -1023.980\n",
      "Iter 40/50 - Loss hyperparam: -1052.103\n",
      "Iter 41/50 - Loss hyperparam: -1074.422\n",
      "Iter 42/50 - Loss hyperparam: -1109.172\n",
      "Iter 43/50 - Loss hyperparam: -1134.850\n",
      "Iter 44/50 - Loss hyperparam: -1159.978\n",
      "Iter 45/50 - Loss hyperparam: -1194.214\n",
      "Iter 46/50 - Loss hyperparam: -1215.491\n",
      "Iter 47/50 - Loss hyperparam: -1248.855\n",
      "Iter 48/50 - Loss hyperparam: -1272.868\n",
      "Iter 49/50 - Loss hyperparam: -1302.969\n",
      "Iter 50/50 - Loss hyperparam: -1330.035\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.346\n",
      "Iter 2/100 - Loss theta2: -7.311\n",
      "Iter 3/100 - Loss theta2: -7.344\n",
      "Iter 4/100 - Loss theta2: -7.339\n",
      "Iter 5/100 - Loss theta2: -7.326\n",
      "Iter 6/100 - Loss theta2: -7.332\n",
      "Iter 7/100 - Loss theta2: -7.342\n",
      "Iter 8/100 - Loss theta2: -7.345\n",
      "Iter 9/100 - Loss theta2: -7.344\n",
      "Iter 10/100 - Loss theta2: -7.338\n",
      "Iter 11/100 - Loss theta2: -7.337\n",
      "Iter 12/100 - Loss theta2: -7.342\n",
      "Iter 13/100 - Loss theta2: -7.346\n",
      "Iter 14/100 - Loss theta2: -7.347\n",
      "Iter 15/100 - Loss theta2: -7.345\n",
      "Iter 16/100 - Loss theta2: -7.343\n",
      "Iter 17/100 - Loss theta2: -7.343\n",
      "Iter 18/100 - Loss theta2: -7.346\n",
      "Iter 19/100 - Loss theta2: -7.349\n",
      "Iter 20/100 - Loss theta2: -7.347\n",
      "Iter 21/100 - Loss theta2: -7.348\n",
      "Iter 22/100 - Loss theta2: -7.347\n",
      "Iter 23/100 - Loss theta2: -7.346\n",
      "Iter 24/100 - Loss theta2: -7.349\n",
      "Iter 25/100 - Loss theta2: -7.349\n",
      "Iter 26/100 - Loss theta2: -7.348\n",
      "Iter 27/100 - Loss theta2: -7.348\n",
      "Iter 28/100 - Loss theta2: -7.348\n",
      "Iter 29/100 - Loss theta2: -7.349\n",
      "Iter 30/100 - Loss theta2: -7.349\n",
      "Iter 31/100 - Loss theta2: -7.350\n",
      "Iter 32/100 - Loss theta2: -7.349\n",
      "Iter 33/100 - Loss theta2: -7.349\n",
      "Iter 34/100 - Loss theta2: -7.350\n",
      "Iter 35/100 - Loss theta2: -7.350\n",
      "Iter 36/100 - Loss theta2: -7.351\n",
      "Iter 37/100 - Loss theta2: -7.350\n",
      "Iter 38/100 - Loss theta2: -7.351\n",
      "Iter 39/100 - Loss theta2: -7.351\n",
      "Iter 40/100 - Loss theta2: -7.350\n",
      "Iter 41/100 - Loss theta2: -7.352\n",
      "Iter 42/100 - Loss theta2: -7.351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 43/100 - Loss theta2: -7.351\n",
      "Iter 44/100 - Loss theta2: -7.351\n",
      "Iter 45/100 - Loss theta2: -7.350\n",
      "Iter 46/100 - Loss theta2: -7.350\n",
      "Iter 47/100 - Loss theta2: -7.352\n",
      "Iter 48/100 - Loss theta2: -7.352\n",
      "Iter 49/100 - Loss theta2: -7.352\n",
      "Iter 50/100 - Loss theta2: -7.352\n",
      "Iter 51/100 - Loss theta2: -7.352\n",
      "Iter 52/100 - Loss theta2: -7.352\n",
      "Iter 53/100 - Loss theta2: -7.352\n",
      "Iter 54/100 - Loss theta2: -7.353\n",
      "Iter 55/100 - Loss theta2: -7.353\n",
      "Iter 56/100 - Loss theta2: -7.352\n",
      "Iter 57/100 - Loss theta2: -7.352\n",
      "Iter 58/100 - Loss theta2: -7.354\n",
      "Iter 59/100 - Loss theta2: -7.354\n",
      "Iter 60/100 - Loss theta2: -7.353\n",
      "Iter 61/100 - Loss theta2: -7.352\n",
      "Iter 62/100 - Loss theta2: -7.353\n",
      "Iter 63/100 - Loss theta2: -7.353\n",
      "Iter 64/100 - Loss theta2: -7.354\n",
      "Iter 65/100 - Loss theta2: -7.354\n",
      "Iter 66/100 - Loss theta2: -7.353\n",
      "Iter 67/100 - Loss theta2: -7.353\n",
      "Iter 68/100 - Loss theta2: -7.355\n",
      "Iter 69/100 - Loss theta2: -7.353\n",
      "Iter 70/100 - Loss theta2: -7.353\n",
      "Iter 71/100 - Loss theta2: -7.354\n",
      "Iter 72/100 - Loss theta2: -7.353\n",
      "Iter 73/100 - Loss theta2: -7.355\n",
      "Iter 74/100 - Loss theta2: -7.354\n",
      "Iter 75/100 - Loss theta2: -7.353\n",
      "Iter 76/100 - Loss theta2: -7.353\n",
      "Iter 77/100 - Loss theta2: -7.355\n",
      "Iter 78/100 - Loss theta2: -7.354\n",
      "Iter 79/100 - Loss theta2: -7.354\n",
      "Iter 80/100 - Loss theta2: -7.354\n",
      "Iter 81/100 - Loss theta2: -7.355\n",
      "Iter 82/100 - Loss theta2: -7.355\n",
      "Iter 83/100 - Loss theta2: -7.356\n",
      "Iter 84/100 - Loss theta2: -7.354\n",
      "Iter 85/100 - Loss theta2: -7.354\n",
      "Iter 86/100 - Loss theta2: -7.356\n",
      "Iter 87/100 - Loss theta2: -7.356\n",
      "Iter 88/100 - Loss theta2: -7.355\n",
      "Iter 89/100 - Loss theta2: -7.355\n",
      "Iter 90/100 - Loss theta2: -7.355\n",
      "Iter 91/100 - Loss theta2: -7.356\n",
      "Iter 92/100 - Loss theta2: -7.356\n",
      "Iter 93/100 - Loss theta2: -7.355\n",
      "Iter 94/100 - Loss theta2: -7.356\n",
      "Iter 95/100 - Loss theta2: -7.355\n",
      "Iter 96/100 - Loss theta2: -7.355\n",
      "Iter 97/100 - Loss theta2: -7.355\n",
      "Iter 98/100 - Loss theta2: -7.356\n",
      "Iter 99/100 - Loss theta2: -7.356\n",
      "Iter 100/100 - Loss theta2: -7.356\n",
      "tensor([[0.6881],\n",
      "        [0.6881]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7172]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 141.066\n",
      "Iter 2/50 - Loss hyperparam: 103.891\n",
      "Iter 3/50 - Loss hyperparam: 66.068\n",
      "Iter 4/50 - Loss hyperparam: 26.137\n",
      "Iter 5/50 - Loss hyperparam: -16.812\n",
      "Iter 6/50 - Loss hyperparam: -63.001\n",
      "Iter 7/50 - Loss hyperparam: -111.333\n",
      "Iter 8/50 - Loss hyperparam: -159.740\n",
      "Iter 9/50 - Loss hyperparam: -205.765\n",
      "Iter 10/50 - Loss hyperparam: -246.398\n",
      "Iter 11/50 - Loss hyperparam: -278.890\n",
      "Iter 12/50 - Loss hyperparam: -303.531\n",
      "Iter 13/50 - Loss hyperparam: -323.141\n",
      "Iter 14/50 - Loss hyperparam: -340.353\n",
      "Iter 15/50 - Loss hyperparam: -357.995\n",
      "Iter 16/50 - Loss hyperparam: -378.687\n",
      "Iter 17/50 - Loss hyperparam: -403.598\n",
      "Iter 18/50 - Loss hyperparam: -432.317\n",
      "Iter 19/50 - Loss hyperparam: -463.588\n",
      "Iter 20/50 - Loss hyperparam: -496.446\n",
      "Iter 21/50 - Loss hyperparam: -530.766\n",
      "Iter 22/50 - Loss hyperparam: -565.564\n",
      "Iter 23/50 - Loss hyperparam: -597.202\n",
      "Iter 24/50 - Loss hyperparam: -622.297\n",
      "Iter 25/50 - Loss hyperparam: -642.898\n",
      "Iter 26/50 - Loss hyperparam: -665.089\n",
      "Iter 27/50 - Loss hyperparam: -694.445\n",
      "Iter 28/50 - Loss hyperparam: -732.610\n",
      "Iter 29/50 - Loss hyperparam: -773.795\n",
      "Iter 30/50 - Loss hyperparam: -804.887\n",
      "Iter 31/50 - Loss hyperparam: -822.648\n",
      "Iter 32/50 - Loss hyperparam: -843.728\n",
      "Iter 33/50 - Loss hyperparam: -878.151\n",
      "Iter 34/50 - Loss hyperparam: -918.615\n",
      "Iter 35/50 - Loss hyperparam: -947.756\n",
      "Iter 36/50 - Loss hyperparam: -968.211\n",
      "Iter 37/50 - Loss hyperparam: -996.613\n",
      "Iter 38/50 - Loss hyperparam: -1032.133\n",
      "Iter 39/50 - Loss hyperparam: -1064.864\n",
      "Iter 40/50 - Loss hyperparam: -1091.668\n",
      "Iter 41/50 - Loss hyperparam: -1116.583\n",
      "Iter 42/50 - Loss hyperparam: -1151.666\n",
      "Iter 43/50 - Loss hyperparam: -1184.772\n",
      "Iter 44/50 - Loss hyperparam: -1203.542\n",
      "Iter 45/50 - Loss hyperparam: -1241.519\n",
      "Iter 46/50 - Loss hyperparam: -1269.133\n",
      "Iter 47/50 - Loss hyperparam: -1294.353\n",
      "Iter 48/50 - Loss hyperparam: -1329.163\n",
      "Iter 49/50 - Loss hyperparam: -1354.909\n",
      "Iter 50/50 - Loss hyperparam: -1383.586\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.438\n",
      "Iter 2/100 - Loss theta2: -7.403\n",
      "Iter 3/100 - Loss theta2: -7.436\n",
      "Iter 4/100 - Loss theta2: -7.431\n",
      "Iter 5/100 - Loss theta2: -7.418\n",
      "Iter 6/100 - Loss theta2: -7.423\n",
      "Iter 7/100 - Loss theta2: -7.436\n",
      "Iter 8/100 - Loss theta2: -7.438\n",
      "Iter 9/100 - Loss theta2: -7.434\n",
      "Iter 10/100 - Loss theta2: -7.429\n",
      "Iter 11/100 - Loss theta2: -7.430\n",
      "Iter 12/100 - Loss theta2: -7.434\n",
      "Iter 13/100 - Loss theta2: -7.439\n",
      "Iter 14/100 - Loss theta2: -7.438\n",
      "Iter 15/100 - Loss theta2: -7.435\n",
      "Iter 16/100 - Loss theta2: -7.433\n",
      "Iter 17/100 - Loss theta2: -7.437\n",
      "Iter 18/100 - Loss theta2: -7.439\n",
      "Iter 19/100 - Loss theta2: -7.440\n",
      "Iter 20/100 - Loss theta2: -7.440\n",
      "Iter 21/100 - Loss theta2: -7.438\n",
      "Iter 22/100 - Loss theta2: -7.436\n",
      "Iter 23/100 - Loss theta2: -7.438\n",
      "Iter 24/100 - Loss theta2: -7.439\n",
      "Iter 25/100 - Loss theta2: -7.440\n",
      "Iter 26/100 - Loss theta2: -7.439\n",
      "Iter 27/100 - Loss theta2: -7.438\n",
      "Iter 28/100 - Loss theta2: -7.438\n",
      "Iter 29/100 - Loss theta2: -7.441\n",
      "Iter 30/100 - Loss theta2: -7.441\n",
      "Iter 31/100 - Loss theta2: -7.442\n",
      "Iter 32/100 - Loss theta2: -7.441\n",
      "Iter 33/100 - Loss theta2: -7.439\n",
      "Iter 34/100 - Loss theta2: -7.440\n",
      "Iter 35/100 - Loss theta2: -7.443\n",
      "Iter 36/100 - Loss theta2: -7.442\n",
      "Iter 37/100 - Loss theta2: -7.442\n",
      "Iter 38/100 - Loss theta2: -7.441\n",
      "Iter 39/100 - Loss theta2: -7.441\n",
      "Iter 40/100 - Loss theta2: -7.441\n",
      "Iter 41/100 - Loss theta2: -7.444\n",
      "Iter 42/100 - Loss theta2: -7.443\n",
      "Iter 43/100 - Loss theta2: -7.442\n",
      "Iter 44/100 - Loss theta2: -7.442\n",
      "Iter 45/100 - Loss theta2: -7.442\n",
      "Iter 46/100 - Loss theta2: -7.442\n",
      "Iter 47/100 - Loss theta2: -7.443\n",
      "Iter 48/100 - Loss theta2: -7.444\n",
      "Iter 49/100 - Loss theta2: -7.443\n",
      "Iter 50/100 - Loss theta2: -7.443\n",
      "Iter 51/100 - Loss theta2: -7.442\n",
      "Iter 52/100 - Loss theta2: -7.443\n",
      "Iter 53/100 - Loss theta2: -7.445\n",
      "Iter 54/100 - Loss theta2: -7.444\n",
      "Iter 55/100 - Loss theta2: -7.444\n",
      "Iter 56/100 - Loss theta2: -7.444\n",
      "Iter 57/100 - Loss theta2: -7.444\n",
      "Iter 58/100 - Loss theta2: -7.443\n",
      "Iter 59/100 - Loss theta2: -7.444\n",
      "Iter 60/100 - Loss theta2: -7.445\n",
      "Iter 61/100 - Loss theta2: -7.445\n",
      "Iter 62/100 - Loss theta2: -7.445\n",
      "Iter 63/100 - Loss theta2: -7.445\n",
      "Iter 64/100 - Loss theta2: -7.444\n",
      "Iter 65/100 - Loss theta2: -7.445\n",
      "Iter 66/100 - Loss theta2: -7.445\n",
      "Iter 67/100 - Loss theta2: -7.446\n",
      "Iter 68/100 - Loss theta2: -7.445\n",
      "Iter 69/100 - Loss theta2: -7.446\n",
      "Iter 70/100 - Loss theta2: -7.446\n",
      "Iter 71/100 - Loss theta2: -7.446\n",
      "Iter 72/100 - Loss theta2: -7.445\n",
      "Iter 73/100 - Loss theta2: -7.445\n",
      "Iter 74/100 - Loss theta2: -7.445\n",
      "Iter 75/100 - Loss theta2: -7.446\n",
      "Iter 76/100 - Loss theta2: -7.446\n",
      "Iter 77/100 - Loss theta2: -7.447\n",
      "Iter 78/100 - Loss theta2: -7.448\n",
      "Iter 79/100 - Loss theta2: -7.446\n",
      "Iter 80/100 - Loss theta2: -7.447\n",
      "Iter 81/100 - Loss theta2: -7.446\n",
      "Iter 82/100 - Loss theta2: -7.446\n",
      "Iter 83/100 - Loss theta2: -7.448\n",
      "Iter 84/100 - Loss theta2: -7.447\n",
      "Iter 85/100 - Loss theta2: -7.447\n",
      "Iter 86/100 - Loss theta2: -7.448\n",
      "Iter 87/100 - Loss theta2: -7.447\n",
      "Iter 88/100 - Loss theta2: -7.446\n",
      "Iter 89/100 - Loss theta2: -7.448\n",
      "Iter 90/100 - Loss theta2: -7.447\n",
      "Iter 91/100 - Loss theta2: -7.448\n",
      "Iter 92/100 - Loss theta2: -7.446\n",
      "Iter 93/100 - Loss theta2: -7.448\n",
      "Iter 94/100 - Loss theta2: -7.448\n",
      "Iter 95/100 - Loss theta2: -7.448\n",
      "Iter 96/100 - Loss theta2: -7.447\n",
      "Iter 97/100 - Loss theta2: -7.449\n",
      "Iter 98/100 - Loss theta2: -7.447\n",
      "Iter 99/100 - Loss theta2: -7.447\n",
      "Iter 100/100 - Loss theta2: -7.448\n",
      "tensor([[0.6893],\n",
      "        [0.6895]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7178],\n",
      "        [0.7176]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 88.127\n",
      "Iter 2/50 - Loss hyperparam: 47.770\n",
      "Iter 3/50 - Loss hyperparam: 5.371\n",
      "Iter 4/50 - Loss hyperparam: -39.104\n",
      "Iter 5/50 - Loss hyperparam: -85.937\n",
      "Iter 6/50 - Loss hyperparam: -134.133\n",
      "Iter 7/50 - Loss hyperparam: -180.206\n",
      "Iter 8/50 - Loss hyperparam: -219.432\n",
      "Iter 9/50 - Loss hyperparam: -248.488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/50 - Loss hyperparam: -268.584\n",
      "Iter 11/50 - Loss hyperparam: -284.574\n",
      "Iter 12/50 - Loss hyperparam: -301.061\n",
      "Iter 13/50 - Loss hyperparam: -321.110\n",
      "Iter 14/50 - Loss hyperparam: -345.984\n",
      "Iter 15/50 - Loss hyperparam: -375.304\n",
      "Iter 16/50 - Loss hyperparam: -407.412\n",
      "Iter 17/50 - Loss hyperparam: -439.917\n",
      "Iter 18/50 - Loss hyperparam: -470.893\n",
      "Iter 19/50 - Loss hyperparam: -499.907\n",
      "Iter 20/50 - Loss hyperparam: -527.059\n",
      "Iter 21/50 - Loss hyperparam: -552.591\n",
      "Iter 22/50 - Loss hyperparam: -578.005\n",
      "Iter 23/50 - Loss hyperparam: -605.058\n",
      "Iter 24/50 - Loss hyperparam: -634.658\n",
      "Iter 25/50 - Loss hyperparam: -667.578\n",
      "Iter 26/50 - Loss hyperparam: -702.764\n",
      "Iter 27/50 - Loss hyperparam: -735.030\n",
      "Iter 28/50 - Loss hyperparam: -760.359\n",
      "Iter 29/50 - Loss hyperparam: -783.796\n",
      "Iter 30/50 - Loss hyperparam: -813.974\n",
      "Iter 31/50 - Loss hyperparam: -851.512\n",
      "Iter 32/50 - Loss hyperparam: -885.335\n",
      "Iter 33/50 - Loss hyperparam: -907.602\n",
      "Iter 34/50 - Loss hyperparam: -932.407\n",
      "Iter 35/50 - Loss hyperparam: -968.329\n",
      "Iter 36/50 - Loss hyperparam: -1002.430\n",
      "Iter 37/50 - Loss hyperparam: -1026.726\n",
      "Iter 38/50 - Loss hyperparam: -1055.947\n",
      "Iter 39/50 - Loss hyperparam: -1090.371\n",
      "Iter 40/50 - Loss hyperparam: -1120.514\n",
      "Iter 41/50 - Loss hyperparam: -1147.763\n",
      "Iter 42/50 - Loss hyperparam: -1178.068\n",
      "Iter 43/50 - Loss hyperparam: -1212.080\n",
      "Iter 44/50 - Loss hyperparam: -1235.461\n",
      "Iter 45/50 - Loss hyperparam: -1270.492\n",
      "Iter 46/50 - Loss hyperparam: -1297.331\n",
      "Iter 47/50 - Loss hyperparam: -1327.685\n",
      "Iter 48/50 - Loss hyperparam: -1357.894\n",
      "Iter 49/50 - Loss hyperparam: -1386.436\n",
      "Iter 50/50 - Loss hyperparam: -1417.701\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.390\n",
      "Iter 2/100 - Loss theta2: -7.355\n",
      "Iter 3/100 - Loss theta2: -7.387\n",
      "Iter 4/100 - Loss theta2: -7.380\n",
      "Iter 5/100 - Loss theta2: -7.372\n",
      "Iter 6/100 - Loss theta2: -7.377\n",
      "Iter 7/100 - Loss theta2: -7.388\n",
      "Iter 8/100 - Loss theta2: -7.391\n",
      "Iter 9/100 - Loss theta2: -7.386\n",
      "Iter 10/100 - Loss theta2: -7.381\n",
      "Iter 11/100 - Loss theta2: -7.382\n",
      "Iter 12/100 - Loss theta2: -7.387\n",
      "Iter 13/100 - Loss theta2: -7.392\n",
      "Iter 14/100 - Loss theta2: -7.390\n",
      "Iter 15/100 - Loss theta2: -7.387\n",
      "Iter 16/100 - Loss theta2: -7.387\n",
      "Iter 17/100 - Loss theta2: -7.387\n",
      "Iter 18/100 - Loss theta2: -7.391\n",
      "Iter 19/100 - Loss theta2: -7.392\n",
      "Iter 20/100 - Loss theta2: -7.391\n",
      "Iter 21/100 - Loss theta2: -7.387\n",
      "Iter 22/100 - Loss theta2: -7.388\n",
      "Iter 23/100 - Loss theta2: -7.390\n",
      "Iter 24/100 - Loss theta2: -7.389\n",
      "Iter 25/100 - Loss theta2: -7.392\n",
      "Iter 26/100 - Loss theta2: -7.391\n",
      "Iter 27/100 - Loss theta2: -7.389\n",
      "Iter 28/100 - Loss theta2: -7.389\n",
      "Iter 29/100 - Loss theta2: -7.391\n",
      "Iter 30/100 - Loss theta2: -7.391\n",
      "Iter 31/100 - Loss theta2: -7.393\n",
      "Iter 32/100 - Loss theta2: -7.392\n",
      "Iter 33/100 - Loss theta2: -7.392\n",
      "Iter 34/100 - Loss theta2: -7.394\n",
      "Iter 35/100 - Loss theta2: -7.393\n",
      "Iter 36/100 - Loss theta2: -7.394\n",
      "Iter 37/100 - Loss theta2: -7.394\n",
      "Iter 38/100 - Loss theta2: -7.393\n",
      "Iter 39/100 - Loss theta2: -7.392\n",
      "Iter 40/100 - Loss theta2: -7.394\n",
      "Iter 41/100 - Loss theta2: -7.393\n",
      "Iter 42/100 - Loss theta2: -7.394\n",
      "Iter 43/100 - Loss theta2: -7.392\n",
      "Iter 44/100 - Loss theta2: -7.393\n",
      "Iter 45/100 - Loss theta2: -7.393\n",
      "Iter 46/100 - Loss theta2: -7.394\n",
      "Iter 47/100 - Loss theta2: -7.394\n",
      "Iter 48/100 - Loss theta2: -7.394\n",
      "Iter 49/100 - Loss theta2: -7.394\n",
      "Iter 50/100 - Loss theta2: -7.394\n",
      "Iter 51/100 - Loss theta2: -7.396\n",
      "Iter 52/100 - Loss theta2: -7.394\n",
      "Iter 53/100 - Loss theta2: -7.395\n",
      "Iter 54/100 - Loss theta2: -7.395\n",
      "Iter 55/100 - Loss theta2: -7.395\n",
      "Iter 56/100 - Loss theta2: -7.395\n",
      "Iter 57/100 - Loss theta2: -7.394\n",
      "Iter 58/100 - Loss theta2: -7.395\n",
      "Iter 59/100 - Loss theta2: -7.395\n",
      "Iter 60/100 - Loss theta2: -7.395\n",
      "Iter 61/100 - Loss theta2: -7.397\n",
      "Iter 62/100 - Loss theta2: -7.396\n",
      "Iter 63/100 - Loss theta2: -7.397\n",
      "Iter 64/100 - Loss theta2: -7.395\n",
      "Iter 65/100 - Loss theta2: -7.397\n",
      "Iter 66/100 - Loss theta2: -7.395\n",
      "Iter 67/100 - Loss theta2: -7.396\n",
      "Iter 68/100 - Loss theta2: -7.395\n",
      "Iter 69/100 - Loss theta2: -7.396\n",
      "Iter 70/100 - Loss theta2: -7.397\n",
      "Iter 71/100 - Loss theta2: -7.398\n",
      "Iter 72/100 - Loss theta2: -7.396\n",
      "Iter 73/100 - Loss theta2: -7.396\n",
      "Iter 74/100 - Loss theta2: -7.395\n",
      "Iter 75/100 - Loss theta2: -7.396\n",
      "Iter 76/100 - Loss theta2: -7.396\n",
      "Iter 77/100 - Loss theta2: -7.397\n",
      "Iter 78/100 - Loss theta2: -7.396\n",
      "Iter 79/100 - Loss theta2: -7.397\n",
      "Iter 80/100 - Loss theta2: -7.398\n",
      "Iter 81/100 - Loss theta2: -7.396\n",
      "Iter 82/100 - Loss theta2: -7.397\n",
      "Iter 83/100 - Loss theta2: -7.397\n",
      "Iter 84/100 - Loss theta2: -7.398\n",
      "Iter 85/100 - Loss theta2: -7.398\n",
      "Iter 86/100 - Loss theta2: -7.398\n",
      "Iter 87/100 - Loss theta2: -7.399\n",
      "Iter 88/100 - Loss theta2: -7.398\n",
      "Iter 89/100 - Loss theta2: -7.396\n",
      "Iter 90/100 - Loss theta2: -7.398\n",
      "Iter 91/100 - Loss theta2: -7.398\n",
      "Iter 92/100 - Loss theta2: -7.396\n",
      "Iter 93/100 - Loss theta2: -7.398\n",
      "Iter 94/100 - Loss theta2: -7.399\n",
      "Iter 95/100 - Loss theta2: -7.396\n",
      "Iter 96/100 - Loss theta2: -7.398\n",
      "Iter 97/100 - Loss theta2: -7.398\n",
      "Iter 98/100 - Loss theta2: -7.398\n",
      "Iter 99/100 - Loss theta2: -7.399\n",
      "Iter 100/100 - Loss theta2: -7.397\n",
      "tensor([[0.6885],\n",
      "        [0.6886]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7172]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 78.823\n",
      "Iter 2/50 - Loss hyperparam: 39.225\n",
      "Iter 3/50 - Loss hyperparam: -2.257\n",
      "Iter 4/50 - Loss hyperparam: -45.464\n",
      "Iter 5/50 - Loss hyperparam: -90.280\n",
      "Iter 6/50 - Loss hyperparam: -134.980\n",
      "Iter 7/50 - Loss hyperparam: -177.745\n",
      "Iter 8/50 - Loss hyperparam: -217.847\n",
      "Iter 9/50 - Loss hyperparam: -254.518\n",
      "Iter 10/50 - Loss hyperparam: -285.342\n",
      "Iter 11/50 - Loss hyperparam: -309.145\n",
      "Iter 12/50 - Loss hyperparam: -327.743\n",
      "Iter 13/50 - Loss hyperparam: -344.149\n",
      "Iter 14/50 - Loss hyperparam: -362.227\n",
      "Iter 15/50 - Loss hyperparam: -384.670\n",
      "Iter 16/50 - Loss hyperparam: -411.743\n",
      "Iter 17/50 - Loss hyperparam: -443.173\n",
      "Iter 18/50 - Loss hyperparam: -478.220\n",
      "Iter 19/50 - Loss hyperparam: -514.194\n",
      "Iter 20/50 - Loss hyperparam: -548.407\n",
      "Iter 21/50 - Loss hyperparam: -577.410\n",
      "Iter 22/50 - Loss hyperparam: -601.045\n",
      "Iter 23/50 - Loss hyperparam: -623.198\n",
      "Iter 24/50 - Loss hyperparam: -648.829\n",
      "Iter 25/50 - Loss hyperparam: -680.269\n",
      "Iter 26/50 - Loss hyperparam: -716.661\n",
      "Iter 27/50 - Loss hyperparam: -753.673\n",
      "Iter 28/50 - Loss hyperparam: -785.936\n",
      "Iter 29/50 - Loss hyperparam: -811.150\n",
      "Iter 30/50 - Loss hyperparam: -835.570\n",
      "Iter 31/50 - Loss hyperparam: -866.366\n",
      "Iter 32/50 - Loss hyperparam: -903.293\n",
      "Iter 33/50 - Loss hyperparam: -938.789\n",
      "Iter 34/50 - Loss hyperparam: -965.524\n",
      "Iter 35/50 - Loss hyperparam: -989.099\n",
      "Iter 36/50 - Loss hyperparam: -1020.330\n",
      "Iter 37/50 - Loss hyperparam: -1058.444\n",
      "Iter 38/50 - Loss hyperparam: -1087.885\n",
      "Iter 39/50 - Loss hyperparam: -1111.290\n",
      "Iter 40/50 - Loss hyperparam: -1146.033\n",
      "Iter 41/50 - Loss hyperparam: -1181.992\n",
      "Iter 42/50 - Loss hyperparam: -1205.835\n",
      "Iter 43/50 - Loss hyperparam: -1236.915\n",
      "Iter 44/50 - Loss hyperparam: -1274.107\n",
      "Iter 45/50 - Loss hyperparam: -1297.212\n",
      "Iter 46/50 - Loss hyperparam: -1328.305\n",
      "Iter 47/50 - Loss hyperparam: -1364.116\n",
      "Iter 48/50 - Loss hyperparam: -1388.686\n",
      "Iter 49/50 - Loss hyperparam: -1422.107\n",
      "Iter 50/50 - Loss hyperparam: -1445.093\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.430\n",
      "Iter 2/100 - Loss theta2: -7.395\n",
      "Iter 3/100 - Loss theta2: -7.429\n",
      "Iter 4/100 - Loss theta2: -7.423\n",
      "Iter 5/100 - Loss theta2: -7.408\n",
      "Iter 6/100 - Loss theta2: -7.415\n",
      "Iter 7/100 - Loss theta2: -7.426\n",
      "Iter 8/100 - Loss theta2: -7.430\n",
      "Iter 9/100 - Loss theta2: -7.426\n",
      "Iter 10/100 - Loss theta2: -7.422\n",
      "Iter 11/100 - Loss theta2: -7.420\n",
      "Iter 12/100 - Loss theta2: -7.426\n",
      "Iter 13/100 - Loss theta2: -7.431\n",
      "Iter 14/100 - Loss theta2: -7.430\n",
      "Iter 15/100 - Loss theta2: -7.429\n",
      "Iter 16/100 - Loss theta2: -7.425\n",
      "Iter 17/100 - Loss theta2: -7.425\n",
      "Iter 18/100 - Loss theta2: -7.430\n",
      "Iter 19/100 - Loss theta2: -7.431\n",
      "Iter 20/100 - Loss theta2: -7.431\n",
      "Iter 21/100 - Loss theta2: -7.430\n",
      "Iter 22/100 - Loss theta2: -7.427\n",
      "Iter 23/100 - Loss theta2: -7.429\n",
      "Iter 24/100 - Loss theta2: -7.429\n",
      "Iter 25/100 - Loss theta2: -7.432\n",
      "Iter 26/100 - Loss theta2: -7.430\n",
      "Iter 27/100 - Loss theta2: -7.431\n",
      "Iter 28/100 - Loss theta2: -7.431\n",
      "Iter 29/100 - Loss theta2: -7.430\n",
      "Iter 30/100 - Loss theta2: -7.432\n",
      "Iter 31/100 - Loss theta2: -7.431\n",
      "Iter 32/100 - Loss theta2: -7.432\n",
      "Iter 33/100 - Loss theta2: -7.433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34/100 - Loss theta2: -7.431\n",
      "Iter 35/100 - Loss theta2: -7.431\n",
      "Iter 36/100 - Loss theta2: -7.431\n",
      "Iter 37/100 - Loss theta2: -7.433\n",
      "Iter 38/100 - Loss theta2: -7.433\n",
      "Iter 39/100 - Loss theta2: -7.431\n",
      "Iter 40/100 - Loss theta2: -7.432\n",
      "Iter 41/100 - Loss theta2: -7.431\n",
      "Iter 42/100 - Loss theta2: -7.434\n",
      "Iter 43/100 - Loss theta2: -7.433\n",
      "Iter 44/100 - Loss theta2: -7.433\n",
      "Iter 45/100 - Loss theta2: -7.431\n",
      "Iter 46/100 - Loss theta2: -7.432\n",
      "Iter 47/100 - Loss theta2: -7.433\n",
      "Iter 48/100 - Loss theta2: -7.431\n",
      "Iter 49/100 - Loss theta2: -7.433\n",
      "Iter 50/100 - Loss theta2: -7.433\n",
      "Iter 51/100 - Loss theta2: -7.433\n",
      "Iter 52/100 - Loss theta2: -7.432\n",
      "Iter 53/100 - Loss theta2: -7.433\n",
      "Iter 54/100 - Loss theta2: -7.432\n",
      "Iter 55/100 - Loss theta2: -7.433\n",
      "Iter 56/100 - Loss theta2: -7.433\n",
      "Iter 57/100 - Loss theta2: -7.434\n",
      "Iter 58/100 - Loss theta2: -7.432\n",
      "Iter 59/100 - Loss theta2: -7.433\n",
      "Iter 60/100 - Loss theta2: -7.432\n",
      "Iter 61/100 - Loss theta2: -7.434\n",
      "Iter 62/100 - Loss theta2: -7.434\n",
      "Iter 63/100 - Loss theta2: -7.435\n",
      "Iter 64/100 - Loss theta2: -7.433\n",
      "Iter 65/100 - Loss theta2: -7.433\n",
      "Iter 66/100 - Loss theta2: -7.433\n",
      "Iter 67/100 - Loss theta2: -7.432\n",
      "Iter 68/100 - Loss theta2: -7.434\n",
      "Iter 69/100 - Loss theta2: -7.433\n",
      "Iter 70/100 - Loss theta2: -7.433\n",
      "Iter 71/100 - Loss theta2: -7.434\n",
      "Iter 72/100 - Loss theta2: -7.433\n",
      "Iter 73/100 - Loss theta2: -7.433\n",
      "Iter 74/100 - Loss theta2: -7.434\n",
      "Iter 75/100 - Loss theta2: -7.432\n",
      "Iter 76/100 - Loss theta2: -7.433\n",
      "Iter 77/100 - Loss theta2: -7.435\n",
      "Iter 78/100 - Loss theta2: -7.435\n",
      "Iter 79/100 - Loss theta2: -7.432\n",
      "Iter 80/100 - Loss theta2: -7.434\n",
      "Iter 81/100 - Loss theta2: -7.434\n",
      "Iter 82/100 - Loss theta2: -7.434\n",
      "Iter 83/100 - Loss theta2: -7.434\n",
      "Iter 84/100 - Loss theta2: -7.434\n",
      "Iter 85/100 - Loss theta2: -7.434\n",
      "Iter 86/100 - Loss theta2: -7.433\n",
      "Iter 87/100 - Loss theta2: -7.433\n",
      "Iter 88/100 - Loss theta2: -7.434\n",
      "Iter 89/100 - Loss theta2: -7.434\n",
      "Iter 90/100 - Loss theta2: -7.433\n",
      "Iter 91/100 - Loss theta2: -7.433\n",
      "Iter 92/100 - Loss theta2: -7.434\n",
      "Iter 93/100 - Loss theta2: -7.435\n",
      "Iter 94/100 - Loss theta2: -7.434\n",
      "Iter 95/100 - Loss theta2: -7.433\n",
      "Iter 96/100 - Loss theta2: -7.435\n",
      "Iter 97/100 - Loss theta2: -7.434\n",
      "Iter 98/100 - Loss theta2: -7.436\n",
      "Iter 99/100 - Loss theta2: -7.435\n",
      "Iter 100/100 - Loss theta2: -7.434\n",
      "tensor([[0.6892],\n",
      "        [0.6895]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7181],\n",
      "        [0.7179]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 197.366\n",
      "Iter 2/50 - Loss hyperparam: 159.157\n",
      "Iter 3/50 - Loss hyperparam: 117.920\n",
      "Iter 4/50 - Loss hyperparam: 73.860\n",
      "Iter 5/50 - Loss hyperparam: 28.438\n",
      "Iter 6/50 - Loss hyperparam: -16.875\n",
      "Iter 7/50 - Loss hyperparam: -61.644\n",
      "Iter 8/50 - Loss hyperparam: -106.017\n",
      "Iter 9/50 - Loss hyperparam: -149.898\n",
      "Iter 10/50 - Loss hyperparam: -192.855\n",
      "Iter 11/50 - Loss hyperparam: -235.084\n",
      "Iter 12/50 - Loss hyperparam: -278.416\n",
      "Iter 13/50 - Loss hyperparam: -324.613\n",
      "Iter 14/50 - Loss hyperparam: -372.661\n",
      "Iter 15/50 - Loss hyperparam: -417.660\n",
      "Iter 16/50 - Loss hyperparam: -451.595\n",
      "Iter 17/50 - Loss hyperparam: -469.915\n",
      "Iter 18/50 - Loss hyperparam: -479.261\n",
      "Iter 19/50 - Loss hyperparam: -490.992\n",
      "Iter 20/50 - Loss hyperparam: -511.442\n",
      "Iter 21/50 - Loss hyperparam: -542.045\n",
      "Iter 22/50 - Loss hyperparam: -581.887\n",
      "Iter 23/50 - Loss hyperparam: -628.084\n",
      "Iter 24/50 - Loss hyperparam: -674.985\n",
      "Iter 25/50 - Loss hyperparam: -716.294\n",
      "Iter 26/50 - Loss hyperparam: -749.816\n",
      "Iter 27/50 - Loss hyperparam: -775.080\n",
      "Iter 28/50 - Loss hyperparam: -795.449\n",
      "Iter 29/50 - Loss hyperparam: -818.292\n",
      "Iter 30/50 - Loss hyperparam: -850.895\n",
      "Iter 31/50 - Loss hyperparam: -896.576\n",
      "Iter 32/50 - Loss hyperparam: -946.471\n",
      "Iter 33/50 - Loss hyperparam: -978.972\n",
      "Iter 34/50 - Loss hyperparam: -995.833\n",
      "Iter 35/50 - Loss hyperparam: -1020.561\n",
      "Iter 36/50 - Loss hyperparam: -1057.824\n",
      "Iter 37/50 - Loss hyperparam: -1100.876\n",
      "Iter 38/50 - Loss hyperparam: -1141.514\n",
      "Iter 39/50 - Loss hyperparam: -1162.492\n",
      "Iter 40/50 - Loss hyperparam: -1185.719\n",
      "Iter 41/50 - Loss hyperparam: -1231.855\n",
      "Iter 42/50 - Loss hyperparam: -1271.221\n",
      "Iter 43/50 - Loss hyperparam: -1294.376\n",
      "Iter 44/50 - Loss hyperparam: -1321.493\n",
      "Iter 45/50 - Loss hyperparam: -1367.085\n",
      "Iter 46/50 - Loss hyperparam: -1396.362\n",
      "Iter 47/50 - Loss hyperparam: -1420.269\n",
      "Iter 48/50 - Loss hyperparam: -1462.052\n",
      "Iter 49/50 - Loss hyperparam: -1493.958\n",
      "Iter 50/50 - Loss hyperparam: -1517.712\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.564\n",
      "Iter 2/100 - Loss theta2: -7.527\n",
      "Iter 3/100 - Loss theta2: -7.563\n",
      "Iter 4/100 - Loss theta2: -7.558\n",
      "Iter 5/100 - Loss theta2: -7.545\n",
      "Iter 6/100 - Loss theta2: -7.551\n",
      "Iter 7/100 - Loss theta2: -7.562\n",
      "Iter 8/100 - Loss theta2: -7.564\n",
      "Iter 9/100 - Loss theta2: -7.558\n",
      "Iter 10/100 - Loss theta2: -7.554\n",
      "Iter 11/100 - Loss theta2: -7.556\n",
      "Iter 12/100 - Loss theta2: -7.561\n",
      "Iter 13/100 - Loss theta2: -7.565\n",
      "Iter 14/100 - Loss theta2: -7.565\n",
      "Iter 15/100 - Loss theta2: -7.562\n",
      "Iter 16/100 - Loss theta2: -7.559\n",
      "Iter 17/100 - Loss theta2: -7.561\n",
      "Iter 18/100 - Loss theta2: -7.566\n",
      "Iter 19/100 - Loss theta2: -7.565\n",
      "Iter 20/100 - Loss theta2: -7.563\n",
      "Iter 21/100 - Loss theta2: -7.563\n",
      "Iter 22/100 - Loss theta2: -7.562\n",
      "Iter 23/100 - Loss theta2: -7.561\n",
      "Iter 24/100 - Loss theta2: -7.565\n",
      "Iter 25/100 - Loss theta2: -7.564\n",
      "Iter 26/100 - Loss theta2: -7.564\n",
      "Iter 27/100 - Loss theta2: -7.563\n",
      "Iter 28/100 - Loss theta2: -7.565\n",
      "Iter 29/100 - Loss theta2: -7.565\n",
      "Iter 30/100 - Loss theta2: -7.564\n",
      "Iter 31/100 - Loss theta2: -7.565\n",
      "Iter 32/100 - Loss theta2: -7.566\n",
      "Iter 33/100 - Loss theta2: -7.565\n",
      "Iter 34/100 - Loss theta2: -7.565\n",
      "Iter 35/100 - Loss theta2: -7.564\n",
      "Iter 36/100 - Loss theta2: -7.565\n",
      "Iter 37/100 - Loss theta2: -7.565\n",
      "Iter 38/100 - Loss theta2: -7.566\n",
      "Iter 39/100 - Loss theta2: -7.565\n",
      "Iter 40/100 - Loss theta2: -7.566\n",
      "Iter 41/100 - Loss theta2: -7.566\n",
      "Iter 42/100 - Loss theta2: -7.565\n",
      "Iter 43/100 - Loss theta2: -7.566\n",
      "Iter 44/100 - Loss theta2: -7.567\n",
      "Iter 45/100 - Loss theta2: -7.566\n",
      "Iter 46/100 - Loss theta2: -7.567\n",
      "Iter 47/100 - Loss theta2: -7.566\n",
      "Iter 48/100 - Loss theta2: -7.567\n",
      "Iter 49/100 - Loss theta2: -7.568\n",
      "Iter 50/100 - Loss theta2: -7.567\n",
      "Iter 51/100 - Loss theta2: -7.569\n",
      "Iter 52/100 - Loss theta2: -7.566\n",
      "Iter 53/100 - Loss theta2: -7.564\n",
      "Iter 54/100 - Loss theta2: -7.567\n",
      "Iter 55/100 - Loss theta2: -7.567\n",
      "Iter 56/100 - Loss theta2: -7.565\n",
      "Iter 57/100 - Loss theta2: -7.564\n",
      "Iter 58/100 - Loss theta2: -7.569\n",
      "Iter 59/100 - Loss theta2: -7.569\n",
      "Iter 60/100 - Loss theta2: -7.566\n",
      "Iter 61/100 - Loss theta2: -7.564\n",
      "Iter 62/100 - Loss theta2: -7.566\n",
      "Iter 63/100 - Loss theta2: -7.566\n",
      "Iter 64/100 - Loss theta2: -7.566\n",
      "Iter 65/100 - Loss theta2: -7.566\n",
      "Iter 66/100 - Loss theta2: -7.566\n",
      "Iter 67/100 - Loss theta2: -7.566\n",
      "Iter 68/100 - Loss theta2: -7.565\n",
      "Iter 69/100 - Loss theta2: -7.567\n",
      "Iter 70/100 - Loss theta2: -7.566\n",
      "Iter 71/100 - Loss theta2: -7.565\n",
      "Iter 72/100 - Loss theta2: -7.566\n",
      "Iter 73/100 - Loss theta2: -7.568\n",
      "Iter 74/100 - Loss theta2: -7.568\n",
      "Iter 75/100 - Loss theta2: -7.567\n",
      "Iter 76/100 - Loss theta2: -7.567\n",
      "Iter 77/100 - Loss theta2: -7.565\n",
      "Iter 78/100 - Loss theta2: -7.567\n",
      "Iter 79/100 - Loss theta2: -7.566\n",
      "Iter 80/100 - Loss theta2: -7.568\n",
      "Iter 81/100 - Loss theta2: -7.566\n",
      "Iter 82/100 - Loss theta2: -7.566\n",
      "Iter 83/100 - Loss theta2: -7.567\n",
      "Iter 84/100 - Loss theta2: -7.565\n",
      "Iter 85/100 - Loss theta2: -7.565\n",
      "Iter 86/100 - Loss theta2: -7.567\n",
      "Iter 87/100 - Loss theta2: -7.568\n",
      "Iter 88/100 - Loss theta2: -7.566\n",
      "Iter 89/100 - Loss theta2: -7.567\n",
      "Iter 90/100 - Loss theta2: -7.565\n",
      "Iter 91/100 - Loss theta2: -7.566\n",
      "Iter 92/100 - Loss theta2: -7.565\n",
      "Iter 93/100 - Loss theta2: -7.565\n",
      "Iter 94/100 - Loss theta2: -7.567\n",
      "Iter 95/100 - Loss theta2: -7.570\n",
      "Iter 96/100 - Loss theta2: -7.567\n",
      "Iter 97/100 - Loss theta2: -7.567\n",
      "Iter 98/100 - Loss theta2: -7.569\n",
      "Iter 99/100 - Loss theta2: -7.565\n",
      "Iter 100/100 - Loss theta2: -7.567\n",
      "tensor([[0.6903],\n",
      "        [0.6906]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7171]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 199.016\n",
      "Iter 2/50 - Loss hyperparam: 164.388\n",
      "Iter 3/50 - Loss hyperparam: 127.856\n",
      "Iter 4/50 - Loss hyperparam: 90.117\n",
      "Iter 5/50 - Loss hyperparam: 51.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6/50 - Loss hyperparam: 11.687\n",
      "Iter 7/50 - Loss hyperparam: -27.809\n",
      "Iter 8/50 - Loss hyperparam: -67.264\n",
      "Iter 9/50 - Loss hyperparam: -107.141\n",
      "Iter 10/50 - Loss hyperparam: -148.827\n",
      "Iter 11/50 - Loss hyperparam: -193.680\n",
      "Iter 12/50 - Loss hyperparam: -241.930\n",
      "Iter 13/50 - Loss hyperparam: -292.843\n",
      "Iter 14/50 - Loss hyperparam: -345.229\n",
      "Iter 15/50 - Loss hyperparam: -399.070\n",
      "Iter 16/50 - Loss hyperparam: -451.640\n",
      "Iter 17/50 - Loss hyperparam: -494.194\n",
      "Iter 18/50 - Loss hyperparam: -520.264\n",
      "Iter 19/50 - Loss hyperparam: -534.530\n",
      "Iter 20/50 - Loss hyperparam: -545.273\n",
      "Iter 21/50 - Loss hyperparam: -560.095\n",
      "Iter 22/50 - Loss hyperparam: -583.849\n",
      "Iter 23/50 - Loss hyperparam: -617.272\n",
      "Iter 24/50 - Loss hyperparam: -658.784\n",
      "Iter 25/50 - Loss hyperparam: -707.600\n",
      "Iter 26/50 - Loss hyperparam: -761.413\n",
      "Iter 27/50 - Loss hyperparam: -809.341\n",
      "Iter 28/50 - Loss hyperparam: -842.161\n",
      "Iter 29/50 - Loss hyperparam: -859.006\n",
      "Iter 30/50 - Loss hyperparam: -875.235\n",
      "Iter 31/50 - Loss hyperparam: -902.923\n",
      "Iter 32/50 - Loss hyperparam: -943.405\n",
      "Iter 33/50 - Loss hyperparam: -995.040\n",
      "Iter 34/50 - Loss hyperparam: -1042.741\n",
      "Iter 35/50 - Loss hyperparam: -1071.419\n",
      "Iter 36/50 - Loss hyperparam: -1087.260\n",
      "Iter 37/50 - Loss hyperparam: -1113.248\n",
      "Iter 38/50 - Loss hyperparam: -1158.597\n",
      "Iter 39/50 - Loss hyperparam: -1209.745\n",
      "Iter 40/50 - Loss hyperparam: -1240.258\n",
      "Iter 41/50 - Loss hyperparam: -1256.838\n",
      "Iter 42/50 - Loss hyperparam: -1291.063\n",
      "Iter 43/50 - Loss hyperparam: -1340.364\n",
      "Iter 44/50 - Loss hyperparam: -1373.920\n",
      "Iter 45/50 - Loss hyperparam: -1392.734\n",
      "Iter 46/50 - Loss hyperparam: -1431.719\n",
      "Iter 47/50 - Loss hyperparam: -1476.419\n",
      "Iter 48/50 - Loss hyperparam: -1496.313\n",
      "Iter 49/50 - Loss hyperparam: -1531.388\n",
      "Iter 50/50 - Loss hyperparam: -1575.554\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.560\n",
      "Iter 2/100 - Loss theta2: -7.546\n",
      "Iter 3/100 - Loss theta2: -7.562\n",
      "Iter 4/100 - Loss theta2: -7.563\n",
      "Iter 5/100 - Loss theta2: -7.557\n",
      "Iter 6/100 - Loss theta2: -7.559\n",
      "Iter 7/100 - Loss theta2: -7.564\n",
      "Iter 8/100 - Loss theta2: -7.562\n",
      "Iter 9/100 - Loss theta2: -7.560\n",
      "Iter 10/100 - Loss theta2: -7.560\n",
      "Iter 11/100 - Loss theta2: -7.561\n",
      "Iter 12/100 - Loss theta2: -7.565\n",
      "Iter 13/100 - Loss theta2: -7.564\n",
      "Iter 14/100 - Loss theta2: -7.563\n",
      "Iter 15/100 - Loss theta2: -7.564\n",
      "Iter 16/100 - Loss theta2: -7.564\n",
      "Iter 17/100 - Loss theta2: -7.565\n",
      "Iter 18/100 - Loss theta2: -7.563\n",
      "Iter 19/100 - Loss theta2: -7.561\n",
      "Iter 20/100 - Loss theta2: -7.563\n",
      "Iter 21/100 - Loss theta2: -7.565\n",
      "Iter 22/100 - Loss theta2: -7.564\n",
      "Iter 23/100 - Loss theta2: -7.563\n",
      "Iter 24/100 - Loss theta2: -7.563\n",
      "Iter 25/100 - Loss theta2: -7.563\n",
      "Iter 26/100 - Loss theta2: -7.563\n",
      "Iter 27/100 - Loss theta2: -7.566\n",
      "Iter 28/100 - Loss theta2: -7.565\n",
      "Iter 29/100 - Loss theta2: -7.565\n",
      "Iter 30/100 - Loss theta2: -7.565\n",
      "Iter 31/100 - Loss theta2: -7.565\n",
      "Iter 32/100 - Loss theta2: -7.564\n",
      "Iter 33/100 - Loss theta2: -7.564\n",
      "Iter 34/100 - Loss theta2: -7.565\n",
      "Iter 35/100 - Loss theta2: -7.566\n",
      "Iter 36/100 - Loss theta2: -7.565\n",
      "Iter 37/100 - Loss theta2: -7.564\n",
      "Iter 38/100 - Loss theta2: -7.565\n",
      "Iter 39/100 - Loss theta2: -7.565\n",
      "Iter 40/100 - Loss theta2: -7.565\n",
      "Iter 41/100 - Loss theta2: -7.565\n",
      "Iter 42/100 - Loss theta2: -7.565\n",
      "Iter 43/100 - Loss theta2: -7.565\n",
      "Iter 44/100 - Loss theta2: -7.567\n",
      "Iter 45/100 - Loss theta2: -7.565\n",
      "Iter 46/100 - Loss theta2: -7.564\n",
      "Iter 47/100 - Loss theta2: -7.567\n",
      "Iter 48/100 - Loss theta2: -7.564\n",
      "Iter 49/100 - Loss theta2: -7.566\n",
      "Iter 50/100 - Loss theta2: -7.564\n",
      "Iter 51/100 - Loss theta2: -7.564\n",
      "Iter 52/100 - Loss theta2: -7.565\n",
      "Iter 53/100 - Loss theta2: -7.565\n",
      "Iter 54/100 - Loss theta2: -7.564\n",
      "Iter 55/100 - Loss theta2: -7.566\n",
      "Iter 56/100 - Loss theta2: -7.564\n",
      "Iter 57/100 - Loss theta2: -7.565\n",
      "Iter 58/100 - Loss theta2: -7.565\n",
      "Iter 59/100 - Loss theta2: -7.566\n",
      "Iter 60/100 - Loss theta2: -7.566\n",
      "Iter 61/100 - Loss theta2: -7.566\n",
      "Iter 62/100 - Loss theta2: -7.565\n",
      "Iter 63/100 - Loss theta2: -7.567\n",
      "Iter 64/100 - Loss theta2: -7.566\n",
      "Iter 65/100 - Loss theta2: -7.566\n",
      "Iter 66/100 - Loss theta2: -7.567\n",
      "Iter 67/100 - Loss theta2: -7.567\n",
      "Iter 68/100 - Loss theta2: -7.566\n",
      "Iter 69/100 - Loss theta2: -7.567\n",
      "Iter 70/100 - Loss theta2: -7.566\n",
      "Iter 71/100 - Loss theta2: -7.565\n",
      "Iter 72/100 - Loss theta2: -7.566\n",
      "Iter 73/100 - Loss theta2: -7.566\n",
      "Iter 74/100 - Loss theta2: -7.565\n",
      "Iter 75/100 - Loss theta2: -7.566\n",
      "Iter 76/100 - Loss theta2: -7.568\n",
      "Iter 77/100 - Loss theta2: -7.566\n",
      "Iter 78/100 - Loss theta2: -7.566\n",
      "Iter 79/100 - Loss theta2: -7.566\n",
      "Iter 80/100 - Loss theta2: -7.566\n",
      "Iter 81/100 - Loss theta2: -7.567\n",
      "Iter 82/100 - Loss theta2: -7.564\n",
      "Iter 83/100 - Loss theta2: -7.564\n",
      "Iter 84/100 - Loss theta2: -7.565\n",
      "Iter 85/100 - Loss theta2: -7.565\n",
      "Iter 86/100 - Loss theta2: -7.564\n",
      "Iter 87/100 - Loss theta2: -7.565\n",
      "Iter 88/100 - Loss theta2: -7.567\n",
      "Iter 89/100 - Loss theta2: -7.565\n",
      "Iter 90/100 - Loss theta2: -7.565\n",
      "Iter 91/100 - Loss theta2: -7.565\n",
      "Iter 92/100 - Loss theta2: -7.566\n",
      "Iter 93/100 - Loss theta2: -7.564\n",
      "Iter 94/100 - Loss theta2: -7.567\n",
      "Iter 95/100 - Loss theta2: -7.563\n",
      "Iter 96/100 - Loss theta2: -7.567\n",
      "Iter 97/100 - Loss theta2: -7.567\n",
      "Iter 98/100 - Loss theta2: -7.565\n",
      "Iter 99/100 - Loss theta2: -7.568\n",
      "Iter 100/100 - Loss theta2: -7.565\n",
      "tensor([[0.6907],\n",
      "        [0.6904]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7175]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1242]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 195.741\n",
      "Iter 2/50 - Loss hyperparam: 161.482\n",
      "Iter 3/50 - Loss hyperparam: 126.924\n",
      "Iter 4/50 - Loss hyperparam: 91.796\n",
      "Iter 5/50 - Loss hyperparam: 55.764\n",
      "Iter 6/50 - Loss hyperparam: 18.528\n",
      "Iter 7/50 - Loss hyperparam: -19.992\n",
      "Iter 8/50 - Loss hyperparam: -59.578\n",
      "Iter 9/50 - Loss hyperparam: -100.197\n",
      "Iter 10/50 - Loss hyperparam: -142.613\n",
      "Iter 11/50 - Loss hyperparam: -187.986\n",
      "Iter 12/50 - Loss hyperparam: -237.125\n",
      "Iter 13/50 - Loss hyperparam: -290.072\n",
      "Iter 14/50 - Loss hyperparam: -345.627\n",
      "Iter 15/50 - Loss hyperparam: -400.928\n",
      "Iter 16/50 - Loss hyperparam: -453.167\n",
      "Iter 17/50 - Loss hyperparam: -499.973\n",
      "Iter 18/50 - Loss hyperparam: -534.055\n",
      "Iter 19/50 - Loss hyperparam: -551.089\n",
      "Iter 20/50 - Loss hyperparam: -560.012\n",
      "Iter 21/50 - Loss hyperparam: -573.687\n",
      "Iter 22/50 - Loss hyperparam: -597.488\n",
      "Iter 23/50 - Loss hyperparam: -631.247\n",
      "Iter 24/50 - Loss hyperparam: -673.015\n",
      "Iter 25/50 - Loss hyperparam: -721.354\n",
      "Iter 26/50 - Loss hyperparam: -775.799\n",
      "Iter 27/50 - Loss hyperparam: -829.040\n",
      "Iter 28/50 - Loss hyperparam: -864.148\n",
      "Iter 29/50 - Loss hyperparam: -881.402\n",
      "Iter 30/50 - Loss hyperparam: -898.082\n",
      "Iter 31/50 - Loss hyperparam: -923.424\n",
      "Iter 32/50 - Loss hyperparam: -962.934\n",
      "Iter 33/50 - Loss hyperparam: -1016.710\n",
      "Iter 34/50 - Loss hyperparam: -1067.758\n",
      "Iter 35/50 - Loss hyperparam: -1100.695\n",
      "Iter 36/50 - Loss hyperparam: -1117.129\n",
      "Iter 37/50 - Loss hyperparam: -1141.217\n",
      "Iter 38/50 - Loss hyperparam: -1186.134\n",
      "Iter 39/50 - Loss hyperparam: -1237.981\n",
      "Iter 40/50 - Loss hyperparam: -1274.865\n",
      "Iter 41/50 - Loss hyperparam: -1290.651\n",
      "Iter 42/50 - Loss hyperparam: -1322.179\n",
      "Iter 43/50 - Loss hyperparam: -1374.283\n",
      "Iter 44/50 - Loss hyperparam: -1412.236\n",
      "Iter 45/50 - Loss hyperparam: -1429.719\n",
      "Iter 46/50 - Loss hyperparam: -1467.986\n",
      "Iter 47/50 - Loss hyperparam: -1516.783\n",
      "Iter 48/50 - Loss hyperparam: -1535.264\n",
      "Iter 49/50 - Loss hyperparam: -1568.464\n",
      "Iter 50/50 - Loss hyperparam: -1620.441\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.496\n",
      "Iter 2/100 - Loss theta2: -7.470\n",
      "Iter 3/100 - Loss theta2: -7.495\n",
      "Iter 4/100 - Loss theta2: -7.491\n",
      "Iter 5/100 - Loss theta2: -7.483\n",
      "Iter 6/100 - Loss theta2: -7.488\n",
      "Iter 7/100 - Loss theta2: -7.497\n",
      "Iter 8/100 - Loss theta2: -7.496\n",
      "Iter 9/100 - Loss theta2: -7.492\n",
      "Iter 10/100 - Loss theta2: -7.491\n",
      "Iter 11/100 - Loss theta2: -7.494\n",
      "Iter 12/100 - Loss theta2: -7.497\n",
      "Iter 13/100 - Loss theta2: -7.498\n",
      "Iter 14/100 - Loss theta2: -7.494\n",
      "Iter 15/100 - Loss theta2: -7.494\n",
      "Iter 16/100 - Loss theta2: -7.494\n",
      "Iter 17/100 - Loss theta2: -7.498\n",
      "Iter 18/100 - Loss theta2: -7.500\n",
      "Iter 19/100 - Loss theta2: -7.497\n",
      "Iter 20/100 - Loss theta2: -7.496\n",
      "Iter 21/100 - Loss theta2: -7.497\n",
      "Iter 22/100 - Loss theta2: -7.498\n",
      "Iter 23/100 - Loss theta2: -7.499\n",
      "Iter 24/100 - Loss theta2: -7.497\n",
      "Iter 25/100 - Loss theta2: -7.498\n",
      "Iter 26/100 - Loss theta2: -7.498\n",
      "Iter 27/100 - Loss theta2: -7.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28/100 - Loss theta2: -7.500\n",
      "Iter 29/100 - Loss theta2: -7.499\n",
      "Iter 30/100 - Loss theta2: -7.498\n",
      "Iter 31/100 - Loss theta2: -7.499\n",
      "Iter 32/100 - Loss theta2: -7.501\n",
      "Iter 33/100 - Loss theta2: -7.500\n",
      "Iter 34/100 - Loss theta2: -7.499\n",
      "Iter 35/100 - Loss theta2: -7.499\n",
      "Iter 36/100 - Loss theta2: -7.499\n",
      "Iter 37/100 - Loss theta2: -7.501\n",
      "Iter 38/100 - Loss theta2: -7.500\n",
      "Iter 39/100 - Loss theta2: -7.501\n",
      "Iter 40/100 - Loss theta2: -7.501\n",
      "Iter 41/100 - Loss theta2: -7.500\n",
      "Iter 42/100 - Loss theta2: -7.500\n",
      "Iter 43/100 - Loss theta2: -7.501\n",
      "Iter 44/100 - Loss theta2: -7.501\n",
      "Iter 45/100 - Loss theta2: -7.501\n",
      "Iter 46/100 - Loss theta2: -7.502\n",
      "Iter 47/100 - Loss theta2: -7.503\n",
      "Iter 48/100 - Loss theta2: -7.502\n",
      "Iter 49/100 - Loss theta2: -7.501\n",
      "Iter 50/100 - Loss theta2: -7.502\n",
      "Iter 51/100 - Loss theta2: -7.502\n",
      "Iter 52/100 - Loss theta2: -7.502\n",
      "Iter 53/100 - Loss theta2: -7.501\n",
      "Iter 54/100 - Loss theta2: -7.502\n",
      "Iter 55/100 - Loss theta2: -7.501\n",
      "Iter 56/100 - Loss theta2: -7.502\n",
      "Iter 57/100 - Loss theta2: -7.501\n",
      "Iter 58/100 - Loss theta2: -7.502\n",
      "Iter 59/100 - Loss theta2: -7.502\n",
      "Iter 60/100 - Loss theta2: -7.503\n",
      "Iter 61/100 - Loss theta2: -7.501\n",
      "Iter 62/100 - Loss theta2: -7.503\n",
      "Iter 63/100 - Loss theta2: -7.502\n",
      "Iter 64/100 - Loss theta2: -7.501\n",
      "Iter 65/100 - Loss theta2: -7.502\n",
      "Iter 66/100 - Loss theta2: -7.503\n",
      "Iter 67/100 - Loss theta2: -7.502\n",
      "Iter 68/100 - Loss theta2: -7.502\n",
      "Iter 69/100 - Loss theta2: -7.502\n",
      "Iter 70/100 - Loss theta2: -7.504\n",
      "Iter 71/100 - Loss theta2: -7.504\n",
      "Iter 72/100 - Loss theta2: -7.502\n",
      "Iter 73/100 - Loss theta2: -7.503\n",
      "Iter 74/100 - Loss theta2: -7.502\n",
      "Iter 75/100 - Loss theta2: -7.504\n",
      "Iter 76/100 - Loss theta2: -7.503\n",
      "Iter 77/100 - Loss theta2: -7.502\n",
      "Iter 78/100 - Loss theta2: -7.503\n",
      "Iter 79/100 - Loss theta2: -7.503\n",
      "Iter 80/100 - Loss theta2: -7.502\n",
      "Iter 81/100 - Loss theta2: -7.503\n",
      "Iter 82/100 - Loss theta2: -7.504\n",
      "Iter 83/100 - Loss theta2: -7.503\n",
      "Iter 84/100 - Loss theta2: -7.503\n",
      "Iter 85/100 - Loss theta2: -7.504\n",
      "Iter 86/100 - Loss theta2: -7.504\n",
      "Iter 87/100 - Loss theta2: -7.504\n",
      "Iter 88/100 - Loss theta2: -7.503\n",
      "Iter 89/100 - Loss theta2: -7.504\n",
      "Iter 90/100 - Loss theta2: -7.503\n",
      "Iter 91/100 - Loss theta2: -7.503\n",
      "Iter 92/100 - Loss theta2: -7.505\n",
      "Iter 93/100 - Loss theta2: -7.504\n",
      "Iter 94/100 - Loss theta2: -7.504\n",
      "Iter 95/100 - Loss theta2: -7.503\n",
      "Iter 96/100 - Loss theta2: -7.503\n",
      "Iter 97/100 - Loss theta2: -7.502\n",
      "Iter 98/100 - Loss theta2: -7.504\n",
      "Iter 99/100 - Loss theta2: -7.503\n",
      "Iter 100/100 - Loss theta2: -7.504\n",
      "tensor([[0.6884],\n",
      "        [0.6894]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7153],\n",
      "        [0.7151]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1241]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 222.793\n",
      "Iter 2/50 - Loss hyperparam: 193.017\n",
      "Iter 3/50 - Loss hyperparam: 162.665\n",
      "Iter 4/50 - Loss hyperparam: 132.208\n",
      "Iter 5/50 - Loss hyperparam: 100.942\n",
      "Iter 6/50 - Loss hyperparam: 67.881\n",
      "Iter 7/50 - Loss hyperparam: 32.807\n",
      "Iter 8/50 - Loss hyperparam: -4.059\n",
      "Iter 9/50 - Loss hyperparam: -42.524\n",
      "Iter 10/50 - Loss hyperparam: -82.739\n",
      "Iter 11/50 - Loss hyperparam: -124.844\n",
      "Iter 12/50 - Loss hyperparam: -168.923\n",
      "Iter 13/50 - Loss hyperparam: -215.436\n",
      "Iter 14/50 - Loss hyperparam: -265.608\n",
      "Iter 15/50 - Loss hyperparam: -320.037\n",
      "Iter 16/50 - Loss hyperparam: -378.400\n",
      "Iter 17/50 - Loss hyperparam: -441.057\n",
      "Iter 18/50 - Loss hyperparam: -505.304\n",
      "Iter 19/50 - Loss hyperparam: -564.408\n",
      "Iter 20/50 - Loss hyperparam: -607.263\n",
      "Iter 21/50 - Loss hyperparam: -626.463\n",
      "Iter 22/50 - Loss hyperparam: -633.051\n",
      "Iter 23/50 - Loss hyperparam: -642.455\n",
      "Iter 24/50 - Loss hyperparam: -661.831\n",
      "Iter 25/50 - Loss hyperparam: -692.267\n",
      "Iter 26/50 - Loss hyperparam: -734.255\n",
      "Iter 27/50 - Loss hyperparam: -786.476\n",
      "Iter 28/50 - Loss hyperparam: -847.010\n",
      "Iter 29/50 - Loss hyperparam: -906.816\n",
      "Iter 30/50 - Loss hyperparam: -948.401\n",
      "Iter 31/50 - Loss hyperparam: -964.577\n",
      "Iter 32/50 - Loss hyperparam: -975.970\n",
      "Iter 33/50 - Loss hyperparam: -999.482\n",
      "Iter 34/50 - Loss hyperparam: -1042.301\n",
      "Iter 35/50 - Loss hyperparam: -1100.069\n",
      "Iter 36/50 - Loss hyperparam: -1157.970\n",
      "Iter 37/50 - Loss hyperparam: -1190.735\n",
      "Iter 38/50 - Loss hyperparam: -1203.016\n",
      "Iter 39/50 - Loss hyperparam: -1226.368\n",
      "Iter 40/50 - Loss hyperparam: -1274.984\n",
      "Iter 41/50 - Loss hyperparam: -1335.632\n",
      "Iter 42/50 - Loss hyperparam: -1368.615\n",
      "Iter 43/50 - Loss hyperparam: -1378.746\n",
      "Iter 44/50 - Loss hyperparam: -1414.309\n",
      "Iter 45/50 - Loss hyperparam: -1475.507\n",
      "Iter 46/50 - Loss hyperparam: -1510.517\n",
      "Iter 47/50 - Loss hyperparam: -1519.238\n",
      "Iter 48/50 - Loss hyperparam: -1566.178\n",
      "Iter 49/50 - Loss hyperparam: -1623.186\n",
      "Iter 50/50 - Loss hyperparam: -1631.282\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.524\n",
      "Iter 2/100 - Loss theta2: -7.488\n",
      "Iter 3/100 - Loss theta2: -7.523\n",
      "Iter 4/100 - Loss theta2: -7.516\n",
      "Iter 5/100 - Loss theta2: -7.504\n",
      "Iter 6/100 - Loss theta2: -7.509\n",
      "Iter 7/100 - Loss theta2: -7.519\n",
      "Iter 8/100 - Loss theta2: -7.522\n",
      "Iter 9/100 - Loss theta2: -7.518\n",
      "Iter 10/100 - Loss theta2: -7.513\n",
      "Iter 11/100 - Loss theta2: -7.514\n",
      "Iter 12/100 - Loss theta2: -7.520\n",
      "Iter 13/100 - Loss theta2: -7.524\n",
      "Iter 14/100 - Loss theta2: -7.522\n",
      "Iter 15/100 - Loss theta2: -7.520\n",
      "Iter 16/100 - Loss theta2: -7.520\n",
      "Iter 17/100 - Loss theta2: -7.520\n",
      "Iter 18/100 - Loss theta2: -7.523\n",
      "Iter 19/100 - Loss theta2: -7.522\n",
      "Iter 20/100 - Loss theta2: -7.524\n",
      "Iter 21/100 - Loss theta2: -7.521\n",
      "Iter 22/100 - Loss theta2: -7.521\n",
      "Iter 23/100 - Loss theta2: -7.523\n",
      "Iter 24/100 - Loss theta2: -7.525\n",
      "Iter 25/100 - Loss theta2: -7.524\n",
      "Iter 26/100 - Loss theta2: -7.525\n",
      "Iter 27/100 - Loss theta2: -7.523\n",
      "Iter 28/100 - Loss theta2: -7.523\n",
      "Iter 29/100 - Loss theta2: -7.522\n",
      "Iter 30/100 - Loss theta2: -7.523\n",
      "Iter 31/100 - Loss theta2: -7.525\n",
      "Iter 32/100 - Loss theta2: -7.524\n",
      "Iter 33/100 - Loss theta2: -7.523\n",
      "Iter 34/100 - Loss theta2: -7.523\n",
      "Iter 35/100 - Loss theta2: -7.523\n",
      "Iter 36/100 - Loss theta2: -7.525\n",
      "Iter 37/100 - Loss theta2: -7.524\n",
      "Iter 38/100 - Loss theta2: -7.523\n",
      "Iter 39/100 - Loss theta2: -7.525\n",
      "Iter 40/100 - Loss theta2: -7.523\n",
      "Iter 41/100 - Loss theta2: -7.525\n",
      "Iter 42/100 - Loss theta2: -7.523\n",
      "Iter 43/100 - Loss theta2: -7.525\n",
      "Iter 44/100 - Loss theta2: -7.524\n",
      "Iter 45/100 - Loss theta2: -7.526\n",
      "Iter 46/100 - Loss theta2: -7.526\n",
      "Iter 47/100 - Loss theta2: -7.523\n",
      "Iter 48/100 - Loss theta2: -7.526\n",
      "Iter 49/100 - Loss theta2: -7.525\n",
      "Iter 50/100 - Loss theta2: -7.524\n",
      "Iter 51/100 - Loss theta2: -7.525\n",
      "Iter 52/100 - Loss theta2: -7.525\n",
      "Iter 53/100 - Loss theta2: -7.527\n",
      "Iter 54/100 - Loss theta2: -7.525\n",
      "Iter 55/100 - Loss theta2: -7.525\n",
      "Iter 56/100 - Loss theta2: -7.525\n",
      "Iter 57/100 - Loss theta2: -7.525\n",
      "Iter 58/100 - Loss theta2: -7.526\n",
      "Iter 59/100 - Loss theta2: -7.526\n",
      "Iter 60/100 - Loss theta2: -7.525\n",
      "Iter 61/100 - Loss theta2: -7.525\n",
      "Iter 62/100 - Loss theta2: -7.526\n",
      "Iter 63/100 - Loss theta2: -7.524\n",
      "Iter 64/100 - Loss theta2: -7.524\n",
      "Iter 65/100 - Loss theta2: -7.526\n",
      "Iter 66/100 - Loss theta2: -7.525\n",
      "Iter 67/100 - Loss theta2: -7.525\n",
      "Iter 68/100 - Loss theta2: -7.524\n",
      "Iter 69/100 - Loss theta2: -7.526\n",
      "Iter 70/100 - Loss theta2: -7.526\n",
      "Iter 71/100 - Loss theta2: -7.527\n",
      "Iter 72/100 - Loss theta2: -7.525\n",
      "Iter 73/100 - Loss theta2: -7.525\n",
      "Iter 74/100 - Loss theta2: -7.525\n",
      "Iter 75/100 - Loss theta2: -7.524\n",
      "Iter 76/100 - Loss theta2: -7.525\n",
      "Iter 77/100 - Loss theta2: -7.524\n",
      "Iter 78/100 - Loss theta2: -7.526\n",
      "Iter 79/100 - Loss theta2: -7.525\n",
      "Iter 80/100 - Loss theta2: -7.525\n",
      "Iter 81/100 - Loss theta2: -7.525\n",
      "Iter 82/100 - Loss theta2: -7.526\n",
      "Iter 83/100 - Loss theta2: -7.525\n",
      "Iter 84/100 - Loss theta2: -7.525\n",
      "Iter 85/100 - Loss theta2: -7.525\n",
      "Iter 86/100 - Loss theta2: -7.524\n",
      "Iter 87/100 - Loss theta2: -7.525\n",
      "Iter 88/100 - Loss theta2: -7.525\n",
      "Iter 89/100 - Loss theta2: -7.525\n",
      "Iter 90/100 - Loss theta2: -7.525\n",
      "Iter 91/100 - Loss theta2: -7.525\n",
      "Iter 92/100 - Loss theta2: -7.524\n",
      "Iter 93/100 - Loss theta2: -7.526\n",
      "Iter 94/100 - Loss theta2: -7.526\n",
      "Iter 95/100 - Loss theta2: -7.524\n",
      "Iter 96/100 - Loss theta2: -7.525\n",
      "Iter 97/100 - Loss theta2: -7.525\n",
      "Iter 98/100 - Loss theta2: -7.525\n",
      "Iter 99/100 - Loss theta2: -7.525\n",
      "Iter 100/100 - Loss theta2: -7.528\n",
      "tensor([[0.6884],\n",
      "        [0.6899]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7153],\n",
      "        [0.7151]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1241]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss hyperparam: 82.085\n",
      "Iter 2/50 - Loss hyperparam: 39.277\n",
      "Iter 3/50 - Loss hyperparam: -3.936\n",
      "Iter 4/50 - Loss hyperparam: -47.938\n",
      "Iter 5/50 - Loss hyperparam: -93.568\n",
      "Iter 6/50 - Loss hyperparam: -141.292\n",
      "Iter 7/50 - Loss hyperparam: -190.200\n",
      "Iter 8/50 - Loss hyperparam: -238.240\n",
      "Iter 9/50 - Loss hyperparam: -282.589\n",
      "Iter 10/50 - Loss hyperparam: -319.861\n",
      "Iter 11/50 - Loss hyperparam: -348.705\n",
      "Iter 12/50 - Loss hyperparam: -371.190\n",
      "Iter 13/50 - Loss hyperparam: -390.469\n",
      "Iter 14/50 - Loss hyperparam: -410.010\n",
      "Iter 15/50 - Loss hyperparam: -432.702\n",
      "Iter 16/50 - Loss hyperparam: -460.774\n",
      "Iter 17/50 - Loss hyperparam: -495.212\n",
      "Iter 18/50 - Loss hyperparam: -534.708\n",
      "Iter 19/50 - Loss hyperparam: -575.906\n",
      "Iter 20/50 - Loss hyperparam: -615.339\n",
      "Iter 21/50 - Loss hyperparam: -650.802\n",
      "Iter 22/50 - Loss hyperparam: -679.754\n",
      "Iter 23/50 - Loss hyperparam: -703.773\n",
      "Iter 24/50 - Loss hyperparam: -730.553\n",
      "Iter 25/50 - Loss hyperparam: -764.667\n",
      "Iter 26/50 - Loss hyperparam: -804.306\n",
      "Iter 27/50 - Loss hyperparam: -846.590\n",
      "Iter 28/50 - Loss hyperparam: -886.005\n",
      "Iter 29/50 - Loss hyperparam: -915.393\n",
      "Iter 30/50 - Loss hyperparam: -942.100\n",
      "Iter 31/50 - Loss hyperparam: -973.962\n",
      "Iter 32/50 - Loss hyperparam: -1013.680\n",
      "Iter 33/50 - Loss hyperparam: -1055.547\n",
      "Iter 34/50 - Loss hyperparam: -1087.912\n",
      "Iter 35/50 - Loss hyperparam: -1114.344\n",
      "Iter 36/50 - Loss hyperparam: -1147.442\n",
      "Iter 37/50 - Loss hyperparam: -1189.160\n",
      "Iter 38/50 - Loss hyperparam: -1226.682\n",
      "Iter 39/50 - Loss hyperparam: -1253.386\n",
      "Iter 40/50 - Loss hyperparam: -1287.976\n",
      "Iter 41/50 - Loss hyperparam: -1330.583\n",
      "Iter 42/50 - Loss hyperparam: -1361.474\n",
      "Iter 43/50 - Loss hyperparam: -1390.756\n",
      "Iter 44/50 - Loss hyperparam: -1430.560\n",
      "Iter 45/50 - Loss hyperparam: -1460.860\n",
      "Iter 46/50 - Loss hyperparam: -1494.808\n",
      "Iter 47/50 - Loss hyperparam: -1535.868\n",
      "Iter 48/50 - Loss hyperparam: -1559.453\n",
      "Iter 49/50 - Loss hyperparam: -1598.155\n",
      "Iter 50/50 - Loss hyperparam: -1632.990\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.436\n",
      "Iter 2/100 - Loss theta2: -7.445\n",
      "Iter 3/100 - Loss theta2: -7.441\n",
      "Iter 4/100 - Loss theta2: -7.451\n",
      "Iter 5/100 - Loss theta2: -7.451\n",
      "Iter 6/100 - Loss theta2: -7.447\n",
      "Iter 7/100 - Loss theta2: -7.448\n",
      "Iter 8/100 - Loss theta2: -7.452\n",
      "Iter 9/100 - Loss theta2: -7.452\n",
      "Iter 10/100 - Loss theta2: -7.450\n",
      "Iter 11/100 - Loss theta2: -7.449\n",
      "Iter 12/100 - Loss theta2: -7.451\n",
      "Iter 13/100 - Loss theta2: -7.452\n",
      "Iter 14/100 - Loss theta2: -7.453\n",
      "Iter 15/100 - Loss theta2: -7.451\n",
      "Iter 16/100 - Loss theta2: -7.451\n",
      "Iter 17/100 - Loss theta2: -7.453\n",
      "Iter 18/100 - Loss theta2: -7.454\n",
      "Iter 19/100 - Loss theta2: -7.454\n",
      "Iter 20/100 - Loss theta2: -7.453\n",
      "Iter 21/100 - Loss theta2: -7.454\n",
      "Iter 22/100 - Loss theta2: -7.456\n",
      "Iter 23/100 - Loss theta2: -7.454\n",
      "Iter 24/100 - Loss theta2: -7.454\n",
      "Iter 25/100 - Loss theta2: -7.453\n",
      "Iter 26/100 - Loss theta2: -7.455\n",
      "Iter 27/100 - Loss theta2: -7.454\n",
      "Iter 28/100 - Loss theta2: -7.455\n",
      "Iter 29/100 - Loss theta2: -7.456\n",
      "Iter 30/100 - Loss theta2: -7.454\n",
      "Iter 31/100 - Loss theta2: -7.455\n",
      "Iter 32/100 - Loss theta2: -7.456\n",
      "Iter 33/100 - Loss theta2: -7.453\n",
      "Iter 34/100 - Loss theta2: -7.457\n",
      "Iter 35/100 - Loss theta2: -7.454\n",
      "Iter 36/100 - Loss theta2: -7.457\n",
      "Iter 37/100 - Loss theta2: -7.454\n",
      "Iter 38/100 - Loss theta2: -7.455\n",
      "Iter 39/100 - Loss theta2: -7.457\n",
      "Iter 40/100 - Loss theta2: -7.457\n",
      "Iter 41/100 - Loss theta2: -7.456\n",
      "Iter 42/100 - Loss theta2: -7.456\n",
      "Iter 43/100 - Loss theta2: -7.457\n",
      "Iter 44/100 - Loss theta2: -7.458\n",
      "Iter 45/100 - Loss theta2: -7.456\n",
      "Iter 46/100 - Loss theta2: -7.458\n",
      "Iter 47/100 - Loss theta2: -7.455\n",
      "Iter 48/100 - Loss theta2: -7.457\n",
      "Iter 49/100 - Loss theta2: -7.457\n",
      "Iter 50/100 - Loss theta2: -7.457\n",
      "Iter 51/100 - Loss theta2: -7.458\n",
      "Iter 52/100 - Loss theta2: -7.458\n",
      "Iter 53/100 - Loss theta2: -7.458\n",
      "Iter 54/100 - Loss theta2: -7.458\n",
      "Iter 55/100 - Loss theta2: -7.458\n",
      "Iter 56/100 - Loss theta2: -7.459\n",
      "Iter 57/100 - Loss theta2: -7.459\n",
      "Iter 58/100 - Loss theta2: -7.458\n",
      "Iter 59/100 - Loss theta2: -7.458\n",
      "Iter 60/100 - Loss theta2: -7.460\n",
      "Iter 61/100 - Loss theta2: -7.460\n",
      "Iter 62/100 - Loss theta2: -7.460\n",
      "Iter 63/100 - Loss theta2: -7.458\n",
      "Iter 64/100 - Loss theta2: -7.461\n",
      "Iter 65/100 - Loss theta2: -7.459\n",
      "Iter 66/100 - Loss theta2: -7.459\n",
      "Iter 67/100 - Loss theta2: -7.461\n",
      "Iter 68/100 - Loss theta2: -7.461\n",
      "Iter 69/100 - Loss theta2: -7.461\n",
      "Iter 70/100 - Loss theta2: -7.458\n",
      "Iter 71/100 - Loss theta2: -7.462\n",
      "Iter 72/100 - Loss theta2: -7.459\n",
      "Iter 73/100 - Loss theta2: -7.459\n",
      "Iter 74/100 - Loss theta2: -7.461\n",
      "Iter 75/100 - Loss theta2: -7.461\n",
      "Iter 76/100 - Loss theta2: -7.460\n",
      "Iter 77/100 - Loss theta2: -7.460\n",
      "Iter 78/100 - Loss theta2: -7.462\n",
      "Iter 79/100 - Loss theta2: -7.460\n",
      "Iter 80/100 - Loss theta2: -7.459\n",
      "Iter 81/100 - Loss theta2: -7.461\n",
      "Iter 82/100 - Loss theta2: -7.461\n",
      "Iter 83/100 - Loss theta2: -7.460\n",
      "Iter 84/100 - Loss theta2: -7.461\n",
      "Iter 85/100 - Loss theta2: -7.461\n",
      "Iter 86/100 - Loss theta2: -7.461\n",
      "Iter 87/100 - Loss theta2: -7.461\n",
      "Iter 88/100 - Loss theta2: -7.461\n",
      "Iter 89/100 - Loss theta2: -7.461\n",
      "Iter 90/100 - Loss theta2: -7.461\n",
      "Iter 91/100 - Loss theta2: -7.462\n",
      "Iter 92/100 - Loss theta2: -7.462\n",
      "Iter 93/100 - Loss theta2: -7.461\n",
      "Iter 94/100 - Loss theta2: -7.461\n",
      "Iter 95/100 - Loss theta2: -7.460\n",
      "Iter 96/100 - Loss theta2: -7.461\n",
      "Iter 97/100 - Loss theta2: -7.461\n",
      "Iter 98/100 - Loss theta2: -7.462\n",
      "Iter 99/100 - Loss theta2: -7.461\n",
      "Iter 100/100 - Loss theta2: -7.462\n",
      "tensor([[0.6895],\n",
      "        [0.6895]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7177],\n",
      "        [0.7175]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1245]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 101.271\n",
      "Iter 2/50 - Loss hyperparam: 52.884\n",
      "Iter 3/50 - Loss hyperparam: 1.910\n",
      "Iter 4/50 - Loss hyperparam: -51.866\n",
      "Iter 5/50 - Loss hyperparam: -108.760\n",
      "Iter 6/50 - Loss hyperparam: -167.060\n",
      "Iter 7/50 - Loss hyperparam: -222.418\n",
      "Iter 8/50 - Loss hyperparam: -269.228\n",
      "Iter 9/50 - Loss hyperparam: -302.622\n",
      "Iter 10/50 - Loss hyperparam: -323.029\n",
      "Iter 11/50 - Loss hyperparam: -337.938\n",
      "Iter 12/50 - Loss hyperparam: -355.338\n",
      "Iter 13/50 - Loss hyperparam: -379.320\n",
      "Iter 14/50 - Loss hyperparam: -410.802\n",
      "Iter 15/50 - Loss hyperparam: -448.876\n",
      "Iter 16/50 - Loss hyperparam: -491.128\n",
      "Iter 17/50 - Loss hyperparam: -533.365\n",
      "Iter 18/50 - Loss hyperparam: -570.358\n",
      "Iter 19/50 - Loss hyperparam: -599.516\n",
      "Iter 20/50 - Loss hyperparam: -624.163\n",
      "Iter 21/50 - Loss hyperparam: -650.582\n",
      "Iter 22/50 - Loss hyperparam: -683.222\n",
      "Iter 23/50 - Loss hyperparam: -722.452\n",
      "Iter 24/50 - Loss hyperparam: -764.354\n",
      "Iter 25/50 - Loss hyperparam: -802.915\n",
      "Iter 26/50 - Loss hyperparam: -835.702\n",
      "Iter 27/50 - Loss hyperparam: -866.038\n",
      "Iter 28/50 - Loss hyperparam: -899.133\n",
      "Iter 29/50 - Loss hyperparam: -936.924\n",
      "Iter 30/50 - Loss hyperparam: -975.932\n",
      "Iter 31/50 - Loss hyperparam: -1013.075\n",
      "Iter 32/50 - Loss hyperparam: -1047.254\n",
      "Iter 33/50 - Loss hyperparam: -1079.675\n",
      "Iter 34/50 - Loss hyperparam: -1114.331\n",
      "Iter 35/50 - Loss hyperparam: -1153.477\n",
      "Iter 36/50 - Loss hyperparam: -1190.902\n",
      "Iter 37/50 - Loss hyperparam: -1220.410\n",
      "Iter 38/50 - Loss hyperparam: -1255.594\n",
      "Iter 39/50 - Loss hyperparam: -1299.165\n",
      "Iter 40/50 - Loss hyperparam: -1329.596\n",
      "Iter 41/50 - Loss hyperparam: -1362.597\n",
      "Iter 42/50 - Loss hyperparam: -1407.197\n",
      "Iter 43/50 - Loss hyperparam: -1434.431\n",
      "Iter 44/50 - Loss hyperparam: -1473.191\n",
      "Iter 45/50 - Loss hyperparam: -1510.117\n",
      "Iter 46/50 - Loss hyperparam: -1541.313\n",
      "Iter 47/50 - Loss hyperparam: -1580.976\n",
      "Iter 48/50 - Loss hyperparam: -1612.574\n",
      "Iter 49/50 - Loss hyperparam: -1649.637\n",
      "Iter 50/50 - Loss hyperparam: -1683.240\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.491\n",
      "Iter 2/100 - Loss theta2: -7.451\n",
      "Iter 3/100 - Loss theta2: -7.487\n",
      "Iter 4/100 - Loss theta2: -7.482\n",
      "Iter 5/100 - Loss theta2: -7.468\n",
      "Iter 6/100 - Loss theta2: -7.473\n",
      "Iter 7/100 - Loss theta2: -7.484\n",
      "Iter 8/100 - Loss theta2: -7.488\n",
      "Iter 9/100 - Loss theta2: -7.485\n",
      "Iter 10/100 - Loss theta2: -7.479\n",
      "Iter 11/100 - Loss theta2: -7.481\n",
      "Iter 12/100 - Loss theta2: -7.484\n",
      "Iter 13/100 - Loss theta2: -7.491\n",
      "Iter 14/100 - Loss theta2: -7.490\n",
      "Iter 15/100 - Loss theta2: -7.487\n",
      "Iter 16/100 - Loss theta2: -7.483\n",
      "Iter 17/100 - Loss theta2: -7.486\n",
      "Iter 18/100 - Loss theta2: -7.489\n",
      "Iter 19/100 - Loss theta2: -7.490\n",
      "Iter 20/100 - Loss theta2: -7.492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21/100 - Loss theta2: -7.488\n",
      "Iter 22/100 - Loss theta2: -7.487\n",
      "Iter 23/100 - Loss theta2: -7.487\n",
      "Iter 24/100 - Loss theta2: -7.486\n",
      "Iter 25/100 - Loss theta2: -7.488\n",
      "Iter 26/100 - Loss theta2: -7.487\n",
      "Iter 27/100 - Loss theta2: -7.487\n",
      "Iter 28/100 - Loss theta2: -7.485\n",
      "Iter 29/100 - Loss theta2: -7.487\n",
      "Iter 30/100 - Loss theta2: -7.486\n",
      "Iter 31/100 - Loss theta2: -7.487\n",
      "Iter 32/100 - Loss theta2: -7.486\n",
      "Iter 33/100 - Loss theta2: -7.487\n",
      "Iter 34/100 - Loss theta2: -7.487\n",
      "Iter 35/100 - Loss theta2: -7.488\n",
      "Iter 36/100 - Loss theta2: -7.489\n",
      "Iter 37/100 - Loss theta2: -7.488\n",
      "Iter 38/100 - Loss theta2: -7.489\n",
      "Iter 39/100 - Loss theta2: -7.489\n",
      "Iter 40/100 - Loss theta2: -7.488\n",
      "Iter 41/100 - Loss theta2: -7.489\n",
      "Iter 42/100 - Loss theta2: -7.489\n",
      "Iter 43/100 - Loss theta2: -7.493\n",
      "Iter 44/100 - Loss theta2: -7.490\n",
      "Iter 45/100 - Loss theta2: -7.491\n",
      "Iter 46/100 - Loss theta2: -7.492\n",
      "Iter 47/100 - Loss theta2: -7.491\n",
      "Iter 48/100 - Loss theta2: -7.488\n",
      "Iter 49/100 - Loss theta2: -7.489\n",
      "Iter 50/100 - Loss theta2: -7.488\n",
      "Iter 51/100 - Loss theta2: -7.489\n",
      "Iter 52/100 - Loss theta2: -7.492\n",
      "Iter 53/100 - Loss theta2: -7.489\n",
      "Iter 54/100 - Loss theta2: -7.491\n",
      "Iter 55/100 - Loss theta2: -7.492\n",
      "Iter 56/100 - Loss theta2: -7.491\n",
      "Iter 57/100 - Loss theta2: -7.491\n",
      "Iter 58/100 - Loss theta2: -7.492\n",
      "Iter 59/100 - Loss theta2: -7.492\n",
      "Iter 60/100 - Loss theta2: -7.491\n",
      "Iter 61/100 - Loss theta2: -7.489\n",
      "Iter 62/100 - Loss theta2: -7.492\n",
      "Iter 63/100 - Loss theta2: -7.491\n",
      "Iter 64/100 - Loss theta2: -7.490\n",
      "Iter 65/100 - Loss theta2: -7.490\n",
      "Iter 66/100 - Loss theta2: -7.495\n",
      "Iter 67/100 - Loss theta2: -7.491\n",
      "Iter 68/100 - Loss theta2: -7.493\n",
      "Iter 69/100 - Loss theta2: -7.491\n",
      "Iter 70/100 - Loss theta2: -7.491\n",
      "Iter 71/100 - Loss theta2: -7.491\n",
      "Iter 72/100 - Loss theta2: -7.495\n",
      "Iter 73/100 - Loss theta2: -7.493\n",
      "Iter 74/100 - Loss theta2: -7.491\n",
      "Iter 75/100 - Loss theta2: -7.492\n",
      "Iter 76/100 - Loss theta2: -7.493\n",
      "Iter 77/100 - Loss theta2: -7.492\n",
      "Iter 78/100 - Loss theta2: -7.495\n",
      "Iter 79/100 - Loss theta2: -7.493\n",
      "Iter 80/100 - Loss theta2: -7.494\n",
      "Iter 81/100 - Loss theta2: -7.492\n",
      "Iter 82/100 - Loss theta2: -7.491\n",
      "Iter 83/100 - Loss theta2: -7.492\n",
      "Iter 84/100 - Loss theta2: -7.493\n",
      "Iter 85/100 - Loss theta2: -7.494\n",
      "Iter 86/100 - Loss theta2: -7.493\n",
      "Iter 87/100 - Loss theta2: -7.493\n",
      "Iter 88/100 - Loss theta2: -7.493\n",
      "Iter 89/100 - Loss theta2: -7.492\n",
      "Iter 90/100 - Loss theta2: -7.492\n",
      "Iter 91/100 - Loss theta2: -7.494\n",
      "Iter 92/100 - Loss theta2: -7.493\n",
      "Iter 93/100 - Loss theta2: -7.493\n",
      "Iter 94/100 - Loss theta2: -7.492\n",
      "Iter 95/100 - Loss theta2: -7.492\n",
      "Iter 96/100 - Loss theta2: -7.491\n",
      "Iter 97/100 - Loss theta2: -7.494\n",
      "Iter 98/100 - Loss theta2: -7.493\n",
      "Iter 99/100 - Loss theta2: -7.494\n",
      "Iter 100/100 - Loss theta2: -7.494\n",
      "tensor([[0.6898],\n",
      "        [0.6898]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7176],\n",
      "        [0.7175]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 160.082\n",
      "Iter 2/50 - Loss hyperparam: 114.752\n",
      "Iter 3/50 - Loss hyperparam: 67.043\n",
      "Iter 4/50 - Loss hyperparam: 16.826\n",
      "Iter 5/50 - Loss hyperparam: -36.595\n",
      "Iter 6/50 - Loss hyperparam: -93.692\n",
      "Iter 7/50 - Loss hyperparam: -153.696\n",
      "Iter 8/50 - Loss hyperparam: -214.785\n",
      "Iter 9/50 - Loss hyperparam: -274.195\n",
      "Iter 10/50 - Loss hyperparam: -326.980\n",
      "Iter 11/50 - Loss hyperparam: -366.296\n",
      "Iter 12/50 - Loss hyperparam: -389.335\n",
      "Iter 13/50 - Loss hyperparam: -402.936\n",
      "Iter 14/50 - Loss hyperparam: -417.009\n",
      "Iter 15/50 - Loss hyperparam: -437.582\n",
      "Iter 16/50 - Loss hyperparam: -466.820\n",
      "Iter 17/50 - Loss hyperparam: -504.698\n",
      "Iter 18/50 - Loss hyperparam: -549.687\n",
      "Iter 19/50 - Loss hyperparam: -598.315\n",
      "Iter 20/50 - Loss hyperparam: -644.637\n",
      "Iter 21/50 - Loss hyperparam: -682.528\n",
      "Iter 22/50 - Loss hyperparam: -711.433\n",
      "Iter 23/50 - Loss hyperparam: -736.978\n",
      "Iter 24/50 - Loss hyperparam: -765.959\n",
      "Iter 25/50 - Loss hyperparam: -802.327\n",
      "Iter 26/50 - Loss hyperparam: -845.067\n",
      "Iter 27/50 - Loss hyperparam: -888.993\n",
      "Iter 28/50 - Loss hyperparam: -929.402\n",
      "Iter 29/50 - Loss hyperparam: -965.195\n",
      "Iter 30/50 - Loss hyperparam: -997.159\n",
      "Iter 31/50 - Loss hyperparam: -1029.609\n",
      "Iter 32/50 - Loss hyperparam: -1066.034\n",
      "Iter 33/50 - Loss hyperparam: -1107.827\n",
      "Iter 34/50 - Loss hyperparam: -1151.173\n",
      "Iter 35/50 - Loss hyperparam: -1184.242\n",
      "Iter 36/50 - Loss hyperparam: -1211.488\n",
      "Iter 37/50 - Loss hyperparam: -1251.149\n",
      "Iter 38/50 - Loss hyperparam: -1301.028\n",
      "Iter 39/50 - Loss hyperparam: -1332.972\n",
      "Iter 40/50 - Loss hyperparam: -1359.129\n",
      "Iter 41/50 - Loss hyperparam: -1407.232\n",
      "Iter 42/50 - Loss hyperparam: -1447.379\n",
      "Iter 43/50 - Loss hyperparam: -1471.879\n",
      "Iter 44/50 - Loss hyperparam: -1517.493\n",
      "Iter 45/50 - Loss hyperparam: -1554.106\n",
      "Iter 46/50 - Loss hyperparam: -1585.408\n",
      "Iter 47/50 - Loss hyperparam: -1626.867\n",
      "Iter 48/50 - Loss hyperparam: -1662.817\n",
      "Iter 49/50 - Loss hyperparam: -1695.312\n",
      "Iter 50/50 - Loss hyperparam: -1739.688\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.526\n",
      "Iter 2/100 - Loss theta2: -7.496\n",
      "Iter 3/100 - Loss theta2: -7.522\n",
      "Iter 4/100 - Loss theta2: -7.518\n",
      "Iter 5/100 - Loss theta2: -7.509\n",
      "Iter 6/100 - Loss theta2: -7.516\n",
      "Iter 7/100 - Loss theta2: -7.524\n",
      "Iter 8/100 - Loss theta2: -7.525\n",
      "Iter 9/100 - Loss theta2: -7.519\n",
      "Iter 10/100 - Loss theta2: -7.517\n",
      "Iter 11/100 - Loss theta2: -7.518\n",
      "Iter 12/100 - Loss theta2: -7.523\n",
      "Iter 13/100 - Loss theta2: -7.525\n",
      "Iter 14/100 - Loss theta2: -7.523\n",
      "Iter 15/100 - Loss theta2: -7.521\n",
      "Iter 16/100 - Loss theta2: -7.521\n",
      "Iter 17/100 - Loss theta2: -7.523\n",
      "Iter 18/100 - Loss theta2: -7.526\n",
      "Iter 19/100 - Loss theta2: -7.526\n",
      "Iter 20/100 - Loss theta2: -7.524\n",
      "Iter 21/100 - Loss theta2: -7.523\n",
      "Iter 22/100 - Loss theta2: -7.523\n",
      "Iter 23/100 - Loss theta2: -7.527\n",
      "Iter 24/100 - Loss theta2: -7.524\n",
      "Iter 25/100 - Loss theta2: -7.525\n",
      "Iter 26/100 - Loss theta2: -7.524\n",
      "Iter 27/100 - Loss theta2: -7.524\n",
      "Iter 28/100 - Loss theta2: -7.525\n",
      "Iter 29/100 - Loss theta2: -7.523\n",
      "Iter 30/100 - Loss theta2: -7.526\n",
      "Iter 31/100 - Loss theta2: -7.525\n",
      "Iter 32/100 - Loss theta2: -7.527\n",
      "Iter 33/100 - Loss theta2: -7.525\n",
      "Iter 34/100 - Loss theta2: -7.526\n",
      "Iter 35/100 - Loss theta2: -7.525\n",
      "Iter 36/100 - Loss theta2: -7.523\n",
      "Iter 37/100 - Loss theta2: -7.525\n",
      "Iter 38/100 - Loss theta2: -7.525\n",
      "Iter 39/100 - Loss theta2: -7.527\n",
      "Iter 40/100 - Loss theta2: -7.525\n",
      "Iter 41/100 - Loss theta2: -7.527\n",
      "Iter 42/100 - Loss theta2: -7.526\n",
      "Iter 43/100 - Loss theta2: -7.525\n",
      "Iter 44/100 - Loss theta2: -7.526\n",
      "Iter 45/100 - Loss theta2: -7.525\n",
      "Iter 46/100 - Loss theta2: -7.526\n",
      "Iter 47/100 - Loss theta2: -7.525\n",
      "Iter 48/100 - Loss theta2: -7.526\n",
      "Iter 49/100 - Loss theta2: -7.523\n",
      "Iter 50/100 - Loss theta2: -7.526\n",
      "Iter 51/100 - Loss theta2: -7.528\n",
      "Iter 52/100 - Loss theta2: -7.526\n",
      "Iter 53/100 - Loss theta2: -7.525\n",
      "Iter 54/100 - Loss theta2: -7.526\n",
      "Iter 55/100 - Loss theta2: -7.527\n",
      "Iter 56/100 - Loss theta2: -7.526\n",
      "Iter 57/100 - Loss theta2: -7.524\n",
      "Iter 58/100 - Loss theta2: -7.525\n",
      "Iter 59/100 - Loss theta2: -7.525\n",
      "Iter 60/100 - Loss theta2: -7.525\n",
      "Iter 61/100 - Loss theta2: -7.527\n",
      "Iter 62/100 - Loss theta2: -7.526\n",
      "Iter 63/100 - Loss theta2: -7.526\n",
      "Iter 64/100 - Loss theta2: -7.528\n",
      "Iter 65/100 - Loss theta2: -7.525\n",
      "Iter 66/100 - Loss theta2: -7.525\n",
      "Iter 67/100 - Loss theta2: -7.526\n",
      "Iter 68/100 - Loss theta2: -7.525\n",
      "Iter 69/100 - Loss theta2: -7.525\n",
      "Iter 70/100 - Loss theta2: -7.526\n",
      "Iter 71/100 - Loss theta2: -7.526\n",
      "Iter 72/100 - Loss theta2: -7.525\n",
      "Iter 73/100 - Loss theta2: -7.526\n",
      "Iter 74/100 - Loss theta2: -7.526\n",
      "Iter 75/100 - Loss theta2: -7.526\n",
      "Iter 76/100 - Loss theta2: -7.526\n",
      "Iter 77/100 - Loss theta2: -7.527\n",
      "Iter 78/100 - Loss theta2: -7.526\n",
      "Iter 79/100 - Loss theta2: -7.525\n",
      "Iter 80/100 - Loss theta2: -7.526\n",
      "Iter 81/100 - Loss theta2: -7.526\n",
      "Iter 82/100 - Loss theta2: -7.526\n",
      "Iter 83/100 - Loss theta2: -7.526\n",
      "Iter 84/100 - Loss theta2: -7.527\n",
      "Iter 85/100 - Loss theta2: -7.524\n",
      "Iter 86/100 - Loss theta2: -7.526\n",
      "Iter 87/100 - Loss theta2: -7.526\n",
      "Iter 88/100 - Loss theta2: -7.525\n",
      "Iter 89/100 - Loss theta2: -7.524\n",
      "Iter 90/100 - Loss theta2: -7.528\n",
      "Iter 91/100 - Loss theta2: -7.525\n",
      "Iter 92/100 - Loss theta2: -7.525\n",
      "Iter 93/100 - Loss theta2: -7.524\n",
      "Iter 94/100 - Loss theta2: -7.526\n",
      "Iter 95/100 - Loss theta2: -7.526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 96/100 - Loss theta2: -7.527\n",
      "Iter 97/100 - Loss theta2: -7.527\n",
      "Iter 98/100 - Loss theta2: -7.526\n",
      "Iter 99/100 - Loss theta2: -7.526\n",
      "Iter 100/100 - Loss theta2: -7.525\n",
      "tensor([[0.6897],\n",
      "        [0.6898]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7168],\n",
      "        [0.7167]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 232.484\n",
      "Iter 2/50 - Loss hyperparam: 187.689\n",
      "Iter 3/50 - Loss hyperparam: 139.368\n",
      "Iter 4/50 - Loss hyperparam: 88.102\n",
      "Iter 5/50 - Loss hyperparam: 35.760\n",
      "Iter 6/50 - Loss hyperparam: -16.228\n",
      "Iter 7/50 - Loss hyperparam: -67.725\n",
      "Iter 8/50 - Loss hyperparam: -119.076\n",
      "Iter 9/50 - Loss hyperparam: -170.220\n",
      "Iter 10/50 - Loss hyperparam: -220.628\n",
      "Iter 11/50 - Loss hyperparam: -270.278\n",
      "Iter 12/50 - Loss hyperparam: -320.994\n",
      "Iter 13/50 - Loss hyperparam: -375.139\n",
      "Iter 14/50 - Loss hyperparam: -432.374\n",
      "Iter 15/50 - Loss hyperparam: -487.927\n",
      "Iter 16/50 - Loss hyperparam: -532.467\n",
      "Iter 17/50 - Loss hyperparam: -558.037\n",
      "Iter 18/50 - Loss hyperparam: -569.544\n",
      "Iter 19/50 - Loss hyperparam: -581.158\n",
      "Iter 20/50 - Loss hyperparam: -602.219\n",
      "Iter 21/50 - Loss hyperparam: -635.294\n",
      "Iter 22/50 - Loss hyperparam: -679.706\n",
      "Iter 23/50 - Loss hyperparam: -732.626\n",
      "Iter 24/50 - Loss hyperparam: -788.205\n",
      "Iter 25/50 - Loss hyperparam: -838.916\n",
      "Iter 26/50 - Loss hyperparam: -881.142\n",
      "Iter 27/50 - Loss hyperparam: -913.180\n",
      "Iter 28/50 - Loss hyperparam: -936.972\n",
      "Iter 29/50 - Loss hyperparam: -961.373\n",
      "Iter 30/50 - Loss hyperparam: -996.217\n",
      "Iter 31/50 - Loss hyperparam: -1047.251\n",
      "Iter 32/50 - Loss hyperparam: -1107.389\n",
      "Iter 33/50 - Loss hyperparam: -1151.768\n",
      "Iter 34/50 - Loss hyperparam: -1173.118\n",
      "Iter 35/50 - Loss hyperparam: -1198.897\n",
      "Iter 36/50 - Loss hyperparam: -1239.531\n",
      "Iter 37/50 - Loss hyperparam: -1289.125\n",
      "Iter 38/50 - Loss hyperparam: -1340.136\n",
      "Iter 39/50 - Loss hyperparam: -1368.685\n",
      "Iter 40/50 - Loss hyperparam: -1390.959\n",
      "Iter 41/50 - Loss hyperparam: -1441.144\n",
      "Iter 42/50 - Loss hyperparam: -1492.419\n",
      "Iter 43/50 - Loss hyperparam: -1522.135\n",
      "Iter 44/50 - Loss hyperparam: -1550.315\n",
      "Iter 45/50 - Loss hyperparam: -1600.664\n",
      "Iter 46/50 - Loss hyperparam: -1643.584\n",
      "Iter 47/50 - Loss hyperparam: -1666.671\n",
      "Iter 48/50 - Loss hyperparam: -1713.830\n",
      "Iter 49/50 - Loss hyperparam: -1757.382\n",
      "Iter 50/50 - Loss hyperparam: -1779.515\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.614\n",
      "Iter 2/100 - Loss theta2: -7.581\n",
      "Iter 3/100 - Loss theta2: -7.611\n",
      "Iter 4/100 - Loss theta2: -7.606\n",
      "Iter 5/100 - Loss theta2: -7.596\n",
      "Iter 6/100 - Loss theta2: -7.602\n",
      "Iter 7/100 - Loss theta2: -7.612\n",
      "Iter 8/100 - Loss theta2: -7.615\n",
      "Iter 9/100 - Loss theta2: -7.610\n",
      "Iter 10/100 - Loss theta2: -7.604\n",
      "Iter 11/100 - Loss theta2: -7.608\n",
      "Iter 12/100 - Loss theta2: -7.613\n",
      "Iter 13/100 - Loss theta2: -7.614\n",
      "Iter 14/100 - Loss theta2: -7.614\n",
      "Iter 15/100 - Loss theta2: -7.610\n",
      "Iter 16/100 - Loss theta2: -7.611\n",
      "Iter 17/100 - Loss theta2: -7.612\n",
      "Iter 18/100 - Loss theta2: -7.616\n",
      "Iter 19/100 - Loss theta2: -7.615\n",
      "Iter 20/100 - Loss theta2: -7.614\n",
      "Iter 21/100 - Loss theta2: -7.613\n",
      "Iter 22/100 - Loss theta2: -7.614\n",
      "Iter 23/100 - Loss theta2: -7.613\n",
      "Iter 24/100 - Loss theta2: -7.614\n",
      "Iter 25/100 - Loss theta2: -7.612\n",
      "Iter 26/100 - Loss theta2: -7.613\n",
      "Iter 27/100 - Loss theta2: -7.610\n",
      "Iter 28/100 - Loss theta2: -7.614\n",
      "Iter 29/100 - Loss theta2: -7.616\n",
      "Iter 30/100 - Loss theta2: -7.616\n",
      "Iter 31/100 - Loss theta2: -7.614\n",
      "Iter 32/100 - Loss theta2: -7.616\n",
      "Iter 33/100 - Loss theta2: -7.617\n",
      "Iter 34/100 - Loss theta2: -7.615\n",
      "Iter 35/100 - Loss theta2: -7.616\n",
      "Iter 36/100 - Loss theta2: -7.613\n",
      "Iter 37/100 - Loss theta2: -7.614\n",
      "Iter 38/100 - Loss theta2: -7.615\n",
      "Iter 39/100 - Loss theta2: -7.616\n",
      "Iter 40/100 - Loss theta2: -7.617\n",
      "Iter 41/100 - Loss theta2: -7.618\n",
      "Iter 42/100 - Loss theta2: -7.617\n",
      "Iter 43/100 - Loss theta2: -7.615\n",
      "Iter 44/100 - Loss theta2: -7.617\n",
      "Iter 45/100 - Loss theta2: -7.618\n",
      "Iter 46/100 - Loss theta2: -7.617\n",
      "Iter 47/100 - Loss theta2: -7.617\n",
      "Iter 48/100 - Loss theta2: -7.616\n",
      "Iter 49/100 - Loss theta2: -7.618\n",
      "Iter 50/100 - Loss theta2: -7.618\n",
      "Iter 51/100 - Loss theta2: -7.615\n",
      "Iter 52/100 - Loss theta2: -7.617\n",
      "Iter 53/100 - Loss theta2: -7.616\n",
      "Iter 54/100 - Loss theta2: -7.615\n",
      "Iter 55/100 - Loss theta2: -7.614\n",
      "Iter 56/100 - Loss theta2: -7.616\n",
      "Iter 57/100 - Loss theta2: -7.616\n",
      "Iter 58/100 - Loss theta2: -7.615\n",
      "Iter 59/100 - Loss theta2: -7.614\n",
      "Iter 60/100 - Loss theta2: -7.615\n",
      "Iter 61/100 - Loss theta2: -7.617\n",
      "Iter 62/100 - Loss theta2: -7.617\n",
      "Iter 63/100 - Loss theta2: -7.615\n",
      "Iter 64/100 - Loss theta2: -7.617\n",
      "Iter 65/100 - Loss theta2: -7.617\n",
      "Iter 66/100 - Loss theta2: -7.616\n",
      "Iter 67/100 - Loss theta2: -7.618\n",
      "Iter 68/100 - Loss theta2: -7.615\n",
      "Iter 69/100 - Loss theta2: -7.615\n",
      "Iter 70/100 - Loss theta2: -7.617\n",
      "Iter 71/100 - Loss theta2: -7.617\n",
      "Iter 72/100 - Loss theta2: -7.617\n",
      "Iter 73/100 - Loss theta2: -7.615\n",
      "Iter 74/100 - Loss theta2: -7.617\n",
      "Iter 75/100 - Loss theta2: -7.617\n",
      "Iter 76/100 - Loss theta2: -7.617\n",
      "Iter 77/100 - Loss theta2: -7.619\n",
      "Iter 78/100 - Loss theta2: -7.617\n",
      "Iter 79/100 - Loss theta2: -7.617\n",
      "Iter 80/100 - Loss theta2: -7.617\n",
      "Iter 81/100 - Loss theta2: -7.618\n",
      "Iter 82/100 - Loss theta2: -7.618\n",
      "Iter 83/100 - Loss theta2: -7.615\n",
      "Iter 84/100 - Loss theta2: -7.615\n",
      "Iter 85/100 - Loss theta2: -7.616\n",
      "Iter 86/100 - Loss theta2: -7.617\n",
      "Iter 87/100 - Loss theta2: -7.617\n",
      "Iter 88/100 - Loss theta2: -7.617\n",
      "Iter 89/100 - Loss theta2: -7.618\n",
      "Iter 90/100 - Loss theta2: -7.618\n",
      "Iter 91/100 - Loss theta2: -7.615\n",
      "Iter 92/100 - Loss theta2: -7.616\n",
      "Iter 93/100 - Loss theta2: -7.617\n",
      "Iter 94/100 - Loss theta2: -7.614\n",
      "Iter 95/100 - Loss theta2: -7.616\n",
      "Iter 96/100 - Loss theta2: -7.617\n",
      "Iter 97/100 - Loss theta2: -7.617\n",
      "Iter 98/100 - Loss theta2: -7.616\n",
      "Iter 99/100 - Loss theta2: -7.619\n",
      "Iter 100/100 - Loss theta2: -7.618\n",
      "tensor([[0.6908],\n",
      "        [0.6910]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7171],\n",
      "        [0.7169]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 102.415\n",
      "Iter 2/50 - Loss hyperparam: 48.792\n",
      "Iter 3/50 - Loss hyperparam: -7.746\n",
      "Iter 4/50 - Loss hyperparam: -64.584\n",
      "Iter 5/50 - Loss hyperparam: -118.437\n",
      "Iter 6/50 - Loss hyperparam: -168.629\n",
      "Iter 7/50 - Loss hyperparam: -217.859\n",
      "Iter 8/50 - Loss hyperparam: -267.053\n",
      "Iter 9/50 - Loss hyperparam: -313.770\n",
      "Iter 10/50 - Loss hyperparam: -353.623\n",
      "Iter 11/50 - Loss hyperparam: -382.985\n",
      "Iter 12/50 - Loss hyperparam: -403.629\n",
      "Iter 13/50 - Loss hyperparam: -422.699\n",
      "Iter 14/50 - Loss hyperparam: -445.906\n",
      "Iter 15/50 - Loss hyperparam: -475.109\n",
      "Iter 16/50 - Loss hyperparam: -509.740\n",
      "Iter 17/50 - Loss hyperparam: -548.243\n",
      "Iter 18/50 - Loss hyperparam: -589.499\n",
      "Iter 19/50 - Loss hyperparam: -632.971\n",
      "Iter 20/50 - Loss hyperparam: -675.793\n",
      "Iter 21/50 - Loss hyperparam: -712.169\n",
      "Iter 22/50 - Loss hyperparam: -739.805\n",
      "Iter 23/50 - Loss hyperparam: -765.727\n",
      "Iter 24/50 - Loss hyperparam: -798.063\n",
      "Iter 25/50 - Loss hyperparam: -837.949\n",
      "Iter 26/50 - Loss hyperparam: -881.636\n",
      "Iter 27/50 - Loss hyperparam: -925.754\n",
      "Iter 28/50 - Loss hyperparam: -965.787\n",
      "Iter 29/50 - Loss hyperparam: -996.263\n",
      "Iter 30/50 - Loss hyperparam: -1025.438\n",
      "Iter 31/50 - Loss hyperparam: -1064.835\n",
      "Iter 32/50 - Loss hyperparam: -1110.156\n",
      "Iter 33/50 - Loss hyperparam: -1152.370\n",
      "Iter 34/50 - Loss hyperparam: -1186.409\n",
      "Iter 35/50 - Loss hyperparam: -1215.714\n",
      "Iter 36/50 - Loss hyperparam: -1256.955\n",
      "Iter 37/50 - Loss hyperparam: -1301.331\n",
      "Iter 38/50 - Loss hyperparam: -1337.079\n",
      "Iter 39/50 - Loss hyperparam: -1366.661\n",
      "Iter 40/50 - Loss hyperparam: -1409.246\n",
      "Iter 41/50 - Loss hyperparam: -1450.912\n",
      "Iter 42/50 - Loss hyperparam: -1482.813\n",
      "Iter 43/50 - Loss hyperparam: -1520.490\n",
      "Iter 44/50 - Loss hyperparam: -1563.772\n",
      "Iter 45/50 - Loss hyperparam: -1595.467\n",
      "Iter 46/50 - Loss hyperparam: -1636.148\n",
      "Iter 47/50 - Loss hyperparam: -1673.459\n",
      "Iter 48/50 - Loss hyperparam: -1703.639\n",
      "Iter 49/50 - Loss hyperparam: -1749.193\n",
      "Iter 50/50 - Loss hyperparam: -1781.013\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.511\n",
      "Iter 2/100 - Loss theta2: -7.486\n",
      "Iter 3/100 - Loss theta2: -7.511\n",
      "Iter 4/100 - Loss theta2: -7.507\n",
      "Iter 5/100 - Loss theta2: -7.499\n",
      "Iter 6/100 - Loss theta2: -7.502\n",
      "Iter 7/100 - Loss theta2: -7.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8/100 - Loss theta2: -7.510\n",
      "Iter 9/100 - Loss theta2: -7.506\n",
      "Iter 10/100 - Loss theta2: -7.506\n",
      "Iter 11/100 - Loss theta2: -7.508\n",
      "Iter 12/100 - Loss theta2: -7.512\n",
      "Iter 13/100 - Loss theta2: -7.512\n",
      "Iter 14/100 - Loss theta2: -7.508\n",
      "Iter 15/100 - Loss theta2: -7.507\n",
      "Iter 16/100 - Loss theta2: -7.510\n",
      "Iter 17/100 - Loss theta2: -7.513\n",
      "Iter 18/100 - Loss theta2: -7.512\n",
      "Iter 19/100 - Loss theta2: -7.514\n",
      "Iter 20/100 - Loss theta2: -7.512\n",
      "Iter 21/100 - Loss theta2: -7.510\n",
      "Iter 22/100 - Loss theta2: -7.511\n",
      "Iter 23/100 - Loss theta2: -7.512\n",
      "Iter 24/100 - Loss theta2: -7.513\n",
      "Iter 25/100 - Loss theta2: -7.510\n",
      "Iter 26/100 - Loss theta2: -7.511\n",
      "Iter 27/100 - Loss theta2: -7.513\n",
      "Iter 28/100 - Loss theta2: -7.511\n",
      "Iter 29/100 - Loss theta2: -7.513\n",
      "Iter 30/100 - Loss theta2: -7.513\n",
      "Iter 31/100 - Loss theta2: -7.512\n",
      "Iter 32/100 - Loss theta2: -7.511\n",
      "Iter 33/100 - Loss theta2: -7.515\n",
      "Iter 34/100 - Loss theta2: -7.513\n",
      "Iter 35/100 - Loss theta2: -7.513\n",
      "Iter 36/100 - Loss theta2: -7.514\n",
      "Iter 37/100 - Loss theta2: -7.512\n",
      "Iter 38/100 - Loss theta2: -7.515\n",
      "Iter 39/100 - Loss theta2: -7.514\n",
      "Iter 40/100 - Loss theta2: -7.514\n",
      "Iter 41/100 - Loss theta2: -7.512\n",
      "Iter 42/100 - Loss theta2: -7.515\n",
      "Iter 43/100 - Loss theta2: -7.514\n",
      "Iter 44/100 - Loss theta2: -7.515\n",
      "Iter 45/100 - Loss theta2: -7.512\n",
      "Iter 46/100 - Loss theta2: -7.514\n",
      "Iter 47/100 - Loss theta2: -7.512\n",
      "Iter 48/100 - Loss theta2: -7.511\n",
      "Iter 49/100 - Loss theta2: -7.513\n",
      "Iter 50/100 - Loss theta2: -7.513\n",
      "Iter 51/100 - Loss theta2: -7.513\n",
      "Iter 52/100 - Loss theta2: -7.514\n",
      "Iter 53/100 - Loss theta2: -7.514\n",
      "Iter 54/100 - Loss theta2: -7.515\n",
      "Iter 55/100 - Loss theta2: -7.515\n",
      "Iter 56/100 - Loss theta2: -7.513\n",
      "Iter 57/100 - Loss theta2: -7.513\n",
      "Iter 58/100 - Loss theta2: -7.513\n",
      "Iter 59/100 - Loss theta2: -7.514\n",
      "Iter 60/100 - Loss theta2: -7.513\n",
      "Iter 61/100 - Loss theta2: -7.515\n",
      "Iter 62/100 - Loss theta2: -7.514\n",
      "Iter 63/100 - Loss theta2: -7.513\n",
      "Iter 64/100 - Loss theta2: -7.513\n",
      "Iter 65/100 - Loss theta2: -7.515\n",
      "Iter 66/100 - Loss theta2: -7.514\n",
      "Iter 67/100 - Loss theta2: -7.513\n",
      "Iter 68/100 - Loss theta2: -7.516\n",
      "Iter 69/100 - Loss theta2: -7.512\n",
      "Iter 70/100 - Loss theta2: -7.514\n",
      "Iter 71/100 - Loss theta2: -7.514\n",
      "Iter 72/100 - Loss theta2: -7.514\n",
      "Iter 73/100 - Loss theta2: -7.514\n",
      "Iter 74/100 - Loss theta2: -7.514\n",
      "Iter 75/100 - Loss theta2: -7.515\n",
      "Iter 76/100 - Loss theta2: -7.514\n",
      "Iter 77/100 - Loss theta2: -7.514\n",
      "Iter 78/100 - Loss theta2: -7.513\n",
      "Iter 79/100 - Loss theta2: -7.513\n",
      "Iter 80/100 - Loss theta2: -7.513\n",
      "Iter 81/100 - Loss theta2: -7.514\n",
      "Iter 82/100 - Loss theta2: -7.514\n",
      "Iter 83/100 - Loss theta2: -7.513\n",
      "Iter 84/100 - Loss theta2: -7.514\n",
      "Iter 85/100 - Loss theta2: -7.514\n",
      "Iter 86/100 - Loss theta2: -7.513\n",
      "Iter 87/100 - Loss theta2: -7.515\n",
      "Iter 88/100 - Loss theta2: -7.515\n",
      "Iter 89/100 - Loss theta2: -7.513\n",
      "Iter 90/100 - Loss theta2: -7.514\n",
      "Iter 91/100 - Loss theta2: -7.514\n",
      "Iter 92/100 - Loss theta2: -7.516\n",
      "Iter 93/100 - Loss theta2: -7.514\n",
      "Iter 94/100 - Loss theta2: -7.514\n",
      "Iter 95/100 - Loss theta2: -7.515\n",
      "Iter 96/100 - Loss theta2: -7.514\n",
      "Iter 97/100 - Loss theta2: -7.515\n",
      "Iter 98/100 - Loss theta2: -7.513\n",
      "Iter 99/100 - Loss theta2: -7.514\n",
      "Iter 100/100 - Loss theta2: -7.513\n",
      "tensor([[0.6898],\n",
      "        [0.6901]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7175],\n",
      "        [0.7173]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 36.241\n",
      "Iter 2/50 - Loss hyperparam: -14.424\n",
      "Iter 3/50 - Loss hyperparam: -68.752\n",
      "Iter 4/50 - Loss hyperparam: -122.508\n",
      "Iter 5/50 - Loss hyperparam: -175.377\n",
      "Iter 6/50 - Loss hyperparam: -223.501\n",
      "Iter 7/50 - Loss hyperparam: -264.518\n",
      "Iter 8/50 - Loss hyperparam: -298.857\n",
      "Iter 9/50 - Loss hyperparam: -326.159\n",
      "Iter 10/50 - Loss hyperparam: -349.291\n",
      "Iter 11/50 - Loss hyperparam: -373.587\n",
      "Iter 12/50 - Loss hyperparam: -401.520\n",
      "Iter 13/50 - Loss hyperparam: -433.437\n",
      "Iter 14/50 - Loss hyperparam: -469.637\n",
      "Iter 15/50 - Loss hyperparam: -508.820\n",
      "Iter 16/50 - Loss hyperparam: -547.987\n",
      "Iter 17/50 - Loss hyperparam: -585.166\n",
      "Iter 18/50 - Loss hyperparam: -618.255\n",
      "Iter 19/50 - Loss hyperparam: -647.399\n",
      "Iter 20/50 - Loss hyperparam: -677.171\n",
      "Iter 21/50 - Loss hyperparam: -710.398\n",
      "Iter 22/50 - Loss hyperparam: -748.738\n",
      "Iter 23/50 - Loss hyperparam: -790.288\n",
      "Iter 24/50 - Loss hyperparam: -830.858\n",
      "Iter 25/50 - Loss hyperparam: -866.848\n",
      "Iter 26/50 - Loss hyperparam: -898.712\n",
      "Iter 27/50 - Loss hyperparam: -932.395\n",
      "Iter 28/50 - Loss hyperparam: -971.930\n",
      "Iter 29/50 - Loss hyperparam: -1014.491\n",
      "Iter 30/50 - Loss hyperparam: -1054.131\n",
      "Iter 31/50 - Loss hyperparam: -1087.681\n",
      "Iter 32/50 - Loss hyperparam: -1122.201\n",
      "Iter 33/50 - Loss hyperparam: -1163.367\n",
      "Iter 34/50 - Loss hyperparam: -1205.359\n",
      "Iter 35/50 - Loss hyperparam: -1240.186\n",
      "Iter 36/50 - Loss hyperparam: -1274.853\n",
      "Iter 37/50 - Loss hyperparam: -1316.589\n",
      "Iter 38/50 - Loss hyperparam: -1355.319\n",
      "Iter 39/50 - Loss hyperparam: -1386.069\n",
      "Iter 40/50 - Loss hyperparam: -1423.479\n",
      "Iter 41/50 - Loss hyperparam: -1468.077\n",
      "Iter 42/50 - Loss hyperparam: -1498.514\n",
      "Iter 43/50 - Loss hyperparam: -1540.901\n",
      "Iter 44/50 - Loss hyperparam: -1579.454\n",
      "Iter 45/50 - Loss hyperparam: -1612.528\n",
      "Iter 46/50 - Loss hyperparam: -1656.788\n",
      "Iter 47/50 - Loss hyperparam: -1689.170\n",
      "Iter 48/50 - Loss hyperparam: -1728.446\n",
      "Iter 49/50 - Loss hyperparam: -1765.479\n",
      "Iter 50/50 - Loss hyperparam: -1804.654\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.448\n",
      "Iter 2/100 - Loss theta2: -7.416\n",
      "Iter 3/100 - Loss theta2: -7.446\n",
      "Iter 4/100 - Loss theta2: -7.440\n",
      "Iter 5/100 - Loss theta2: -7.428\n",
      "Iter 6/100 - Loss theta2: -7.435\n",
      "Iter 7/100 - Loss theta2: -7.444\n",
      "Iter 8/100 - Loss theta2: -7.446\n",
      "Iter 9/100 - Loss theta2: -7.440\n",
      "Iter 10/100 - Loss theta2: -7.437\n",
      "Iter 11/100 - Loss theta2: -7.440\n",
      "Iter 12/100 - Loss theta2: -7.446\n",
      "Iter 13/100 - Loss theta2: -7.450\n",
      "Iter 14/100 - Loss theta2: -7.446\n",
      "Iter 15/100 - Loss theta2: -7.442\n",
      "Iter 16/100 - Loss theta2: -7.445\n",
      "Iter 17/100 - Loss theta2: -7.446\n",
      "Iter 18/100 - Loss theta2: -7.447\n",
      "Iter 19/100 - Loss theta2: -7.449\n",
      "Iter 20/100 - Loss theta2: -7.447\n",
      "Iter 21/100 - Loss theta2: -7.444\n",
      "Iter 22/100 - Loss theta2: -7.447\n",
      "Iter 23/100 - Loss theta2: -7.448\n",
      "Iter 24/100 - Loss theta2: -7.449\n",
      "Iter 25/100 - Loss theta2: -7.448\n",
      "Iter 26/100 - Loss theta2: -7.447\n",
      "Iter 27/100 - Loss theta2: -7.448\n",
      "Iter 28/100 - Loss theta2: -7.450\n",
      "Iter 29/100 - Loss theta2: -7.449\n",
      "Iter 30/100 - Loss theta2: -7.451\n",
      "Iter 31/100 - Loss theta2: -7.449\n",
      "Iter 32/100 - Loss theta2: -7.449\n",
      "Iter 33/100 - Loss theta2: -7.447\n",
      "Iter 34/100 - Loss theta2: -7.450\n",
      "Iter 35/100 - Loss theta2: -7.451\n",
      "Iter 36/100 - Loss theta2: -7.450\n",
      "Iter 37/100 - Loss theta2: -7.451\n",
      "Iter 38/100 - Loss theta2: -7.451\n",
      "Iter 39/100 - Loss theta2: -7.451\n",
      "Iter 40/100 - Loss theta2: -7.449\n",
      "Iter 41/100 - Loss theta2: -7.451\n",
      "Iter 42/100 - Loss theta2: -7.452\n",
      "Iter 43/100 - Loss theta2: -7.451\n",
      "Iter 44/100 - Loss theta2: -7.450\n",
      "Iter 45/100 - Loss theta2: -7.450\n",
      "Iter 46/100 - Loss theta2: -7.450\n",
      "Iter 47/100 - Loss theta2: -7.449\n",
      "Iter 48/100 - Loss theta2: -7.450\n",
      "Iter 49/100 - Loss theta2: -7.451\n",
      "Iter 50/100 - Loss theta2: -7.451\n",
      "Iter 51/100 - Loss theta2: -7.451\n",
      "Iter 52/100 - Loss theta2: -7.449\n",
      "Iter 53/100 - Loss theta2: -7.451\n",
      "Iter 54/100 - Loss theta2: -7.451\n",
      "Iter 55/100 - Loss theta2: -7.451\n",
      "Iter 56/100 - Loss theta2: -7.452\n",
      "Iter 57/100 - Loss theta2: -7.451\n",
      "Iter 58/100 - Loss theta2: -7.452\n",
      "Iter 59/100 - Loss theta2: -7.453\n",
      "Iter 60/100 - Loss theta2: -7.451\n",
      "Iter 61/100 - Loss theta2: -7.452\n",
      "Iter 62/100 - Loss theta2: -7.453\n",
      "Iter 63/100 - Loss theta2: -7.450\n",
      "Iter 64/100 - Loss theta2: -7.451\n",
      "Iter 65/100 - Loss theta2: -7.453\n",
      "Iter 66/100 - Loss theta2: -7.453\n",
      "Iter 67/100 - Loss theta2: -7.451\n",
      "Iter 68/100 - Loss theta2: -7.452\n",
      "Iter 69/100 - Loss theta2: -7.450\n",
      "Iter 70/100 - Loss theta2: -7.453\n",
      "Iter 71/100 - Loss theta2: -7.452\n",
      "Iter 72/100 - Loss theta2: -7.453\n",
      "Iter 73/100 - Loss theta2: -7.451\n",
      "Iter 74/100 - Loss theta2: -7.453\n",
      "Iter 75/100 - Loss theta2: -7.452\n",
      "Iter 76/100 - Loss theta2: -7.454\n",
      "Iter 77/100 - Loss theta2: -7.452\n",
      "Iter 78/100 - Loss theta2: -7.453\n",
      "Iter 79/100 - Loss theta2: -7.453\n",
      "Iter 80/100 - Loss theta2: -7.453\n",
      "Iter 81/100 - Loss theta2: -7.452\n",
      "Iter 82/100 - Loss theta2: -7.452\n",
      "Iter 83/100 - Loss theta2: -7.451\n",
      "Iter 84/100 - Loss theta2: -7.453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 85/100 - Loss theta2: -7.452\n",
      "Iter 86/100 - Loss theta2: -7.453\n",
      "Iter 87/100 - Loss theta2: -7.454\n",
      "Iter 88/100 - Loss theta2: -7.452\n",
      "Iter 89/100 - Loss theta2: -7.453\n",
      "Iter 90/100 - Loss theta2: -7.453\n",
      "Iter 91/100 - Loss theta2: -7.453\n",
      "Iter 92/100 - Loss theta2: -7.452\n",
      "Iter 93/100 - Loss theta2: -7.452\n",
      "Iter 94/100 - Loss theta2: -7.453\n",
      "Iter 95/100 - Loss theta2: -7.452\n",
      "Iter 96/100 - Loss theta2: -7.454\n",
      "Iter 97/100 - Loss theta2: -7.453\n",
      "Iter 98/100 - Loss theta2: -7.454\n",
      "Iter 99/100 - Loss theta2: -7.454\n",
      "Iter 100/100 - Loss theta2: -7.453\n",
      "tensor([[0.6890],\n",
      "        [0.6895]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7176],\n",
      "        [0.7173]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: -23.099\n",
      "Iter 2/50 - Loss hyperparam: -73.383\n",
      "Iter 3/50 - Loss hyperparam: -120.171\n",
      "Iter 4/50 - Loss hyperparam: -161.378\n",
      "Iter 5/50 - Loss hyperparam: -197.942\n",
      "Iter 6/50 - Loss hyperparam: -231.585\n",
      "Iter 7/50 - Loss hyperparam: -264.924\n",
      "Iter 8/50 - Loss hyperparam: -298.573\n",
      "Iter 9/50 - Loss hyperparam: -332.048\n",
      "Iter 10/50 - Loss hyperparam: -365.529\n",
      "Iter 11/50 - Loss hyperparam: -398.578\n",
      "Iter 12/50 - Loss hyperparam: -429.941\n",
      "Iter 13/50 - Loss hyperparam: -459.745\n",
      "Iter 14/50 - Loss hyperparam: -490.279\n",
      "Iter 15/50 - Loss hyperparam: -523.839\n",
      "Iter 16/50 - Loss hyperparam: -559.935\n",
      "Iter 17/50 - Loss hyperparam: -595.471\n",
      "Iter 18/50 - Loss hyperparam: -628.290\n",
      "Iter 19/50 - Loss hyperparam: -660.816\n",
      "Iter 20/50 - Loss hyperparam: -696.332\n",
      "Iter 21/50 - Loss hyperparam: -733.799\n",
      "Iter 22/50 - Loss hyperparam: -770.382\n",
      "Iter 23/50 - Loss hyperparam: -805.933\n",
      "Iter 24/50 - Loss hyperparam: -841.669\n",
      "Iter 25/50 - Loss hyperparam: -878.784\n",
      "Iter 26/50 - Loss hyperparam: -917.695\n",
      "Iter 27/50 - Loss hyperparam: -954.894\n",
      "Iter 28/50 - Loss hyperparam: -990.677\n",
      "Iter 29/50 - Loss hyperparam: -1030.352\n",
      "Iter 30/50 - Loss hyperparam: -1068.726\n",
      "Iter 31/50 - Loss hyperparam: -1105.385\n",
      "Iter 32/50 - Loss hyperparam: -1144.495\n",
      "Iter 33/50 - Loss hyperparam: -1183.336\n",
      "Iter 34/50 - Loss hyperparam: -1220.182\n",
      "Iter 35/50 - Loss hyperparam: -1259.868\n",
      "Iter 36/50 - Loss hyperparam: -1296.950\n",
      "Iter 37/50 - Loss hyperparam: -1335.574\n",
      "Iter 38/50 - Loss hyperparam: -1373.528\n",
      "Iter 39/50 - Loss hyperparam: -1410.479\n",
      "Iter 40/50 - Loss hyperparam: -1449.791\n",
      "Iter 41/50 - Loss hyperparam: -1487.047\n",
      "Iter 42/50 - Loss hyperparam: -1524.677\n",
      "Iter 43/50 - Loss hyperparam: -1561.888\n",
      "Iter 44/50 - Loss hyperparam: -1598.816\n",
      "Iter 45/50 - Loss hyperparam: -1640.232\n",
      "Iter 46/50 - Loss hyperparam: -1678.419\n",
      "Iter 47/50 - Loss hyperparam: -1712.549\n",
      "Iter 48/50 - Loss hyperparam: -1747.936\n",
      "Iter 49/50 - Loss hyperparam: -1789.467\n",
      "Iter 50/50 - Loss hyperparam: -1830.123\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.447\n",
      "Iter 2/100 - Loss theta2: -7.413\n",
      "Iter 3/100 - Loss theta2: -7.446\n",
      "Iter 4/100 - Loss theta2: -7.442\n",
      "Iter 5/100 - Loss theta2: -7.428\n",
      "Iter 6/100 - Loss theta2: -7.433\n",
      "Iter 7/100 - Loss theta2: -7.445\n",
      "Iter 8/100 - Loss theta2: -7.448\n",
      "Iter 9/100 - Loss theta2: -7.442\n",
      "Iter 10/100 - Loss theta2: -7.438\n",
      "Iter 11/100 - Loss theta2: -7.440\n",
      "Iter 12/100 - Loss theta2: -7.445\n",
      "Iter 13/100 - Loss theta2: -7.448\n",
      "Iter 14/100 - Loss theta2: -7.449\n",
      "Iter 15/100 - Loss theta2: -7.444\n",
      "Iter 16/100 - Loss theta2: -7.443\n",
      "Iter 17/100 - Loss theta2: -7.445\n",
      "Iter 18/100 - Loss theta2: -7.449\n",
      "Iter 19/100 - Loss theta2: -7.449\n",
      "Iter 20/100 - Loss theta2: -7.449\n",
      "Iter 21/100 - Loss theta2: -7.445\n",
      "Iter 22/100 - Loss theta2: -7.446\n",
      "Iter 23/100 - Loss theta2: -7.448\n",
      "Iter 24/100 - Loss theta2: -7.447\n",
      "Iter 25/100 - Loss theta2: -7.449\n",
      "Iter 26/100 - Loss theta2: -7.448\n",
      "Iter 27/100 - Loss theta2: -7.449\n",
      "Iter 28/100 - Loss theta2: -7.449\n",
      "Iter 29/100 - Loss theta2: -7.448\n",
      "Iter 30/100 - Loss theta2: -7.448\n",
      "Iter 31/100 - Loss theta2: -7.448\n",
      "Iter 32/100 - Loss theta2: -7.450\n",
      "Iter 33/100 - Loss theta2: -7.450\n",
      "Iter 34/100 - Loss theta2: -7.448\n",
      "Iter 35/100 - Loss theta2: -7.450\n",
      "Iter 36/100 - Loss theta2: -7.449\n",
      "Iter 37/100 - Loss theta2: -7.448\n",
      "Iter 38/100 - Loss theta2: -7.449\n",
      "Iter 39/100 - Loss theta2: -7.451\n",
      "Iter 40/100 - Loss theta2: -7.450\n",
      "Iter 41/100 - Loss theta2: -7.450\n",
      "Iter 42/100 - Loss theta2: -7.450\n",
      "Iter 43/100 - Loss theta2: -7.451\n",
      "Iter 44/100 - Loss theta2: -7.452\n",
      "Iter 45/100 - Loss theta2: -7.448\n",
      "Iter 46/100 - Loss theta2: -7.448\n",
      "Iter 47/100 - Loss theta2: -7.452\n",
      "Iter 48/100 - Loss theta2: -7.450\n",
      "Iter 49/100 - Loss theta2: -7.451\n",
      "Iter 50/100 - Loss theta2: -7.451\n",
      "Iter 51/100 - Loss theta2: -7.450\n",
      "Iter 52/100 - Loss theta2: -7.451\n",
      "Iter 53/100 - Loss theta2: -7.450\n",
      "Iter 54/100 - Loss theta2: -7.451\n",
      "Iter 55/100 - Loss theta2: -7.449\n",
      "Iter 56/100 - Loss theta2: -7.450\n",
      "Iter 57/100 - Loss theta2: -7.451\n",
      "Iter 58/100 - Loss theta2: -7.451\n",
      "Iter 59/100 - Loss theta2: -7.450\n",
      "Iter 60/100 - Loss theta2: -7.451\n",
      "Iter 61/100 - Loss theta2: -7.451\n",
      "Iter 62/100 - Loss theta2: -7.452\n",
      "Iter 63/100 - Loss theta2: -7.451\n",
      "Iter 64/100 - Loss theta2: -7.451\n",
      "Iter 65/100 - Loss theta2: -7.451\n",
      "Iter 66/100 - Loss theta2: -7.453\n",
      "Iter 67/100 - Loss theta2: -7.451\n",
      "Iter 68/100 - Loss theta2: -7.451\n",
      "Iter 69/100 - Loss theta2: -7.452\n",
      "Iter 70/100 - Loss theta2: -7.451\n",
      "Iter 71/100 - Loss theta2: -7.452\n",
      "Iter 72/100 - Loss theta2: -7.449\n",
      "Iter 73/100 - Loss theta2: -7.451\n",
      "Iter 74/100 - Loss theta2: -7.450\n",
      "Iter 75/100 - Loss theta2: -7.452\n",
      "Iter 76/100 - Loss theta2: -7.451\n",
      "Iter 77/100 - Loss theta2: -7.451\n",
      "Iter 78/100 - Loss theta2: -7.451\n",
      "Iter 79/100 - Loss theta2: -7.452\n",
      "Iter 80/100 - Loss theta2: -7.453\n",
      "Iter 81/100 - Loss theta2: -7.453\n",
      "Iter 82/100 - Loss theta2: -7.454\n",
      "Iter 83/100 - Loss theta2: -7.451\n",
      "Iter 84/100 - Loss theta2: -7.451\n",
      "Iter 85/100 - Loss theta2: -7.453\n",
      "Iter 86/100 - Loss theta2: -7.453\n",
      "Iter 87/100 - Loss theta2: -7.453\n",
      "Iter 88/100 - Loss theta2: -7.452\n",
      "Iter 89/100 - Loss theta2: -7.452\n",
      "Iter 90/100 - Loss theta2: -7.452\n",
      "Iter 91/100 - Loss theta2: -7.452\n",
      "Iter 92/100 - Loss theta2: -7.454\n",
      "Iter 93/100 - Loss theta2: -7.452\n",
      "Iter 94/100 - Loss theta2: -7.453\n",
      "Iter 95/100 - Loss theta2: -7.452\n",
      "Iter 96/100 - Loss theta2: -7.454\n",
      "Iter 97/100 - Loss theta2: -7.454\n",
      "Iter 98/100 - Loss theta2: -7.454\n",
      "Iter 99/100 - Loss theta2: -7.453\n",
      "Iter 100/100 - Loss theta2: -7.452\n",
      "tensor([[0.6891],\n",
      "        [0.6894]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7173]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 300.378\n",
      "Iter 2/50 - Loss hyperparam: 256.950\n",
      "Iter 3/50 - Loss hyperparam: 213.175\n",
      "Iter 4/50 - Loss hyperparam: 168.232\n",
      "Iter 5/50 - Loss hyperparam: 120.826\n",
      "Iter 6/50 - Loss hyperparam: 70.164\n",
      "Iter 7/50 - Loss hyperparam: 16.078\n",
      "Iter 8/50 - Loss hyperparam: -40.827\n",
      "Iter 9/50 - Loss hyperparam: -100.061\n",
      "Iter 10/50 - Loss hyperparam: -161.991\n",
      "Iter 11/50 - Loss hyperparam: -227.452\n",
      "Iter 12/50 - Loss hyperparam: -296.880\n",
      "Iter 13/50 - Loss hyperparam: -369.306\n",
      "Iter 14/50 - Loss hyperparam: -440.812\n",
      "Iter 15/50 - Loss hyperparam: -503.373\n",
      "Iter 16/50 - Loss hyperparam: -549.687\n",
      "Iter 17/50 - Loss hyperparam: -581.941\n",
      "Iter 18/50 - Loss hyperparam: -605.715\n",
      "Iter 19/50 - Loss hyperparam: -626.472\n",
      "Iter 20/50 - Loss hyperparam: -650.763\n",
      "Iter 21/50 - Loss hyperparam: -682.706\n",
      "Iter 22/50 - Loss hyperparam: -722.806\n",
      "Iter 23/50 - Loss hyperparam: -769.294\n",
      "Iter 24/50 - Loss hyperparam: -820.141\n",
      "Iter 25/50 - Loss hyperparam: -875.113\n",
      "Iter 26/50 - Loss hyperparam: -933.819\n",
      "Iter 27/50 - Loss hyperparam: -988.570\n",
      "Iter 28/50 - Loss hyperparam: -1026.125\n",
      "Iter 29/50 - Loss hyperparam: -1047.875\n",
      "Iter 30/50 - Loss hyperparam: -1070.812\n",
      "Iter 31/50 - Loss hyperparam: -1109.550\n",
      "Iter 32/50 - Loss hyperparam: -1169.463\n",
      "Iter 33/50 - Loss hyperparam: -1240.478\n",
      "Iter 34/50 - Loss hyperparam: -1288.113\n",
      "Iter 35/50 - Loss hyperparam: -1301.214\n",
      "Iter 36/50 - Loss hyperparam: -1325.639\n",
      "Iter 37/50 - Loss hyperparam: -1381.236\n",
      "Iter 38/50 - Loss hyperparam: -1449.214\n",
      "Iter 39/50 - Loss hyperparam: -1487.092\n",
      "Iter 40/50 - Loss hyperparam: -1510.382\n",
      "Iter 41/50 - Loss hyperparam: -1553.702\n",
      "Iter 42/50 - Loss hyperparam: -1608.495\n",
      "Iter 43/50 - Loss hyperparam: -1656.322\n",
      "Iter 44/50 - Loss hyperparam: -1686.661\n",
      "Iter 45/50 - Loss hyperparam: -1721.564\n",
      "Iter 46/50 - Loss hyperparam: -1785.107\n",
      "Iter 47/50 - Loss hyperparam: -1815.353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 48/50 - Loss hyperparam: -1846.294\n",
      "Iter 49/50 - Loss hyperparam: -1913.386\n",
      "Iter 50/50 - Loss hyperparam: -1931.897\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.691\n",
      "Iter 2/100 - Loss theta2: -7.667\n",
      "Iter 3/100 - Loss theta2: -7.691\n",
      "Iter 4/100 - Loss theta2: -7.686\n",
      "Iter 5/100 - Loss theta2: -7.681\n",
      "Iter 6/100 - Loss theta2: -7.683\n",
      "Iter 7/100 - Loss theta2: -7.695\n",
      "Iter 8/100 - Loss theta2: -7.692\n",
      "Iter 9/100 - Loss theta2: -7.688\n",
      "Iter 10/100 - Loss theta2: -7.686\n",
      "Iter 11/100 - Loss theta2: -7.691\n",
      "Iter 12/100 - Loss theta2: -7.691\n",
      "Iter 13/100 - Loss theta2: -7.692\n",
      "Iter 14/100 - Loss theta2: -7.689\n",
      "Iter 15/100 - Loss theta2: -7.689\n",
      "Iter 16/100 - Loss theta2: -7.691\n",
      "Iter 17/100 - Loss theta2: -7.690\n",
      "Iter 18/100 - Loss theta2: -7.691\n",
      "Iter 19/100 - Loss theta2: -7.690\n",
      "Iter 20/100 - Loss theta2: -7.693\n",
      "Iter 21/100 - Loss theta2: -7.694\n",
      "Iter 22/100 - Loss theta2: -7.692\n",
      "Iter 23/100 - Loss theta2: -7.695\n",
      "Iter 24/100 - Loss theta2: -7.690\n",
      "Iter 25/100 - Loss theta2: -7.692\n",
      "Iter 26/100 - Loss theta2: -7.692\n",
      "Iter 27/100 - Loss theta2: -7.693\n",
      "Iter 28/100 - Loss theta2: -7.692\n",
      "Iter 29/100 - Loss theta2: -7.693\n",
      "Iter 30/100 - Loss theta2: -7.694\n",
      "Iter 31/100 - Loss theta2: -7.693\n",
      "Iter 32/100 - Loss theta2: -7.693\n",
      "Iter 33/100 - Loss theta2: -7.692\n",
      "Iter 34/100 - Loss theta2: -7.694\n",
      "Iter 35/100 - Loss theta2: -7.690\n",
      "Iter 36/100 - Loss theta2: -7.693\n",
      "Iter 37/100 - Loss theta2: -7.693\n",
      "Iter 38/100 - Loss theta2: -7.692\n",
      "Iter 39/100 - Loss theta2: -7.691\n",
      "Iter 40/100 - Loss theta2: -7.692\n",
      "Iter 41/100 - Loss theta2: -7.696\n",
      "Iter 42/100 - Loss theta2: -7.693\n",
      "Iter 43/100 - Loss theta2: -7.692\n",
      "Iter 44/100 - Loss theta2: -7.695\n",
      "Iter 45/100 - Loss theta2: -7.695\n",
      "Iter 46/100 - Loss theta2: -7.696\n",
      "Iter 47/100 - Loss theta2: -7.693\n",
      "Iter 48/100 - Loss theta2: -7.694\n",
      "Iter 49/100 - Loss theta2: -7.691\n",
      "Iter 50/100 - Loss theta2: -7.693\n",
      "Iter 51/100 - Loss theta2: -7.696\n",
      "Iter 52/100 - Loss theta2: -7.693\n",
      "Iter 53/100 - Loss theta2: -7.696\n",
      "Iter 54/100 - Loss theta2: -7.692\n",
      "Iter 55/100 - Loss theta2: -7.690\n",
      "Iter 56/100 - Loss theta2: -7.693\n",
      "Iter 57/100 - Loss theta2: -7.693\n",
      "Iter 58/100 - Loss theta2: -7.694\n",
      "Iter 59/100 - Loss theta2: -7.691\n",
      "Iter 60/100 - Loss theta2: -7.696\n",
      "Iter 61/100 - Loss theta2: -7.692\n",
      "Iter 62/100 - Loss theta2: -7.693\n",
      "Iter 63/100 - Loss theta2: -7.693\n",
      "Iter 64/100 - Loss theta2: -7.693\n",
      "Iter 65/100 - Loss theta2: -7.695\n",
      "Iter 66/100 - Loss theta2: -7.691\n",
      "Iter 67/100 - Loss theta2: -7.696\n",
      "Iter 68/100 - Loss theta2: -7.691\n",
      "Iter 69/100 - Loss theta2: -7.694\n",
      "Iter 70/100 - Loss theta2: -7.696\n",
      "Iter 71/100 - Loss theta2: -7.695\n",
      "Iter 72/100 - Loss theta2: -7.693\n",
      "Iter 73/100 - Loss theta2: -7.695\n",
      "Iter 74/100 - Loss theta2: -7.693\n",
      "Iter 75/100 - Loss theta2: -7.691\n",
      "Iter 76/100 - Loss theta2: -7.692\n",
      "Iter 77/100 - Loss theta2: -7.695\n",
      "Iter 78/100 - Loss theta2: -7.691\n",
      "Iter 79/100 - Loss theta2: -7.691\n",
      "Iter 80/100 - Loss theta2: -7.696\n",
      "Iter 81/100 - Loss theta2: -7.694\n",
      "Iter 82/100 - Loss theta2: -7.691\n",
      "Iter 83/100 - Loss theta2: -7.691\n",
      "Iter 84/100 - Loss theta2: -7.694\n",
      "Iter 85/100 - Loss theta2: -7.692\n",
      "Iter 86/100 - Loss theta2: -7.691\n",
      "Iter 87/100 - Loss theta2: -7.691\n",
      "Iter 88/100 - Loss theta2: -7.693\n",
      "Iter 89/100 - Loss theta2: -7.693\n",
      "Iter 90/100 - Loss theta2: -7.693\n",
      "Iter 91/100 - Loss theta2: -7.693\n",
      "Iter 92/100 - Loss theta2: -7.695\n",
      "Iter 93/100 - Loss theta2: -7.696\n",
      "Iter 94/100 - Loss theta2: -7.694\n",
      "Iter 95/100 - Loss theta2: -7.694\n",
      "Iter 96/100 - Loss theta2: -7.693\n",
      "Iter 97/100 - Loss theta2: -7.693\n",
      "Iter 98/100 - Loss theta2: -7.692\n",
      "Iter 99/100 - Loss theta2: -7.692\n",
      "Iter 100/100 - Loss theta2: -7.693\n",
      "tensor([[0.6915],\n",
      "        [0.6920]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7172],\n",
      "        [0.7169]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 406.928\n",
      "Iter 2/50 - Loss hyperparam: 370.173\n",
      "Iter 3/50 - Loss hyperparam: 331.955\n",
      "Iter 4/50 - Loss hyperparam: 293.465\n",
      "Iter 5/50 - Loss hyperparam: 255.291\n",
      "Iter 6/50 - Loss hyperparam: 217.368\n",
      "Iter 7/50 - Loss hyperparam: 179.368\n",
      "Iter 8/50 - Loss hyperparam: 141.001\n",
      "Iter 9/50 - Loss hyperparam: 102.096\n",
      "Iter 10/50 - Loss hyperparam: 62.545\n",
      "Iter 11/50 - Loss hyperparam: 22.262\n",
      "Iter 12/50 - Loss hyperparam: -18.814\n",
      "Iter 13/50 - Loss hyperparam: -60.733\n",
      "Iter 14/50 - Loss hyperparam: -103.633\n",
      "Iter 15/50 - Loss hyperparam: -147.909\n",
      "Iter 16/50 - Loss hyperparam: -194.397\n",
      "Iter 17/50 - Loss hyperparam: -244.353\n",
      "Iter 18/50 - Loss hyperparam: -299.191\n",
      "Iter 19/50 - Loss hyperparam: -360.244\n",
      "Iter 20/50 - Loss hyperparam: -428.725\n",
      "Iter 21/50 - Loss hyperparam: -505.616\n",
      "Iter 22/50 - Loss hyperparam: -591.047\n",
      "Iter 23/50 - Loss hyperparam: -682.749\n",
      "Iter 24/50 - Loss hyperparam: -777.632\n",
      "Iter 25/50 - Loss hyperparam: -876.937\n",
      "Iter 26/50 - Loss hyperparam: -954.915\n",
      "Iter 27/50 - Loss hyperparam: -965.461\n",
      "Iter 28/50 - Loss hyperparam: -946.460\n",
      "Iter 29/50 - Loss hyperparam: -947.093\n",
      "Iter 30/50 - Loss hyperparam: -972.682\n",
      "Iter 31/50 - Loss hyperparam: -1019.123\n",
      "Iter 32/50 - Loss hyperparam: -1082.427\n",
      "Iter 33/50 - Loss hyperparam: -1161.456\n",
      "Iter 34/50 - Loss hyperparam: -1258.366\n",
      "Iter 35/50 - Loss hyperparam: -1341.428\n",
      "Iter 36/50 - Loss hyperparam: -1348.089\n",
      "Iter 37/50 - Loss hyperparam: -1344.382\n",
      "Iter 38/50 - Loss hyperparam: -1370.135\n",
      "Iter 39/50 - Loss hyperparam: -1423.913\n",
      "Iter 40/50 - Loss hyperparam: -1509.860\n",
      "Iter 41/50 - Loss hyperparam: -1603.259\n",
      "Iter 42/50 - Loss hyperparam: -1617.958\n",
      "Iter 43/50 - Loss hyperparam: -1621.625\n",
      "Iter 44/50 - Loss hyperparam: -1658.023\n",
      "Iter 45/50 - Loss hyperparam: -1744.257\n",
      "Iter 46/50 - Loss hyperparam: -1820.349\n",
      "Iter 47/50 - Loss hyperparam: -1826.992\n",
      "Iter 48/50 - Loss hyperparam: -1844.364\n",
      "Iter 49/50 - Loss hyperparam: -1928.425\n",
      "Iter 50/50 - Loss hyperparam: -1990.658\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.688\n",
      "Iter 2/100 - Loss theta2: -7.655\n",
      "Iter 3/100 - Loss theta2: -7.685\n",
      "Iter 4/100 - Loss theta2: -7.683\n",
      "Iter 5/100 - Loss theta2: -7.671\n",
      "Iter 6/100 - Loss theta2: -7.680\n",
      "Iter 7/100 - Loss theta2: -7.687\n",
      "Iter 8/100 - Loss theta2: -7.687\n",
      "Iter 9/100 - Loss theta2: -7.680\n",
      "Iter 10/100 - Loss theta2: -7.678\n",
      "Iter 11/100 - Loss theta2: -7.685\n",
      "Iter 12/100 - Loss theta2: -7.688\n",
      "Iter 13/100 - Loss theta2: -7.686\n",
      "Iter 14/100 - Loss theta2: -7.684\n",
      "Iter 15/100 - Loss theta2: -7.686\n",
      "Iter 16/100 - Loss theta2: -7.689\n",
      "Iter 17/100 - Loss theta2: -7.688\n",
      "Iter 18/100 - Loss theta2: -7.689\n",
      "Iter 19/100 - Loss theta2: -7.688\n",
      "Iter 20/100 - Loss theta2: -7.685\n",
      "Iter 21/100 - Loss theta2: -7.690\n",
      "Iter 22/100 - Loss theta2: -7.689\n",
      "Iter 23/100 - Loss theta2: -7.690\n",
      "Iter 24/100 - Loss theta2: -7.690\n",
      "Iter 25/100 - Loss theta2: -7.688\n",
      "Iter 26/100 - Loss theta2: -7.690\n",
      "Iter 27/100 - Loss theta2: -7.690\n",
      "Iter 28/100 - Loss theta2: -7.687\n",
      "Iter 29/100 - Loss theta2: -7.691\n",
      "Iter 30/100 - Loss theta2: -7.689\n",
      "Iter 31/100 - Loss theta2: -7.689\n",
      "Iter 32/100 - Loss theta2: -7.692\n",
      "Iter 33/100 - Loss theta2: -7.690\n",
      "Iter 34/100 - Loss theta2: -7.689\n",
      "Iter 35/100 - Loss theta2: -7.690\n",
      "Iter 36/100 - Loss theta2: -7.690\n",
      "Iter 37/100 - Loss theta2: -7.693\n",
      "Iter 38/100 - Loss theta2: -7.690\n",
      "Iter 39/100 - Loss theta2: -7.690\n",
      "Iter 40/100 - Loss theta2: -7.688\n",
      "Iter 41/100 - Loss theta2: -7.689\n",
      "Iter 42/100 - Loss theta2: -7.691\n",
      "Iter 43/100 - Loss theta2: -7.693\n",
      "Iter 44/100 - Loss theta2: -7.690\n",
      "Iter 45/100 - Loss theta2: -7.691\n",
      "Iter 46/100 - Loss theta2: -7.692\n",
      "Iter 47/100 - Loss theta2: -7.694\n",
      "Iter 48/100 - Loss theta2: -7.690\n",
      "Iter 49/100 - Loss theta2: -7.689\n",
      "Iter 50/100 - Loss theta2: -7.693\n",
      "Iter 51/100 - Loss theta2: -7.693\n",
      "Iter 52/100 - Loss theta2: -7.691\n",
      "Iter 53/100 - Loss theta2: -7.693\n",
      "Iter 54/100 - Loss theta2: -7.690\n",
      "Iter 55/100 - Loss theta2: -7.691\n",
      "Iter 56/100 - Loss theta2: -7.691\n",
      "Iter 57/100 - Loss theta2: -7.689\n",
      "Iter 58/100 - Loss theta2: -7.692\n",
      "Iter 59/100 - Loss theta2: -7.691\n",
      "Iter 60/100 - Loss theta2: -7.691\n",
      "Iter 61/100 - Loss theta2: -7.694\n",
      "Iter 62/100 - Loss theta2: -7.693\n",
      "Iter 63/100 - Loss theta2: -7.688\n",
      "Iter 64/100 - Loss theta2: -7.689\n",
      "Iter 65/100 - Loss theta2: -7.690\n",
      "Iter 66/100 - Loss theta2: -7.690\n",
      "Iter 67/100 - Loss theta2: -7.690\n",
      "Iter 68/100 - Loss theta2: -7.691\n",
      "Iter 69/100 - Loss theta2: -7.693\n",
      "Iter 70/100 - Loss theta2: -7.691\n",
      "Iter 71/100 - Loss theta2: -7.693\n",
      "Iter 72/100 - Loss theta2: -7.693\n",
      "Iter 73/100 - Loss theta2: -7.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 74/100 - Loss theta2: -7.691\n",
      "Iter 75/100 - Loss theta2: -7.692\n",
      "Iter 76/100 - Loss theta2: -7.692\n",
      "Iter 77/100 - Loss theta2: -7.692\n",
      "Iter 78/100 - Loss theta2: -7.690\n",
      "Iter 79/100 - Loss theta2: -7.692\n",
      "Iter 80/100 - Loss theta2: -7.690\n",
      "Iter 81/100 - Loss theta2: -7.691\n",
      "Iter 82/100 - Loss theta2: -7.692\n",
      "Iter 83/100 - Loss theta2: -7.691\n",
      "Iter 84/100 - Loss theta2: -7.690\n",
      "Iter 85/100 - Loss theta2: -7.693\n",
      "Iter 86/100 - Loss theta2: -7.691\n",
      "Iter 87/100 - Loss theta2: -7.693\n",
      "Iter 88/100 - Loss theta2: -7.689\n",
      "Iter 89/100 - Loss theta2: -7.691\n",
      "Iter 90/100 - Loss theta2: -7.691\n",
      "Iter 91/100 - Loss theta2: -7.695\n",
      "Iter 92/100 - Loss theta2: -7.690\n",
      "Iter 93/100 - Loss theta2: -7.693\n",
      "Iter 94/100 - Loss theta2: -7.691\n",
      "Iter 95/100 - Loss theta2: -7.691\n",
      "Iter 96/100 - Loss theta2: -7.691\n",
      "Iter 97/100 - Loss theta2: -7.691\n",
      "Iter 98/100 - Loss theta2: -7.692\n",
      "Iter 99/100 - Loss theta2: -7.691\n",
      "Iter 100/100 - Loss theta2: -7.690\n",
      "tensor([[0.6919],\n",
      "        [0.6899]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7156],\n",
      "        [0.7161]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 84.592\n",
      "Iter 2/50 - Loss hyperparam: 22.671\n",
      "Iter 3/50 - Loss hyperparam: -43.145\n",
      "Iter 4/50 - Loss hyperparam: -110.426\n",
      "Iter 5/50 - Loss hyperparam: -174.642\n",
      "Iter 6/50 - Loss hyperparam: -231.077\n",
      "Iter 7/50 - Loss hyperparam: -277.004\n",
      "Iter 8/50 - Loss hyperparam: -314.163\n",
      "Iter 9/50 - Loss hyperparam: -346.190\n",
      "Iter 10/50 - Loss hyperparam: -375.133\n",
      "Iter 11/50 - Loss hyperparam: -403.128\n",
      "Iter 12/50 - Loss hyperparam: -432.599\n",
      "Iter 13/50 - Loss hyperparam: -464.803\n",
      "Iter 14/50 - Loss hyperparam: -499.799\n",
      "Iter 15/50 - Loss hyperparam: -537.724\n",
      "Iter 16/50 - Loss hyperparam: -579.208\n",
      "Iter 17/50 - Loss hyperparam: -623.669\n",
      "Iter 18/50 - Loss hyperparam: -667.714\n",
      "Iter 19/50 - Loss hyperparam: -706.348\n",
      "Iter 20/50 - Loss hyperparam: -737.712\n",
      "Iter 21/50 - Loss hyperparam: -766.843\n",
      "Iter 22/50 - Loss hyperparam: -801.533\n",
      "Iter 23/50 - Loss hyperparam: -845.143\n",
      "Iter 24/50 - Loss hyperparam: -894.206\n",
      "Iter 25/50 - Loss hyperparam: -939.941\n",
      "Iter 26/50 - Loss hyperparam: -976.305\n",
      "Iter 27/50 - Loss hyperparam: -1009.387\n",
      "Iter 28/50 - Loss hyperparam: -1048.052\n",
      "Iter 29/50 - Loss hyperparam: -1092.714\n",
      "Iter 30/50 - Loss hyperparam: -1137.859\n",
      "Iter 31/50 - Loss hyperparam: -1180.615\n",
      "Iter 32/50 - Loss hyperparam: -1218.645\n",
      "Iter 33/50 - Loss hyperparam: -1254.987\n",
      "Iter 34/50 - Loss hyperparam: -1299.253\n",
      "Iter 35/50 - Loss hyperparam: -1347.671\n",
      "Iter 36/50 - Loss hyperparam: -1382.963\n",
      "Iter 37/50 - Loss hyperparam: -1419.887\n",
      "Iter 38/50 - Loss hyperparam: -1468.131\n",
      "Iter 39/50 - Loss hyperparam: -1507.745\n",
      "Iter 40/50 - Loss hyperparam: -1544.972\n",
      "Iter 41/50 - Loss hyperparam: -1587.672\n",
      "Iter 42/50 - Loss hyperparam: -1632.575\n",
      "Iter 43/50 - Loss hyperparam: -1666.379\n",
      "Iter 44/50 - Loss hyperparam: -1714.922\n",
      "Iter 45/50 - Loss hyperparam: -1750.753\n",
      "Iter 46/50 - Loss hyperparam: -1793.980\n",
      "Iter 47/50 - Loss hyperparam: -1836.045\n",
      "Iter 48/50 - Loss hyperparam: -1871.763\n",
      "Iter 49/50 - Loss hyperparam: -1919.295\n",
      "Iter 50/50 - Loss hyperparam: -1954.749\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.533\n",
      "Iter 2/100 - Loss theta2: -7.484\n",
      "Iter 3/100 - Loss theta2: -7.522\n",
      "Iter 4/100 - Loss theta2: -7.518\n",
      "Iter 5/100 - Loss theta2: -7.504\n",
      "Iter 6/100 - Loss theta2: -7.508\n",
      "Iter 7/100 - Loss theta2: -7.522\n",
      "Iter 8/100 - Loss theta2: -7.528\n",
      "Iter 9/100 - Loss theta2: -7.520\n",
      "Iter 10/100 - Loss theta2: -7.515\n",
      "Iter 11/100 - Loss theta2: -7.517\n",
      "Iter 12/100 - Loss theta2: -7.521\n",
      "Iter 13/100 - Loss theta2: -7.526\n",
      "Iter 14/100 - Loss theta2: -7.526\n",
      "Iter 15/100 - Loss theta2: -7.526\n",
      "Iter 16/100 - Loss theta2: -7.522\n",
      "Iter 17/100 - Loss theta2: -7.521\n",
      "Iter 18/100 - Loss theta2: -7.526\n",
      "Iter 19/100 - Loss theta2: -7.529\n",
      "Iter 20/100 - Loss theta2: -7.527\n",
      "Iter 21/100 - Loss theta2: -7.526\n",
      "Iter 22/100 - Loss theta2: -7.524\n",
      "Iter 23/100 - Loss theta2: -7.525\n",
      "Iter 24/100 - Loss theta2: -7.527\n",
      "Iter 25/100 - Loss theta2: -7.530\n",
      "Iter 26/100 - Loss theta2: -7.526\n",
      "Iter 27/100 - Loss theta2: -7.526\n",
      "Iter 28/100 - Loss theta2: -7.527\n",
      "Iter 29/100 - Loss theta2: -7.526\n",
      "Iter 30/100 - Loss theta2: -7.531\n",
      "Iter 31/100 - Loss theta2: -7.530\n",
      "Iter 32/100 - Loss theta2: -7.528\n",
      "Iter 33/100 - Loss theta2: -7.526\n",
      "Iter 34/100 - Loss theta2: -7.530\n",
      "Iter 35/100 - Loss theta2: -7.528\n",
      "Iter 36/100 - Loss theta2: -7.529\n",
      "Iter 37/100 - Loss theta2: -7.531\n",
      "Iter 38/100 - Loss theta2: -7.530\n",
      "Iter 39/100 - Loss theta2: -7.528\n",
      "Iter 40/100 - Loss theta2: -7.532\n",
      "Iter 41/100 - Loss theta2: -7.534\n",
      "Iter 42/100 - Loss theta2: -7.537\n",
      "Iter 43/100 - Loss theta2: -7.537\n",
      "Iter 44/100 - Loss theta2: -7.533\n",
      "Iter 45/100 - Loss theta2: -7.533\n",
      "Iter 46/100 - Loss theta2: -7.534\n",
      "Iter 47/100 - Loss theta2: -7.532\n",
      "Iter 48/100 - Loss theta2: -7.532\n",
      "Iter 49/100 - Loss theta2: -7.531\n",
      "Iter 50/100 - Loss theta2: -7.533\n",
      "Iter 51/100 - Loss theta2: -7.532\n",
      "Iter 52/100 - Loss theta2: -7.533\n",
      "Iter 53/100 - Loss theta2: -7.534\n",
      "Iter 54/100 - Loss theta2: -7.534\n",
      "Iter 55/100 - Loss theta2: -7.534\n",
      "Iter 56/100 - Loss theta2: -7.534\n",
      "Iter 57/100 - Loss theta2: -7.535\n",
      "Iter 58/100 - Loss theta2: -7.533\n",
      "Iter 59/100 - Loss theta2: -7.537\n",
      "Iter 60/100 - Loss theta2: -7.537\n",
      "Iter 61/100 - Loss theta2: -7.536\n",
      "Iter 62/100 - Loss theta2: -7.535\n",
      "Iter 63/100 - Loss theta2: -7.533\n",
      "Iter 64/100 - Loss theta2: -7.535\n",
      "Iter 65/100 - Loss theta2: -7.534\n",
      "Iter 66/100 - Loss theta2: -7.534\n",
      "Iter 67/100 - Loss theta2: -7.536\n",
      "Iter 68/100 - Loss theta2: -7.538\n",
      "Iter 69/100 - Loss theta2: -7.538\n",
      "Iter 70/100 - Loss theta2: -7.535\n",
      "Iter 71/100 - Loss theta2: -7.536\n",
      "Iter 72/100 - Loss theta2: -7.536\n",
      "Iter 73/100 - Loss theta2: -7.535\n",
      "Iter 74/100 - Loss theta2: -7.535\n",
      "Iter 75/100 - Loss theta2: -7.535\n",
      "Iter 76/100 - Loss theta2: -7.535\n",
      "Iter 77/100 - Loss theta2: -7.536\n",
      "Iter 78/100 - Loss theta2: -7.537\n",
      "Iter 79/100 - Loss theta2: -7.536\n",
      "Iter 80/100 - Loss theta2: -7.539\n",
      "Iter 81/100 - Loss theta2: -7.539\n",
      "Iter 82/100 - Loss theta2: -7.537\n",
      "Iter 83/100 - Loss theta2: -7.537\n",
      "Iter 84/100 - Loss theta2: -7.537\n",
      "Iter 85/100 - Loss theta2: -7.539\n",
      "Iter 86/100 - Loss theta2: -7.536\n",
      "Iter 87/100 - Loss theta2: -7.536\n",
      "Iter 88/100 - Loss theta2: -7.539\n",
      "Iter 89/100 - Loss theta2: -7.538\n",
      "Iter 90/100 - Loss theta2: -7.538\n",
      "Iter 91/100 - Loss theta2: -7.537\n",
      "Iter 92/100 - Loss theta2: -7.539\n",
      "Iter 93/100 - Loss theta2: -7.537\n",
      "Iter 94/100 - Loss theta2: -7.538\n",
      "Iter 95/100 - Loss theta2: -7.539\n",
      "Iter 96/100 - Loss theta2: -7.537\n",
      "Iter 97/100 - Loss theta2: -7.538\n",
      "Iter 98/100 - Loss theta2: -7.538\n",
      "Iter 99/100 - Loss theta2: -7.540\n",
      "Iter 100/100 - Loss theta2: -7.539\n",
      "tensor([[0.6898],\n",
      "        [0.6902]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7171],\n",
      "        [0.7168]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1243]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 159.810\n",
      "Iter 2/50 - Loss hyperparam: 105.249\n",
      "Iter 3/50 - Loss hyperparam: 47.900\n",
      "Iter 4/50 - Loss hyperparam: -11.992\n",
      "Iter 5/50 - Loss hyperparam: -73.941\n",
      "Iter 6/50 - Loss hyperparam: -135.800\n",
      "Iter 7/50 - Loss hyperparam: -194.919\n",
      "Iter 8/50 - Loss hyperparam: -250.565\n",
      "Iter 9/50 - Loss hyperparam: -305.263\n",
      "Iter 10/50 - Loss hyperparam: -361.188\n",
      "Iter 11/50 - Loss hyperparam: -416.425\n",
      "Iter 12/50 - Loss hyperparam: -465.415\n",
      "Iter 13/50 - Loss hyperparam: -501.730\n",
      "Iter 14/50 - Loss hyperparam: -524.534\n",
      "Iter 15/50 - Loss hyperparam: -542.261\n",
      "Iter 16/50 - Loss hyperparam: -564.770\n",
      "Iter 17/50 - Loss hyperparam: -596.420\n",
      "Iter 18/50 - Loss hyperparam: -637.281\n",
      "Iter 19/50 - Loss hyperparam: -685.152\n",
      "Iter 20/50 - Loss hyperparam: -736.524\n",
      "Iter 21/50 - Loss hyperparam: -788.388\n",
      "Iter 22/50 - Loss hyperparam: -839.020\n",
      "Iter 23/50 - Loss hyperparam: -883.767\n",
      "Iter 24/50 - Loss hyperparam: -917.232\n",
      "Iter 25/50 - Loss hyperparam: -943.761\n",
      "Iter 26/50 - Loss hyperparam: -976.125\n",
      "Iter 27/50 - Loss hyperparam: -1021.824\n",
      "Iter 28/50 - Loss hyperparam: -1077.229\n",
      "Iter 29/50 - Loss hyperparam: -1130.765\n",
      "Iter 30/50 - Loss hyperparam: -1174.934\n",
      "Iter 31/50 - Loss hyperparam: -1209.964\n",
      "Iter 32/50 - Loss hyperparam: -1241.564\n",
      "Iter 33/50 - Loss hyperparam: -1283.150\n",
      "Iter 34/50 - Loss hyperparam: -1339.417\n",
      "Iter 35/50 - Loss hyperparam: -1389.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36/50 - Loss hyperparam: -1423.578\n",
      "Iter 37/50 - Loss hyperparam: -1457.722\n",
      "Iter 38/50 - Loss hyperparam: -1502.586\n",
      "Iter 39/50 - Loss hyperparam: -1558.665\n",
      "Iter 40/50 - Loss hyperparam: -1595.263\n",
      "Iter 41/50 - Loss hyperparam: -1629.782\n",
      "Iter 42/50 - Loss hyperparam: -1678.489\n",
      "Iter 43/50 - Loss hyperparam: -1730.493\n",
      "Iter 44/50 - Loss hyperparam: -1759.445\n",
      "Iter 45/50 - Loss hyperparam: -1806.979\n",
      "Iter 46/50 - Loss hyperparam: -1856.630\n",
      "Iter 47/50 - Loss hyperparam: -1885.762\n",
      "Iter 48/50 - Loss hyperparam: -1937.504\n",
      "Iter 49/50 - Loss hyperparam: -1979.684\n",
      "Iter 50/50 - Loss hyperparam: -2012.092\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/100 - Loss theta2: -7.622\n",
      "Iter 2/100 - Loss theta2: -7.593\n",
      "Iter 3/100 - Loss theta2: -7.625\n",
      "Iter 4/100 - Loss theta2: -7.621\n",
      "Iter 5/100 - Loss theta2: -7.611\n",
      "Iter 6/100 - Loss theta2: -7.618\n",
      "Iter 7/100 - Loss theta2: -7.624\n",
      "Iter 8/100 - Loss theta2: -7.625\n",
      "Iter 9/100 - Loss theta2: -7.618\n",
      "Iter 10/100 - Loss theta2: -7.618\n",
      "Iter 11/100 - Loss theta2: -7.622\n",
      "Iter 12/100 - Loss theta2: -7.627\n",
      "Iter 13/100 - Loss theta2: -7.625\n",
      "Iter 14/100 - Loss theta2: -7.623\n",
      "Iter 15/100 - Loss theta2: -7.619\n",
      "Iter 16/100 - Loss theta2: -7.623\n",
      "Iter 17/100 - Loss theta2: -7.626\n",
      "Iter 18/100 - Loss theta2: -7.627\n",
      "Iter 19/100 - Loss theta2: -7.624\n",
      "Iter 20/100 - Loss theta2: -7.623\n",
      "Iter 21/100 - Loss theta2: -7.622\n",
      "Iter 22/100 - Loss theta2: -7.624\n",
      "Iter 23/100 - Loss theta2: -7.626\n",
      "Iter 24/100 - Loss theta2: -7.626\n",
      "Iter 25/100 - Loss theta2: -7.626\n",
      "Iter 26/100 - Loss theta2: -7.624\n",
      "Iter 27/100 - Loss theta2: -7.624\n",
      "Iter 28/100 - Loss theta2: -7.628\n",
      "Iter 29/100 - Loss theta2: -7.626\n",
      "Iter 30/100 - Loss theta2: -7.625\n",
      "Iter 31/100 - Loss theta2: -7.625\n",
      "Iter 32/100 - Loss theta2: -7.626\n",
      "Iter 33/100 - Loss theta2: -7.626\n",
      "Iter 34/100 - Loss theta2: -7.624\n",
      "Iter 35/100 - Loss theta2: -7.627\n",
      "Iter 36/100 - Loss theta2: -7.626\n",
      "Iter 37/100 - Loss theta2: -7.627\n",
      "Iter 38/100 - Loss theta2: -7.627\n",
      "Iter 39/100 - Loss theta2: -7.628\n",
      "Iter 40/100 - Loss theta2: -7.628\n",
      "Iter 41/100 - Loss theta2: -7.626\n",
      "Iter 42/100 - Loss theta2: -7.626\n",
      "Iter 43/100 - Loss theta2: -7.626\n",
      "Iter 44/100 - Loss theta2: -7.628\n",
      "Iter 45/100 - Loss theta2: -7.629\n",
      "Iter 46/100 - Loss theta2: -7.625\n",
      "Iter 47/100 - Loss theta2: -7.627\n",
      "Iter 48/100 - Loss theta2: -7.629\n",
      "Iter 49/100 - Loss theta2: -7.629\n",
      "Iter 50/100 - Loss theta2: -7.626\n",
      "Iter 51/100 - Loss theta2: -7.627\n",
      "Iter 52/100 - Loss theta2: -7.629\n",
      "Iter 53/100 - Loss theta2: -7.626\n",
      "Iter 54/100 - Loss theta2: -7.628\n",
      "Iter 55/100 - Loss theta2: -7.627\n",
      "Iter 56/100 - Loss theta2: -7.626\n",
      "Iter 57/100 - Loss theta2: -7.630\n",
      "Iter 58/100 - Loss theta2: -7.629\n",
      "Iter 59/100 - Loss theta2: -7.626\n",
      "Iter 60/100 - Loss theta2: -7.630\n",
      "Iter 61/100 - Loss theta2: -7.627\n",
      "Iter 62/100 - Loss theta2: -7.629\n",
      "Iter 63/100 - Loss theta2: -7.630\n",
      "Iter 64/100 - Loss theta2: -7.628\n",
      "Iter 65/100 - Loss theta2: -7.629\n",
      "Iter 66/100 - Loss theta2: -7.629\n",
      "Iter 67/100 - Loss theta2: -7.628\n",
      "Iter 68/100 - Loss theta2: -7.627\n",
      "Iter 69/100 - Loss theta2: -7.626\n",
      "Iter 70/100 - Loss theta2: -7.629\n",
      "Iter 71/100 - Loss theta2: -7.631\n",
      "Iter 72/100 - Loss theta2: -7.629\n",
      "Iter 73/100 - Loss theta2: -7.629\n",
      "Iter 74/100 - Loss theta2: -7.629\n",
      "Iter 75/100 - Loss theta2: -7.629\n",
      "Iter 76/100 - Loss theta2: -7.630\n",
      "Iter 77/100 - Loss theta2: -7.628\n",
      "Iter 78/100 - Loss theta2: -7.629\n",
      "Iter 79/100 - Loss theta2: -7.630\n",
      "Iter 80/100 - Loss theta2: -7.630\n",
      "Iter 81/100 - Loss theta2: -7.628\n",
      "Iter 82/100 - Loss theta2: -7.628\n",
      "Iter 83/100 - Loss theta2: -7.628\n",
      "Iter 84/100 - Loss theta2: -7.630\n",
      "Iter 85/100 - Loss theta2: -7.628\n",
      "Iter 86/100 - Loss theta2: -7.630\n",
      "Iter 87/100 - Loss theta2: -7.628\n",
      "Iter 88/100 - Loss theta2: -7.629\n",
      "Iter 89/100 - Loss theta2: -7.627\n",
      "Iter 90/100 - Loss theta2: -7.630\n",
      "Iter 91/100 - Loss theta2: -7.631\n",
      "Iter 92/100 - Loss theta2: -7.631\n",
      "Iter 93/100 - Loss theta2: -7.629\n",
      "Iter 94/100 - Loss theta2: -7.627\n",
      "Iter 95/100 - Loss theta2: -7.631\n",
      "Iter 96/100 - Loss theta2: -7.630\n",
      "Iter 97/100 - Loss theta2: -7.629\n",
      "Iter 98/100 - Loss theta2: -7.629\n",
      "Iter 99/100 - Loss theta2: -7.630\n",
      "Iter 100/100 - Loss theta2: -7.628\n",
      "tensor([[0.6909],\n",
      "        [0.6913]], grad_fn=<CopySlices>)\n",
      "tensor([[0.7174],\n",
      "        [0.7170]], grad_fn=<CopySlices>)\n",
      "tensor([[0.6971],\n",
      "        [0.6971]])\n",
      "tensor([[0.7171],\n",
      "        [0.7171]])\n",
      "Parameter containing:\n",
      "tensor([[0.1244]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 74.918\n",
      "Iter 2/50 - Loss hyperparam: 14.268\n",
      "Iter 3/50 - Loss hyperparam: -54.802\n",
      "Iter 4/50 - Loss hyperparam: -118.544\n",
      "Iter 5/50 - Loss hyperparam: -178.441\n",
      "Iter 6/50 - Loss hyperparam: -225.522\n",
      "Iter 7/50 - Loss hyperparam: -280.262\n",
      "Iter 8/50 - Loss hyperparam: -327.972\n",
      "Iter 9/50 - Loss hyperparam: -362.736\n",
      "Iter 10/50 - Loss hyperparam: -409.188\n",
      "Iter 11/50 - Loss hyperparam: -422.177\n",
      "Iter 12/50 - Loss hyperparam: -446.745\n",
      "Iter 13/50 - Loss hyperparam: -480.497\n",
      "Iter 14/50 - Loss hyperparam: -517.910\n",
      "Iter 15/50 - Loss hyperparam: -560.267\n",
      "Iter 16/50 - Loss hyperparam: -610.345\n",
      "Iter 17/50 - Loss hyperparam: -653.830\n",
      "Iter 18/50 - Loss hyperparam: -703.564\n",
      "Iter 19/50 - Loss hyperparam: -740.694\n",
      "Iter 20/50 - Loss hyperparam: -773.865\n",
      "Iter 21/50 - Loss hyperparam: -809.056\n",
      "Iter 22/50 - Loss hyperparam: -835.005\n",
      "Iter 23/50 - Loss hyperparam: -878.531\n",
      "Iter 24/50 - Loss hyperparam: -922.618\n",
      "Iter 25/50 - Loss hyperparam: -987.400\n",
      "Iter 26/50 - Loss hyperparam: -1019.752\n",
      "Iter 27/50 - Loss hyperparam: -1051.233\n",
      "Iter 28/50 - Loss hyperparam: -1084.141\n",
      "Iter 29/50 - Loss hyperparam: -1131.477\n",
      "Iter 30/50 - Loss hyperparam: -1182.758\n",
      "Iter 31/50 - Loss hyperparam: -1233.617\n",
      "Iter 32/50 - Loss hyperparam: -1270.169\n",
      "Iter 33/50 - Loss hyperparam: -1309.704\n",
      "Iter 34/50 - Loss hyperparam: -1357.272\n",
      "Iter 35/50 - Loss hyperparam: -1406.897\n",
      "Iter 36/50 - Loss hyperparam: -1452.775\n",
      "Iter 37/50 - Loss hyperparam: -1476.533\n",
      "Iter 38/50 - Loss hyperparam: -1523.318\n",
      "Iter 39/50 - Loss hyperparam: -1565.659\n",
      "Iter 40/50 - Loss hyperparam: -1620.835\n",
      "Iter 41/50 - Loss hyperparam: -1646.945\n",
      "Iter 42/50 - Loss hyperparam: -1697.880\n",
      "Iter 43/50 - Loss hyperparam: -1733.412\n",
      "Iter 44/50 - Loss hyperparam: -1777.814\n",
      "Iter 45/50 - Loss hyperparam: -1834.637\n",
      "Iter 46/50 - Loss hyperparam: -1869.784\n",
      "Iter 47/50 - Loss hyperparam: -1900.365\n",
      "Iter 48/50 - Loss hyperparam: -1933.950\n",
      "Iter 49/50 - Loss hyperparam: -1976.647\n",
      "Iter 50/50 - Loss hyperparam: -2021.489\n",
      "END HYPERPARAMETERS optimization\n",
      "tensor([[0.5056],\n",
      "        [0.8497],\n",
      "        [0.7184],\n",
      "        [0.2491],\n",
      "        [0.3126],\n",
      "        [0.1857],\n",
      "        [0.8741],\n",
      "        [0.2859]], grad_fn=<CopySlices>)\n",
      "tensor([[3.1794, 0.5620, 2.0765, 0.3676, 2.6988, 0.4778, 2.5080, 0.4440, 2.7779,\n",
      "         0.4918, 2.2003, 0.3895, 1.9510, 0.3454, 2.6705, 0.4728],\n",
      "        [0.5620, 2.8424, 0.3676, 1.8560, 0.4778, 2.4123, 0.4440, 2.2418, 0.4918,\n",
      "         2.4830, 0.3895, 1.9667, 0.3454, 1.7439, 0.4728, 2.3870],\n",
      "        [2.0765, 0.3676, 3.1794, 0.5620, 2.9841, 0.5283, 0.8717, 0.1543, 1.1290,\n",
      "         0.1999, 0.6540, 0.1158, 3.1676, 0.5608, 1.0162, 0.1799],\n",
      "        [0.3676, 1.8560, 0.5620, 2.8424, 0.5283, 2.6673, 0.1543, 0.7791, 0.1999,\n",
      "         1.0092, 0.1158, 0.5846, 0.5608, 2.8313, 0.1799, 0.9084],\n",
      "        [2.6988, 0.4778, 2.9841, 0.5283, 3.1794, 0.5620, 1.4421, 0.2553, 1.7596,\n",
      "         0.3115, 1.1486, 0.2033, 2.9101, 0.5152, 1.6241, 0.2875],\n",
      "        [0.4778, 2.4123, 0.5283, 2.6673, 0.5620, 2.8424, 0.2553, 1.2890, 0.3115,\n",
      "         1.5728, 0.2033, 1.0266, 0.5152, 2.6011, 0.2875, 1.4517],\n",
      "        [2.5080, 0.4440, 0.8717, 0.1543, 1.4421, 0.2553, 3.1794, 0.5620, 3.1289,\n",
      "         0.5539, 3.1289, 0.5540, 0.7831, 0.1386, 3.1590, 0.5593],\n",
      "        [0.4440, 2.2418, 0.1543, 0.7791, 0.2553, 1.2890, 0.5620, 2.8424, 0.5539,\n",
      "         2.7967, 0.5540, 2.7968, 0.1386, 0.6999, 0.5593, 2.8236],\n",
      "        [2.7779, 0.4918, 1.1290, 0.1999, 1.7596, 0.3115, 3.1289, 0.5539, 3.1794,\n",
      "         0.5620, 2.9964, 0.5305, 1.0256, 0.1816, 3.1662, 0.5606],\n",
      "        [0.4918, 2.4830, 0.1999, 1.0092, 0.3115, 1.5728, 0.5539, 2.7967, 0.5620,\n",
      "         2.8424, 0.5305, 2.6784, 0.1816, 0.9167, 0.5606, 2.8301],\n",
      "        [2.2003, 0.3895, 0.6540, 0.1158, 1.1486, 0.2033, 3.1289, 0.5540, 2.9964,\n",
      "         0.5305, 3.1794, 0.5620, 0.5810, 0.1029, 3.0622, 0.5421],\n",
      "        [0.3895, 1.9667, 0.1158, 0.5846, 0.2033, 1.0266, 0.5540, 2.7968, 0.5305,\n",
      "         2.6784, 0.5620, 2.8424, 0.1029, 0.5194, 0.5421, 2.7371],\n",
      "        [1.9510, 0.3454, 3.1676, 0.5608, 2.9101, 0.5152, 0.7831, 0.1386, 1.0256,\n",
      "         0.1816, 0.5810, 0.1029, 3.1794, 0.5620, 0.9189, 0.1627],\n",
      "        [0.3454, 1.7439, 0.5608, 2.8313, 0.5152, 2.6011, 0.1386, 0.6999, 0.1816,\n",
      "         0.9167, 0.1029, 0.5194, 0.5620, 2.8424, 0.1627, 0.8213],\n",
      "        [2.6705, 0.4728, 1.0162, 0.1799, 1.6241, 0.2875, 3.1590, 0.5593, 3.1662,\n",
      "         0.5606, 3.0622, 0.5421, 0.9189, 0.1627, 3.1794, 0.5620],\n",
      "        [0.4728, 2.3870, 0.1799, 0.9084, 0.2875, 1.4517, 0.5593, 2.8236, 0.5606,\n",
      "         2.8301, 0.5421, 2.7371, 0.1627, 0.8213, 0.5620, 2.8424]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 5.3432e-03, -8.6600e-04,  7.0810e-05,  7.1672e-04, -1.3309e-03,\n",
      "          2.7475e-04, -2.6202e-04,  8.9523e-04, -1.1086e-04,  1.9592e-04,\n",
      "          3.4523e-04,  7.0468e-04,  4.2269e-03,  8.1560e-04, -1.6403e-04,\n",
      "          2.1142e-04],\n",
      "        [-6.6191e-04,  5.2800e-03,  2.0271e-03, -5.1260e-05,  5.1126e-04,\n",
      "         -1.6975e-04,  3.2681e-04,  1.4901e-04,  2.7508e-05,  1.4305e-06,\n",
      "          1.4467e-03, -2.7037e-04, -3.7161e-03,  1.0419e-04,  2.3705e-04,\n",
      "         -1.4687e-04],\n",
      "        [ 5.0306e-04,  7.1168e-04,  1.1509e-02, -9.3579e-04, -6.5823e-03,\n",
      "         -2.6914e-03, -1.9608e-03,  1.6102e-03, -8.8227e-04, -3.4429e-04,\n",
      "          4.3366e-03, -2.5810e-03,  1.0530e-02,  1.1286e-03, -8.2266e-04,\n",
      "         -2.0117e-03],\n",
      "        [ 1.1362e-03, -1.5008e-04,  5.2264e-03,  5.6834e-03, -5.0319e-03,\n",
      "         -1.2605e-03, -2.0715e-03,  1.5907e-03, -1.2747e-03,  2.2590e-04,\n",
      "          3.6608e-03, -1.9637e-03,  2.1584e-03,  1.6172e-03, -1.2028e-03,\n",
      "         -6.1554e-04],\n",
      "        [ 4.2915e-04,  4.3169e-04,  4.0615e-03, -9.2787e-04,  1.3876e-04,\n",
      "         -2.0516e-03, -1.3644e-03,  1.3319e-03, -1.0141e-03, -3.0595e-04,\n",
      "          4.0030e-03, -1.8194e-03,  8.6572e-03,  8.6099e-04, -7.8595e-04,\n",
      "         -1.5737e-03],\n",
      "        [ 7.7727e-04, -7.3671e-05,  4.5779e-03, -1.2946e-04, -3.6382e-03,\n",
      "          4.4448e-03, -1.5675e-03,  1.4066e-03, -1.0460e-03, -1.1802e-04,\n",
      "          3.0685e-03, -9.7966e-04, -5.7554e-04,  5.0688e-04, -9.4157e-04,\n",
      "         -6.0880e-04],\n",
      "        [-5.0378e-04,  7.5114e-04, -4.6529e-03, -3.4772e-04,  3.9419e-03,\n",
      "          9.9689e-04,  6.6192e-03, -2.1633e-03,  7.7391e-04, -6.6698e-05,\n",
      "         -3.3283e-03,  4.7779e-04, -1.8567e-03, -1.7089e-03,  7.4697e-04,\n",
      "          7.6121e-04],\n",
      "        [ 2.6998e-04, -1.0419e-04, -3.8143e-03, -9.2566e-05,  3.6712e-03,\n",
      "          9.3675e-04,  4.9001e-04,  4.2431e-03,  5.9587e-04, -2.0742e-05,\n",
      "         -3.2603e-03,  1.2622e-03, -8.2556e-03, -7.3379e-04,  3.0905e-04,\n",
      "          6.3157e-04],\n",
      "        [-3.6621e-04,  2.8810e-04, -3.3450e-03,  1.1948e-04,  2.6988e-03,\n",
      "          9.3716e-04,  1.0681e-03, -1.4266e-03,  5.9352e-03,  4.5061e-05,\n",
      "         -3.0608e-03,  6.1172e-04, -2.2006e-04, -1.0406e-03,  6.7568e-04,\n",
      "          7.5179e-04],\n",
      "        [-3.0637e-05, -5.3406e-05, -2.5400e-03,  2.2888e-04,  3.1126e-03,\n",
      "          5.5945e-04,  5.6589e-04, -7.4506e-04,  5.3614e-04,  5.2903e-03,\n",
      "         -2.2449e-03,  5.5146e-04, -7.1198e-03, -1.9717e-04,  3.8147e-04,\n",
      "          5.7173e-04],\n",
      "        [-5.5504e-04,  9.3034e-04, -5.7669e-03, -6.1534e-04,  4.8929e-03,\n",
      "          1.1472e-03,  1.8561e-03, -2.5517e-03,  6.3038e-04, -1.3924e-04,\n",
      "          1.7211e-03,  5.5349e-04, -3.2577e-03, -2.0904e-03,  7.3123e-04,\n",
      "          8.3727e-04],\n",
      "        [ 3.5134e-04, -8.5354e-05, -4.5789e-03, -4.2188e-04,  4.1722e-03,\n",
      "          1.2372e-03,  5.7727e-04, -9.3913e-04,  6.4456e-04, -2.7084e-04,\n",
      "         -3.8930e-03,  7.0093e-03, -9.1395e-03, -1.2506e-03,  3.1191e-04,\n",
      "          6.0940e-04],\n",
      "        [ 5.3215e-04,  5.6487e-04,  6.5813e-03, -7.1251e-04, -6.7396e-03,\n",
      "         -2.6212e-03, -2.0242e-03,  1.7847e-03, -8.9014e-04, -3.0157e-04,\n",
      "          4.3921e-03, -2.4511e-03,  1.5632e-02,  1.2539e-03, -8.4871e-04,\n",
      "         -1.9571e-03],\n",
      "        [ 1.0261e-03, -1.3041e-04,  5.3660e-03,  5.9295e-04, -5.0741e-03,\n",
      "         -1.3309e-03, -1.9889e-03,  1.5898e-03, -1.2563e-03,  2.3842e-04,\n",
      "          3.8846e-03, -2.0267e-03,  2.7188e-03,  6.7849e-03, -1.1483e-03,\n",
      "         -6.4135e-04],\n",
      "        [-4.3392e-04,  5.0509e-04, -3.9008e-03, -8.7082e-05,  3.2455e-03,\n",
      "          9.5746e-04,  1.2660e-03, -1.7551e-03,  8.2922e-04,  7.4506e-06,\n",
      "         -3.2182e-03,  5.3656e-04, -9.1827e-04, -1.3438e-03,  5.8188e-03,\n",
      "          7.3355e-04],\n",
      "        [ 1.1230e-04, -8.1778e-05, -3.1233e-03,  1.0878e-04,  3.3580e-03,\n",
      "          7.1955e-04,  5.3471e-04, -7.9608e-04,  5.7578e-04,  1.1730e-04,\n",
      "         -2.7116e-03,  8.3780e-04, -7.6159e-03, -4.0919e-04,  3.2705e-04,\n",
      "          5.7037e-03]], grad_fn=<SubBackward0>)\n",
      "likelihood.task_noise_covar_factor\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048],\n",
      "        [-0.0043]], requires_grad=True)\n",
      "likelihood.raw_noise\n",
      "Parameter containing:\n",
      "tensor([-5.3007], requires_grad=True)\n",
      "mean_module.base_means.0.constant\n",
      "Parameter containing:\n",
      "tensor([0.0258], requires_grad=True)\n",
      "mean_module.base_means.1.constant\n",
      "Parameter containing:\n",
      "tensor([-0.1533], requires_grad=True)\n",
      "covar_module.chol_11\n",
      "Parameter containing:\n",
      "tensor(1.7817, requires_grad=True)\n",
      "covar_module.chol_22\n",
      "Parameter containing:\n",
      "tensor(1.6546, requires_grad=True)\n",
      "covar_module.chol_21\n",
      "Parameter containing:\n",
      "tensor(0.3154, requires_grad=True)\n",
      "covar_module.task_covar_module.covar_factor\n",
      "Parameter containing:\n",
      "tensor([[-1.7248],\n",
      "        [-0.9583]], requires_grad=True)\n",
      "covar_module.task_covar_module.raw_var\n",
      "Parameter containing:\n",
      "tensor([-0.3752, -0.6968], requires_grad=True)\n",
      "covar_module.data_covar_module.raw_lengthscale\n",
      "Parameter containing:\n",
      "tensor([[-0.7921]], requires_grad=True)\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04. Original error on first attempt: cholesky_cpu: U(5,5) is zero, singular U.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cholesky_cpu: U(5,5) is zero, singular U.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ed912f0adf1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'END HYPERPARAMETERS optimization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mg_theta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconduct_param_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_design\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-28047e80bee6>\u001b[0m in \u001b[0;36mconduct_param_opti\u001b[0;34m(x0, loc_sample0, f_target, g_theta1, agg_data, model, likelihood, training_iter)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mg_theta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_par\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ForCarlo/Targeted_Adaptive_Design/code/vvlikelihood.py\u001b[0m in \u001b[0;36mget_ell\u001b[0;34m(self, agg_data, f_target, x, g_theta1, g_theta2, model, likelihood)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mQf12_sec_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_term_Qf12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_term_Qf12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_theta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/functions/__init__.py\u001b[0m in \u001b[0;36minv_matmul\u001b[0;34m(mat, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlazify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_matmul\u001b[0;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minv_quad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/functions/_inv_matmul.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, representation_tree, has_left, *args)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mrhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0msolves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_tensor\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/functions/_inv_matmul.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(lazy_tsr, rhs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_computations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cholesky_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mLazyTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mCholesky\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtriangular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlower\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m\"upper\"\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \"\"\"\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transpose_nonbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mkwargs_pkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_add_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# contiguous call is necessary here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mcholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluated_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTriangularLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mattempts\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mwith\u001b[0m \u001b[0msuccessively\u001b[0m \u001b[0mincreasing\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmake\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mraising\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_psd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             raise NotPSDError(\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;34mf\"Matrix not positive definite after repeatedly adding jitter up to {jitter_new:.1e}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;34mf\"Original error on first attempt: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             )\n",
      "\u001b[0;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04. Original error on first attempt: cholesky_cpu: U(5,5) is zero, singular U."
     ]
    }
   ],
   "source": [
    "iter_hp = 50\n",
    "iter_design = 100\n",
    "iter_param = 10\n",
    "\n",
    "\n",
    "f_target  = 0.5 *torch.sqrt(Tensor([2.])) * torch.ones(2, 1) #Tensor(vf.tgt_vec)\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size = 10\n",
    "loc_sample = np.random.random_sample((loc_size,1))\n",
    "\n",
    "\n",
    "\n",
    "g_theta1 = x_train\n",
    "g_theta1 = g_theta1.reshape(g_theta1.shape[0], 1)\n",
    "agg_data = y_train.flatten()\n",
    "\n",
    "# agg_data.append(y_train.flatten())\n",
    "# agg_data = torch.cat(agg_data)\n",
    "\n",
    "x0 = Tensor([1./(8.)])\n",
    "x0 = x0.reshape(x0.shape[0],1)\n",
    "x00 = x0\n",
    "vec_x = x00\n",
    "SUCCESS = False\n",
    "FAILURE = False\n",
    "iter = 0\n",
    "\n",
    "while(SUCCESS == False and iter < 100):\n",
    "    \n",
    "    \n",
    "    print('START HYPERPARAMETERS optimization')\n",
    "    model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    g_theta2,x0_new,lower_bound, upper_bound = conduct_param_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design)\n",
    "    print(lower_bound)\n",
    "    print(upper_bound)\n",
    "    print(f_target-tol_vector)\n",
    "    print(f_target+tol_vector)\n",
    "    loc_sample = np.random.random_sample((loc_size,1)) #g_theta2 #\n",
    "    x0 = x0_new #Tensor(np.random.random_sample((1,D)))\n",
    "    vec_x = torch.cat([vec_x, x0_new])\n",
    "    print(x0_new)\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "#     pred = likelihood(model(g_theta2))\n",
    "    #1y_train_new = (pred.mean)\n",
    "    #agg_data.append(y_train_new.flatten())\n",
    "    g_theta2_detach = g_theta2.detach()\n",
    "    new_data = vfield_(g_theta2_detach)\n",
    "    agg_data = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "    g_theta1= torch.cat([g_theta1, g_theta2], 0)\n",
    "    SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    iter = iter + 1\n",
    "\n",
    "print(x0_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x0 = Tensor(np.array([0.1998, 0.1004]))\n",
    "#\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([0.1732, 0.1299]))\n",
    "print(vf(x0))\n",
    "x0 = x0.reshape(1,2)\n",
    "print(x0)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "pr = likelihood(model(x0))\n",
    "print(pr.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
