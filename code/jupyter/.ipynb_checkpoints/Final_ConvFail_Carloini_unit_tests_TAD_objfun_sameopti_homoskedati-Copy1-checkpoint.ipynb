{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(\"..\")\n",
    "from LBFGS import FullBatchLBFGS\n",
    "from kernels import vvkernels as vvk, sep_vvkernels as svvk, vvk_rbfkernel as vvk_rbf\n",
    "from means import vvmeans as vvm\n",
    "from likelihood import vvlikelihood as vvll\n",
    "from mlikelihoods import MarginalLogLikelihood as exmll\n",
    "from predstrategies import GPprediction\n",
    "from utils import ObjFun, get_vertices, stopping_criteria\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If model is bad, initialize at previous 2 sample!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=30)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=28)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=24)    # legend fontsize\n",
    "plt.rc('font', size=24)          # controls default text sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "\n",
    "We sample from $$V_1(x_1, x_2) = 3(1 - x_1)^2 e^{-x_1^2 - (x_2 +1)^2} - 10 (x_1/5 - x_1 ^3 - x_2^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- (x_1 + 2) ^2 - x_2^2} + 0.5(2x_1 + x_2)$$\n",
    "$$V_2(x_1, x_2) = 3(1 +x_2)^2 e^{-x_2^2 - (x_1 +1)^2} - 10 (-x_2/5 + x_2 ^3 + x_1^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- ( 2- x_2) ^2 - x_1^2} + 0.5(2x_1 + x_2)$$\n",
    "\n",
    "where $(x_1, x_2) \\in [-3, 3]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3380, 0.3502], dtype=torch.float32)\n",
      "tensor([[ 1.8356, -1.0358],\n",
      "        [ 1.2213, -1.6843],\n",
      "        [ 1.8712, -1.0909],\n",
      "        [ 1.0003, -1.5199]])\n"
     ]
    }
   ],
   "source": [
    "vf = ObjFun()\n",
    "f_target = vf.tgt_vec\n",
    "print(f_target)\n",
    "sample_size = 4\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -3.\n",
    "vf.high = 3.\n",
    "\n",
    "high_minus_low = vf.high- vf.low\n",
    "#high_minus_low = -\n",
    "def g_theta(sample_size, D):\n",
    "    loc_x = (2. - 1.0 )  * np.random.random_sample((sample_size,1)) + 1.0\n",
    "    \n",
    "    loc_y = (2.  -1.0)  * np.random.random_sample((sample_size,1)) - 2.\n",
    "    loc = np.concatenate((loc_x, loc_y), 1)\n",
    "    #loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "#train_x = Tensor([[-1.5, 1.5], [-1.5, 1.3]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    \n",
    "    out = vf(x[:,0], x[:,1]) + torch.randn(Tensor(vf(x[:,0], x[:,1])).size()) * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)\n",
    "\n",
    "# print(train_y)\n",
    "# train_y = (train_y - train_y.mean())/train_y.std(dim=-2, keepdim=True)\n",
    "# train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "# print(train_y)\n",
    "# print(train_y.std(dim=-2, keepdim=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5540983606557375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0779/ 0.0305\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP model initialization\n",
    "We inialize the GP model following https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)  #vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)#vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)  # \n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters oprimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels,noise_value, current_model = None, current_likelihood = None):\n",
    "    noises = torch.ones(agg_data.shape[0]) * (noise_value) #  torch.zeros(agg_data.shape[0]) # \n",
    "    noises = noises.reshape(g_theta1.shape[0], 2)\n",
    "    \n",
    "#     if (current_model is not None):\n",
    "#         likelihood = current_likelihood #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)  #\n",
    "\n",
    "#         model = current_model#.get_fantasy_model(g_theta1, agg_data) #MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#         model.set_train_data(g_theta1, agg_data,  strict=False)\n",
    "#     else:\n",
    "#         likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2)#vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #\n",
    "#         model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "        \n",
    "    likelihood =  vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2) #\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.double()\n",
    "    likelihood.double()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #, weight_decay=0.001)  # Includes GaussianLikelihood parameters\n",
    "    mll = exmll(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, chi_square = mll(agg_data,g_theta1, model, likelihood, noise_value)\n",
    "        loss = -1. * loss\n",
    "#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \n",
    "        #print('loss is %.3f' %loss)\n",
    "        df = agg_data.shape[0]\n",
    "        chi_square = chi_square.clone().detach()\n",
    "        \n",
    "        p_val = 1. - stats.chi2.cdf(chi_square, df)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "       # print(p_val)\n",
    "#         if (p_val > 0.99999):\n",
    "#             return model, likelihood\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    print('loss is %.3f' %loss)\n",
    "#     for params in model.named_parameters():\n",
    "#         print(params)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design parameters and sampling point optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "#     g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "#     x_d= nn.Parameter(Tensor(x0))\n",
    "#     #optimizer = torch.optim.LBFGS(list([g_theta2,x_d]), lr= .000001, max_iter = 100,line_search_fn=\"strong_wolfe\")  #\n",
    "    \n",
    "#     optimizer = FullBatchLBFGS(list([g_theta2,x_d]), lr=.001)\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "        \n",
    "#         loss2 = -1. * loss2\n",
    "\n",
    "       \n",
    "\n",
    "       \n",
    "#         return loss2\n",
    "        \n",
    "#     loss2 = closure()\n",
    "#     loss2.backward()\n",
    "#     #fail = False\n",
    "#     for i in range(training_design_iter):\n",
    "#         options = {'closure': closure, 'current_loss': loss2, 'max_ls': 10}\n",
    "#         loss2, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "#         #print('design Iter %d/%d - Loss: %.3f' %(i + 1, training_design_iter, loss2.item()))\n",
    "  \n",
    "#         if fail:\n",
    "#             print('Convergence reached!')\n",
    "\n",
    "#             break\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    "#     loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "#     loss2 = -1. * loss2\n",
    "#     print('Loss design: %.3f' % ( loss2))\n",
    "#     #print(x_d)\n",
    "#     return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': g_theta2, 'lr': 0.1},{'params': x_d, 'lr': 0.1}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "    loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    #print(x_d)\n",
    "    return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "    \n",
    "#     fun = partial(likelihood.get_ell, agg_data,f_target, g_theta1, model, likelihood, noise_value)\n",
    "#     def tad_obj(samples):\n",
    "#         loss2, pf1, Qf1, Qf12, data_fit, Q21 = fun(samples)\n",
    "#         return -loss2.detach().numpy()\n",
    "    \n",
    "    \n",
    "#     sample0 = torch.cat([x0, loc_sample], 0)\n",
    "#     print(sample0)\n",
    "#     #print(sample0.shape)\n",
    "#     res = scipy.optimize.minimize(tad_obj, sample0)#, method='nelder-mead')\n",
    "#     sample_opt = res.x\n",
    "#     loss2, pf1, Qf1, Qf12, data_fit, Q21 = fun(sample_opt)\n",
    "#     loss2 = -1. * loss2\n",
    "#     print('Loss design: %.3f' % ( loss2))\n",
    "\n",
    "#     x_d = sample_opt[0:g_theta1.shape[1]]\n",
    "#     x_d = Tensor(x_d.reshape(math.ceil(x_d.shape[0]/g_theta1.shape[1]), g_theta1.shape[1]))\n",
    "#     g_theta2 = sample_opt[g_theta1.shape[1]:]\n",
    "#     g_theta2 = Tensor(g_theta2.reshape(math.ceil(g_theta2.shape[0]/g_theta1.shape[1]), g_theta1.shape[1]))\n",
    "#     return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conducting the TAD experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_size = 2\n",
    "#loc_sample0 = Tensor((2. - 1.5)  * np.random.random_sample((loc_size,2)) + 1.5)\n",
    "x0 = Tensor(np.array([-2. , -2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "x0 = x0.reshape(1,2)\n",
    "\n",
    "dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(loc_size) )\n",
    "                    #loc_size = 4\n",
    "loc_sample = dis_2sample.sample((loc_size + 1,))\n",
    "\n",
    "loc_sample0 = loc_sample.reshape(loc_size + 1, 2)\n",
    "#loc_sample0[-1] = train_x[-1] + 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9904, -1.9393],\n",
      "        [-1.9602, -2.0397],\n",
      "        [-1.9529, -1.9500]])\n",
      "0\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.544\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 61.150\n",
      "expected info is tensor([[0.8740]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(121.4251, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.6205, -2.6829]])\n",
      "new data istensor([[-0.4384, -0.0069]])\n",
      "g_theta2 istensor([[-1.3733, -1.2788],\n",
      "        [-1.3006, -1.3059],\n",
      "        [-1.3500, -1.2856]])\n",
      "p21val is 0.000000000118672\n",
      "pf12val is 0.011507973613331\n",
      "chi_f12 is 8.929430252386647\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "0\n",
      "Loss design: 54.526\n",
      "expected info is tensor([[0.8201]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(108.7174, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.3934, -1.2369]])\n",
      "new data istensor([[-0.4503,  0.0688]])\n",
      "g_theta2 istensor([[-2.8910, -2.9978],\n",
      "        [-1.6867, -2.8147],\n",
      "        [-1.1583, -2.6746]])\n",
      "p21val is 0.000000000000013\n",
      "pf12val is 0.079462482084580\n",
      "chi_f12 is 5.064940584354365\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is2\n",
      "acquiring, new size is7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "1\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.028\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 51.069\n",
      "expected info is tensor([[0.0014]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(107.9813, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7413,  1.4068]])\n",
      "new data istensor([[-0.2526,  0.0039]])\n",
      "g_theta2 istensor([[-0.8901, -2.8827],\n",
      "        [-0.9544, -2.8262],\n",
      "        [-0.8658, -2.8640]])\n",
      "p21val is 0.480678115011412\n",
      "pf12val is 0.000000000035242\n",
      "chi_f12 is 48.137560376769969\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.1440, -0.0259],\n",
      "        [-1.3891,  1.1914],\n",
      "        [-2.7408,  1.4076]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.241\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 0.777\n",
      "expected info is tensor([[0.4076]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.0350, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.6426,  2.5041]])\n",
      "new data istensor([[-0.1545, -0.0189]])\n",
      "g_theta2 istensor([[-0.4042, -1.3534],\n",
      "        [-0.8289,  1.8428],\n",
      "        [-2.1992,  1.4727]])\n",
      "p21val is 0.344194915438826\n",
      "pf12val is 0.821920214094612\n",
      "chi_f12 is 0.392223903564259\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.3615019540105856\n",
      "new 2 points\n",
      "tensor([[-0.0656,  2.1448],\n",
      "        [-0.9186,  1.3858],\n",
      "        [-2.6418,  2.5052]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "3\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.135\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 712.980\n",
      "expected info is tensor([[0.0152]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1427.4927, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0035,  3.0050]])\n",
      "new data istensor([[-0.1392,  0.0023]])\n",
      "g_theta2 istensor([[ 0.7317,  2.7052],\n",
      "        [-0.5192,  2.1175],\n",
      "        [-1.5210,  1.4534]])\n",
      "p21val is 0.170906498068219\n",
      "pf12val is 0.828794391933336\n",
      "chi_f12 is 0.375566347979023\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.3010,  1.1192],\n",
      "        [ 0.5426,  2.4647],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "4\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.098\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 436.039\n",
      "expected info is tensor([[0.0109]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(879.1548, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.3312,  3.0010]])\n",
      "new data istensor([[0.0263, 0.0038]])\n",
      "g_theta2 istensor([[-2.3277,  1.3089],\n",
      "        [ 0.4358,  2.5022],\n",
      "        [-2.8986,  0.9527]])\n",
      "p21val is 0.615662069076881\n",
      "pf12val is 0.530075426141546\n",
      "chi_f12 is 1.269471938173912\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.6820, -2.9817],\n",
      "        [-2.6344,  2.9391],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "5\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.129\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 256.027\n",
      "expected info is tensor([[0.0007]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(520.8486, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5968,  2.3182]])\n",
      "new data istensor([[-0.0260, -0.0189]])\n",
      "g_theta2 istensor([[-1.5926, -2.4797],\n",
      "        [-2.7191,  2.4832],\n",
      "        [-2.8873,  1.0405]])\n",
      "p21val is 0.574575805999278\n",
      "pf12val is 0.065823360785636\n",
      "chi_f12 is 5.441560953041537\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.7824,  0.2094],\n",
      "        [-0.8090, -1.3343],\n",
      "        [-1.5969,  2.3169]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "6\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.965\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 289.955\n",
      "expected info is tensor([[0.0016]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(581.0272, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0066,  2.0615]])\n",
      "new data istensor([[-0.2084, -0.0034]])\n",
      "g_theta2 istensor([[-2.0526, -1.3633],\n",
      "        [ 0.1329, -2.3259],\n",
      "        [-1.4011,  2.7553]])\n",
      "p21val is 0.797414244065469\n",
      "pf12val is 0.836129205845364\n",
      "chi_f12 is 0.357944250785652\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.7161, -2.3844],\n",
      "        [-0.0676, -0.4654],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.329\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1384.471\n",
      "expected info is tensor([[0.0023]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2750.4048, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1779,  3.0120]])\n",
      "new data istensor([[-0.0865,  0.0258]])\n",
      "g_theta2 istensor([[ 0.0621, -2.6649],\n",
      "        [ 0.5781, -1.5757],\n",
      "        [-1.1810,  0.7165]])\n",
      "p21val is 0.000000008253648\n",
      "pf12val is 0.764651245328826\n",
      "chi_f12 is 0.536670875126780\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "Loss design: 1384.400\n",
      "expected info is tensor([[0.0336]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2746.8269, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1772,  3.0127]])\n",
      "new data istensor([[-0.0341,  0.0325]])\n",
      "g_theta2 istensor([[-2.7494,  2.2784],\n",
      "        [-0.6656, -1.5868],\n",
      "        [-1.1786,  0.7130]])\n",
      "p21val is 0.434900799587941\n",
      "pf12val is 0.146446132604598\n",
      "chi_f12 is 3.842195227346692\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.1551,  1.4171],\n",
      "        [-0.4133, -0.2180],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "8\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.886\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3265.771\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6545.5901, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.0398,  2.1719]])\n",
      "new data istensor([[-0.1166,  0.0146]])\n",
      "g_theta2 istensor([[ 2.4846, -0.1452],\n",
      "        [ 1.0832, -1.7268],\n",
      "        [-2.9127,  0.7754]])\n",
      "p21val is 0.178372079919249\n",
      "pf12val is 0.939187637635514\n",
      "chi_f12 is 0.125479985297547\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2267,  0.5991],\n",
      "        [ 1.4512, -0.8517],\n",
      "        [-2.0398,  2.1697]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.211\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1869.854\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3482.8256, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0367,  1.4740]])\n",
      "new data istensor([[-0.2341,  0.0058]])\n",
      "g_theta2 istensor([[ 1.6506, -0.8936],\n",
      "        [ 2.4076, -1.8589],\n",
      "        [-1.8902,  2.8151]])\n",
      "p21val is 0.002819367436018\n",
      "pf12val is 0.678836477076850\n",
      "chi_f12 is 0.774750018949523\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: 1870.333\n",
      "expected info is tensor([[0.0016]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3484.6998, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0367,  1.4742]])\n",
      "new data istensor([[-0.2493, -0.0041]])\n",
      "g_theta2 istensor([[ 1.4217, -1.1192],\n",
      "        [ 2.6702, -1.8470],\n",
      "        [-1.1300,  2.8872]])\n",
      "p21val is 0.023485566203859\n",
      "pf12val is 0.967259297788452\n",
      "chi_f12 is 0.066577345693153\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8409,  2.9286],\n",
      "        [-1.7043, -0.6299],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "10\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.135\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4412.550\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8839.6681, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.3767,  1.8978]])\n",
      "new data istensor([[-0.1823,  0.0128]])\n",
      "g_theta2 istensor([[ 2.6149,  2.4726],\n",
      "        [-1.0230, -1.4096],\n",
      "        [-1.0086,  2.8213]])\n",
      "p21val is 0.038929056663271\n",
      "pf12val is 0.690958440325271\n",
      "chi_f12 is 0.739351202540560\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.0757, -1.8373],\n",
      "        [ 0.3388,  1.5856],\n",
      "        [-2.3766,  1.8984]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "11\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.144\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2164.667\n",
      "expected info is tensor([[0.0010]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3735.0750, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0551,  2.5598]])\n",
      "new data istensor([[-0.2034,  0.0328]])\n",
      "g_theta2 istensor([[ 2.9009, -1.4872],\n",
      "        [ 1.1724,  1.7610],\n",
      "        [-1.2156,  2.5927]])\n",
      "p21val is 0.759243706757040\n",
      "pf12val is 0.443223688254912\n",
      "chi_f12 is 1.627361393497554\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8982, -2.9281],\n",
      "        [-0.2834,  0.6095],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "12\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.336\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 218.269\n",
      "expected info is tensor([[5.1980e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(443.3472, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.5047, 0.1828]])\n",
      "new data istensor([[0.0820, 0.3132]])\n",
      "g_theta2 istensor([[ 0.1078, -2.4723],\n",
      "        [ 2.8642, -1.6146],\n",
      "        [-2.9765,  2.8963]])\n",
      "p21val is 0.795953384588722\n",
      "pf12val is 0.002445225410953\n",
      "chi_f12 is 12.027235935371497\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.7580,  1.3742],\n",
      "        [ 0.1070,  0.7855],\n",
      "        [ 0.5054,  0.1819]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "13\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.065\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 132.662\n",
      "expected info is tensor([[0.0724]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(271.3968, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.7111, 0.5332]])\n",
      "new data istensor([[ 0.4195, -0.3246]])\n",
      "g_theta2 istensor([[-1.4184,  1.7223],\n",
      "        [ 0.6150,  1.5867],\n",
      "        [-0.2180,  1.3653]])\n",
      "p21val is 0.000000340039464\n",
      "pf12val is 0.000982537849012\n",
      "chi_f12 is 13.850743383591496\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "13\n",
      "Loss design: 103.674\n",
      "expected info is tensor([[0.0099]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(212.9412, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.0185, 0.9833]])\n",
      "new data istensor([[ 0.3340, -0.0710]])\n",
      "g_theta2 istensor([[ 1.8866, -1.2994],\n",
      "        [-0.3065, -0.2288],\n",
      "        [-0.2193, -0.4902]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.441037582697201\n",
      "chi_f12 is 1.637250371261675\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is3\n",
      "acquiring, new size is58\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "14\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.238\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 195.084\n",
      "expected info is tensor([[3.7259e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(396.5246, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.7345, 1.1303]])\n",
      "new data istensor([[ 0.3835, -0.0087]])\n",
      "g_theta2 istensor([[-2.0215,  1.8858],\n",
      "        [ 0.5318,  0.1490],\n",
      "        [ 0.6580, -0.1236]])\n",
      "p21val is 0.017217285021917\n",
      "pf12val is 0.943214435304561\n",
      "chi_f12 is 0.116923250539796\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.7688, -1.2089],\n",
      "        [-2.9531, -1.5297],\n",
      "        [ 2.7328,  1.1307]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "15\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.224\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 474.710\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(950.5089, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[3.0057, 0.2793]])\n",
      "new data istensor([[0.3452, 0.0051]])\n",
      "g_theta2 istensor([[-2.9915, -1.8152],\n",
      "        [-2.4955, -1.4913],\n",
      "        [ 2.2468,  0.5342]])\n",
      "p21val is 0.302117601073296\n",
      "pf12val is 0.763404657271082\n",
      "chi_f12 is 0.539934075987875\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9476, -1.9613],\n",
      "        [ 0.7192,  1.0747],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "16\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.914\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2236.309\n",
      "expected info is tensor([[6.0928e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4485.6540, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1609,  2.1168]])\n",
      "new data istensor([[ 0.1511, -0.0881]])\n",
      "g_theta2 istensor([[-1.2555, -2.5944],\n",
      "        [ 1.1695,  1.7503],\n",
      "        [-2.8902,  1.1909]])\n",
      "p21val is 0.927688157554932\n",
      "pf12val is 0.002620189369124\n",
      "chi_f12 is 11.889017371058928\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6604,  0.7602],\n",
      "        [-2.8284,  2.3574],\n",
      "        [-1.1589,  2.1174]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "17\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.253\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 139.069\n",
      "expected info is tensor([[1.1032e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(284.8471, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.3176, -0.2509]])\n",
      "new data istensor([[-0.5928,  0.3565]])\n",
      "g_theta2 istensor([[ 1.3281,  0.1508],\n",
      "        [-2.5271,  1.5796],\n",
      "        [-0.3846,  2.9731]])\n",
      "p21val is 0.022328062602098\n",
      "pf12val is 0.135365810030762\n",
      "chi_f12 is 3.999548922482579\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.8832, -0.8553],\n",
      "        [-0.8226, -1.8344],\n",
      "        [-2.3182, -0.2496]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.227\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 332.553\n",
      "expected info is tensor([[0.0012]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(672.5084, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.3981, -0.3106]])\n",
      "new data istensor([[-0.5922,  0.7813]])\n",
      "g_theta2 istensor([[-2.4455, -1.8642],\n",
      "        [ 0.1233, -2.4335],\n",
      "        [-2.8825, -1.6364]])\n",
      "p21val is 0.883387204054754\n",
      "pf12val is 0.000027285008166\n",
      "chi_f12 is 21.018346316048337\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.4658, -0.8275],\n",
      "        [-0.2099, -1.0792],\n",
      "        [-1.3982, -0.3101]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "19\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.209\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 261.885\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(530.3395, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.1221,  1.0772]])\n",
      "new data istensor([[ 0.5770, -0.3817]])\n",
      "g_theta2 istensor([[-1.1180, -1.5866],\n",
      "        [-1.1605, -1.5593],\n",
      "        [-2.1812, -1.0497]])\n",
      "p21val is 0.198490839168969\n",
      "pf12val is 0.045585147971619\n",
      "chi_f12 is 6.176346635782219\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.7282, -0.3389],\n",
      "        [-1.5541, -2.5632],\n",
      "        [-0.1217,  1.0793]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "20\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -1.821\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 235.785\n",
      "expected info is tensor([[0.0064]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(478.3887, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.9787, 1.0341]])\n",
      "new data istensor([[0.4509, 0.1880]])\n",
      "g_theta2 istensor([[-0.7419, -0.0522],\n",
      "        [-2.0711, -2.6816],\n",
      "        [-0.8888, -0.7890]])\n",
      "p21val is 0.461081693107089\n",
      "pf12val is 0.881989527039885\n",
      "chi_f12 is 0.251150194300566\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[2.7528, 2.1787],\n",
      "        [0.4631, 0.4070],\n",
      "        [0.9788, 1.0331]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "21\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.181\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 412.349\n",
      "expected info is tensor([[0.0068]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(831.4729, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.0442, 1.7214]])\n",
      "new data istensor([[0.3565, 0.0310]])\n",
      "g_theta2 istensor([[ 2.3516,  2.7369],\n",
      "        [-0.2383, -0.0823],\n",
      "        [-0.0943,  1.7172]])\n",
      "p21val is 0.001520195881609\n",
      "pf12val is 0.847597758029531\n",
      "chi_f12 is 0.330698195344541\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "21\n",
      "Loss design: 411.806\n",
      "expected info is tensor([[0.0091]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(830.3865, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.0407, 1.7222]])\n",
      "new data istensor([[0.3513, 0.0129]])\n",
      "g_theta2 istensor([[ 2.7465,  2.3798],\n",
      "        [-0.8001, -0.0193],\n",
      "        [-0.0730,  1.9483]])\n",
      "p21val is 0.307685570211814\n",
      "pf12val is 0.968245648249399\n",
      "chi_f12 is 0.064538910093489\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.9039,  2.1692],\n",
      "        [-1.3539, -0.1308],\n",
      "        [ 2.0398,  1.7223]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "22\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.197\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 623.935\n",
      "expected info is tensor([[0.0046]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1256.0253, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.7416, 1.0409]])\n",
      "new data istensor([[ 0.3786, -0.0483]])\n",
      "g_theta2 istensor([[ 2.5263,  2.6006],\n",
      "        [-2.3003, -0.3125],\n",
      "        [ 2.9413,  2.1531]])\n",
      "p21val is 0.776964067646265\n",
      "pf12val is 0.929098945657046\n",
      "chi_f12 is 0.147080076271619\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3835, -1.5920],\n",
      "        [ 0.1423,  0.1070],\n",
      "        [ 1.7419,  1.0412]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "23\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.191\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1168.626\n",
      "expected info is tensor([[0.0077]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2240.9787, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[3.0230, 1.4376]])\n",
      "new data istensor([[ 0.3590, -0.0185]])\n",
      "g_theta2 istensor([[ 0.0630, -1.9088],\n",
      "        [ 0.0423,  0.9196],\n",
      "        [ 1.1811,  0.1550]])\n",
      "p21val is 0.000083504320507\n",
      "pf12val is 0.174950093819917\n",
      "chi_f12 is 3.486509047803102\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "23\n",
      "Loss design: 1168.710\n",
      "expected info is tensor([[0.0076]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2249.1476, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[3.0221, 1.4378]])\n",
      "new data istensor([[0.4394, 0.0351]])\n",
      "g_theta2 istensor([[ 1.1425,  1.4609],\n",
      "        [-1.1036, -0.6256],\n",
      "        [ 1.1725,  0.1653]])\n",
      "p21val is 0.708432824462919\n",
      "pf12val is 0.913836738814369\n",
      "chi_f12 is 0.180206692448638\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.9680, -0.9727],\n",
      "        [ 0.3603,  1.0879],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "24\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.239\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2665.702\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5113.0950, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8016,  3.0340]])\n",
      "new data istensor([[-0.0240, -0.0243]])\n",
      "g_theta2 istensor([[ 1.5180, -2.0707],\n",
      "        [-0.1692,  0.9752],\n",
      "        [-2.9434,  1.1514]])\n",
      "p21val is 0.452515513549586\n",
      "pf12val is 0.050874552597754\n",
      "chi_f12 is 5.956784858810763\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.0743,  1.6131],\n",
      "        [-0.9025,  2.1996],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "25\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.208\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1071.604\n",
      "expected info is tensor([[0.0013]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2154.4800, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.6407,  0.6982]])\n",
      "new data istensor([[-0.0122, -0.0328]])\n",
      "g_theta2 istensor([[ 0.7029, -0.2947],\n",
      "        [-0.3746,  2.0633],\n",
      "        [-2.9059,  2.6945]])\n",
      "p21val is 0.706034679881061\n",
      "pf12val is 0.058904721391196\n",
      "chi_f12 is 5.663668064188274\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.6269, -0.2341],\n",
      "        [ 1.2653,  2.9767],\n",
      "        [-0.6395,  0.6984]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "26\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.993\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1356.967\n",
      "expected info is tensor([[9.5780e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2725.7258, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.7903,  1.2838]])\n",
      "new data istensor([[ 0.3293, -0.2824]])\n",
      "g_theta2 istensor([[-0.2509, -0.2838],\n",
      "        [ 2.4229,  2.5045],\n",
      "        [-1.3933, -0.3947]])\n",
      "p21val is 0.843753857338505\n",
      "pf12val is 0.939047874198406\n",
      "chi_f12 is 0.125777633673344\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4908, -0.4294],\n",
      "        [ 1.7323,  2.4368],\n",
      "        [-0.7892,  1.2843]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "27\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.337\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 426.216\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(860.9968, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1169,  0.5023]])\n",
      "new data istensor([[-0.5452,  0.3892]])\n",
      "g_theta2 istensor([[ 0.8049, -0.3316],\n",
      "        [ 1.9152,  1.7106],\n",
      "        [ 0.0578, -0.0482]])\n",
      "p21val is 0.401726247733169\n",
      "pf12val is 0.939172049638562\n",
      "chi_f12 is 0.125513180211295\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4213, -1.3218],\n",
      "        [-2.6347,  0.8439],\n",
      "        [-2.1167,  0.5034]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "28\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.356\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1786.032\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3583.6424, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.6082,  0.3272]])\n",
      "new data istensor([[-0.6860,  0.8244]])\n",
      "g_theta2 istensor([[-0.6262, -1.5517],\n",
      "        [-2.8103,  1.8173],\n",
      "        [-2.9366,  1.6882]])\n",
      "p21val is 0.690958048530474\n",
      "pf12val is 0.001446937844282\n",
      "chi_f12 is 13.076611574279385\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.0059,  1.0307],\n",
      "        [-0.2412,  2.5705],\n",
      "        [-1.6091,  0.3260]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "29\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.366\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1955.733\n",
      "expected info is tensor([[0.0026]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3923.5754, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8112,  0.9684]])\n",
      "new data istensor([[-0.3580,  0.2971]])\n",
      "g_theta2 istensor([[-2.9412,  1.8718],\n",
      "        [ 0.8992,  2.5005],\n",
      "        [-2.6101, -0.1786]])\n",
      "p21val is 0.298902744496008\n",
      "pf12val is 0.986193525098215\n",
      "chi_f12 is 0.027805341438858\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.5185, -0.0499],\n",
      "        [ 0.8009, -1.9736],\n",
      "        [-1.8112,  0.9683]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "30\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.379\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3598.651\n",
      "expected info is tensor([[0.0009]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7210.8406, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.2488,  1.1599]])\n",
      "new data istensor([[-0.0506,  0.0723]])\n",
      "g_theta2 istensor([[-2.4218, -0.3854],\n",
      "        [ 1.6936, -1.8990],\n",
      "        [-2.8045,  0.4597]])\n",
      "p21val is 0.583159423843875\n",
      "pf12val is 0.662576094242261\n",
      "chi_f12 is 0.823239736967951\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.8417,  0.5554],\n",
      "        [-0.9506,  0.3011],\n",
      "        [-1.2494,  1.1590]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "31\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.403\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1632.512\n",
      "expected info is tensor([[0.0020]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3275.3978, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.4782, 1.6515]])\n",
      "new data istensor([[ 0.8473, -0.3880]])\n",
      "g_theta2 istensor([[-2.5892, -0.1693],\n",
      "        [-1.0234,  0.9723],\n",
      "        [-2.0946,  0.3074]])\n",
      "p21val is 0.600532430977389\n",
      "pf12val is 0.219047059314333\n",
      "chi_f12 is 3.036937379071539\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.3547,  0.0161],\n",
      "        [ 2.9177, -2.5048],\n",
      "        [ 0.4765,  1.6505]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "32\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.330\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 170.064\n",
      "expected info is tensor([[7.7227e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(346.3758, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.7162, 3.0009]])\n",
      "new data istensor([[0.3624, 0.0213]])\n",
      "g_theta2 istensor([[-2.3871, -0.5918],\n",
      "        [ 2.4619, -1.9143],\n",
      "        [-0.4492,  0.9466]])\n",
      "p21val is 0.715595140198969\n",
      "pf12val is 0.962139866674194\n",
      "chi_f12 is 0.077190894663084\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.1982,  1.0805],\n",
      "        [ 2.9671, -2.2218],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "33\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.437\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3143.825\n",
      "expected info is tensor([[0.0023]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6300.8344, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.0662,  2.6063]])\n",
      "new data istensor([[-0.0801, -0.0095]])\n",
      "g_theta2 istensor([[-0.9006,  1.5675],\n",
      "        [ 2.5112, -2.5247],\n",
      "        [-2.9977,  1.0304]])\n",
      "p21val is 0.996219291690376\n",
      "pf12val is 0.871401948801343\n",
      "chi_f12 is 0.275303857797123\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.7364, -1.3792],\n",
      "        [-1.3522,  2.0299],\n",
      "        [-2.0647,  2.6069]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "34\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.344\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4597.096\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9208.9935, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8875,  2.6556]])\n",
      "new data istensor([[-0.0401, -0.0060]])\n",
      "g_theta2 istensor([[ 2.1693, -1.2571],\n",
      "        [-1.2158,  1.2875],\n",
      "        [-2.8580,  1.6284]])\n",
      "p21val is 0.634372219088884\n",
      "pf12val is 0.423544262579436\n",
      "chi_f12 is 1.718194508450562\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.8553, -1.5638],\n",
      "        [-1.9384, -2.0200],\n",
      "        [-1.8890,  2.6566]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "35\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.481\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4016.354\n",
      "expected info is tensor([[0.0012]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8046.7680, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5790,  1.8148]])\n",
      "new data istensor([[0.0022, 0.0043]])\n",
      "g_theta2 istensor([[-1.1937, -1.5733],\n",
      "        [-2.1262, -2.5009],\n",
      "        [-2.6620,  2.6969]])\n",
      "p21val is 0.944869279419846\n",
      "pf12val is 0.488429670645401\n",
      "chi_f12 is 1.433119575767255\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.7584, -0.7053],\n",
      "        [-1.0687,  0.2994],\n",
      "        [-1.5774,  1.8169]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "36\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.512\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2695.404\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5404.2035, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.4686,  0.7396]])\n",
      "new data istensor([[-0.4626,  0.1554]])\n",
      "g_theta2 istensor([[-0.6758, -1.5351],\n",
      "        [-0.2194, -0.7772],\n",
      "        [-2.6346,  2.8397]])\n",
      "p21val is 0.242376459049885\n",
      "pf12val is 0.015053765468449\n",
      "chi_f12 is 8.392254243684448\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.8926,  0.6786],\n",
      "        [-2.5627, -0.3447],\n",
      "        [-2.4685,  0.7393]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "37\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.503\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4155.246\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8325.1019, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.4100,  0.9010]])\n",
      "new data istensor([[-0.3245,  0.1156]])\n",
      "g_theta2 istensor([[-1.0515,  1.0342],\n",
      "        [-1.6856, -0.6515],\n",
      "        [-2.5702, -0.2788]])\n",
      "p21val is 0.485352285058300\n",
      "pf12val is 0.013801638440092\n",
      "chi_f12 is 8.565935932647223\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5315,  0.3988],\n",
      "        [ 1.5460,  0.1197],\n",
      "        [-2.4115,  0.9013]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "38\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.519\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 214.786\n",
      "expected info is tensor([[7.8078e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(436.0436, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0032, -0.7900]])\n",
      "new data istensor([[-0.4455,  0.0098]])\n",
      "g_theta2 istensor([[-0.7041,  0.8278],\n",
      "        [ 1.8694, -0.2767],\n",
      "        [-2.5827,  1.8909]])\n",
      "p21val is 0.040219946839839\n",
      "pf12val is 0.247846673330242\n",
      "chi_f12 is 2.789889953429601\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.3940, -0.4804],\n",
      "        [ 1.6395, -1.2653],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "39\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.522\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 663.472\n",
      "expected info is tensor([[5.0969e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1313.5859, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.3942, 3.0107]])\n",
      "new data istensor([[ 0.2265, -0.0694]])\n",
      "g_theta2 istensor([[-1.7956, -0.3834],\n",
      "        [ 1.5172, -1.2109],\n",
      "        [-2.9014,  1.1779]])\n",
      "p21val is 0.168469666835651\n",
      "pf12val is 0.352500539225292\n",
      "chi_f12 is 2.085406254027686\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2561,  2.3699],\n",
      "        [ 1.6546,  0.5506],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "40\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.507\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1779.617\n",
      "expected info is tensor([[5.6858e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3570.9678, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.1353,  2.6167]])\n",
      "new data istensor([[ 0.2599, -0.2713]])\n",
      "g_theta2 istensor([[ 1.1050,  1.6708],\n",
      "        [ 1.5619, -1.0988],\n",
      "        [-2.9003,  1.1753]])\n",
      "p21val is 0.600093023795787\n",
      "pf12val is 0.056798137883528\n",
      "chi_f12 is 5.736503475066272\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.1777,  2.9411],\n",
      "        [ 0.8178,  0.9609],\n",
      "        [-0.1332,  2.6158]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "41\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.531\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2521.886\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4649.9299, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.0252,  3.0451]])\n",
      "new data istensor([[ 0.2101, -0.1450]])\n",
      "g_theta2 istensor([[-1.1602,  2.2107],\n",
      "        [ 1.5668,  0.1635],\n",
      "        [-1.1049,  1.7719]])\n",
      "p21val is 0.045981934617707\n",
      "pf12val is 0.028280547673845\n",
      "chi_f12 is 7.131162144044596\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5549,  0.7173],\n",
      "        [ 2.3331,  2.0727],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "42\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.525\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4368.668\n",
      "expected info is tensor([[1.0900e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8751.7967, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8486,  1.0025]])\n",
      "new data istensor([[-0.3575,  0.2505]])\n",
      "g_theta2 istensor([[ 1.4529,  0.2038],\n",
      "        [ 1.9037,  2.8128],\n",
      "        [-2.9399,  1.2012]])\n",
      "p21val is 0.442280751509800\n",
      "pf12val is 0.411775683711576\n",
      "chi_f12 is 1.774553069827775\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9370, -2.7121],\n",
      "        [ 0.3412,  0.4394],\n",
      "        [-1.8484,  1.0031]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "43\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.521\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5674.817\n",
      "expected info is tensor([[0.0035]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11364.6412, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8752,  1.5113]])\n",
      "new data istensor([[-0.1437,  0.0521]])\n",
      "g_theta2 istensor([[-0.0383, -2.5280],\n",
      "        [ 0.7891, -0.2723],\n",
      "        [-2.8897,  0.0927]])\n",
      "p21val is 0.776893152465199\n",
      "pf12val is 0.746703172005408\n",
      "chi_f12 is 0.584175065835239\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.4337,  1.1554],\n",
      "        [ 0.1295,  0.2399],\n",
      "        [-1.8755,  1.5115]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "44\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.558\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 6458.441\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(12932.1537, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.6868,  2.1687]])\n",
      "new data istensor([[-0.0247, -0.0289]])\n",
      "g_theta2 istensor([[-0.7839,  0.6071],\n",
      "        [ 0.7833, -0.2478],\n",
      "        [-2.7360,  0.5949]])\n",
      "p21val is 0.735746046501348\n",
      "pf12val is 0.389269171543822\n",
      "chi_f12 is 1.886968433919672\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.9105,  2.0567],\n",
      "        [-0.8291,  1.6002],\n",
      "        [-1.6881,  2.1686]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "45\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.575\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5781.554\n",
      "expected info is tensor([[0.0014]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11578.1540, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.2860,  2.3384]])\n",
      "new data istensor([[-0.1070,  0.0252]])\n",
      "g_theta2 istensor([[-2.5802,  1.6602],\n",
      "        [-1.1559,  1.5786],\n",
      "        [-2.7618,  1.0150]])\n",
      "p21val is 0.454234460825859\n",
      "pf12val is 0.347739561541642\n",
      "chi_f12 is 2.112602931526273\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5982, -1.7816],\n",
      "        [ 2.5975, -1.6677],\n",
      "        [-2.2860,  2.3369]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "46\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.593\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2656.126\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4592.8058, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.5743,  3.0605]])\n",
      "new data istensor([[-0.0863,  0.0201]])\n",
      "g_theta2 istensor([[-1.1720, -1.7276],\n",
      "        [ 2.5184, -1.9282],\n",
      "        [-2.8616,  1.4342]])\n",
      "p21val is 0.028234342056086\n",
      "pf12val is 0.980605403872264\n",
      "chi_f12 is 0.039170277997530\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.1840, -1.2313],\n",
      "        [-1.2272, -2.4084],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "47\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.594\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3950.053\n",
      "expected info is tensor([[4.6269e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7913.7620, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1401,  0.2635]])\n",
      "new data istensor([[-0.5247,  0.5549]])\n",
      "g_theta2 istensor([[-2.0171, -2.4559],\n",
      "        [-0.8669, -2.8646],\n",
      "        [-2.9409,  2.9376]])\n",
      "p21val is 0.596555166504939\n",
      "pf12val is 0.758493966027964\n",
      "chi_f12 is 0.552840870726023\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.5685,  0.0112],\n",
      "        [ 2.9028,  2.3331],\n",
      "        [-1.1394,  0.2651]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "48\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.612\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1544.439\n",
      "expected info is tensor([[0.0014]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3100.6363, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.1131,  0.4565]])\n",
      "new data istensor([[0.0918, 0.1239]])\n",
      "g_theta2 istensor([[-2.1758, -0.6097],\n",
      "        [ 2.3603,  2.6075],\n",
      "        [-0.4718, -0.7785]])\n",
      "p21val is 0.321033763724148\n",
      "pf12val is 0.021231125346714\n",
      "chi_f12 is 7.704573994580195\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2682,  0.9671],\n",
      "        [ 0.3805,  0.7854],\n",
      "        [-0.1125,  0.4564]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "49\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.608\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4033.450\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8080.8086, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.4046,  0.3002]])\n",
      "new data istensor([[0.0926, 0.1317]])\n",
      "g_theta2 istensor([[1.4246, 1.7633],\n",
      "        [1.1016, 1.6027],\n",
      "        [0.8118, 1.4916]])\n",
      "p21val is 0.674086068002820\n",
      "pf12val is 0.000001480135326\n",
      "chi_f12 is 26.846754075414157\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.3152,  0.2161],\n",
      "        [ 2.4798,  2.7560],\n",
      "        [-0.4072,  0.3003]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "50\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.591\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5841.104\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11696.6226, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.8176, -0.3571]])\n",
      "new data istensor([[0.0073, 0.1701]])\n",
      "g_theta2 istensor([[ 0.9345, -0.6969],\n",
      "        [ 2.6677,  2.4444],\n",
      "        [ 0.9438, -0.7674]])\n",
      "p21val is 0.723978884957192\n",
      "pf12val is 0.850545262356948\n",
      "chi_f12 is 0.323755300071176\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.4878, -2.4759],\n",
      "        [-0.8762,  2.0254],\n",
      "        [-0.8174, -0.3580]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "51\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.539\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 770.137\n",
      "expected info is tensor([[3.4444e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1551.4324, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8101, -1.8753]])\n",
      "new data istensor([[-0.2940,  0.0546]])\n",
      "g_theta2 istensor([[ 2.6514, -2.5152],\n",
      "        [-0.5519,  2.0044],\n",
      "        [-1.6316,  0.4851]])\n",
      "p21val is 0.622763352081745\n",
      "pf12val is 0.710770803735569\n",
      "chi_f12 is 0.682810517525029\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.3876, -2.8769],\n",
      "        [ 1.8000,  0.4043],\n",
      "        [-1.8104, -1.8757]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "52\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.614\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 181.365\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(369.2116, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0033, -2.4917]])\n",
      "new data istensor([[-0.4595,  0.0021]])\n",
      "g_theta2 istensor([[-1.9339, -1.4829],\n",
      "        [ 2.2448,  0.0999],\n",
      "        [-1.2304, -0.9829]])\n",
      "p21val is 0.596972617741469\n",
      "pf12val is 0.932434617661849\n",
      "chi_f12 is 0.139912490060564\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.5899, -2.0933],\n",
      "        [ 1.5465,  0.0466],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "53\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.632\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5770.343\n",
      "expected info is tensor([[5.3174e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11556.0858, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.2404,  0.9907]])\n",
      "new data istensor([[-0.3413,  0.1722]])\n",
      "g_theta2 istensor([[-2.3268, -2.7662],\n",
      "        [ 2.7790,  0.5795],\n",
      "        [-2.9140,  2.9372]])\n",
      "p21val is 0.500996725070739\n",
      "pf12val is 0.631898247847418\n",
      "chi_f12 is 0.918053796077274\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.7108, -1.5288],\n",
      "        [-0.5451, -2.6424],\n",
      "        [-2.2403,  0.9916]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "54\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.637\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7123.211\n",
      "expected info is tensor([[2.8869e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(14262.1040, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8812,  1.4769]])\n",
      "new data istensor([[-0.1437,  0.0547]])\n",
      "g_theta2 istensor([[ 1.4947, -2.4165],\n",
      "        [-1.3429, -2.5201],\n",
      "        [-2.7680, -0.0339]])\n",
      "p21val is 0.909745243780577\n",
      "pf12val is 0.705495053719064\n",
      "chi_f12 is 0.697711037485626\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.2071, -1.8060],\n",
      "        [-2.0909,  1.7493],\n",
      "        [-1.8816,  1.4755]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "55\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.649\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 6822.544\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(13660.2457, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5169,  0.8711]])\n",
      "new data istensor([[-0.3819,  0.3913]])\n",
      "g_theta2 istensor([[ 1.5257, -1.2980],\n",
      "        [-2.7154,  2.3703],\n",
      "        [-2.7566,  2.3451]])\n",
      "p21val is 0.470943903105317\n",
      "pf12val is 0.823951078951057\n",
      "chi_f12 is 0.387288242080346\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.3900,  1.7695],\n",
      "        [ 0.7399,  1.1211],\n",
      "        [-1.5157,  0.8714]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "56\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.663\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3615.541\n",
      "expected info is tensor([[3.1448e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7245.5535, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.4945,  0.2517]])\n",
      "new data istensor([[-0.5788,  0.2010]])\n",
      "g_theta2 istensor([[-2.4118,  2.0038],\n",
      "        [ 1.5467,  1.9247],\n",
      "        [-0.4781,  2.0295]])\n",
      "p21val is 0.113782320633068\n",
      "pf12val is 0.421510319861195\n",
      "chi_f12 is 1.727822036366988\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.9505, -0.9042],\n",
      "        [ 2.1179, -2.4855],\n",
      "        [-2.4957,  0.2507]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "57\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.665\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5379.570\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10774.5622, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.4954,  0.2472]])\n",
      "new data istensor([[-0.5706,  0.2180]])\n",
      "g_theta2 istensor([[-1.9342, -0.7726],\n",
      "        [ 2.6418, -2.5324],\n",
      "        [-2.7221,  1.3779]])\n",
      "p21val is 0.572033685168024\n",
      "pf12val is 0.616792876192041\n",
      "chi_f12 is 0.966444012847028\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.3573, -0.8523],\n",
      "        [-0.3722,  1.1194],\n",
      "        [-2.4957,  0.2472]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "58\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.677\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 900.628\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1688.6219, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0250, -0.2846]])\n",
      "new data istensor([[-0.4199,  0.0373]])\n",
      "g_theta2 istensor([[-0.3582, -1.0577],\n",
      "        [ 0.1991,  0.2975],\n",
      "        [-2.1452, -0.6335]])\n",
      "p21val is 0.639318298783706\n",
      "pf12val is 0.320633577782689\n",
      "chi_f12 is 2.274912620193275\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6052, -2.6464],\n",
      "        [-1.9319, -1.2168],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "59\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.690\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7753.662\n",
      "expected info is tensor([[7.3785e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(15523.0900, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5990,  1.9178]])\n",
      "new data istensor([[-0.0753, -0.0240]])\n",
      "g_theta2 istensor([[ 1.4600, -2.5116],\n",
      "        [-1.6641, -1.1319],\n",
      "        [-2.9768,  2.8392]])\n",
      "p21val is 0.073912889811451\n",
      "pf12val is 0.023461856854371\n",
      "chi_f12 is 7.504758578172587\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.7745, -2.6009],\n",
      "        [-2.4713, -1.6476],\n",
      "        [-1.5976,  1.9178]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "60\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.668\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2124.917\n",
      "expected info is tensor([[2.8963e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3739.3253, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.7390,  3.0511]])\n",
      "new data istensor([[ 0.1106, -0.0333]])\n",
      "g_theta2 istensor([[ 1.4860, -2.4919],\n",
      "        [-1.4463, -1.7600],\n",
      "        [-0.9223,  1.1656]])\n",
      "p21val is 0.673397784810405\n",
      "pf12val is 0.729031831030159\n",
      "chi_f12 is 0.632075767924337\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.7606,  0.2056],\n",
      "        [-1.4963,  2.7010],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "61\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.689\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3820.759\n",
      "expected info is tensor([[0.0015]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5798.8982, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.4055,  3.0963]])\n",
      "new data istensor([[0.0479, 0.0090]])\n",
      "g_theta2 istensor([[ 1.6173,  0.2906],\n",
      "        [-0.9797,  2.8583],\n",
      "        [-0.9298,  0.6814]])\n",
      "p21val is 0.467060823050217\n",
      "pf12val is 0.623837455273131\n",
      "chi_f12 is 0.943730865780064\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.8550,  2.6861],\n",
      "        [ 2.2792, -1.5106],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "62\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.660\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4096.223\n",
      "expected info is tensor([[6.6836e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5997.8270, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1344,  3.1051]])\n",
      "new data istensor([[-0.0580, -0.0074]])\n",
      "g_theta2 istensor([[-1.1363,  2.6803],\n",
      "        [ 1.8845, -1.0329],\n",
      "        [-1.2026,  1.1774]])\n",
      "p21val is 0.845819190485476\n",
      "pf12val is 0.347357889146827\n",
      "chi_f12 is 2.114799299432931\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.4543,  2.6699],\n",
      "        [-0.4198,  1.3816],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "63\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.711\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 6677.912\n",
      "expected info is tensor([[2.8772e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(13371.3047, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.2402,  2.7109]])\n",
      "new data istensor([[-0.0805,  0.0261]])\n",
      "g_theta2 istensor([[-0.2462,  2.4278],\n",
      "        [ 0.3008,  0.2691],\n",
      "        [-1.1759,  1.1670]])\n",
      "p21val is 0.834516741635837\n",
      "pf12val is 0.721681720362762\n",
      "chi_f12 is 0.652342135612281\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.2589,  2.8947],\n",
      "        [-0.4160, -0.1462],\n",
      "        [-2.2398,  2.7101]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "64\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.716\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3252.548\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4400.2688, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0666,  3.0785]])\n",
      "new data istensor([[-0.1918,  0.0366]])\n",
      "g_theta2 istensor([[-1.8104,  2.3271],\n",
      "        [ 0.6947, -1.4926],\n",
      "        [-1.5162,  1.8105]])\n",
      "p21val is 0.108405457519459\n",
      "pf12val is 0.223918190166000\n",
      "chi_f12 is 2.992949032620568\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.2183,  1.3742],\n",
      "        [-2.1552,  2.4393],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "65\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.725\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1612.077\n",
      "expected info is tensor([[1.8331e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2777.7013, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0479,  0.3550]])\n",
      "new data istensor([[-0.3996,  0.0121]])\n",
      "g_theta2 istensor([[-0.9663,  0.8971],\n",
      "        [-2.7911,  2.9265],\n",
      "        [-1.1499,  2.8858]])\n",
      "p21val is 0.073318026135213\n",
      "pf12val is 0.943928992278034\n",
      "chi_f12 is 0.115408671420522\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.1006,  0.2716],\n",
      "        [-2.0100, -0.5883],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "66\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.711\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 774.858\n",
      "expected info is tensor([[9.3748e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1465.3473, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0219, -1.2425]])\n",
      "new data istensor([[-0.4387,  0.0125]])\n",
      "g_theta2 istensor([[ 2.4634,  0.5911],\n",
      "        [-0.4371, -1.9221],\n",
      "        [-1.1426,  2.8433]])\n",
      "p21val is 0.816950601229774\n",
      "pf12val is 0.713828351062495\n",
      "chi_f12 is 0.674225500437531\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.4153, -0.9556],\n",
      "        [-2.2718,  0.3127],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "67\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.731\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1663.404\n",
      "expected info is tensor([[3.7709e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3339.8642, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7114, -1.0038]])\n",
      "new data istensor([[-0.4713,  0.0376]])\n",
      "g_theta2 istensor([[ 2.1433, -1.4345],\n",
      "        [ 1.0393, -1.5889],\n",
      "        [-1.1362,  2.8468]])\n",
      "p21val is 0.375837495899206\n",
      "pf12val is 0.980933024679872\n",
      "chi_f12 is 0.038502188490058\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.7934, -1.1417],\n",
      "        [-2.3031,  2.3154],\n",
      "        [-2.7099, -1.0043]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "68\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.732\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2993.894\n",
      "expected info is tensor([[4.5172e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6002.3708, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7464, -0.7419]])\n",
      "new data istensor([[-0.4622,  0.0358]])\n",
      "g_theta2 istensor([[-1.1071, -1.4828],\n",
      "        [-1.6829,  2.0082],\n",
      "        [-2.6587, -1.7637]])\n",
      "p21val is 0.767188736666707\n",
      "pf12val is 0.712540243726896\n",
      "chi_f12 is 0.677837772021379\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.8778,  0.0996],\n",
      "        [-2.6656, -2.2559],\n",
      "        [-2.7453, -0.7418]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "69\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.752\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2522.563\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4327.7005, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0605, -0.5768]])\n",
      "new data istensor([[-0.4786, -0.0096]])\n",
      "g_theta2 istensor([[-1.8008,  0.5434],\n",
      "        [-1.3995, -2.4678],\n",
      "        [-2.1566, -1.4870]])\n",
      "p21val is 0.561672114441504\n",
      "pf12val is 0.057569299891958\n",
      "chi_f12 is 5.709531682711440\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.0105,  2.2732],\n",
      "        [ 2.4141, -2.3853],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "70\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.740\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2596.228\n",
      "expected info is tensor([[2.1664e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4354.4754, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0653,  0.0090]])\n",
      "new data istensor([[-0.4528,  0.0243]])\n",
      "g_theta2 istensor([[-0.9993,  2.3575],\n",
      "        [ 2.6713, -2.2412],\n",
      "        [-1.1205,  2.8250]])\n",
      "p21val is 0.757537234411001\n",
      "pf12val is 0.729690459626926\n",
      "chi_f12 is 0.630269725346550\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.2336, -1.3852],\n",
      "        [-0.5204, -1.0092],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "71\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.767\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3076.235\n",
      "expected info is tensor([[1.6483e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4665.2177, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0866,  0.6168]])\n",
      "new data istensor([[-0.4044, -0.0120]])\n",
      "g_theta2 istensor([[ 1.7826, -1.0378],\n",
      "        [-0.3652, -1.1153],\n",
      "        [-1.1072,  2.8254]])\n",
      "p21val is 0.124654960519059\n",
      "pf12val is 0.118673550000719\n",
      "chi_f12 is 4.262757665712885\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.0570,  2.8521],\n",
      "        [-0.7555,  2.9784],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "72\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.772\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1406.367\n",
      "expected info is tensor([[3.8491e-10]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1845.7038, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0700, -2.8517]])\n",
      "new data istensor([[-0.5014, -0.0078]])\n",
      "g_theta2 istensor([[ 0.8087,  2.5525],\n",
      "        [ 0.1613,  2.4964],\n",
      "        [-1.1014,  2.8294]])\n",
      "p21val is 0.953135292513316\n",
      "pf12val is 0.479235376711249\n",
      "chi_f12 is 1.471126820766307\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9690, -1.0238],\n",
      "        [-2.5755, -1.6045],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "current sol istensor([[-3.0700, -2.8517]])\n",
      "Success is False and failure is True after 73 iterations\n"
     ]
    }
   ],
   "source": [
    "loc_sample = loc_sample0.clone()\n",
    "iter_hp = 50\n",
    "iter_design = 100\n",
    "iter_param = 100\n",
    "num_base_kernels = 1\n",
    "max_iter = 50\n",
    "\n",
    "\n",
    "f_target = Tensor([[-1., -1.]]).reshape(2,1) \n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    " #np.random.random_sample((loc_size,2))\n",
    "#loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "#train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "\n",
    "#loc_sample = Tensor([[0.0, 0.1], [0.0, -0.1]]) #T\n",
    "# loc_x = (-1.5 + 2.)  * np.random.random_sample((loc_size,1)) +2.\n",
    "\n",
    "# # loc_y = (2. - 1.5)  * np.random.random_sample((loc_size,1)) - 1.5\n",
    "# # loc = np.concatenate((loc_x, loc_y), 1)\n",
    "print(loc_sample)\n",
    "\n",
    "\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten()\n",
    "\n",
    "data_fit_vec = torch.empty((1,1))\n",
    "entropy_vec = torch.empty((1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "vec_x = x0.clone() #Tensor(np.array([0.0,0.0])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "var_vec = torch.zeros([max_iter, 1])\n",
    "p21_vec = torch.empty((1,1))\n",
    "\n",
    "lr_new = .01\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "show_TTRBox = False\n",
    "iter = 0    \n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "patience = 0.0\n",
    "patience_f = 0.0\n",
    "patience_2 = 0.0\n",
    "checking_model = False\n",
    "model_double_check = False\n",
    "while(SUCCESS == False and FAILURE == False):\n",
    "    print(iter)\n",
    "    model_double_check = False\n",
    "    if (checking_model == False):\n",
    "        print('START HYPERPARAMETERS optimization')\n",
    "        if (iter == 0):\n",
    "            cur_model = None\n",
    "            cur_likelihood = None\n",
    "\n",
    "\n",
    "        loc_sample_old = loc_sample.clone()\n",
    "        x0_old = x0.clone()\n",
    "        model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "\n",
    "\n",
    "        print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "   \n",
    "    \n",
    "    x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new,noise_value)\n",
    "  \n",
    "    cur_model = model\n",
    "    cur_likelihood = likelihood\n",
    "    \n",
    "  \n",
    "    lower_bound = torch.zeros(pf1.shape)\n",
    "    upper_bound = torch.zeros(pf1.shape)\n",
    "        \n",
    "    for i in range(pf1.shape[0]):\n",
    "        lower_bound[i] = pf1[i] -  torch.sqrt(Qf12[i,i])\n",
    "        upper_bound[i] = pf1[i] +  torch.sqrt(Qf12[i,i])\n",
    "#     d = torch.sqrt(gpytorch.inv_quad(Qf1, f_target - pf1))\n",
    "#     print(d)\n",
    "    SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    \n",
    "    \n",
    "    entropy = ( 0.5 * torch.log( torch.det(Qf1.evaluate()) / torch.det(Qf12.evaluate()) ) ).reshape(1,1)\n",
    "    \n",
    "    print('expected info is '+str(entropy))\n",
    "    print('mohabb disatance is' + str(Qf12.inv_quad(f_target - pf1)))\n",
    "    if not SUCCESS:\n",
    "    \n",
    "    #var_vec[iter] = var\n",
    "        \n",
    "        \n",
    "\n",
    "         #np.random.random_sample((loc_size,2))\n",
    "\n",
    "        \n",
    "\n",
    "        new_data = vfield_(g_theta2.detach())  \n",
    "        agg_data12 = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "        g_theta12= torch.cat([g_theta1, g_theta2.detach()], 0)\n",
    "        new_data_x = vfield_(x0_new.detach() )  \n",
    "        print('current sol is'+str(x0_new.detach()))\n",
    "        print('new data is' + str(new_data_x))\n",
    "        print('g_theta2 is' + str(g_theta2.detach()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            if iter >= 0:\n",
    "                \n",
    "                \n",
    "                p21 = likelihood.get_p21(g_theta1, g_theta2.detach(), agg_data, model, noise_value)\n",
    "                \n",
    "#                 Q21 = Q21 + noise_value*torch.eye(Q21.shape[0])\n",
    "                chi_21 = (Q21).inv_quad(new_data.flatten() - p21.reshape(new_data.flatten().shape))\n",
    "                p_val = 1. - stats.chi2.cdf(chi_21, Q21.shape[0])\n",
    "                pf12 = likelihood.get_pf12(Q21,g_theta1, g_theta2.detach(), x0_new.detach(), new_data.flatten(), pf1, p21, model, noise_value)\n",
    "               \n",
    "                #Qf12 = Qf12 + \n",
    "                chi_f12 = (Qf12 + noise_value*torch.eye(Qf12.shape[0])).inv_quad(new_data_x.flatten() - pf12.reshape(new_data_x.flatten().shape))\n",
    "                p_val_f12 = 1. - stats.chi2.cdf(chi_f12, Qf12.shape[0])\n",
    "                print('p21val is %.15f' %p_val)\n",
    "                p21_vec = torch.cat([p21_vec, Tensor([p_val]).reshape(1,1)], 0)\n",
    "                print('pf12val is %.15f' %p_val_f12)\n",
    "                print('chi_f12 is %.15f' %chi_f12 )\n",
    "                \n",
    "                if (p_val < 0.01):# or p_val_f12 < 0.001:\n",
    "                    model_double_check = True\n",
    "                    checking_model = True\n",
    "                    patience = patience+1\n",
    "                    print('patience is %.3f' %patience)\n",
    "\n",
    "                if (model_double_check == True):\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_sample.shape[0],2)) + vf.low)\n",
    "                    sum = torch.zeros(2, 2) #replace with num_tasks\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_old).t(), (g_theta2.detach()[i] - x0_old) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum #+ torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_old, covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_old],0)\n",
    "                    \n",
    "                    x0 = x0_old #Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    if (patience >= 2):# or patience_2 >= 2 or patience_f >= 2):\n",
    "                        entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                        data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                        iter = iter + 1\n",
    "                        patience = 0\n",
    "#                         patience_2 = 0\n",
    "#                         patience_f = 0\n",
    "                        model_double_check = False\n",
    "                        checking_model = False\n",
    "                        num_base_kernels = num_base_kernels + 1\n",
    "                        print('adding complexity to model')\n",
    "                        print('num base is' + str(num_base_kernels))\n",
    "#     #                         \n",
    "                        loc_sample = loc_sample_old\n",
    "                        #x0 = x0_old\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                        print('acquiring, new size is' + str(g_theta1.shape[0]))\n",
    "                 \n",
    "                    #iter_hp = iter_hp + 10\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                    entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                    data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                    model_double_check = False\n",
    "                    iter = iter + 1\n",
    "                    patience = 0\n",
    "                    patience_2 = 0\n",
    "                    patience_f = 0\n",
    "                    checking_model = False\n",
    "                    if (entropy < 1e-4 * tol_vector[0,0]):\n",
    "                        FAILURE = True\n",
    "                    \n",
    "                    x0 = (x0_new.detach())# + torch.randn(x0_new.detach().size()) * .001)#/torch.norm(x0_new.detach())\n",
    "                    sum = torch.zeros(2, 2)\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum# + torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_new.detach(), covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = loc_sample#/torch.norm(loc_sample)\n",
    "                    #loc_sample = 2. *  (loc_sample - torch.min(loc_sample)) / (torch.max(loc_sample) - torch.min(loc_sample)) - 1.\n",
    "                    #loc_sample[0] = x0_new.detach() #+ torch.randn(x0_new.detach().size()) * .001 #g_theta2.detach() #loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_size,2)) + vf.low)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_new.detach()],0)\n",
    "                    for i in range(loc_sample.shape[0]):\n",
    "                        if loc_sample[i,0] < -3. or loc_sample[i,0] > 3. or loc_sample[i,1] < -3. or loc_sample[i,1] > 3.:\n",
    "                            print('samples escaped box')\n",
    "                            loc_sample[i] = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    \n",
    "                    \n",
    "#                     if p_val > 0.99 and p_val_f12 > 0.99:\n",
    "#                         num_base_kernels = max(num_base_kernels - 1, 3)\n",
    "                        #iter_hp = iter_hp - 10\n",
    "                    chi_f_target = (Qf12 ).inv_quad(f_target - pf1)\n",
    "                    p_val_f_target = 1. - stats.chi2.cdf(chi_f_target, Qf12.shape[0])\n",
    "                    print('p_val_ftarget is '+str(p_val_f_target))\n",
    "                    if (p_val_f_target > .95):\n",
    "                        print('acquiring target point becuse p_val_ftarget is '+str(p_val_f_target))\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "        \n",
    "\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                    else:\n",
    "#                         x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "#                         g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                       \n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                        \n",
    "                    if x0_new.detach()[0,0] < -3. or x0_new.detach()[0,0] > 3. or x0_new.detach()[0,1] < -3. or x0_new.detach()[0,1] > 3.:\n",
    "#                         x0 = Tensor(np.array([0.0,-1.0])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = x0.reshape(1,2) \n",
    "                        x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = x0.reshape(1,2)\n",
    "                        #x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                 \n",
    "             #       loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "                        loc_sample[-1] = x0 #(x0_new.detach()) \n",
    "                    print('new 2 points')\n",
    "                    print(loc_sample)\n",
    "                  \n",
    " #                    agg_data  = (agg_data  - agg_data.mean())/agg_data .std(dim=-1, keepdim=True)\n",
    "#                     g_theta1 = (g_theta1 - g_theta1.mean())/g_theta1.std(dim=-2, keepdim=True)\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            #clear_output(wait=False)\n",
    "           \n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "print('current sol is'+str(x0_new.detach()))\n",
    "    \n",
    "print('Success is ' + str(SUCCESS) + ' and failure is ' + str(FAILURE)+' after '+ str(iter) + ' iterations')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5722],\n",
      "        [-0.0274]], grad_fn=<CopySlices>)\n",
      "tensor([[-0.5151],\n",
      "        [ 0.0320]], grad_fn=<CopySlices>)\n",
      "tensor([[-1.0010],\n",
      "        [-1.0010]])\n",
      "tensor([[-0.9990],\n",
      "        [-0.9990]])\n",
      "tensor([[-0.5436],\n",
      "        [ 0.0023]], grad_fn=<ReshapeAliasBackward0>)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target - 0.001)\n",
    "print(f_target + 0.001)\n",
    "print(pf1)\n",
    "print(num_base_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('ticks')\n",
    "#plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "p21_vec_plot = p21_vec.detach()[1:]\n",
    "f, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "#print(p21_vec_plot)\n",
    "ax.plot(p21_vec_plot,'s',color = 'blue', markersize=6)\n",
    "ax.axhline(.01,linestyle = '--',color = 'red', markersize=12, alpha = 1.0)\n",
    "ax.set_xlim(-0.3, p21_vec_plot.shape[0])\n",
    "ax.set_ylim(-0.3, 1.)\n",
    "#ax.set_yscale('log')\n",
    "plt.xticks(np.arange(0, 21, step=5.))\n",
    "ax.tick_params(labelsize='small', width=3)\n",
    "ax.set_xlabel('# of model checks')\n",
    "ax.set_ylabel('Q')\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.03, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(1.5,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.04, 0.34), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(0.5,        #x start point\n",
    "             0.17,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.1,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "#############################\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.16, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(11.5,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.11, 0.34), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(8.5,        #x start point\n",
    "             0.17,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.1,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.legend(['Q', 'Threshold for Q ($0.01$)'], loc = 'lower right')\n",
    "plt.savefig('figures/qvalue_fail.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (73,) and torch.Size([74, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vd/tt7x9mbn35b5wxq8_t6p0mt80000gn/T/ipykernel_77586/2463639934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtight_layout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentropy_vec_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#ax1.set_yscale('log')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (73,) and torch.Size([74, 1])"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('ticks')\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(18, 8), tight_layout=True)\n",
    "\n",
    "ax1.plot(np.array(range(1,iter+1)), (entropy_vec_plot), '--o', color = 'blue', markersize=12)\n",
    "#ax1.set_yscale('log')\n",
    "# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2.plot(np.array(range(1,iter+1)), data_fit_vec_plot, '--o', color = 'red', markersize=12)\n",
    "\n",
    "ax1.set_xlabel('iteration #', size=24)\n",
    "ax2.set_xlabel('iteration #', size=24)\n",
    "ax1.set_ylabel('Expected information (log scale)', size = 24)\n",
    "ax2.set_ylabel('Data fit', size = 24)\n",
    "#ax2.yfmt.set_useOffset(10000)\n",
    "\n",
    "plt.savefig('figures/expectedinfo_vs_datafit_acquiring_fail.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'v', color = 'black',markersize=8, alpha = 0.3)\n",
    "ax.plot(vec_x[-1,0], vec_x[-1,1],'rs',markersize=8)\n",
    "#ax.plot(0.8731, 0.5664,'gD',markersize=10)\n",
    "ax.set_title('TAD solution (converge/fail)')\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "plt.savefig('figures/tad_sol_allpoints_fail.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 * 2 + 4 + 3 *31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
