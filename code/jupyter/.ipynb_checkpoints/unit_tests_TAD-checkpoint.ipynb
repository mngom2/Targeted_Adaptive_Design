{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = torch.linspace(0, 1, 300)\n",
    "k = 100\n",
    "N = sample_x.shape[0]\n",
    "indices = torch.randperm(N)[:k]\n",
    "train_x = sample_x[indices]\n",
    "#train_x = train_x.reshape(train_x.shape[0],1)\n",
    "\n",
    "train_y = torch.stack([\n",
    "    torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,\n",
    "    torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,\n",
    "], -1)\n",
    "\n",
    "def g_theta(theta):\n",
    "    return torch.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "#         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "        self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "    model = MultitaskGPModel(g_theta1, agg_data.reshape(g_theta1.shape[0],2), likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "        output_ll = likelihood(output)\n",
    "\n",
    "        loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        print('Iter %d/%d - Loss hyperparam: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "        optimizer.step()\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class param_opti(nn.Module):\n",
    "    def __init__(self, _theta2,k):\n",
    "        super(param_opti, self).__init__()\n",
    "        N = _theta2.shape[0]\n",
    "        indices = torch.randperm(N)[:k]\n",
    "        sampled_values= _theta2[indices]\n",
    "        sampled_values = sampled_values.reshape(sampled_values.shape[0], 1)\n",
    "        self.g_theta2 = nn.Parameter(sampled_values)\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return ((self.g_theta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_param_opti(x,g_theta1, agg_data, model, likelihood, training_iter):\n",
    "    m = x.shape[0]\n",
    "    f_target = 0.5 *torch.sqrt(Tensor([2.])) * torch.ones(2 * m, 1)  #torch.zeros(2,1)\n",
    "    \n",
    "#     f_target[0] = 0.5 *torch.sqrt(Tensor([2.]))\n",
    "#     f_target[1] = 0.5 * torch.sqrt(Tensor([2.]))\n",
    "    #f_target = f_target.reshape(2,1)\n",
    "\n",
    "    \n",
    "    _par = param_opti(sample_x,10)\n",
    "\n",
    "    optimizer = torch.optim.Adam(_par.parameters(), lr=0.1)\n",
    "    \n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        g_theta2 = _par.forward()\n",
    "        \n",
    "        loss1, lb, ub = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model.mean_module, model.covar_module, likelihood)\n",
    "        loss1 = -1. * loss1\n",
    "        loss1.backward(retain_graph=True)\n",
    "        print('Iter %d/%d - Loss theta2: %.3f' % (i + 1, training_iter, loss1.item()))\n",
    "        optimizer.step()\n",
    "    return loss1, g_theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class design_opti(nn.Module):\n",
    "    def __init__(self, _x):\n",
    "        super(design_opti, self).__init__()\n",
    "        self.x_design = nn.Parameter(_x)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.x_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0, g_theta1, agg_data, model, likelihood, training_iter_out, training_iter_in):\n",
    "    design = design_opti(x0)\n",
    "    optimizer = torch.optim.Adam(design.parameters(), lr=0.1)\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for i in range(training_iter_out):\n",
    "            optimizer.zero_grad()\n",
    "            x_d = design.forward()\n",
    "            loss2, g_theta2 = conduct_param_opti(x_d, g_theta1, agg_data, model, likelihood,training_iter_in)\n",
    "            loss2.backward(retain_graph=True)\n",
    "            print('Iter %d/%d - Loss design: %.3f' % (i + 1, training_iter_out, loss2.item()))\n",
    "            optimizer.step()\n",
    "    return x_d, g_theta2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START HYPERPARAMETERS optimization\n",
      "Iter 1/50 - Loss hyperparam: 47.435\n",
      "Iter 2/50 - Loss hyperparam: 33.364\n",
      "Iter 3/50 - Loss hyperparam: 19.734\n",
      "Iter 4/50 - Loss hyperparam: 6.729\n",
      "Iter 5/50 - Loss hyperparam: -5.858\n",
      "Iter 6/50 - Loss hyperparam: -18.108\n",
      "Iter 7/50 - Loss hyperparam: -30.135\n",
      "Iter 8/50 - Loss hyperparam: -42.393\n",
      "Iter 9/50 - Loss hyperparam: -55.226\n",
      "Iter 10/50 - Loss hyperparam: -67.933\n",
      "Iter 11/50 - Loss hyperparam: -79.065\n",
      "Iter 12/50 - Loss hyperparam: -87.447\n",
      "Iter 13/50 - Loss hyperparam: -92.928\n",
      "Iter 14/50 - Loss hyperparam: -96.661\n",
      "Iter 15/50 - Loss hyperparam: -100.429\n",
      "Iter 16/50 - Loss hyperparam: -105.595\n",
      "Iter 17/50 - Loss hyperparam: -112.684\n",
      "Iter 18/50 - Loss hyperparam: -121.511\n",
      "Iter 19/50 - Loss hyperparam: -131.332\n",
      "Iter 20/50 - Loss hyperparam: -140.940\n",
      "Iter 21/50 - Loss hyperparam: -149.090\n",
      "Iter 22/50 - Loss hyperparam: -155.425\n",
      "Iter 23/50 - Loss hyperparam: -160.643\n",
      "Iter 24/50 - Loss hyperparam: -165.532\n",
      "Iter 25/50 - Loss hyperparam: -170.402\n",
      "Iter 26/50 - Loss hyperparam: -175.091\n",
      "Iter 27/50 - Loss hyperparam: -179.311\n",
      "Iter 28/50 - Loss hyperparam: -183.071\n",
      "Iter 29/50 - Loss hyperparam: -186.482\n",
      "Iter 30/50 - Loss hyperparam: -189.295\n",
      "Iter 31/50 - Loss hyperparam: -191.214\n",
      "Iter 32/50 - Loss hyperparam: -192.412\n",
      "Iter 33/50 - Loss hyperparam: -193.297\n",
      "Iter 34/50 - Loss hyperparam: -194.102\n",
      "Iter 35/50 - Loss hyperparam: -194.737\n",
      "Iter 36/50 - Loss hyperparam: -194.983\n",
      "Iter 37/50 - Loss hyperparam: -194.979\n",
      "Iter 38/50 - Loss hyperparam: -194.901\n",
      "Iter 39/50 - Loss hyperparam: -194.656\n",
      "Iter 40/50 - Loss hyperparam: -194.286\n",
      "Iter 41/50 - Loss hyperparam: -194.049\n",
      "Iter 42/50 - Loss hyperparam: -193.998\n",
      "Iter 43/50 - Loss hyperparam: -193.919\n",
      "Iter 44/50 - Loss hyperparam: -193.594\n",
      "Iter 45/50 - Loss hyperparam: -193.251\n",
      "Iter 46/50 - Loss hyperparam: -193.326\n",
      "Iter 47/50 - Loss hyperparam: -193.565\n",
      "Iter 48/50 - Loss hyperparam: -193.531\n",
      "Iter 49/50 - Loss hyperparam: -193.398\n",
      "Iter 50/50 - Loss hyperparam: -193.531\n",
      "END HYPERPARAMETERS optimization\n",
      "Iter 1/10 - Loss theta2: -5.236\n",
      "Iter 2/10 - Loss theta2: -5.226\n",
      "Iter 3/10 - Loss theta2: -5.240\n",
      "Iter 4/10 - Loss theta2: -5.244\n",
      "Iter 5/10 - Loss theta2: -5.238\n",
      "Iter 6/10 - Loss theta2: -5.237\n",
      "Iter 7/10 - Loss theta2: -5.240\n",
      "Iter 8/10 - Loss theta2: -5.245\n",
      "Iter 9/10 - Loss theta2: -5.245\n",
      "Iter 10/10 - Loss theta2: -5.242\n",
      "Iter 1/10 - Loss design: -5.242\n",
      "Iter 1/10 - Loss theta2: 70.663\n",
      "Iter 2/10 - Loss theta2: 73.520\n",
      "Iter 3/10 - Loss theta2: 70.396\n",
      "Iter 4/10 - Loss theta2: 70.506\n",
      "Iter 5/10 - Loss theta2: 70.507\n",
      "Iter 6/10 - Loss theta2: 70.350\n",
      "Iter 7/10 - Loss theta2: 70.166\n",
      "Iter 8/10 - Loss theta2: 70.053\n",
      "Iter 9/10 - Loss theta2: 70.036\n",
      "Iter 10/10 - Loss theta2: 70.080\n",
      "Iter 2/10 - Loss design: 70.080\n",
      "Iter 1/10 - Loss theta2: 28.960\n",
      "Iter 2/10 - Loss theta2: 28.826\n",
      "Iter 3/10 - Loss theta2: 28.563\n",
      "Iter 4/10 - Loss theta2: 28.762\n",
      "Iter 5/10 - Loss theta2: 28.649\n",
      "Iter 6/10 - Loss theta2: 28.431\n",
      "Iter 7/10 - Loss theta2: 28.284\n",
      "Iter 8/10 - Loss theta2: 28.279\n",
      "Iter 9/10 - Loss theta2: 28.289\n",
      "Iter 10/10 - Loss theta2: 28.298\n",
      "Iter 3/10 - Loss design: 28.298\n",
      "Iter 1/10 - Loss theta2: 10.635\n",
      "Iter 2/10 - Loss theta2: 9.648\n",
      "Iter 3/10 - Loss theta2: 9.709\n",
      "Iter 4/10 - Loss theta2: 9.651\n",
      "Iter 5/10 - Loss theta2: 9.605\n",
      "Iter 6/10 - Loss theta2: 9.588\n",
      "Iter 7/10 - Loss theta2: 9.594\n",
      "Iter 8/10 - Loss theta2: 9.604\n",
      "Iter 9/10 - Loss theta2: 9.607\n",
      "Iter 10/10 - Loss theta2: 9.604\n",
      "Iter 4/10 - Loss design: 9.604\n",
      "Iter 1/10 - Loss theta2: 4.674\n",
      "Iter 2/10 - Loss theta2: 5.373\n",
      "Iter 3/10 - Loss theta2: 4.358\n",
      "Iter 4/10 - Loss theta2: 4.351\n",
      "Iter 5/10 - Loss theta2: 4.340\n",
      "Iter 6/10 - Loss theta2: 4.310\n",
      "Iter 7/10 - Loss theta2: 4.295\n",
      "Iter 8/10 - Loss theta2: 4.295\n",
      "Iter 9/10 - Loss theta2: 4.300\n",
      "Iter 10/10 - Loss theta2: 4.303\n",
      "Iter 5/10 - Loss design: 4.303\n",
      "Iter 1/10 - Loss theta2: 2.523\n",
      "Iter 2/10 - Loss theta2: 3.188\n",
      "Iter 3/10 - Loss theta2: 2.532\n",
      "Iter 4/10 - Loss theta2: 2.535\n",
      "Iter 5/10 - Loss theta2: 2.540\n",
      "Iter 6/10 - Loss theta2: 2.527\n",
      "Iter 7/10 - Loss theta2: 2.520\n",
      "Iter 8/10 - Loss theta2: 2.519\n",
      "Iter 9/10 - Loss theta2: 2.521\n",
      "Iter 10/10 - Loss theta2: 2.522\n",
      "Iter 6/10 - Loss design: 2.522\n",
      "Iter 1/10 - Loss theta2: 1.819\n",
      "Iter 2/10 - Loss theta2: 1.883\n",
      "Iter 3/10 - Loss theta2: 1.817\n",
      "Iter 4/10 - Loss theta2: 1.821\n",
      "Iter 5/10 - Loss theta2: 1.820\n",
      "Iter 6/10 - Loss theta2: 1.817\n",
      "Iter 7/10 - Loss theta2: 1.816\n",
      "Iter 8/10 - Loss theta2: 1.816\n",
      "Iter 9/10 - Loss theta2: 1.817\n",
      "Iter 10/10 - Loss theta2: 1.817\n",
      "Iter 7/10 - Loss design: 1.817\n",
      "Iter 1/10 - Loss theta2: 1.502\n",
      "Iter 2/10 - Loss theta2: 1.501\n",
      "Iter 3/10 - Loss theta2: 1.500\n",
      "Iter 4/10 - Loss theta2: 1.500\n",
      "Iter 5/10 - Loss theta2: 1.500\n",
      "Iter 6/10 - Loss theta2: 1.499\n",
      "Iter 7/10 - Loss theta2: 1.499\n",
      "Iter 8/10 - Loss theta2: 1.499\n",
      "Iter 9/10 - Loss theta2: 1.499\n",
      "Iter 10/10 - Loss theta2: 1.499\n",
      "Iter 8/10 - Loss design: 1.499\n",
      "Iter 1/10 - Loss theta2: 1.338\n",
      "Iter 2/10 - Loss theta2: 1.395\n",
      "Iter 3/10 - Loss theta2: 1.338\n",
      "Iter 4/10 - Loss theta2: 1.338\n",
      "Iter 5/10 - Loss theta2: 1.338\n",
      "Iter 6/10 - Loss theta2: 1.337\n",
      "Iter 7/10 - Loss theta2: 1.337\n",
      "Iter 8/10 - Loss theta2: 1.337\n",
      "Iter 9/10 - Loss theta2: 1.337\n",
      "Iter 10/10 - Loss theta2: 1.337\n",
      "Iter 9/10 - Loss design: 1.337\n",
      "Iter 1/10 - Loss theta2: 1.245\n",
      "Iter 2/10 - Loss theta2: 1.268\n",
      "Iter 3/10 - Loss theta2: 1.245\n",
      "Iter 4/10 - Loss theta2: 1.245\n",
      "Iter 5/10 - Loss theta2: 1.245\n",
      "Iter 6/10 - Loss theta2: 1.245\n",
      "Iter 7/10 - Loss theta2: 1.245\n",
      "Iter 8/10 - Loss theta2: 1.245\n",
      "Iter 9/10 - Loss theta2: 1.245\n",
      "Iter 10/10 - Loss theta2: 1.245\n",
      "Iter 10/10 - Loss design: 1.245\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[110, 2]' is invalid for input of size 420",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8ab459c8c679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'START HYPERPARAMETERS optimization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_theta11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magg_data1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_hp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'END HYPERPARAMETERS optimization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx0_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconduct_design_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magg_data1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_design\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8cb43eb7cb5a>\u001b[0m in \u001b[0;36mhyper_opti\u001b[0;34m(g_theta1, agg_data, training_iter)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhyper_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_theta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvvll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorProductLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultitaskGPModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_theta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_theta1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[110, 2]' is invalid for input of size 420"
     ]
    }
   ],
   "source": [
    "iter_hp = 50\n",
    "iter_design = 10\n",
    "iter_param = 10\n",
    "g_theta1 = []\n",
    "agg_data = []\n",
    "g_theta1.append((x_train).flatten())\n",
    "g_theta11 = torch.cat(g_theta1)\n",
    "g_theta11= g_theta11.unsqueeze(-1)\n",
    "\n",
    "agg_data.append(y_train.flatten())\n",
    "agg_data1 = torch.cat(agg_data)\n",
    "\n",
    "x0 = Tensor([1./(8.)])\n",
    "x0 = x0.reshape(x0.shape[0],1)\n",
    "\n",
    "# SUCCESS = False\n",
    "# FAILURE = False\n",
    "# while(SUCCESS == False && FAILURE == False):\n",
    "for i in range(10):\n",
    "    print('START HYPERPARAMETERS optimization')\n",
    "    model, likelihood = hyper_opti(g_theta11,agg_data1,iter_hp)\n",
    "    print('END HYPERPARAMETERS optimization')\n",
    "    x0_new, g_theta2 = conduct_design_opti(x0, g_theta11,agg_data1, model, likelihood, iter_design, iter_param)\n",
    "    \n",
    "    x0 = x0_new\n",
    "\n",
    "    \n",
    "    \n",
    "    g_theta1.append(g_theta2.flatten())\n",
    "    g_theta11 = torch.cat(g_theta1)\n",
    "    \n",
    "    y_train_new = torch.stack([\n",
    "    torch.sin(g_theta2 * (2 * math.pi)) + torch.randn(g_theta2.size()) * 0.2,\n",
    "    torch.cos(g_theta2 * (2 * math.pi)) + torch.randn(g_theta2.size()) * 0.2,\n",
    "], -1)\n",
    "    agg_data.append(y_train_new.flatten())\n",
    "    agg_data1 = torch.cat(agg_data)\n",
    "    g_theta11= g_theta11.unsqueeze(-1)\n",
    "\n",
    "print(x0_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = Tensor([-4.1162])\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "pr = likelihood((model(x0)))\n",
    "print(pr.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covar_module(x0,x0).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covar_module.forward(x0,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((model(x0)).covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.linspace(0, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m =1\n",
    "f_target = 0.5 *torch.sqrt(Tensor([2.])) * torch.ones(2 * m, 1)  #torch.zeros(2,1)\n",
    "print(f_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
