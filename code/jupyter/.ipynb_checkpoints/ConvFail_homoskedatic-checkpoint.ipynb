{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(\"..\")\n",
    "from LBFGS import FullBatchLBFGS\n",
    "from kernels import vvkernels as vvk, sep_vvkernels as svvk, vvk_rbfkernel as vvk_rbf\n",
    "from means import vvmeans as vvm\n",
    "from likelihood import vvlikelihood as vvll\n",
    "from mlikelihoods import MarginalLogLikelihood as exmll\n",
    "from predstrategies import GPprediction\n",
    "from utils import ObjFun, get_vertices, stopping_criteria\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If model is bad, initialize at previous 2 sample!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=30)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=28)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=24)    # legend fontsize\n",
    "plt.rc('font', size=24)          # controls default text sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "\n",
    "We sample from $$V_1(x_1, x_2) = 3(1 - x_1)^2 e^{-x_1^2 - (x_2 +1)^2} - 10 (x_1/5 - x_1 ^3 - x_2^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- (x_1 + 2) ^2 - x_2^2} + 0.5(2x_1 + x_2)$$\n",
    "$$V_2(x_1, x_2) = 3(1 +x_2)^2 e^{-x_2^2 - (x_1 +1)^2} - 10 (-x_2/5 + x_2 ^3 + x_1^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- ( 2- x_2) ^2 - x_1^2} + 0.5(2x_1 + x_2)$$\n",
    "\n",
    "where $(x_1, x_2) \\in [-3, 3]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3380, 0.3502], dtype=torch.float32)\n",
      "tensor([[ 1.6029, -1.9832],\n",
      "        [ 1.9337, -1.5859],\n",
      "        [ 1.1758, -1.3692],\n",
      "        [ 1.0292, -1.1847]])\n"
     ]
    }
   ],
   "source": [
    "vf = ObjFun()\n",
    "f_target = vf.tgt_vec\n",
    "print(f_target)\n",
    "sample_size = 4\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -3.\n",
    "vf.high = 3.\n",
    "\n",
    "high_minus_low = vf.high- vf.low\n",
    "#high_minus_low = -\n",
    "def g_theta(sample_size, D):\n",
    "    loc_x = (2. - 1.0 )  * np.random.random_sample((sample_size,1)) + 1.0\n",
    "    \n",
    "    loc_y = (2.  -1.0)  * np.random.random_sample((sample_size,1)) - 2.\n",
    "    loc = np.concatenate((loc_x, loc_y), 1)\n",
    "    #loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "#train_x = Tensor([[-1.5, 1.5], [-1.5, 1.3]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    \n",
    "    out = vf(x[:,0], x[:,1]) + torch.randn(Tensor(vf(x[:,0], x[:,1])).size()) * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)\n",
    "\n",
    "# print(train_y)\n",
    "# train_y = (train_y - train_y.mean())/train_y.std(dim=-2, keepdim=True)\n",
    "# train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "# print(train_y)\n",
    "# print(train_y.std(dim=-2, keepdim=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5540983606557375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0779/ 0.0305\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP model initialization\n",
    "We inialize the GP model following https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)  #vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)#vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)  # \n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters oprimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels,noise_value, current_model = None, current_likelihood = None):\n",
    "    noises = torch.ones(agg_data.shape[0]) * (noise_value) #  torch.zeros(agg_data.shape[0]) # \n",
    "    noises = noises.reshape(g_theta1.shape[0], 2)\n",
    "    \n",
    "#     if (current_model is not None):\n",
    "#         likelihood = current_likelihood #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)  #\n",
    "\n",
    "#         model = current_model#.get_fantasy_model(g_theta1, agg_data) #MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#         model.set_train_data(g_theta1, agg_data,  strict=False)\n",
    "#     else:\n",
    "#         likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2)#vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #\n",
    "#         model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "        \n",
    "    likelihood =  vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2) #\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.double()\n",
    "    likelihood.double()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #, weight_decay=0.001)  # Includes GaussianLikelihood parameters\n",
    "    mll = exmll(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, chi_square = mll(agg_data,g_theta1, model, likelihood, noise_value)\n",
    "        loss = -1. * loss\n",
    "#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \n",
    "        #print('loss is %.3f' %loss)\n",
    "        df = agg_data.shape[0]\n",
    "        chi_square = chi_square.clone().detach()\n",
    "        \n",
    "        p_val = 1. - stats.chi2.cdf(chi_square, df)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "       # print(p_val)\n",
    "#         if (p_val > 0.99999):\n",
    "#             return model, likelihood\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    print('loss is %.3f' %loss)\n",
    "#     for params in model.named_parameters():\n",
    "#         print(params)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design parameters and sampling point optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "#     g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "#     x_d= nn.Parameter(Tensor(x0))\n",
    "#     #optimizer = torch.optim.LBFGS(list([g_theta2,x_d]), lr= .000001, max_iter = 100,line_search_fn=\"strong_wolfe\")  #\n",
    "    \n",
    "#     optimizer = FullBatchLBFGS(list([g_theta2,x_d]), lr=.001)\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "        \n",
    "#         loss2 = -1. * loss2\n",
    "\n",
    "       \n",
    "\n",
    "       \n",
    "#         return loss2\n",
    "        \n",
    "#     loss2 = closure()\n",
    "#     loss2.backward()\n",
    "#     #fail = False\n",
    "#     for i in range(training_design_iter):\n",
    "#         options = {'closure': closure, 'current_loss': loss2, 'max_ls': 10}\n",
    "#         loss2, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "#         #print('design Iter %d/%d - Loss: %.3f' %(i + 1, training_design_iter, loss2.item()))\n",
    "  \n",
    "#         if fail:\n",
    "#             print('Convergence reached!')\n",
    "\n",
    "#             break\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "  # loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "#     loss2 = -1. * loss2\n",
    "#     print('Loss design: %.3f' % ( loss2))\n",
    "#     #print(x_d)\n",
    "#     return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': g_theta2, 'lr': 0.1},{'params': x_d, 'lr': 0.1}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "#     loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "#     loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    #print(x_d)\n",
    "    return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "    \n",
    "#     fun = partial(likelihood.get_ell, agg_data,f_target, g_theta1, model, likelihood, noise_value)\n",
    "#     def tad_obj(samples):\n",
    "#         loss2, pf1, Qf1, Qf12, data_fit, Q21 = fun(samples)\n",
    "#         return -loss2.detach().numpy()\n",
    "    \n",
    "    \n",
    "#     sample0 = torch.cat([x0, loc_sample], 0)\n",
    "#     print(sample0)\n",
    "#     #print(sample0.shape)\n",
    "#     res = scipy.optimize.minimize(tad_obj, sample0)#, method='nelder-mead')\n",
    "#     sample_opt = res.x\n",
    "#     loss2, pf1, Qf1, Qf12, data_fit, Q21 = fun(sample_opt)\n",
    "#     loss2 = -1. * loss2\n",
    "#     print('Loss design: %.3f' % ( loss2))\n",
    "\n",
    "#     x_d = sample_opt[0:g_theta1.shape[1]]\n",
    "#     x_d = Tensor(x_d.reshape(math.ceil(x_d.shape[0]/g_theta1.shape[1]), g_theta1.shape[1]))\n",
    "#     g_theta2 = sample_opt[g_theta1.shape[1]:]\n",
    "#     g_theta2 = Tensor(g_theta2.reshape(math.ceil(g_theta2.shape[0]/g_theta1.shape[1]), g_theta1.shape[1]))\n",
    "#     return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conducting the TAD experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_size = 2\n",
    "#loc_sample0 = Tensor((2. - 1.5)  * np.random.random_sample((loc_size,2)) + 1.5)\n",
    "x0 = Tensor(np.array([-2. , -2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "x0 = x0.reshape(1,2)\n",
    "\n",
    "dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(loc_size) )\n",
    "                    #loc_size = 4\n",
    "loc_sample = dis_2sample.sample((loc_size + 1,))\n",
    "\n",
    "loc_sample0 = loc_sample.reshape(loc_size + 1, 2)\n",
    "#loc_sample0[-1] = train_x[-1] + 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9499, -1.8817],\n",
      "        [-1.9923, -1.9935],\n",
      "        [-1.9578, -2.1218]])\n",
      "0\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /lus/theta-fs0/software/thetagpu/conda/2022-07-01/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.254\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 502.853\n",
      "expected info is tensor([[0.0061]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1017.1429, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1681, -2.9972]])\n",
      "new data istensor([[-0.4475,  0.0363]])\n",
      "g_theta2 istensor([[0.2479, 0.3679],\n",
      "        [0.2797, 0.3158],\n",
      "        [0.3413, 0.2249]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 238.227854808623988\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "0\n",
      "Loss design: 500.990\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1013.4210, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1573, -2.9899]])\n",
      "new data istensor([[-0.4253, -0.0049]])\n",
      "g_theta2 istensor([[ 0.4300,  0.5109],\n",
      "        [ 1.2717, -0.9663],\n",
      "        [ 0.4444,  0.4605]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 274.469210827752818\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is3\n",
      "acquiring, new size is7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "1\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.078\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 86.529\n",
      "expected info is tensor([[0.0066]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(178.5782, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5400, -2.5254]])\n",
      "new data istensor([[-0.3129, -0.0356]])\n",
      "g_theta2 istensor([[0.1861, 0.1827],\n",
      "        [0.2303, 0.0900],\n",
      "        [0.2257, 0.0306]])\n",
      "p21val is 0.478449034882394\n",
      "pf12val is 0.000000000002164\n",
      "chi_f12 is 53.718345846668790\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.2009, -2.0411],\n",
      "        [-1.2131, -2.0480],\n",
      "        [-1.5402, -2.5246]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.375\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 112.605\n",
      "expected info is tensor([[0.0032]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(233.1085, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.9982, -2.9470]])\n",
      "new data istensor([[-0.4584,  0.0014]])\n",
      "g_theta2 istensor([[ 2.2529, -2.6217],\n",
      "        [-1.2287, -1.3916],\n",
      "        [-1.3169, -1.8137]])\n",
      "p21val is 0.122095699182653\n",
      "pf12val is 0.297179781866933\n",
      "chi_f12 is 2.426835994328245\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.1245, -1.3024],\n",
      "        [ 0.1066, -2.9233],\n",
      "        [-2.9980, -2.9456]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "3\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.094\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 134.736\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(276.3878, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1759, -3.0016]])\n",
      "new data istensor([[-0.4268, -0.0201]])\n",
      "g_theta2 istensor([[-0.8371, -1.3709],\n",
      "        [ 1.5622, -2.4663],\n",
      "        [-2.5590, -0.9483]])\n",
      "p21val is 0.088579780477354\n",
      "pf12val is 0.989653081029954\n",
      "chi_f12 is 0.020801640936251\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.6729,  2.6379],\n",
      "        [-2.3637, -1.2673],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "4\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.165\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 85.887\n",
      "expected info is tensor([[0.0028]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(178.7600, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.9000,  0.7496]])\n",
      "new data istensor([[-0.3824, -0.0658]])\n",
      "g_theta2 istensor([[ 0.3642,  2.5309],\n",
      "        [-2.4846, -0.9687],\n",
      "        [-1.0843,  2.8609]])\n",
      "p21val is 0.000005531449924\n",
      "pf12val is 0.018437018504801\n",
      "chi_f12 is 7.986789520433848\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "4\n",
      "Loss design: 109.149\n",
      "expected info is tensor([[0.0062]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(225.2694, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.4132,  1.8981]])\n",
      "new data istensor([[ 0.0423, -0.0341]])\n",
      "g_theta2 istensor([[ 1.6821, -1.1568],\n",
      "        [-2.2982, -0.3635],\n",
      "        [-2.9142,  0.4688]])\n",
      "p21val is 0.007828416310756\n",
      "pf12val is 0.010601113677084\n",
      "chi_f12 is 9.093592439014110\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is4\n",
      "acquiring, new size is22\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "5\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.018\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 118.745\n",
      "expected info is tensor([[0.0290]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(244.3073, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8504,  1.1202]])\n",
      "new data istensor([[-0.3088,  0.2062]])\n",
      "g_theta2 istensor([[ 1.2829,  2.5260],\n",
      "        [-2.1228, -1.4155],\n",
      "        [-2.4885,  2.5906]])\n",
      "p21val is 0.095372153533446\n",
      "pf12val is 0.920218384847866\n",
      "chi_f12 is 0.166288524546725\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.3117,  1.4068],\n",
      "        [ 2.9369, -0.3090],\n",
      "        [-1.8500,  1.1208]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "6\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.010\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 117.762\n",
      "expected info is tensor([[0.0016]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(242.4931, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1286,  0.1634]])\n",
      "new data istensor([[-0.5439,  0.5551]])\n",
      "g_theta2 istensor([[ 1.2903,  2.5567],\n",
      "        [ 2.4810, -0.5947],\n",
      "        [-2.3762,  0.8256]])\n",
      "p21val is 0.901120854927243\n",
      "pf12val is 0.000170593567256\n",
      "chi_f12 is 17.352453260415665\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5451,  0.5656],\n",
      "        [ 1.2052,  2.0730],\n",
      "        [-1.1286,  0.1638]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.950\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 206.521\n",
      "expected info is tensor([[0.0078]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(420.4523, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5541, -0.5969]])\n",
      "new data istensor([[-0.5089,  0.6273]])\n",
      "g_theta2 istensor([[ 0.3558,  1.4589],\n",
      "        [ 1.5714,  2.4258],\n",
      "        [-0.5472,  0.7636]])\n",
      "p21val is 0.000000006808625\n",
      "pf12val is 0.062882947841524\n",
      "chi_f12 is 5.532960503057368\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "Loss design: 152.924\n",
      "expected info is tensor([[0.0011]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(312.3243, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.4494,  1.0991]])\n",
      "new data istensor([[ 0.4798, -0.3597]])\n",
      "g_theta2 istensor([[ 2.5362,  2.7163],\n",
      "        [-1.7599, -0.4730],\n",
      "        [-1.7748, -0.4916]])\n",
      "p21val is 0.053730843930097\n",
      "pf12val is 0.000006104670703\n",
      "chi_f12 is 24.012912781413316\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4687,  1.0392],\n",
      "        [ 2.3594,  2.7383],\n",
      "        [-0.4518,  1.1002]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "8\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.831\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 194.759\n",
      "expected info is tensor([[0.0040]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(395.2513, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.3458, 1.9394]])\n",
      "new data istensor([[ 0.7572, -0.4078]])\n",
      "g_theta2 istensor([[-1.0038,  0.4525],\n",
      "        [ 2.4716,  2.4769],\n",
      "        [-1.1295,  0.4619]])\n",
      "p21val is 0.609792606327469\n",
      "pf12val is 0.015257230414297\n",
      "chi_f12 is 8.365403525526064\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[2.2907, 1.8677],\n",
      "        [1.7249, 1.7722],\n",
      "        [0.3485, 1.9381]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.845\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 105.545\n",
      "expected info is tensor([[0.0036]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(216.1640, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.8949,  3.0006]])\n",
      "new data istensor([[ 0.1008, -0.0451]])\n",
      "g_theta2 istensor([[2.4337, 0.9960],\n",
      "        [2.4926, 2.4584],\n",
      "        [0.8045, 1.9797]])\n",
      "p21val is 0.570001873600096\n",
      "pf12val is 0.861504049766671\n",
      "chi_f12 is 0.298151044386629\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.8005,  1.6850],\n",
      "        [-2.5681, -2.5567],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "10\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -1.878\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 159.361\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(324.7206, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.3527,  2.0875]])\n",
      "new data istensor([[ 0.0618, -0.0621]])\n",
      "g_theta2 istensor([[ 2.4865,  1.4813],\n",
      "        [-1.4846, -2.4894],\n",
      "        [-2.9091,  1.1334]])\n",
      "p21val is 0.137807805932614\n",
      "pf12val is 0.992561121597287\n",
      "chi_f12 is 0.014933369687278\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.8166,  2.0128],\n",
      "        [ 0.1748, -0.0986],\n",
      "        [-1.3523,  2.0874]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "11\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.870\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 299.608\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(605.9869, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.4836,  2.2802]])\n",
      "new data istensor([[ 0.3789, -0.3187]])\n",
      "g_theta2 istensor([[-2.5144,  2.6769],\n",
      "        [-0.0340, -1.0147],\n",
      "        [-1.3867,  1.4588]])\n",
      "p21val is 0.965400392661079\n",
      "pf12val is 0.535022018641807\n",
      "chi_f12 is 1.250894753186408\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2052, -0.3979],\n",
      "        [ 0.2522, -0.3522],\n",
      "        [-0.4823,  2.2801]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "12\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.904\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 256.254\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(519.0396, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8071,  3.0011]])\n",
      "new data istensor([[-0.0430,  0.0138]])\n",
      "g_theta2 istensor([[-0.0455, -0.7478],\n",
      "        [ 0.5472, -0.6681],\n",
      "        [ 0.1998,  1.5767]])\n",
      "p21val is 0.000006046829565\n",
      "pf12val is 0.946612697722815\n",
      "chi_f12 is 0.109730495135651\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "12\n",
      "Loss design: 751.048\n",
      "expected info is tensor([[0.0036]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1510.8897, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.7190,  1.6901]])\n",
      "new data istensor([[ 0.4990, -0.3707]])\n",
      "g_theta2 istensor([[ 0.9403,  2.5721],\n",
      "        [-0.3859, -1.1222],\n",
      "        [ 0.5931,  2.9997]])\n",
      "p21val is 0.634249536464875\n",
      "pf12val is 0.425042509669403\n",
      "chi_f12 is 1.711132184616249\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-9.7462e-04,  2.2728e+00],\n",
      "        [-3.2693e-01,  5.1582e-01],\n",
      "        [-7.1839e-01,  1.6886e+00]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "13\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.953\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 237.659\n",
      "expected info is tensor([[0.0027]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(481.9603, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8276,  3.0007]])\n",
      "new data istensor([[-0.0391,  0.0362]])\n",
      "g_theta2 istensor([[-1.1465,  1.3622],\n",
      "        [-1.5126, -0.3183],\n",
      "        [-0.3812,  0.8010]])\n",
      "p21val is 0.914682315510341\n",
      "pf12val is 0.954575753912368\n",
      "chi_f12 is 0.092976547889512\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.8493,  0.4181],\n",
      "        [-1.5026, -0.9099],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "14\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.996\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 174.573\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(355.0173, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0028,  2.0382]])\n",
      "new data istensor([[-0.2157, -0.0193]])\n",
      "g_theta2 istensor([[-1.0843,  0.8570],\n",
      "        [-2.5134, -1.2339],\n",
      "        [-0.8884,  2.9286]])\n",
      "p21val is 0.990438426553078\n",
      "pf12val is 0.928290193320823\n",
      "chi_f12 is 0.148821773518282\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.2788, -0.5688],\n",
      "        [-2.8050, -0.2887],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "15\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.077\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 361.139\n",
      "expected info is tensor([[2.8639e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(730.5256, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.2978,  1.8175]])\n",
      "new data istensor([[-0.1762, -0.0092]])\n",
      "g_theta2 istensor([[ 0.9769, -1.3886],\n",
      "        [-1.5060, -1.6934],\n",
      "        [-0.9638,  2.9982]])\n",
      "p21val is 0.056803533394891\n",
      "pf12val is 0.892041914081616\n",
      "chi_f12 is 0.228484317250671\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.6359,  1.6852],\n",
      "        [-0.9369,  1.3042],\n",
      "        [-2.2979,  1.8183]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "16\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.065\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1533.481\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3079.3361, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.6204,  1.4667]])\n",
      "new data istensor([[-0.2495,  0.0126]])\n",
      "g_theta2 istensor([[-1.1105,  1.1332],\n",
      "        [ 0.4723,  0.6539],\n",
      "        [-1.0650,  1.0400]])\n",
      "p21val is 0.165601428720324\n",
      "pf12val is 0.389335454735699\n",
      "chi_f12 is 1.886627910938947\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.8791,  2.9279],\n",
      "        [-0.1708,  0.8490],\n",
      "        [-2.6208,  1.4665]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "17\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.145\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1028.315\n",
      "expected info is tensor([[0.0009]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2066.8567, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8974,  2.3906]])\n",
      "new data istensor([[-0.0428,  0.0125]])\n",
      "g_theta2 istensor([[ 2.4528,  2.4720],\n",
      "        [-1.0093,  0.6036],\n",
      "        [-2.6258,  0.6769]])\n",
      "p21val is 0.045756882220586\n",
      "pf12val is 0.813564815461354\n",
      "chi_f12 is 0.412659361365423\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.2435, -0.5717],\n",
      "        [-1.5882,  1.7216],\n",
      "        [-1.8965,  2.3905]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.166\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1593.679\n",
      "expected info is tensor([[0.0023]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3198.6528, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.2439,  2.5826]])\n",
      "new data istensor([[ 0.0580, -0.0475]])\n",
      "g_theta2 istensor([[-2.4289, -0.2840],\n",
      "        [-0.3870,  0.7204],\n",
      "        [-2.9131,  1.4098]])\n",
      "p21val is 0.638103435374313\n",
      "pf12val is 0.995084889434090\n",
      "chi_f12 is 0.009854458897176\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.1860,  1.9312],\n",
      "        [ 0.2074, -0.0131],\n",
      "        [-1.2448,  2.5813]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "19\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.218\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2297.170\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4606.9008, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8912,  1.6880]])\n",
      "new data istensor([[-0.1523,  0.0331]])\n",
      "g_theta2 istensor([[ 2.6412,  2.0304],\n",
      "        [ 0.2258,  0.0879],\n",
      "        [-0.4453,  2.7376]])\n",
      "p21val is 0.800181872119580\n",
      "pf12val is 0.601958557797642\n",
      "chi_f12 is 1.015133353821324\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.0998,  1.5092],\n",
      "        [-0.2431,  1.4709],\n",
      "        [-1.8926,  1.6872]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "20\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.261\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2467.370\n",
      "expected info is tensor([[0.0013]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4947.5074, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.5724,  2.0982]])\n",
      "new data istensor([[-0.1988, -0.0334]])\n",
      "g_theta2 istensor([[ 1.8104,  2.5697],\n",
      "        [ 0.1384,  1.6037],\n",
      "        [-2.8250,  0.6439]])\n",
      "p21val is 0.178089025755837\n",
      "pf12val is 0.307378284654883\n",
      "chi_f12 is 2.359352184708833\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.7061,  2.3923],\n",
      "        [ 0.4043,  0.7790],\n",
      "        [-2.5734,  2.0982]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "21\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.280\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 223.007\n",
      "expected info is tensor([[0.0018]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(449.7013, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0040,  3.0016]])\n",
      "new data istensor([[-0.1526,  0.0037]])\n",
      "g_theta2 istensor([[ 2.0300,  2.5645],\n",
      "        [ 1.0004,  0.2320],\n",
      "        [-1.9286,  1.2312]])\n",
      "p21val is 0.129977256580262\n",
      "pf12val is 0.804529551577048\n",
      "chi_f12 is 0.434995160750598\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.1535,  2.2231],\n",
      "        [-2.8082,  0.9080],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "22\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.276\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2405.587\n",
      "expected info is tensor([[0.0290]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4601.1363, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.4737,  3.0333]])\n",
      "new data istensor([[-0.0057,  0.0015]])\n",
      "g_theta2 istensor([[-1.0862,  2.5522],\n",
      "        [-2.9997,  0.4964],\n",
      "        [-2.9262,  1.0745]])\n",
      "p21val is 0.680835072274006\n",
      "pf12val is 0.710746755323194\n",
      "chi_f12 is 0.682878187212888\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.1080,  2.4079],\n",
      "        [-0.6115, -2.8956],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "23\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.331\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3965.295\n",
      "expected info is tensor([[0.0007]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7944.3608, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.9075,  2.0725]])\n",
      "new data istensor([[ 0.2591, -0.1723]])\n",
      "g_theta2 istensor([[-2.9629,  2.9318],\n",
      "        [ 1.5689, -2.5183],\n",
      "        [-2.9770,  2.9744]])\n",
      "p21val is 0.628589478233287\n",
      "pf12val is 0.309147089645339\n",
      "chi_f12 is 2.347876194150883\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.4342,  0.8819],\n",
      "        [-0.0964,  0.3330],\n",
      "        [-0.9055,  2.0718]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "24\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.368\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3856.220\n",
      "expected info is tensor([[0.0014]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7726.7310, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.9173,  2.2768]])\n",
      "new data istensor([[-0.0803,  0.0051]])\n",
      "g_theta2 istensor([[ 2.8462, -0.1644],\n",
      "        [ 0.9202, -1.1235],\n",
      "        [-2.4700,  2.7608]])\n",
      "p21val is 0.384078794729574\n",
      "pf12val is 0.727560192406613\n",
      "chi_f12 is 0.636117089311797\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.7319, -0.6996],\n",
      "        [ 1.5928,  0.3359],\n",
      "        [-1.9193,  2.2767]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "25\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.375\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3557.575\n",
      "expected info is tensor([[1.8773e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7128.8703, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.4130,  1.2707]])\n",
      "new data istensor([[-0.2493,  0.0601]])\n",
      "g_theta2 istensor([[ 1.7887, -1.1133],\n",
      "        [ 1.5898,  1.1298],\n",
      "        [-1.1980,  2.9347]])\n",
      "p21val is 0.853631125925786\n",
      "pf12val is 0.729242581940866\n",
      "chi_f12 is 0.631497684923501\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.6460,  1.3502],\n",
      "        [-1.3758,  0.6385],\n",
      "        [-2.4130,  1.2727]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "26\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.404\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1668.798\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3178.0717, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0293,  2.4535]])\n",
      "new data istensor([[-0.2022,  0.0312]])\n",
      "g_theta2 istensor([[-0.4704,  1.6886],\n",
      "        [-0.4546,  1.7988],\n",
      "        [-2.7119,  0.5422]])\n",
      "p21val is 0.244848129968713\n",
      "pf12val is 0.711152708620583\n",
      "chi_f12 is 0.681736184367504\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.0144,  1.5857],\n",
      "        [-0.2103, -0.5586],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "27\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.416\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5139.517\n",
      "expected info is tensor([[0.0011]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10293.6361, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5777,  1.8412]])\n",
      "new data istensor([[-0.0512, -0.0012]])\n",
      "g_theta2 istensor([[ 0.2888,  1.2194],\n",
      "        [ 1.0536, -1.7864],\n",
      "        [-3.0009,  2.8469]])\n",
      "p21val is 0.059143186221819\n",
      "pf12val is 0.761558026941637\n",
      "chi_f12 is 0.544777817414922\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.6225,  0.1311],\n",
      "        [-1.9066,  2.5628],\n",
      "        [-1.5766,  1.8434]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "28\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.442\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2310.696\n",
      "expected info is tensor([[7.2746e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4213.5063, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0459,  1.6787]])\n",
      "new data istensor([[-0.2831, -0.0042]])\n",
      "g_theta2 istensor([[ 2.4112, -0.2327],\n",
      "        [-2.6803,  2.8438],\n",
      "        [-0.9836,  2.9279]])\n",
      "p21val is 0.437777228860100\n",
      "pf12val is 0.890089598748689\n",
      "chi_f12 is 0.232866297143554\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.5411, -1.6204],\n",
      "        [-1.5814,  1.9415],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "29\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.467\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4824.274\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9663.5557, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.6241,  2.0069]])\n",
      "new data istensor([[-0.1927,  0.0110]])\n",
      "g_theta2 istensor([[ 1.9718, -2.6631],\n",
      "        [-0.8802,  1.2707],\n",
      "        [-1.0256,  2.8041]])\n",
      "p21val is 0.917533857028768\n",
      "pf12val is 0.413947357031262\n",
      "chi_f12 is 1.764032940342535\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.4211,  2.3873],\n",
      "        [-1.8192,  2.0698],\n",
      "        [-2.6244,  2.0083]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "30\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.484\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3679.673\n",
      "expected info is tensor([[4.5353e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6403.1165, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0697,  2.4280]])\n",
      "new data istensor([[-0.2019, -0.0451]])\n",
      "g_theta2 istensor([[ 2.5628,  2.4269],\n",
      "        [-1.3381,  1.6786],\n",
      "        [-1.9830,  1.0391]])\n",
      "p21val is 0.595246219823742\n",
      "pf12val is 0.093716682759369\n",
      "chi_f12 is 4.734958122391003\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.0495,  0.4584],\n",
      "        [ 2.2466, -2.0377],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "31\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.513\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5752.611\n",
      "expected info is tensor([[3.5799e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11520.3821, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1482,  1.6861]])\n",
      "new data istensor([[-0.1752, -0.0024]])\n",
      "g_theta2 istensor([[ 0.7667, -1.4305],\n",
      "        [ 2.5131, -2.4748],\n",
      "        [-1.1323,  2.8665]])\n",
      "p21val is 0.003911535381168\n",
      "pf12val is 0.516504455706437\n",
      "chi_f12 is 1.321342727457790\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "31\n",
      "Loss design: 5760.939\n",
      "expected info is tensor([[0.0027]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11537.0382, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1410,  1.7016]])\n",
      "new data istensor([[-0.1644,  0.0170]])\n",
      "g_theta2 istensor([[-2.4090,  2.4613],\n",
      "        [-2.5289,  2.5403],\n",
      "        [-1.1354,  2.8672]])\n",
      "p21val is 0.064333424783577\n",
      "pf12val is 0.864740520662790\n",
      "chi_f12 is 0.290651586328947\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.6407,  0.5651],\n",
      "        [-2.3498,  2.2459],\n",
      "        [-2.1400,  1.7008]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "32\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.526\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 939.995\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1890.2040, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.9187,  0.2795]])\n",
      "new data istensor([[-0.6680,  0.6821]])\n",
      "g_theta2 istensor([[ 0.4258, -1.6179],\n",
      "        [-2.9213,  2.8636],\n",
      "        [-1.5805,  2.6520]])\n",
      "p21val is 0.001047225310584\n",
      "pf12val is 0.006082869975376\n",
      "chi_f12 is 10.204557317908069\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "32\n",
      "Loss design: 2321.519\n",
      "expected info is tensor([[0.0032]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4184.8009, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.3190,  3.0485]])\n",
      "new data istensor([[-0.1313, -0.0224]])\n",
      "g_theta2 istensor([[-0.9010,  0.0355],\n",
      "        [-2.3348,  2.3764],\n",
      "        [-2.6192,  0.6983]])\n",
      "p21val is 0.085265565868648\n",
      "pf12val is 0.075432569909288\n",
      "chi_f12 is 5.169032271150316\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.1469,  1.0981],\n",
      "        [-1.1488, -1.8603],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "33\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.515\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4672.893\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9359.7588, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8023,  2.7042]])\n",
      "new data istensor([[-0.0165,  0.0045]])\n",
      "g_theta2 istensor([[-0.8851,  1.0234],\n",
      "        [-1.8554, -2.7903],\n",
      "        [-2.8662,  1.1657]])\n",
      "p21val is 0.131684024841237\n",
      "pf12val is 0.871855494067910\n",
      "chi_f12 is 0.274263173235802\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.7019,  0.1033],\n",
      "        [-2.1397, -0.5460],\n",
      "        [-1.8029,  2.7036]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "34\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.530\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1293.331\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2597.6362, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.1821, 2.6761]])\n",
      "new data istensor([[ 0.2900, -0.2415]])\n",
      "g_theta2 istensor([[-0.1913, -0.1364],\n",
      "        [-2.2114, -1.3670],\n",
      "        [-2.7882,  2.6656]])\n",
      "p21val is 0.005423499504318\n",
      "pf12val is 0.547716025698814\n",
      "chi_f12 is 1.203996655252688\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "34\n",
      "Loss design: 7316.447\n",
      "expected info is tensor([[0.0054]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(14648.2761, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5411,  2.3507]])\n",
      "new data istensor([[-0.0208, -0.0435]])\n",
      "g_theta2 istensor([[ 0.9594,  0.7224],\n",
      "        [-2.8378,  2.5018],\n",
      "        [-2.7687,  2.6638]])\n",
      "p21val is 0.303241571514151\n",
      "pf12val is 0.273653001805486\n",
      "chi_f12 is 2.591788783891511\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.7849, -2.2522],\n",
      "        [-1.6670,  2.7963],\n",
      "        [-1.5418,  2.3507]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "35\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.550\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1097.742\n",
      "expected info is tensor([[8.7511e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2194.7383, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.0337,  3.0075]])\n",
      "new data istensor([[ 0.1721, -0.1055]])\n",
      "g_theta2 istensor([[ 1.5722, -2.4717],\n",
      "        [-2.3741,  2.7199],\n",
      "        [-2.3824,  2.7820]])\n",
      "p21val is 0.118854797602991\n",
      "pf12val is 0.805003135962115\n",
      "chi_f12 is 0.433818211932100\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.5203,  0.0822],\n",
      "        [ 2.6415,  1.8444],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "36\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.557\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7671.538\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(15358.8617, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.9969,  1.9565]])\n",
      "new data istensor([[-0.1058,  0.0392]])\n",
      "g_theta2 istensor([[-2.5394, -0.0152],\n",
      "        [ 2.5146,  2.5426],\n",
      "        [-2.8521,  2.8973]])\n",
      "p21val is 0.342304094352906\n",
      "pf12val is 0.197514421564186\n",
      "chi_f12 is 3.243887353457353\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.5833, -0.8681],\n",
      "        [ 2.9873,  2.9940],\n",
      "        [-1.9978,  1.9559]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "37\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.574\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1396.455\n",
      "expected info is tensor([[1.4526e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2803.8449, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.2263, 2.5239]])\n",
      "new data istensor([[ 0.3582, -0.2789]])\n",
      "g_theta2 istensor([[-2.1111, -2.1907],\n",
      "        [ 2.4911,  2.4400],\n",
      "        [-2.8479,  1.1317]])\n",
      "p21val is 0.810058868052388\n",
      "pf12val is 0.653936222278610\n",
      "chi_f12 is 0.849490903463872\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.3176,  1.3330],\n",
      "        [-2.1547,  1.7607],\n",
      "        [ 0.2263,  2.5244]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "38\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.589\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4245.315\n",
      "expected info is tensor([[0.0026]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8504.4304, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.5020, 2.5302]])\n",
      "new data istensor([[ 0.3552, -0.2111]])\n",
      "g_theta2 istensor([[ 1.8478,  1.3014],\n",
      "        [-2.0944,  1.7261],\n",
      "        [-0.8431,  2.5196]])\n",
      "p21val is 0.230084022987567\n",
      "pf12val is 0.909695559286467\n",
      "chi_f12 is 0.189290571360615\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8944,  0.7564],\n",
      "        [-1.6549,  1.7766],\n",
      "        [ 0.5032,  2.5296]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "39\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.596\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3406.457\n",
      "expected info is tensor([[0.0030]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5971.7677, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.2720,  3.0654]])\n",
      "new data istensor([[ 0.1568, -0.1151]])\n",
      "g_theta2 istensor([[ 0.4578,  0.2700],\n",
      "        [-1.4667,  1.5805],\n",
      "        [ 1.3066,  1.8393]])\n",
      "p21val is 0.966092088067862\n",
      "pf12val is 0.684323905997938\n",
      "chi_f12 is 0.758647853333736\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.3138,  2.2734],\n",
      "        [-2.1771,  0.5921],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "40\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.620\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4082.892\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6172.1186, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.5301,  3.1002]])\n",
      "new data istensor([[-0.1214, -0.0021]])\n",
      "g_theta2 istensor([[ 0.2252,  1.3260],\n",
      "        [-2.5100,  1.5284],\n",
      "        [-2.8300,  1.0885]])\n",
      "p21val is 0.483248868692301\n",
      "pf12val is 0.877412027250729\n",
      "chi_f12 is 0.261557165026384\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.8657,  0.6886],\n",
      "        [-1.0243,  2.1501],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "41\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.616\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4769.385\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7532.2662, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8865,  3.1005]])\n",
      "new data istensor([[-0.0270,  0.0206]])\n",
      "g_theta2 istensor([[ 2.7685, -0.6580],\n",
      "        [-0.2481,  2.1248],\n",
      "        [-2.8462,  1.0992]])\n",
      "p21val is 0.064174813193363\n",
      "pf12val is 0.361303778480267\n",
      "chi_f12 is 2.036072365739512\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.6495, -0.5482],\n",
      "        [-0.2117, -2.3477],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "42\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.638\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 9176.333\n",
      "expected info is tensor([[3.2210e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(18368.4637, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.7226,  2.6806]])\n",
      "new data istensor([[-0.0302,  0.0102]])\n",
      "g_theta2 istensor([[-1.5289, -0.9585],\n",
      "        [ 1.4476, -2.5066],\n",
      "        [-2.8496,  1.1042]])\n",
      "p21val is 0.509890891947816\n",
      "pf12val is 0.725680416896101\n",
      "chi_f12 is 0.641291116370681\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.4324, -1.3778],\n",
      "        [ 0.8705, -0.4979],\n",
      "        [-1.7225,  2.6812]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "43\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.625\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3908.430\n",
      "expected info is tensor([[7.8139e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5376.2220, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0750,  3.0815]])\n",
      "new data istensor([[-0.1418,  0.0092]])\n",
      "g_theta2 istensor([[-2.1343, -1.4225],\n",
      "        [ 1.2999, -1.5335],\n",
      "        [-1.2039,  1.7340]])\n",
      "p21val is 0.403642698118593\n",
      "pf12val is 0.332361032628527\n",
      "chi_f12 is 2.203066906464569\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.2699,  2.2173],\n",
      "        [-0.9210,  1.6211],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "44\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.660\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 8545.526\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(17106.7288, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.9488,  2.1570]])\n",
      "new data istensor([[ 0.2620, -0.1752]])\n",
      "g_theta2 istensor([[-0.9375,  2.8755],\n",
      "        [-0.3059, -0.1645],\n",
      "        [-2.8582,  1.1079]])\n",
      "p21val is 0.020989262500689\n",
      "pf12val is 0.096379868874944\n",
      "chi_f12 is 4.678915856543124\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9428,  2.8095],\n",
      "        [-1.1935,  2.7293],\n",
      "        [-0.9489,  2.1559]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "45\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.656\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 11047.187\n",
      "expected info is tensor([[0.0040]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(22110.5949, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.5116,  2.2159]])\n",
      "new data istensor([[-0.0059, -0.0187]])\n",
      "g_theta2 istensor([[-2.7669,  2.6205],\n",
      "        [-2.7195,  2.6419],\n",
      "        [-1.7866,  2.8848]])\n",
      "p21val is 0.303552431798123\n",
      "pf12val is 0.812749096562848\n",
      "chi_f12 is 0.414665662758727\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5681,  1.9115],\n",
      "        [ 0.8111,  0.8027],\n",
      "        [-1.5114,  2.2149]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "46\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.664\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1281.415\n",
      "expected info is tensor([[5.3918e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2573.4314, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.9315,  0.3231]])\n",
      "new data istensor([[-0.6675,  0.6762]])\n",
      "g_theta2 istensor([[ 0.3292,  2.2458],\n",
      "        [ 1.1089,  0.8474],\n",
      "        [-1.0239,  2.8461]])\n",
      "p21val is 0.847910246723019\n",
      "pf12val is 0.003012984657947\n",
      "chi_f12 is 11.609648221599173\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.3290, -2.1028],\n",
      "        [ 2.3857,  1.9037],\n",
      "        [-1.9307,  0.3222]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "47\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.669\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4219.513\n",
      "expected info is tensor([[1.2554e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8452.9417, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.2411,  0.2102]])\n",
      "new data istensor([[-0.6011,  0.4050]])\n",
      "g_theta2 istensor([[ 1.6780, -2.6527],\n",
      "        [ 2.5638,  1.7331],\n",
      "        [-2.6322,  1.5556]])\n",
      "p21val is 0.308144750793092\n",
      "pf12val is 0.620169935278466\n",
      "chi_f12 is 0.955523498679511\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.5363, -2.6256],\n",
      "        [-1.2429,  0.4557],\n",
      "        [-2.2419,  0.2110]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "48\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.684\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 229.170\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(463.7219, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0039, -1.1460]])\n",
      "new data istensor([[-0.4710,  0.0038]])\n",
      "g_theta2 istensor([[ 2.5226, -2.6189],\n",
      "        [-0.3301,  0.2648],\n",
      "        [-2.9679,  1.3153]])\n",
      "p21val is 0.276366126451160\n",
      "pf12val is 0.891667984176785\n",
      "chi_f12 is 0.229322861479581\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.7417, -0.9064],\n",
      "        [ 1.4270, -1.2276],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "49\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.681\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4320.556\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6655.7873, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1000,  1.7434]])\n",
      "new data istensor([[-0.2373, -0.0102]])\n",
      "g_theta2 istensor([[ 0.7376, -1.5520],\n",
      "        [ 2.2750, -0.9058],\n",
      "        [-1.5343,  1.0003]])\n",
      "p21val is 0.644522830122934\n",
      "pf12val is 0.648232401694987\n",
      "chi_f12 is 0.867012004672864\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.0955, -0.0144],\n",
      "        [-2.3643, -1.7598],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "50\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.694\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5728.886\n",
      "expected info is tensor([[5.7985e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7803.9409, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1354,  2.3391]])\n",
      "new data istensor([[-0.2180, -0.0233]])\n",
      "g_theta2 istensor([[-0.3738, -0.6160],\n",
      "        [-2.1195, -2.9734],\n",
      "        [-1.2219,  0.9693]])\n",
      "p21val is 0.277329171908689\n",
      "pf12val is 0.969023750317907\n",
      "chi_f12 is 0.062932314519481\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.0875,  0.5434],\n",
      "        [ 2.9683,  1.0503],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "51\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.706\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 367.671\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(726.6261, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0095, -0.3751]])\n",
      "new data istensor([[-0.4548,  0.0274]])\n",
      "g_theta2 istensor([[-1.6290, -0.3446],\n",
      "        [ 2.5124,  2.2513],\n",
      "        [-1.1884,  0.9417]])\n",
      "p21val is 0.698119175714068\n",
      "pf12val is 0.615446640299869\n",
      "chi_f12 is 0.970814060746867\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.9007,  1.9551],\n",
      "        [-1.2870, -0.2230],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "52\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.702\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1514.488\n",
      "expected info is tensor([[5.6588e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2952.4227, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0212, -0.7504]])\n",
      "new data istensor([[-0.4650,  0.0138]])\n",
      "g_theta2 istensor([[ 2.7413,  0.8333],\n",
      "        [ 0.6382, -0.8053],\n",
      "        [-1.3145,  0.9055]])\n",
      "p21val is 0.831534142070173\n",
      "pf12val is 0.962548467767419\n",
      "chi_f12 is 0.076341715932062\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.4744, -0.2135],\n",
      "        [-0.3998,  0.8412],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "53\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.718\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1854.680\n",
      "expected info is tensor([[2.0421e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3533.6079, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0307, -0.0665]])\n",
      "new data istensor([[-0.4516,  0.0138]])\n",
      "g_theta2 istensor([[ 0.7470, -1.1869],\n",
      "        [-0.2406,  1.6111],\n",
      "        [-1.2336,  0.9362]])\n",
      "p21val is 0.921487581507437\n",
      "pf12val is 0.923042793684813\n",
      "chi_f12 is 0.160159363730385\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.9950, -1.8924],\n",
      "        [ 0.5693,  1.0429],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "54\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.736\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3199.166\n",
      "expected info is tensor([[1.6924e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5292.4382, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0748,  0.2200]])\n",
      "new data istensor([[-0.4431,  0.0259]])\n",
      "g_theta2 istensor([[ 2.2089, -2.4954],\n",
      "        [ 1.0140,  0.2493],\n",
      "        [-1.2717,  0.9471]])\n",
      "p21val is 0.335682631247019\n",
      "pf12val is 0.744391885711934\n",
      "chi_f12 is 0.590375309428327\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.7735,  2.7331],\n",
      "        [-0.7987, -1.3545],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "55\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.712\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2530.465\n",
      "expected info is tensor([[4.4538e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3675.7077, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0836,  0.8114]])\n",
      "new data istensor([[-0.3510,  0.0072]])\n",
      "g_theta2 istensor([[-2.6377,  2.7145],\n",
      "        [ 0.8357, -1.0989],\n",
      "        [-1.1298,  1.1650]])\n",
      "p21val is 0.409407491779796\n",
      "pf12val is 0.979422536795339\n",
      "chi_f12 is 0.041584258320943\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.5750, -0.7443],\n",
      "        [-1.5584, -1.4969],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "56\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.755\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5016.526\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6911.0983, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1252,  0.7903]])\n",
      "new data istensor([[-0.3440, -0.0310]])\n",
      "g_theta2 istensor([[-1.8725, -1.3923],\n",
      "        [-1.9164, -1.4262],\n",
      "        [-0.9294,  0.9283]])\n",
      "p21val is 0.607201153500140\n",
      "pf12val is 0.420520404130356\n",
      "chi_f12 is 1.732524554499355\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5888,  2.9544],\n",
      "        [-1.8197, -0.9282],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "57\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.763\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5580.676\n",
      "expected info is tensor([[3.1015e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7287.0717, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1394,  1.3639]])\n",
      "new data istensor([[-0.2530, -0.0182]])\n",
      "g_theta2 istensor([[ 0.8094,  2.6588],\n",
      "        [-1.6723, -2.1557],\n",
      "        [-1.0791,  0.9329]])\n",
      "p21val is 0.978175535710285\n",
      "pf12val is 0.529159930311215\n",
      "chi_f12 is 1.272929134169936\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.1350,  2.0564],\n",
      "        [-1.7510,  0.7284],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "58\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.779\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 6937.516\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10105.2537, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1376,  1.9174]])\n",
      "new data istensor([[-0.2934, -0.0042]])\n",
      "g_theta2 istensor([[ 0.4775,  2.4469],\n",
      "        [-1.8177,  0.7840],\n",
      "        [-1.1492,  0.9365]])\n",
      "p21val is 0.824519467939186\n",
      "pf12val is 0.048960119691109\n",
      "chi_f12 is 6.033498392219422\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.6104,  2.7942],\n",
      "        [ 0.9483,  1.4475],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "59\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.779\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 8260.702\n",
      "expected info is tensor([[6.8541e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(16537.5692, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7677,  1.9913]])\n",
      "new data istensor([[-0.2004,  0.0182]])\n",
      "g_theta2 istensor([[-0.8586,  2.3231],\n",
      "        [ 1.6311,  1.9473],\n",
      "        [-1.1344,  0.9293]])\n",
      "p21val is 0.563338761816900\n",
      "pf12val is 0.562348601309793\n",
      "chi_f12 is 1.151266668717653\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.7868, -0.8683],\n",
      "        [ 1.6592,  2.4266],\n",
      "        [-2.7669,  1.9912]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "60\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.794\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 10913.063\n",
      "expected info is tensor([[1.1899e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(21842.8617, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7553,  1.9779]])\n",
      "new data istensor([[-0.2576,  0.0017]])\n",
      "g_theta2 istensor([[ 2.1170, -1.3117],\n",
      "        [ 1.7350,  1.8840],\n",
      "        [-2.6421,  0.6485]])\n",
      "p21val is 0.162489425923173\n",
      "pf12val is 0.104965997682868\n",
      "chi_f12 is 4.508237625722241\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.0162, -1.2040],\n",
      "        [-0.1524,  0.6668],\n",
      "        [-2.7559,  1.9777]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "61\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.759\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5557.594\n",
      "expected info is tensor([[6.5830e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(6827.7082, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1105,  3.0965]])\n",
      "new data istensor([[-0.1769, -0.0003]])\n",
      "g_theta2 istensor([[-0.7936, -1.5747],\n",
      "        [-0.4129, -0.2201],\n",
      "        [-1.9477,  1.1442]])\n",
      "p21val is 0.016897133650009\n",
      "pf12val is 0.683311366142837\n",
      "chi_f12 is 0.761609286285132\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.0131, -2.4387],\n",
      "        [-2.4236,  2.0330],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "62\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.771\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 11661.828\n",
      "expected info is tensor([[0.0012]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(23340.1993, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1242,  2.1583]])\n",
      "new data istensor([[-0.0833, -0.0249]])\n",
      "g_theta2 istensor([[ 0.1636, -2.4843],\n",
      "        [-3.0007,  2.2876],\n",
      "        [-2.8948,  1.0825]])\n",
      "p21val is 0.206188008411019\n",
      "pf12val is 0.136507649071497\n",
      "chi_f12 is 3.982749257546640\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.0143,  1.5003],\n",
      "        [-0.3265, -2.3323],\n",
      "        [-2.1241,  2.1600]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "63\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.785\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 6198.121\n",
      "expected info is tensor([[4.1843e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8522.8542, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.6112,  3.1394]])\n",
      "new data istensor([[-0.0836,  0.0213]])\n",
      "g_theta2 istensor([[-1.6165,  0.5979],\n",
      "        [-0.4019, -2.4987],\n",
      "        [-2.7482,  1.1880]])\n",
      "p21val is 0.060786692317375\n",
      "pf12val is 0.080773826078579\n",
      "chi_f12 is 5.032204601208628\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.1649, -0.5562],\n",
      "        [-0.9851, -1.0112],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "64\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.785\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 12044.208\n",
      "expected info is tensor([[1.5052e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(24104.9430, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.4715,  2.6125]])\n",
      "new data istensor([[-0.0172, -0.0265]])\n",
      "g_theta2 istensor([[-1.7855, -0.4541],\n",
      "        [-0.9115, -1.3372],\n",
      "        [-2.8702,  1.1736]])\n",
      "p21val is 0.942022858489354\n",
      "pf12val is 0.618665559699908\n",
      "chi_f12 is 0.960380887167735\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9556, -1.7254],\n",
      "        [ 2.9197,  2.9096],\n",
      "        [-1.4707,  2.6099]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "65\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.792\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5803.931\n",
      "expected info is tensor([[5.4811e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7941.4221, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.3546,  3.1357]])\n",
      "new data istensor([[ 0.0317, -0.0219]])\n",
      "g_theta2 istensor([[-0.9228, -1.5954],\n",
      "        [ 2.4642,  2.4539],\n",
      "        [-2.3055,  1.7030]])\n",
      "p21val is 0.310216342864875\n",
      "pf12val is 0.452216013109493\n",
      "chi_f12 is 1.587190616401369\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2020,  2.1893],\n",
      "        [ 0.5005, -1.5830],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "66\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.802\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 11478.961\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(22974.3507, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.7538,  1.4817]])\n",
      "new data istensor([[-0.1769,  0.0563]])\n",
      "g_theta2 istensor([[ 0.6188,  2.5537],\n",
      "        [ 1.3122, -2.2518],\n",
      "        [-2.8883,  1.1240]])\n",
      "p21val is 0.460703331714923\n",
      "pf12val is 0.418001141180950\n",
      "chi_f12 is 1.744542232726283\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9751,  1.2874],\n",
      "        [-0.9714,  1.7104],\n",
      "        [-1.7528,  1.4811]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "67\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.805\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 12729.052\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(25474.7709, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.6738,  2.0471]])\n",
      "new data istensor([[-0.0442,  0.0124]])\n",
      "g_theta2 istensor([[-1.6150,  0.8280],\n",
      "        [-1.4283,  0.8458],\n",
      "        [-2.8495,  2.4036]])\n",
      "p21val is 0.997487586165942\n",
      "pf12val is 0.748809089254787\n",
      "chi_f12 is 0.578542430929117\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.5960,  1.7498],\n",
      "        [-1.5349,  0.5803],\n",
      "        [-1.6740,  2.0459]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "68\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.821\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7456.620\n",
      "expected info is tensor([[3.6036e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10155.0601, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.0734,  3.1545]])\n",
      "new data istensor([[-0.0369, -0.0199]])\n",
      "g_theta2 istensor([[-0.8467,  1.0663],\n",
      "        [-0.7449,  1.2371],\n",
      "        [-1.1930,  1.0514]])\n",
      "p21val is 0.172007103292279\n",
      "pf12val is 0.324032680597976\n",
      "chi_f12 is 2.253821804466619\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.3232,  0.6249],\n",
      "        [-2.1139, -0.1908],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "69\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.824\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 13715.045\n",
      "expected info is tensor([[9.6296e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(27447.0782, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1668,  2.2362]])\n",
      "new data istensor([[-0.1247,  0.0108]])\n",
      "g_theta2 istensor([[ 0.0160,  1.0059],\n",
      "        [-2.4690, -0.2998],\n",
      "        [-1.2911,  0.9711]])\n",
      "p21val is 0.416098832135154\n",
      "pf12val is 0.486414916680690\n",
      "chi_f12 is 1.441386562565103\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.9090,  2.6408],\n",
      "        [-2.4351,  2.7158],\n",
      "        [-2.1663,  2.2362]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "70\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.833\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 719.415\n",
      "expected info is tensor([[2.9264e-08]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1398.8070, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8646, 3.0157]])\n",
      "new data istensor([[ 0.3861, -0.0027]])\n",
      "g_theta2 istensor([[-2.5663,  2.9333],\n",
      "        [-2.9355,  2.5818],\n",
      "        [-2.8845,  1.2723]])\n",
      "p21val is 0.039608036741606\n",
      "pf12val is 0.599398357845353\n",
      "chi_f12 is 1.023657727534806\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9684, -2.6775],\n",
      "        [ 0.6670,  2.7273],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "71\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.832\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7623.985\n",
      "expected info is tensor([[2.4526e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(15262.7742, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.9717,  0.0679]])\n",
      "new data istensor([[-0.7429,  0.6721]])\n",
      "g_theta2 istensor([[-1.5139, -2.5616],\n",
      "        [ 0.7827,  2.7385],\n",
      "        [-2.9452,  2.9407]])\n",
      "p21val is 0.087105971924154\n",
      "pf12val is 0.478083444598662\n",
      "chi_f12 is 1.475939982872427\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.3772,  0.6038],\n",
      "        [-1.6757, -2.4939],\n",
      "        [-1.9722,  0.0682]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "72\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.809\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 164.180\n",
      "expected info is tensor([[5.8809e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(336.3094, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.9640, -2.2882]])\n",
      "new data istensor([[-0.4639,  0.0113]])\n",
      "g_theta2 istensor([[ 1.2513,  1.9401],\n",
      "        [-1.5299, -2.5435],\n",
      "        [-2.1650, -1.3445]])\n",
      "p21val is 0.125336628029143\n",
      "pf12val is 0.979860184444204\n",
      "chi_f12 is 0.040690772860371\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.6020, -1.1601],\n",
      "        [-1.0538,  1.7920],\n",
      "        [-2.9651, -2.2874]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "73\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.834\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 583.554\n",
      "expected info is tensor([[0.0009]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(1171.3368, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0057, -1.7318]])\n",
      "new data istensor([[-0.4603, -0.0091]])\n",
      "g_theta2 istensor([[-2.4339, -1.1747],\n",
      "        [-0.7875,  2.2887],\n",
      "        [-1.8711, -2.8466]])\n",
      "p21val is 0.652862045110505\n",
      "pf12val is 0.972462961135972\n",
      "chi_f12 is 0.055846580900677\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.1632,  0.5175],\n",
      "        [-2.5325, -2.3677],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "74\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.839\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 13313.438\n",
      "expected info is tensor([[8.4715e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(26643.8093, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.7884,  1.4701]])\n",
      "new data istensor([[-0.1503,  0.0748]])\n",
      "g_theta2 istensor([[ 1.7750,  1.5968],\n",
      "        [-2.4118, -2.1135],\n",
      "        [-2.9611,  2.7507]])\n",
      "p21val is 0.907156571840624\n",
      "pf12val is 0.702329623867292\n",
      "chi_f12 is 0.706704870983537\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.4974e-02,  1.5397e+00],\n",
      "        [ 1.3973e+00, -9.5950e-04],\n",
      "        [-1.7894e+00,  1.4713e+00]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "75\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.850\n",
      "END HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: 2198.703\n",
      "expected info is tensor([[2.3401e-07]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4080.9924, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0406, -1.9304]])\n",
      "new data istensor([[-0.4035, -0.0239]])\n",
      "g_theta2 istensor([[ 0.5389,  0.6325],\n",
      "        [ 0.9632, -1.7483],\n",
      "        [-1.5362,  2.5922]])\n",
      "p21val is 0.230939757148223\n",
      "pf12val is 0.064185354887412\n",
      "chi_f12 is 5.491960422654753\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.6861, -0.6685],\n",
      "        [-1.6469, -2.1791],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "76\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.845\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 13981.951\n",
      "expected info is tensor([[1.7769e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(27980.3873, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.9761,  1.7723]])\n",
      "new data istensor([[ 0.3042, -0.1851]])\n",
      "g_theta2 istensor([[-2.1236, -0.4202],\n",
      "        [-2.4715, -2.3569],\n",
      "        [-2.8749,  1.1588]])\n",
      "p21val is 0.688625257650484\n",
      "pf12val is 0.657589185107842\n",
      "chi_f12 is 0.838349762758588\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.0705,  0.2046],\n",
      "        [-1.5099,  0.8391],\n",
      "        [-0.9744,  1.7741]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "77\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.732\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 14320.083\n",
      "expected info is tensor([[1.7994e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(28657.2702, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.3072,  2.1820]])\n",
      "new data istensor([[ 0.0368, -0.0876]])\n",
      "g_theta2 istensor([[-0.2414, -0.2042],\n",
      "        [-1.4272,  0.8950],\n",
      "        [-1.8517,  2.5438]])\n",
      "p21val is 0.439961780379841\n",
      "pf12val is 0.168179856844887\n",
      "chi_f12 is 3.565442591559021\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.6441, -1.6538],\n",
      "        [-1.3653,  1.7640],\n",
      "        [-1.3084,  2.1815]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "78\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.863\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1613.629\n",
      "expected info is tensor([[1.9106e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3053.0618, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.2583, 3.0304]])\n",
      "new data istensor([[ 0.3093, -0.0394]])\n",
      "g_theta2 istensor([[ 2.5135, -2.3343],\n",
      "        [-1.9991,  1.1489],\n",
      "        [-2.1643,  1.3537]])\n",
      "p21val is 0.667374878002011\n",
      "pf12val is 0.355398319959911\n",
      "chi_f12 is 2.069032180907303\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.1971, -1.4431],\n",
      "        [-0.9058, -2.4198],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "79\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.861\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 16966.364\n",
      "expected info is tensor([[2.8764e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(33950.0764, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.7882,  1.9307]])\n",
      "new data istensor([[-0.0635,  0.0145]])\n",
      "g_theta2 istensor([[ 0.5584, -0.7490],\n",
      "        [-2.3443, -2.8397],\n",
      "        [-2.8647,  1.1531]])\n",
      "p21val is 0.429832306906308\n",
      "pf12val is 0.482469062118741\n",
      "chi_f12 is 1.457676960440055\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.1303,  0.1446],\n",
      "        [-1.2456,  1.0982],\n",
      "        [-1.7883,  1.9329]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "80\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.866\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2665.655\n",
      "expected info is tensor([[2.6717e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4875.8097, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0485, -1.3315]])\n",
      "new data istensor([[-0.4478,  0.0408]])\n",
      "g_theta2 istensor([[ 0.0159,  0.3354],\n",
      "        [-1.0396,  1.0517],\n",
      "        [-1.9405,  2.9661]])\n",
      "p21val is 0.602564786244047\n",
      "pf12val is 0.247638727368791\n",
      "chi_f12 is 2.791568678763315\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.6822, -2.4092],\n",
      "        [-1.2288, -1.0056],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "81\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.871\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 3451.550\n",
      "expected info is tensor([[4.5357e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5949.0362, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0696, -0.6695]])\n",
      "new data istensor([[-0.4347, -0.0292]])\n",
      "g_theta2 istensor([[ 1.1443, -2.3952],\n",
      "        [-0.3380, -0.8623],\n",
      "        [-1.1631,  1.1660]])\n",
      "p21val is 0.099581492569412\n",
      "pf12val is 0.139455738375261\n",
      "chi_f12 is 3.940016031399256\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.1982,  1.2432],\n",
      "        [-2.9286,  2.5071],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "82\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.872\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4434.772\n",
      "expected info is tensor([[2.7487e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7161.0273, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0928, -0.2136]])\n",
      "new data istensor([[-0.4769,  0.0119]])\n",
      "g_theta2 istensor([[ 0.1868,  1.2828],\n",
      "        [-2.5810,  2.6751],\n",
      "        [-1.1705,  1.1729]])\n",
      "p21val is 0.792501060655703\n",
      "pf12val is 0.444425806379378\n",
      "chi_f12 is 1.621944305484104\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.5721,  1.4004],\n",
      "        [ 2.8120, -1.2646],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "83\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.879\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5561.156\n",
      "expected info is tensor([[3.3641e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7897.9100, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1273,  0.1132]])\n",
      "new data istensor([[-0.4498,  0.0159]])\n",
      "g_theta2 istensor([[-1.5835,  1.7307],\n",
      "        [ 2.4958, -2.6670],\n",
      "        [-1.1731,  1.1764]])\n",
      "p21val is 0.514169552741900\n",
      "pf12val is 0.955136516039929\n",
      "chi_f12 is 0.091801999969387\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.0282, -2.1702],\n",
      "        [ 0.5423, -0.2746],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "84\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.887\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 6790.470\n",
      "expected info is tensor([[5.1326e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8682.3602, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1567,  0.5658]])\n",
      "new data istensor([[-0.3768,  0.0156]])\n",
      "g_theta2 istensor([[ 2.6044, -2.3898],\n",
      "        [ 1.2352, -0.8335],\n",
      "        [-1.1294,  1.1492]])\n",
      "p21val is 0.012432550980052\n",
      "pf12val is 0.686853592734263\n",
      "chi_f12 is 0.751268240967767\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.0484, -0.8235],\n",
      "        [ 2.9098, -1.7731],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "85\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.883\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7383.912\n",
      "expected info is tensor([[3.3890e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(8455.7382, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1779,  1.1846]])\n",
      "new data istensor([[-0.3021, -0.0267]])\n",
      "g_theta2 istensor([[-0.7278, -1.4139],\n",
      "        [ 2.4565, -2.7225],\n",
      "        [-1.1462,  1.1634]])\n",
      "p21val is 0.873702589433242\n",
      "pf12val is 0.753284375939055\n",
      "chi_f12 is 0.566624930374454\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.4127, -0.7707],\n",
      "        [-0.0461,  0.7822],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "86\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.887\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 8664.431\n",
      "expected info is tensor([[2.8260e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10292.1393, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.1878,  1.7852]])\n",
      "new data istensor([[-0.2694,  0.0128]])\n",
      "g_theta2 istensor([[-2.5267, -1.3280],\n",
      "        [ 0.3025,  1.8150],\n",
      "        [-1.1306,  1.1475]])\n",
      "p21val is 0.484257779363855\n",
      "pf12val is 0.429512160587770\n",
      "chi_f12 is 1.690210449198914\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.0433,  1.7447],\n",
      "        [-2.3307,  0.3122],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "87\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.892\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 13152.206\n",
      "expected info is tensor([[4.5169e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(26321.5249, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7918,  1.8317]])\n",
      "new data istensor([[-0.2379,  0.0089]])\n",
      "g_theta2 istensor([[-1.0096,  2.3678],\n",
      "        [-2.5344, -0.6020],\n",
      "        [-1.0984,  1.1503]])\n",
      "p21val is 0.246898350483163\n",
      "pf12val is 0.982663887193805\n",
      "chi_f12 is 0.034976285690436\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.5465,  0.9318],\n",
      "        [-2.2035,  1.2246],\n",
      "        [-2.7925,  1.8318]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "88\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.898\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 15034.317\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(30086.0668, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.6622,  2.0542]])\n",
      "new data istensor([[-0.1749,  0.0068]])\n",
      "g_theta2 istensor([[ 1.9187,  1.4633],\n",
      "        [-1.9306,  1.0066],\n",
      "        [-2.6517,  1.3056]])\n",
      "p21val is 0.669095201160654\n",
      "pf12val is 0.527315786526072\n",
      "chi_f12 is 1.279911388958313\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.2143,  1.5474],\n",
      "        [-0.0920,  0.9031],\n",
      "        [-2.6618,  2.0549]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "89\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.908\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 16063.527\n",
      "expected info is tensor([[8.5927e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(32144.4438, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.3068,  2.1151]])\n",
      "new data istensor([[-0.1247, -0.0285]])\n",
      "g_theta2 istensor([[-0.1498,  0.9472],\n",
      "        [ 1.3656,  0.2848],\n",
      "        [-2.6846,  0.9480]])\n",
      "p21val is 0.000000280411690\n",
      "pf12val is 0.369556862379901\n",
      "chi_f12 is 1.990901320732704\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "89\n",
      "Loss design: 8089.576\n",
      "expected info is tensor([[0.0017]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9093.6205, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0624,  3.1778]])\n",
      "new data istensor([[-0.1402, -0.0076]])\n",
      "g_theta2 istensor([[-2.6538,  2.1098],\n",
      "        [-1.7300,  1.1689],\n",
      "        [-1.8180,  1.2168]])\n",
      "p21val is 0.209546462813959\n",
      "pf12val is 0.770646119479166\n",
      "chi_f12 is 0.521051999577762\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.9473, -2.3066],\n",
      "        [-1.7120,  0.6519],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "90\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.907\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 16979.423\n",
      "expected info is tensor([[4.7704e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(33976.2770, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1955,  2.1843]])\n",
      "new data istensor([[-0.1181,  0.0074]])\n",
      "g_theta2 istensor([[ 2.5703, -2.4333],\n",
      "        [-0.2005,  0.7761],\n",
      "        [-1.1503,  1.1034]])\n",
      "p21val is 0.009255853002832\n",
      "pf12val is 0.933926655534912\n",
      "chi_f12 is 0.136714742202538\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "90\n",
      "Loss design: 5372.036\n",
      "expected info is tensor([[6.6959e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10759.4909, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7582, -0.0473]])\n",
      "new data istensor([[-0.4856,  0.0779]])\n",
      "g_theta2 istensor([[-0.3270, -0.5880],\n",
      "        [-2.9044,  2.8998],\n",
      "        [-1.0457,  2.5488]])\n",
      "p21val is 0.017143712546213\n",
      "pf12val is 0.462490067290381\n",
      "chi_f12 is 1.542260396778359\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.2590, -2.6791],\n",
      "        [-2.7012,  0.2870],\n",
      "        [-2.7582, -0.0472]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "91\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.908\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5402.066\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10819.6050, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7585, -0.7381]])\n",
      "new data istensor([[-0.4911,  0.0537]])\n",
      "g_theta2 istensor([[-1.5823, -2.5465],\n",
      "        [-1.8825,  1.0612],\n",
      "        [-2.6523,  0.8607]])\n",
      "p21val is 0.611797593071156\n",
      "pf12val is 0.688322368284431\n",
      "chi_f12 is 0.746995984470243\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4171,  1.0155],\n",
      "        [-1.4540, -2.5750],\n",
      "        [-2.7582, -0.7378]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "92\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.913\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1320.081\n",
      "expected info is tensor([[2.1230e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2409.4340, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0121, -3.0327]])\n",
      "new data istensor([[-0.5232, -0.0015]])\n",
      "g_theta2 istensor([[-0.1014,  0.6044],\n",
      "        [-1.6274, -2.5095],\n",
      "        [-2.7151,  0.2322]])\n",
      "p21val is 0.536890214329858\n",
      "pf12val is 0.145993328840871\n",
      "chi_f12 is 3.848388702376999\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.0176,  1.5043],\n",
      "        [-0.3566,  0.0991],\n",
      "        [-2.0000,  2.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "93\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.911\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 17487.049\n",
      "expected info is tensor([[5.2085e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(34991.6114, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.1888,  2.1886]])\n",
      "new data istensor([[-0.1462, -0.0178]])\n",
      "g_theta2 istensor([[ 0.2775,  0.2683],\n",
      "        [ 0.0987,  0.1376],\n",
      "        [-1.1602,  1.1111]])\n",
      "p21val is 0.379002111951897\n",
      "pf12val is 0.193468740533786\n",
      "chi_f12 is 3.285278654383075\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.8913,  0.0645],\n",
      "        [-0.1435, -2.6833],\n",
      "        [-2.1886,  2.1882]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "94\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.912\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 8631.290\n",
      "expected info is tensor([[6.5740e-06]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(17278.3494, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.2877,  0.4539]])\n",
      "new data istensor([[-0.5486,  0.2938]])\n",
      "g_theta2 istensor([[ 2.4075,  1.1308],\n",
      "        [ 0.5193, -2.4007],\n",
      "        [-2.8027,  2.8779]])\n",
      "p21val is 0.236492429527692\n",
      "pf12val is 0.208435558831417\n",
      "chi_f12 is 3.136250711736560\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.3717,  1.8646],\n",
      "        [ 0.0520,  2.9807],\n",
      "        [-2.2874,  0.4535]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "95\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.872\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2062.741\n",
      "expected info is tensor([[3.6197e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4139.4482, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.6537, -1.8166]])\n",
      "new data istensor([[-0.4219,  0.0107]])\n",
      "g_theta2 istensor([[-2.6251,  1.3607],\n",
      "        [ 0.3830,  2.7641],\n",
      "        [-2.7333,  1.4838]])\n",
      "p21val is 0.862329713175208\n",
      "pf12val is 0.434187612101463\n",
      "chi_f12 is 1.668557104684626\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.6941,  0.9331],\n",
      "        [ 1.6439, -2.6794],\n",
      "        [-2.6541, -1.8169]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "96\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.929\n",
      "END HYPERPARAMETERS optimization\n"
     ]
    }
   ],
   "source": [
    "loc_sample = loc_sample0.clone()\n",
    "iter_hp = 50\n",
    "iter_design = 100\n",
    "iter_param = 100\n",
    "num_base_kernels = 2\n",
    "max_iter = 50\n",
    "\n",
    "\n",
    "f_target = Tensor([[-1., -1.]]).reshape(2,1) \n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    " #np.random.random_sample((loc_size,2))\n",
    "#loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "#train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "\n",
    "#loc_sample = Tensor([[0.0, 0.1], [0.0, -0.1]]) #T\n",
    "# loc_x = (-1.5 + 2.)  * np.random.random_sample((loc_size,1)) +2.\n",
    "\n",
    "# # loc_y = (2. - 1.5)  * np.random.random_sample((loc_size,1)) - 1.5\n",
    "# # loc = np.concatenate((loc_x, loc_y), 1)\n",
    "print(loc_sample)\n",
    "\n",
    "\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten()\n",
    "\n",
    "data_fit_vec = torch.empty((1,1))\n",
    "entropy_vec = torch.empty((1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "vec_x = x0.clone() #Tensor(np.array([0.0,0.0])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "var_vec = torch.zeros([max_iter, 1])\n",
    "p21_vec = torch.empty((1,1))\n",
    "\n",
    "lr_new = .01\n",
    "thresh_EI = 0.001\n",
    "    \n",
    "count_EI = 0\n",
    "max_count_EI = 50\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "show_TTRBox = False\n",
    "iter = 0    \n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "patience = 0.0\n",
    "patience_f = 0.0\n",
    "patience_2 = 0.0\n",
    "checking_model = False\n",
    "model_double_check = False\n",
    "while(SUCCESS == False and FAILURE == False):\n",
    "    print(iter)\n",
    "    model_double_check = False\n",
    "    if (checking_model == False):\n",
    "        print('START HYPERPARAMETERS optimization')\n",
    "        if (iter == 0):\n",
    "            cur_model = None\n",
    "            cur_likelihood = None\n",
    "\n",
    "\n",
    "        loc_sample_old = loc_sample.clone()\n",
    "        x0_old = x0.clone()\n",
    "        model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "\n",
    "\n",
    "        print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "   \n",
    "    \n",
    "    x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new,noise_value)\n",
    "  \n",
    "    cur_model = model\n",
    "    cur_likelihood = likelihood\n",
    "    \n",
    "  \n",
    "    lower_bound = torch.zeros(pf1.shape)\n",
    "    upper_bound = torch.zeros(pf1.shape)\n",
    "        \n",
    "    for i in range(pf1.shape[0]):\n",
    "        lower_bound[i] = pf1[i] -  torch.sqrt(Qf12[i,i])\n",
    "        upper_bound[i] = pf1[i] +  torch.sqrt(Qf12[i,i])\n",
    "#     d = torch.sqrt(gpytorch.inv_quad(Qf1, f_target - pf1))\n",
    "#     print(d)\n",
    "    SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    \n",
    "    \n",
    "    entropy = ( 0.5 * torch.log( torch.det(Qf1.evaluate()) / torch.det(Qf12.evaluate()) ) ).reshape(1,1)\n",
    "    \n",
    "    print('expected info is '+str(entropy))\n",
    "    print('mohabb disatance is' + str(Qf12.inv_quad(f_target - pf1)))\n",
    "    if not SUCCESS:\n",
    "    \n",
    "    #var_vec[iter] = var\n",
    "        \n",
    "        \n",
    "\n",
    "         #np.random.random_sample((loc_size,2))\n",
    "\n",
    "        \n",
    "\n",
    "        new_data = vfield_(g_theta2.detach())  \n",
    "        agg_data12 = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "        g_theta12= torch.cat([g_theta1, g_theta2.detach()], 0)\n",
    "        new_data_x = vfield_(x0_new.detach() )  \n",
    "        print('current sol is'+str(x0_new.detach()))\n",
    "        print('new data is' + str(new_data_x))\n",
    "        print('g_theta2 is' + str(g_theta2.detach()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            if iter >= 0:\n",
    "                \n",
    "                \n",
    "                p21 = likelihood.get_p21(g_theta1, g_theta2.detach(), agg_data, model, noise_value)\n",
    "                \n",
    "#                 Q21 = Q21 + noise_value*torch.eye(Q21.shape[0])\n",
    "                chi_21 = (Q21).inv_quad(new_data.flatten() - p21.reshape(new_data.flatten().shape))\n",
    "                p_val = 1. - stats.chi2.cdf(chi_21, Q21.shape[0])\n",
    "                pf12 = likelihood.get_pf12(Q21,g_theta1, g_theta2.detach(), x0_new.detach(), new_data.flatten(), pf1, p21, model, noise_value)\n",
    "               \n",
    "                #Qf12 = Qf12 + \n",
    "                chi_f12 = (Qf12 + noise_value*torch.eye(Qf12.shape[0])).inv_quad(new_data_x.flatten() - pf12.reshape(new_data_x.flatten().shape))\n",
    "                p_val_f12 = 1. - stats.chi2.cdf(chi_f12, Qf12.shape[0])\n",
    "                print('p21val is %.15f' %p_val)\n",
    "                p21_vec = torch.cat([p21_vec, Tensor([p_val]).reshape(1,1)], 0)\n",
    "                print('pf12val is %.15f' %p_val_f12)\n",
    "                print('chi_f12 is %.15f' %chi_f12 )\n",
    "                \n",
    "                if (p_val < 0.01):# or p_val_f12 < 0.001:\n",
    "                    model_double_check = True\n",
    "                    checking_model = True\n",
    "                    patience = patience+1\n",
    "                    print('patience is %.3f' %patience)\n",
    "\n",
    "                if (model_double_check == True):\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_sample.shape[0],2)) + vf.low)\n",
    "                    sum = torch.zeros(2, 2) #replace with num_tasks\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_old).t(), (g_theta2.detach()[i] - x0_old) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum #+ torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_old, covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_old],0)\n",
    "                    \n",
    "                    x0 = x0_old #Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    if (patience >= 2):# or patience_2 >= 2 or patience_f >= 2):\n",
    "                        PATH = \".//model_Carlo/model_update/model_fail_EIbehavior_\"+str(iter)+\".pt\"\n",
    "                        torch.save(model, PATH)\n",
    "                        entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                        data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                        iter = iter + 1\n",
    "                        patience = 0\n",
    "#                         patience_2 = 0\n",
    "#                         patience_f = 0\n",
    "                        model_double_check = False\n",
    "                        checking_model = False\n",
    "                        num_base_kernels = num_base_kernels + 1\n",
    "                        print('adding complexity to model')\n",
    "                        print('num base is' + str(num_base_kernels))\n",
    "#     #                         \n",
    "                        loc_sample = loc_sample_old\n",
    "                        #x0 = x0_old\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                        g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                        print('acquiring, new size is' + str(g_theta1.shape[0]))\n",
    "                 \n",
    "                    #iter_hp = iter_hp + 10\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    PATH = \".//model_Carlo/model_goodmodel/model_fail_EIbehavior_\"+str(iter)+\".pt\"\n",
    "                    torch.save(model, PATH)\n",
    "                    vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                    entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                    data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                    model_double_check = False\n",
    "                    iter = iter + 1\n",
    "                    patience = 0\n",
    "                    patience_2 = 0\n",
    "                    patience_f = 0\n",
    "                    checking_model = False\n",
    "                    if (entropy < thresh_EI):\n",
    "                        count_EI = count_EI + 1\n",
    "                    else:\n",
    "                        count_EI = 0\n",
    "                    if (count_EI >= max_count_EI):\n",
    "                        FAILURE = True\n",
    "                    \n",
    "                    x0 = (x0_new.detach())# + torch.randn(x0_new.detach().size()) * .001)#/torch.norm(x0_new.detach())\n",
    "                    sum = torch.zeros(2, 2)\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum# + torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_new.detach(), covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = loc_sample#/torch.norm(loc_sample)\n",
    "                    #loc_sample = 2. *  (loc_sample - torch.min(loc_sample)) / (torch.max(loc_sample) - torch.min(loc_sample)) - 1.\n",
    "                    #loc_sample[0] = x0_new.detach() #+ torch.randn(x0_new.detach().size()) * .001 #g_theta2.detach() #loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_size,2)) + vf.low)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_new.detach()],0)\n",
    "                    for i in range(loc_sample.shape[0]):\n",
    "                        if loc_sample[i,0] < -3. or loc_sample[i,0] > 3. or loc_sample[i,1] < -3. or loc_sample[i,1] > 3.:\n",
    "                            print('samples escaped box')\n",
    "                            loc_sample[i] = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    \n",
    "                    \n",
    "#                     if p_val > 0.99 and p_val_f12 > 0.99:\n",
    "#                         num_base_kernels = max(num_base_kernels - 1, 3)\n",
    "                        #iter_hp = iter_hp - 10\n",
    "                    chi_f_target = (Qf12 ).inv_quad(f_target - pf1)\n",
    "                    p_val_f_target = 1. - stats.chi2.cdf(chi_f_target, Qf12.shape[0])\n",
    "                    print('p_val_ftarget is '+str(p_val_f_target))\n",
    "                    if (p_val_f_target > .95):\n",
    "                        print('acquiring target point becuse p_val_ftarget is '+str(p_val_f_target))\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "        \n",
    "\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                    else:\n",
    "#                         x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "#                         g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                       \n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                        \n",
    "                    if x0_new.detach()[0,0] < -3. or x0_new.detach()[0,0] > 3. or x0_new.detach()[0,1] < -3. or x0_new.detach()[0,1] > 3.:\n",
    "#                         x0 = Tensor(np.array([0.0,-1.0])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = x0.reshape(1,2) \n",
    "                        x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = x0.reshape(1,2)\n",
    "                        #x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                 \n",
    "             #       loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "                        loc_sample[-1] = x0 #(x0_new.detach()) \n",
    "                    print('new 2 points')\n",
    "                    print(loc_sample)\n",
    "                  \n",
    " #                    agg_data  = (agg_data  - agg_data.mean())/agg_data .std(dim=-1, keepdim=True)\n",
    "#                     g_theta1 = (g_theta1 - g_theta1.mean())/g_theta1.std(dim=-2, keepdim=True)\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            #clear_output(wait=False)\n",
    "           \n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "PATH = \".//model_Carlo/model_goodmodel/model_fail_EIbehavior_\"+str(iter)+\".pt\"\n",
    "torch.save(model, PATH)\n",
    "print('current sol is'+str(x0_new.detach()))\n",
    "    \n",
    "print('Success is ' + str(SUCCESS) + ' and failure is ' + str(FAILURE)+' after '+ str(iter) + ' iterations')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target - 0.001)\n",
    "print(f_target + 0.001)\n",
    "print(pf1)\n",
    "print(num_base_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=30)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=28)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=24)    # legend fontsize\n",
    "plt.rc('font', size=24)          # controls default text sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "#plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "p21_vec_plot = p21_vec.detach()[1:]\n",
    "f, ax = plt.subplots(1, 1, figsize=(18, 18))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "#print(p21_vec_plot)\n",
    "ax.plot(p21_vec_plot,'s',color = 'blue', markersize=6)\n",
    "ax.axhline(.01,linestyle = '--',color = 'red', markersize=12, alpha = 1.0)\n",
    "ax.set_xlim(-0.3, p21_vec_plot.shape[0])\n",
    "ax.set_ylim(-0.3, 1.1)\n",
    "#ax.set_yscale('log')\n",
    "plt.xticks(np.arange(0, iter+15, step=5.))\n",
    "ax.tick_params(labelsize='small', width=3)\n",
    "ax.set_xlabel('# of Model Checks')\n",
    "ax.set_ylabel('p-value')\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.03, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(1.5,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.007, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(0.3,        #x start point\n",
    "             0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.18,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "#############################\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.07, 0.1), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(4.4,        #x start point\n",
    "             -0.15,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.1,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.04, 0.3), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(3.4,        #x start point\n",
    "             0.12,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.08,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "###############################################\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.14, 0.15), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(9.5,        #x start point\n",
    "#              -0.09,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.07,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.03,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.09, 0.3), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(10.5,        #x start point\n",
    "             0.11,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.07,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "###############################################\n",
    "\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.2, 0.29), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(23.5,        #x start point\n",
    "             0.12,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.08,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "#############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.23, 0.31), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(27.5,        #x start point\n",
    "             0.13,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.09,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "#############\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.26, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(20.2,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.26, 0.25), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(29.4,        #x start point\n",
    "             0.05,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.03,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.01,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "#############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.3, 0.31), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(36.8,        #x start point\n",
    "             0.13,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.09,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "#############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.37, 0.31), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(43.4,        #x start point\n",
    "             0.13,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.09,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "##########\n",
    "#############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.48, 0.31), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(56.8,        #x start point\n",
    "             0.13,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.09,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "##########\n",
    "ax.annotate('Restart', xy=(0.55, 0.15), xytext=(0.54, 0.33), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(64.4,        #x start point\n",
    "             0.17,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.13,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.03,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "ax.annotate('Threshold Line ($y = 0.01$)', xy=(1.0,0.01), xytext=(6,0), color='red', \n",
    "                xycoords = ax.get_yaxis_transform(), textcoords=\"offset points\",\n",
    "                size=14, fontsize = 20, va=\"center\")\n",
    "\n",
    "\n",
    "\n",
    "#ax.legend(['p-value', 'p-value Threshold ($0.01$)'], loc = 'lower right')\n",
    "plt.savefig('figures/qvalue_fail_EIBehavior.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(18, 8), tight_layout=True)\n",
    "\n",
    "ax1.plot(np.array(range(1,iter+2)), (entropy_vec_plot), '--o', color = 'blue', markersize=12)\n",
    "#ax1.set_yscale('log')\n",
    "# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2.plot(np.array(range(1,iter+2)), data_fit_vec_plot, '--o', color = 'red', markersize=12)\n",
    "\n",
    "ax1.set_xlabel('Iteration #', size=24)\n",
    "ax2.set_xlabel('Iteration #', size=24)\n",
    "ax1.set_xticks(np.arange(0,iter+5, step=30.))\n",
    "ax1.set_ylabel('Expected Information (log scale)', size = 24)\n",
    "ax2.set_ylabel('Log-Gaussian Term', size = 24)\n",
    "ax2.set_xticks(np.arange(0,iter+5, step=30.))\n",
    "#ax2.yfmt.set_useOffset(10000)\n",
    "\n",
    "plt.savefig('figures_Carlo/expectedinfo_vs_datafit_fail_EIbehavior.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/vec_x_fail_EIbehavior.txt',vec_x.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta2_fail_EIbehavior.txt', g_theta2.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta1_fail_EIbehavior.txt', g_theta1.detach().numpy())\n",
    "np.savetxt('data_plots/x_train_ini_fail_EIbehavior.txt', x_train.detach().numpy())\n",
    "np.savetxt('data_plots/y_train_ini_fail_EIbehavior.txt', y_train.detach().numpy())\n",
    "np.savetxt('data_plots/agg_data_fail_EIbehavior.txt', agg_data.detach().numpy())\n",
    "np.savetxt('data_plots/entropy_vec_fail_EIbehavior.txt', entropy_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/datafit_fail_EIbehavior.txt', data_fit_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/p21_vec_fail_EIbehavior.txt',p21_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/loss_fail_EIbehavior.txt',loss.detach().numpy())\n",
    "np.savetxt('data_plots/pf1_fail_EIbehavior.txt',pf1.detach().numpy())\n",
    "np.savetxt('data_plots/Qf1_fail_EIbehavior.txt',Qf1.evaluate().detach().numpy())\n",
    "np.savetxt('data_plots/Qf12_fail_EIbehavior.txt', Qf12.evaluate().detach().numpy())\n",
    "np.savetxt( 'data_plots/Q21_fail_EIbehavior.txt', Q21.evaluate().detach().numpy())\n",
    "#np.savetxt('data_plots/iter_fail_EIbehavior.txt', iter+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/v2_fail_EIbehavior.txt', v2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "#ax.scatter(g_theta1[:, 0].detach(),g_theta1[:, 1].detach(), c=\"b\", alpha=0.8)\n",
    "ax.plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(vec_x[-1,0], vec_x[-1,1],'o', color = 'red',markersize=15)\n",
    "#ax.plot(0.8731, 0.5664,'go', color = 'green',markersize=15)\n",
    "ax.set_title('Final TAD configuration')\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.legend(['1-points'])\n",
    "plt.savefig('figures/tad_sol_allpoints_fail_EIbehavior.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker\n",
    "class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "        self.oom = order\n",
    "        self.fformat = fformat\n",
    "        matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
    "    def _set_order_of_magnitude(self):\n",
    "        self.orderOfMagnitude = self.oom\n",
    "    def _set_format(self, vmin=None, vmax=None):\n",
    "        self.format = self.fformat\n",
    "        if self._useMathText:\n",
    "             self.format = r'$\\mathdefault{%s}$' % self.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.1, 3.1, 100)\n",
    "y_plot = np.linspace(-3.1, 3.1, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "likelihood.eval()\n",
    "\n",
    "#noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "#print(x_concat_)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(True):\n",
    "    pred = GPprediction(model)\n",
    "    pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "    #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "    pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "    mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "    mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "    pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "    \n",
    "    var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "    var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "#     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "# #     print(pr.covariance_matrix.mean(axis=0))\n",
    "# #     print(AA)\n",
    "# #     print(pr.variance)\n",
    "# #     print((pr.covariance_matrix))\n",
    "# #     K = model.covar_module\n",
    "#     print((cov.diag()))\n",
    "#     print(pr_mean)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 2, figsize = (28, 24), tight_layout=True)\n",
    "diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "cs10 = ax1[0].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "#ax1[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[0].set_title('$|v_1 - \\mu(v_1)|$', fontsize = 40)\n",
    "cbar10 = fig.colorbar(cs10, ax = ax1[0],format=OOMFormatter(0, mathText=False));\n",
    "\n",
    "ax1[0].set_xlabel('$d_1$')\n",
    "ax1[0].set_ylabel('$d_2$')\n",
    "diff_mean_v1 = torch.abs(v_1 - mean_v_1.detach())/torch.sqrt(var_v_1)\n",
    "#print(var_v_1)\n",
    "cs11 = ax1[1].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(diff_mean_v1.min(), diff_mean_v1.max(), 100), cmap = 'jet')\n",
    "#ax1[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[1].set_title('$|v_1 - \\mu(v_1)|/\\sigma(v_1)$', fontsize = 40)\n",
    "# ax1[0].set_aspect('equal')\n",
    "# ax1[1].set_aspect('equal')\n",
    "cbar11 = fig.colorbar(cs11, ax = ax1[1],format=OOMFormatter(0, mathText=False));\n",
    "ax1[1].set_xlabel('$d_1$')\n",
    "ax1[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())\n",
    "cs20 = ax2[0].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "#ax2[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[0].set_title('$|v_2 - \\mu(v_2)|$', fontsize = 40)\n",
    "cbar20 = fig.colorbar(cs20, ax = ax2[0],format=OOMFormatter(0, mathText=False));\n",
    "ax2[0].set_xlabel('$d_1$')\n",
    "ax2[0].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())/torch.sqrt(var_v_2)\n",
    "cs21 = ax2[1].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(diff_mean_v2.min(), diff_mean_v2.max(), 100), cmap = 'jet')\n",
    "ax2[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "#ax2[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[1].set_title('$|v_2 - \\mu(v_2)|/\\sigma(v_2)$', fontsize = 40)\n",
    "cbar21 = fig.colorbar(cs21, ax = ax2[1],format=OOMFormatter(0, mathText=False));\n",
    "ax2[1].set_xlabel('$d_1$')\n",
    "ax2[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "# ax2[0].set_aspect('equal')\n",
    "# ax2[1].set_aspect('equal')\n",
    "\n",
    "plt.savefig('figures_Carlo/mean_var/mean_final__fail_2_EIBehavior.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.1, 3.1, 100)\n",
    "y_plot = np.linspace(-3.1, 3.1, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "plot = [iter+1]\n",
    "\n",
    "for ii in plot:\n",
    "    try:\n",
    "        \n",
    "        PATH = \".//model_Carlo/model_goodmodel/model_fail_EIbehavior_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "    except:\n",
    "        PATH = \".//model_Carlo/model_update/model_fail_EIbehavior_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "        \n",
    "    #model_16 = torch.load(PATH)\n",
    "    model_16.eval()\n",
    "\n",
    "    likelihood.eval()\n",
    "\n",
    "    #noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "    #print(x_concat_)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var(False):\n",
    "        pred = GPprediction(model_16)\n",
    "        pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "        #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "        pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "        mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "        mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "        pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "\n",
    "        var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "        var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "    #     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "    # #     print(pr.covariance_matrix.mean(axis=0))\n",
    "    # #     print(AA)\n",
    "    # #     print(pr.variance)\n",
    "    # #     print((pr.covariance_matrix))\n",
    "    # #     K = model.covar_module\n",
    "    #     print((cov.diag()))\n",
    "    #     print(pr_mean)\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (28, 12), tight_layout=True)\n",
    "    diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "    minn = torch.min(var_v_1.detach().min(), var_v_2.detach().min())\n",
    "    maxx = torch.min(var_v_1.detach().max(), var_v_2.detach().max())\n",
    "\n",
    "    cs11 = ax1.contourf(xv_plot, yv_plot, var_v_1.detach(), np.linspace(0.0, .02, 100), cmap = 'jet')\n",
    "   # ax1.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    \n",
    "    ax1.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "  \n",
    "    ax1.set_title('$\\sigma^2(v_1)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    # ax1[0].set_aspect('equal')\n",
    "    # ax1[1].set_aspect('equal')\n",
    "    cbar11 = fig.colorbar(cs11, ax = ax1,format=OOMFormatter(-2, mathText=False));\n",
    "    ax1.set_xlabel('$d_1$')\n",
    "    ax1.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    cs21 = ax2.contourf(xv_plot, yv_plot, var_v_2.detach(), np.linspace(0.0, .02, 100), cmap = 'jet')\n",
    "    ax2.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "    ax2.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    ax2.set_title('$\\sigma^2(v_2)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    cbar21 = fig.colorbar(cs21, ax = ax2,format=OOMFormatter(-2, mathText=False));\n",
    "    ax2.set_xlabel('$d_1$')\n",
    "    ax2.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "    # ax2[0].set_aspect('equal')\n",
    "    # ax2[1].set_aspect('equal')\n",
    "\n",
    "    plt.savefig('figures_Carlo/mean_var/var_iter_fail_EIBehavior'+str(ii)+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3.1, 3.1, 30)\n",
    "# y_plot = np.linspace(-3.1, 3.1, 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "# loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# g2 = g_theta2.detach() #loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# plot = [1, 6, 12, 17, 30, 35, 38, 56, 76]\n",
    "# zz = torch.zeros(n*n, 9)\n",
    "# kk = 0\n",
    "# for jj in plot:\n",
    "#     try:\n",
    "        \n",
    "#         PATH = \".//model_Carlo/model_goodmodel/model_fail_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#     except:\n",
    "#         PATH = \".//model_Carlo/model_update/model_fail_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#    # model_16 = torch.load(PATH)\n",
    "#     model_16.eval()\n",
    "\n",
    "#     likelihood.eval()\n",
    "#     g2 = v2.detach()[jj - 1 +loc_size+1 : jj - 1 +loc_size+1 +loc_size+1]\n",
    "#     g_theta1_cur = g_theta1[0:(4 + (jj - 1)* 3)]\n",
    "#     agg_data_cur = agg_data[0:2 * (4 + (jj - 1)* 3)]\n",
    "#     print(agg_data_cur.shape)\n",
    "#     for ii in range(n*n):\n",
    "#         print(ii)\n",
    "#         x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     #     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#     #                     #loc_size = 4\n",
    "#     #     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     #     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#         #g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "\n",
    "\n",
    "        \n",
    "#         loss2_, pf1_, Qf1_, Qf12_, data_fit_, Q21_ = likelihood.get_ell(agg_data_cur,f_target,x0, g_theta1_cur, model_16, likelihood, noise_value, g2)\n",
    "#         zz[ii, kk] = loss2_\n",
    "#     kk = kk+1\n",
    "# zz = zz.reshape(n,n, 9)\n",
    "# torch.save(zz, 'data_plots/zz_fail.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.ticker\n",
    "# class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "#     def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "#         self.oom = order\n",
    "#         self.fformat = fformat\n",
    "#         matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
    "#     def _set_order_of_magnitude(self):\n",
    "#         self.orderOfMagnitude = self.oom\n",
    "#     def _set_format(self, vmin=None, vmax=None):\n",
    "#         self.format = self.fformat\n",
    "#         if self._useMathText:\n",
    "#              self.format = r'$\\mathdefault{%s}$' % self.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for jj in range(9):\n",
    "#     fig, ax = plt.subplots(figsize = (16,14))\n",
    "#     #\n",
    "#     cs = ax.contour(xv_plot, yv_plot,  zz[:,:,jj].detach(), np.linspace( zz[:,:,jj].detach().numpy().min(), zz[:,:,jj].detach().numpy().max(), 1000), cmap = 'jet')\n",
    "#     cbar = fig.colorbar(cs, ax = ax,format=OOMFormatter(0, mathText=False));\n",
    "#     #ax.plot(vf.tgt_loc[0,0],vf.tgt_loc[0,1], 'o', color = 'magenta', markersize=12)\n",
    "#     kk = plot[jj]\n",
    "#     if kk < plot[8]:\n",
    "#         ax.plot(vec_x[kk,0], vec_x[kk,1],'o', color = 'black',markersize=12)\n",
    "#     if kk == plot[8]:\n",
    "#         ax.plot(vec_x[- 1,0], vec_x[- 1,1],'o', color = 'black',markersize=12)\n",
    "#     ax.set_title('TAD Acquisition Function (Iteration '+str(kk)+')', fontsize = 40)\n",
    "#     ax.set_xlabel('$d_1$')\n",
    "#     ax.set_ylabel('$d_2$')\n",
    "    \n",
    "    \n",
    "#     plt.savefig('figures_Carlo/tad_obj_fail'+str(kk)+'.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "#v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "ii = 0\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "########################\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "ax.plot(vec_x[ii,0], vec_x[ii,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(loc_sample0.detach()[ii:ii+loc_size+1,0], loc_sample0.detach()[ii:ii+loc_size+1,1], 'o', color = 'blue', markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Initial Configuration', fontsize = 40)\n",
    "ax.legend(['Target', 'Initial Target Candidate', 'Initial 1-sample','Initial 2-sample'])\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_ini_2_EI_behavior.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "\n",
    "#data= scipy.io.loadmat('data_plots/p_vec_fail.mat')\n",
    "#p_fail =p_vec_plot\n",
    "count, bins, ignored = ax.hist(p21_vec_plot.detach().numpy(), 30, facecolor='green') \n",
    "\n",
    "ax.set_xlabel('p-value')\n",
    "ax.set_ylabel('Count')\n",
    "#plt.title(\"Uniform Distribution Histogram (Bin size 20)\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 20)\n",
    "plt.xticks(np.arange(0, 1.2, step=.2))\n",
    "plt.yticks(np.arange(0, 20, step=2.))\n",
    "#ax.set_axis([0, 1, 0, 20]) # x_start, x_end, y_start, y_end\n",
    "ax.grid(True)\n",
    "\n",
    "\n",
    "plt.savefig('figures_Carlo/p_value_fail_hist_EI.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show(block = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "fig, ax = plt.subplots(figsize = (14,12))\n",
    "\n",
    "#data= scipy.io.loadmat('data_plots/p_vec_fail.mat')\n",
    "#p_fail =p_vec_plot\n",
    "count, bins, ignored = ax.hist(p21_vec_plot.detach().numpy()[9:], 15, facecolor='green') \n",
    "\n",
    "ax.set_xlabel('p-value')\n",
    "ax.set_ylabel('Count')\n",
    "#plt.title(\"Uniform Distribution Histogram (Bin size 20)\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 12)\n",
    "plt.xticks(np.arange(0, 1.2, step=.2))\n",
    "plt.yticks(np.arange(0, 12, step=2.))\n",
    "#ax.set_axis([0, 1, 0, 20]) # x_start, x_end, y_start, y_end\n",
    "ax.grid(True)\n",
    "\n",
    "\n",
    "plt.savefig('figures_Carlo/p_value_fail_hist_EI.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show(block = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p21_vec_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
