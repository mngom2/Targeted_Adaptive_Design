{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(\"..\")\n",
    "from LBFGS import FullBatchLBFGS\n",
    "from kernels import vvkernels as vvk, sep_vvkernels as svvk, vvk_rbfkernel as vvk_rbf\n",
    "from means import vvmeans as vvm\n",
    "from likelihood import vvlikelihood as vvll\n",
    "from mlikelihoods import MarginalLogLikelihood as exmll\n",
    "from predstrategies import GPprediction\n",
    "from utils import ObjFun, get_vertices, stopping_criteria\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=40)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=32)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=32)    # legend fontsize\n",
    "plt.rc('font', size=32)          # controls default text sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "\n",
    "We sample from $$V_1(x_1, x_2) = 3(1 - x_1)^2 e^{-x_1^2 - (x_2 +1)^2} - 10 (x_1/5 - x_1 ^3 - x_2^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- (x_1 + 2) ^2 - x_2^2} + 0.5(2x_1 + x_2)$$\n",
    "$$V_2(x_1, x_2) = 3(1 +x_2)^2 e^{-x_2^2 - (x_1 +1)^2} - 10 (-x_2/5 + x_2 ^3 + x_1^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- ( 2- x_2) ^2 - x_1^2} + 0.5(2x_1 + x_2)$$\n",
    "\n",
    "where $(x_1, x_2) \\in [-3, 3]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3380, 0.3502], dtype=torch.float32)\n",
      "tensor([[ 1.1487, -1.2708],\n",
      "        [ 1.5070, -1.8768],\n",
      "        [ 1.2688, -1.5524],\n",
      "        [ 1.9843, -1.4909]])\n"
     ]
    }
   ],
   "source": [
    "vf = ObjFun()\n",
    "f_target = vf.tgt_vec\n",
    "print(f_target)\n",
    "sample_size = 4\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -3.\n",
    "vf.high = 3.\n",
    "\n",
    "high_minus_low = vf.high- vf.low\n",
    "#high_minus_low = -\n",
    "def g_theta(sample_size, D):\n",
    "    loc_x = (2. - 1.0 )  * np.random.random_sample((sample_size,1)) + 1.0\n",
    "    \n",
    "    loc_y = (2.  -1.0)  * np.random.random_sample((sample_size,1)) - 2.\n",
    "    loc = np.concatenate((loc_x, loc_y), 1)\n",
    "    #loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "#train_x = Tensor([[-1.5, 1.5], [-1.5, 1.3]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    \n",
    "    out = vf(x[:,0], x[:,1]) + torch.randn(Tensor(vf(x[:,0], x[:,1])).size()) * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)\n",
    "\n",
    "# print(train_y)\n",
    "# train_y = (train_y - train_y.mean())/train_y.std(dim=-2, keepdim=True)\n",
    "# train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "# print(train_y)\n",
    "# print(train_y.std(dim=-2, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP model initialization\n",
    "We inialize the GP model following https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)  #vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)#vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)  # \n",
    "        base_kernels = [] #contain all the base kernels\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters oprimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels,noise_value, current_model = None, current_likelihood = None):\n",
    "    noises = torch.ones(agg_data.shape[0]) * (noise_value) #  torch.zeros(agg_data.shape[0]) # \n",
    "    noises = noises.reshape(g_theta1.shape[0], 2)\n",
    "    \n",
    "#     if (current_model is not None):\n",
    "#         likelihood = current_likelihood #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)  #\n",
    "\n",
    "#         model = current_model#.get_fantasy_model(g_theta1, agg_data) #MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#         model.set_train_data(g_theta1, agg_data,  strict=False)\n",
    "#     else:\n",
    "#         likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2)#vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #\n",
    "#         model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "        \n",
    "    likelihood =  vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2) #\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.double()\n",
    "    likelihood.double()\n",
    "\n",
    "    \"\"\"Put related things on GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        g_theta1 = g_theta1.cuda()\n",
    "        agg_data = agg_data.cuda()\n",
    "        \n",
    "    else:\n",
    "        print(\"Using GPU\")\n",
    "    \n",
    "    \n",
    "    \"\"\"end for GPU\"\"\"\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #, weight_decay=0.001)  # Includes GaussianLikelihood parameters\n",
    "    mll = exmll(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, chi_square = mll(agg_data,g_theta1, model, likelihood, noise_value)\n",
    "        loss = -1. * loss\n",
    "#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \n",
    "        #print('loss is %.3f' %loss)\n",
    "#         df = agg_data.shape[0]\n",
    "#         chi_square = chi_square.clone().detach()\n",
    "        \n",
    "#         p_val = 1. - stats.chi2.cdf(chi_square, df)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "       # print(p_val)\n",
    "#         if (p_val > 0.99999):\n",
    "#             return model, likelihood\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    print('loss is %.3f' %loss)\n",
    "#     for params in model.named_parameters():\n",
    "#         print(params)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design parameters and sampling point optimization (where to explore?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': g_theta2, 'lr': 0.1},{'params': x_d, 'lr': 0.1}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss2)\n",
    "        scheduler.step()\n",
    "        \n",
    "    loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "    loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    #print(x_d)\n",
    "    return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conducting the TAD experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_size = 2\n",
    "#loc_sample0 = Tensor((2. - 1.5)  * np.random.random_sample((loc_size,2)) + 1.5)\n",
    "x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "x0 = x0.reshape(1,2)\n",
    "\n",
    "dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(loc_size) )\n",
    "                    #loc_size = 4\n",
    "loc_sample = dis_2sample.sample((loc_size + 1,))\n",
    "\n",
    "loc_sample0 = loc_sample.reshape(loc_size + 1, 2)\n",
    "#loc_sample0[-1] = train_x[-1] + 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1896,  2.0495],\n",
      "        [-2.1469,  1.9705],\n",
      "        [-1.9492,  1.8899]])\n",
      "0\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CUDA\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         loc_sample_old \u001b[38;5;241m=\u001b[39m loc_sample\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     72\u001b[0m         x0_old \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m---> 73\u001b[0m         model, likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_opti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_theta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43magg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43miter_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_base_kernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnoise_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_likelihood\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_likelihood\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Before this is hyper_opti\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEND HYPERPARAMETERS optimization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mhyper_opti\u001b[0;34m(g_theta1, agg_data, training_iter, num_base_kernels, noise_value, current_model, current_likelihood)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_iter):\n\u001b[1;32m     42\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 44\u001b[0m         loss, chi_square \u001b[38;5;241m=\u001b[39m \u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mg_theta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m*\u001b[39m loss\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m#print('loss is %.3f' %loss)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#         df = agg_data.shape[0]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#         chi_square = chi_square.clone().detach()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         \n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#         p_val = 1. - stats.chi2.cdf(chi_square, df)\u001b[39;00m\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/tiany/Targeted_Adaptive_Design/code/jupyter/../mlikelihoods/tadmlls.py:73\u001b[0m, in \u001b[0;36mMarginalLogLikelihood.forward\u001b[0;34m(self, agg_data, g_theta1, model, likelihood, noise_value)\u001b[0m\n\u001b[1;32m     69\u001b[0m         diff \u001b[38;5;241m=\u001b[39m agg_data \u001b[38;5;241m-\u001b[39m m0\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#        if diff.shape[:-1] != covar.batch_shape:\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#            print('error shape')\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m         inv_quad_C11, logdet_C11 \u001b[38;5;241m=\u001b[39m \u001b[43mgpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m         pi \u001b[38;5;241m=\u001b[39m Tensor([math\u001b[38;5;241m.\u001b[39mpi])\n\u001b[1;32m     75\u001b[0m         N \u001b[38;5;241m=\u001b[39m C11\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/functions/__init__.py:156\u001b[0m, in \u001b[0;36minv_quad_logdet\u001b[0;34m(mat, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mComputes an inverse quadratic form (w.r.t mat) with several right hand sides.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03mI.e. computes tr( tensor^T mat^{-1} tensor )\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    - scalar - log determinant\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazify\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlazify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_inv_quad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_inv_quad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1291\u001b[0m, in \u001b[0;36mLazyTensor.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             will_need_cholesky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m will_need_cholesky:\n\u001b[0;32m-> 1291\u001b[0m         cholesky \u001b[38;5;241m=\u001b[39m CholLazyTensor(TriangularLazyTensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cholesky\u001b[38;5;241m.\u001b[39minv_quad_logdet(\n\u001b[1;32m   1293\u001b[0m         inv_quad_rhs\u001b[38;5;241m=\u001b[39minv_quad_rhs,\n\u001b[1;32m   1294\u001b[0m         logdet\u001b[38;5;241m=\u001b[39mlogdet,\n\u001b[1;32m   1295\u001b[0m         reduce_inv_quad\u001b[38;5;241m=\u001b[39mreduce_inv_quad,\n\u001b[1;32m   1296\u001b[0m     )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# Short circuit to inv_quad function if we're not computing logdet\u001b[39;00m\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1004\u001b[0m, in \u001b[0;36mLazyTensor.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\u001b[38;5;28mself\u001b[39m, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LazyTensor\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m        (LazyTensor) Cholesky factor (triangular, upper/lower depending on \"upper\" arg)\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1006\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:428\u001b[0m, in \u001b[0;36mLazyTensor._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sub_mat, KeOpsLazyTensor) \u001b[38;5;28;01mfor\u001b[39;00m sub_mat \u001b[38;5;129;01min\u001b[39;00m evaluated_kern_mat\u001b[38;5;241m.\u001b[39m_args):\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run Cholesky with KeOps: it will either be really slow or not work.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 428\u001b[0m evaluated_mat \u001b[38;5;241m=\u001b[39m \u001b[43mevaluated_kern_mat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# if the tensor is a scalar, we can just take the square root\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluated_mat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/sum_lazy_tensor.py:66\u001b[0m, in \u001b[0;36mSumLazyTensor.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlazy_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlazy_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "loc_sample = loc_sample0.clone()\n",
    "iter_hp = 50\n",
    "iter_design = 200\n",
    "iter_param = 200\n",
    "num_base_kernels = 2\n",
    "max_iter = 50\n",
    "\n",
    "\n",
    "f_target = f_target.reshape(2,1) \n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    " #np.random.random_sample((loc_size,2))\n",
    "#loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "#train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "\n",
    "#loc_sample = Tensor([[0.0, 0.1], [0.0, -0.1]]) #T\n",
    "# loc_x = (-1.5 + 2.)  * np.random.random_sample((loc_size,1)) +2.\n",
    "\n",
    "# # loc_y = (2. - 1.5)  * np.random.random_sample((loc_size,1)) - 1.5\n",
    "# # loc = np.concatenate((loc_x, loc_y), 1)\n",
    "print(loc_sample)\n",
    "\n",
    "\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten()\n",
    "\n",
    "data_fit_vec = torch.empty((1,1))\n",
    "entropy_vec = torch.empty((1,1))\n",
    "loss_vec = torch.empty((1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "vec_x = x0.clone() #Tensor(np.array([0.0,0.0])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "var_vec = torch.zeros([max_iter, 1])\n",
    "p21_vec = torch.empty((1,1))\n",
    "\n",
    "lr_new = .01\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "show_TTRBox = False\n",
    "iter = 0    \n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "patience = 0.0\n",
    "patience_f = 0.0\n",
    "patience_2 = 0.0\n",
    "checking_model = False\n",
    "model_double_check = False\n",
    "\n",
    "\n",
    "\n",
    "while(SUCCESS == False and FAILURE == False):\n",
    "    print(iter)\n",
    "    model_double_check = False\n",
    "    if (checking_model == False):\n",
    "        print('START HYPERPARAMETERS optimization')\n",
    "        if (iter == 0):\n",
    "            cur_model = None\n",
    "            cur_likelihood = None\n",
    "\n",
    "\n",
    "        loc_sample_old = loc_sample.clone()\n",
    "        x0_old = x0.clone()\n",
    "        model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "\n",
    "# Before this is hyper_opti\n",
    "\n",
    "        print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "   \n",
    "    \n",
    "    x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new,noise_value)\n",
    "  \n",
    "    cur_model = model\n",
    "    cur_likelihood = likelihood\n",
    "    \n",
    "  \n",
    "    lower_bound = torch.zeros(pf1.shape)\n",
    "    upper_bound = torch.zeros(pf1.shape)\n",
    "        \n",
    "    for i in range(pf1.shape[0]):\n",
    "        lower_bound[i] = pf1[i] -  torch.sqrt(Qf12[i,i])\n",
    "        upper_bound[i] = pf1[i] +  torch.sqrt(Qf12[i,i])\n",
    "\n",
    "    SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    \n",
    "    \n",
    "    entropy = ( 0.5 * torch.log( torch.det(Qf1.evaluate()) / torch.det(Qf12.evaluate()) ) ).reshape(1,1)\n",
    "    \n",
    "    print('expected info is '+str(entropy))\n",
    "    print('mohabb disatance is' + str(Qf12.inv_quad(f_target - pf1)))\n",
    "    if not SUCCESS:\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        new_data = vfield_(g_theta2.detach())  \n",
    "        agg_data12 = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "        g_theta12= torch.cat([g_theta1, g_theta2.detach()], 0)\n",
    "        new_data_x = vfield_(x0_new.detach() )  \n",
    "        print('current sol is'+str(x0_new.detach()))\n",
    "        print('new data is' + str(new_data_x))\n",
    "        print('g_theta2 is' + str(g_theta2.detach()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            if iter >= 0:\n",
    "                \n",
    "                \n",
    "                p21 = likelihood.get_p21(g_theta1, g_theta2.detach(), agg_data, model, noise_value)\n",
    "                \n",
    "#                 Q21 = Q21 + noise_value*torch.eye(Q21.shape[0])\n",
    "                chi_21 = (Q21).inv_quad(new_data.flatten() - p21.reshape(new_data.flatten().shape))\n",
    "                p_val = 1. - stats.chi2.cdf(chi_21, Q21.shape[0])\n",
    "                pf12 = likelihood.get_pf12(Q21,g_theta1, g_theta2.detach(), x0_new.detach(), new_data.flatten(), pf1, p21, model, noise_value)\n",
    "               \n",
    "                #Qf12 = Qf12 + \n",
    "                chi_f12 = (Qf12 + noise_value*torch.eye(Qf12.shape[0])).inv_quad(new_data_x.flatten() - pf12.reshape(new_data_x.flatten().shape))\n",
    "                p_val_f12 = 1. - stats.chi2.cdf(chi_f12, Qf12.shape[0])\n",
    "                print('p21val is %.15f' %p_val)\n",
    "                p21_vec = torch.cat([p21_vec, Tensor([p_val]).reshape(1,1)], 0)\n",
    "                print('pf12val is %.15f' %p_val_f12)\n",
    "                print('chi_f12 is %.15f' %chi_f12 )\n",
    "                \n",
    "                if (p_val < 0.01):# or p_val_f12 < 0.001:\n",
    "                    model_double_check = True\n",
    "                    checking_model = True\n",
    "                    patience = patience+1\n",
    "                    print('patience is %.3f' %patience)\n",
    "\n",
    "                if (model_double_check == True):\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_sample.shape[0],2)) + vf.low)\n",
    "                    sum = torch.zeros(2, 2) #replace with num_tasks\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_old).t(), (g_theta2.detach()[i] - x0_old) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum #+ torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_old, covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_old],0)\n",
    "                    \n",
    "                    x0 = x0_old #Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    if (patience >= 2):# or patience_2 >= 2 or patience_f >= 2):\n",
    "                        PATH = \".//model_Carlo/model_update/model_base_\"+str(iter)+\".pt\"\n",
    "                        torch.save(model, PATH)\n",
    "                        entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                        data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                        iter = iter + 1\n",
    "                        patience = 0\n",
    "#                         patience_2 = 0\n",
    "#                         patience_f = 0\n",
    "                        model_double_check = False\n",
    "                        checking_model = False\n",
    "                        num_base_kernels = num_base_kernels + 1\n",
    "                        print('adding complexity to model')\n",
    "                        print('num base is' + str(num_base_kernels))\n",
    "#     #                         \n",
    "                        loc_sample = loc_sample_old\n",
    "                        #x0 = x0_old\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                        g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                        print('acquiring 2, new size is' + str(g_theta1.shape[0]))\n",
    "                 \n",
    "                    #iter_hp = iter_hp + 10\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "                    torch.save(model, PATH)\n",
    "                    vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                    loss_vec = torch.cat([loss_vec, -loss])\n",
    "                    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                    entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                    data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                    model_double_check = False\n",
    "                    iter = iter + 1\n",
    "                    patience = 0\n",
    "                    patience_2 = 0\n",
    "                    patience_f = 0\n",
    "                    checking_model = False\n",
    "                    if (entropy < 1e-4 * tol_vector[0,0]):\n",
    "                        FAILURE = True\n",
    "                    \n",
    "                    x0 = (x0_new.detach())# + torch.randn(x0_new.detach().size()) * .001)#/torch.norm(x0_new.detach())\n",
    "                    sum = torch.zeros(2, 2)\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum# + torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_new.detach(), covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = loc_sample#/torch.norm(loc_sample)\n",
    "                    #loc_sample = 2. *  (loc_sample - torch.min(loc_sample)) / (torch.max(loc_sample) - torch.min(loc_sample)) - 1.\n",
    "                    #loc_sample[0] = x0_new.detach() #+ torch.randn(x0_new.detach().size()) * .001 #g_theta2.detach() #loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_size,2)) + vf.low)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_new.detach()],0)\n",
    "                    for i in range(loc_sample.shape[0]):\n",
    "                        if loc_sample[i,0] < -3. or loc_sample[i,0] > 3. or loc_sample[i,1] < -3. or loc_sample[i,1] > 3.:\n",
    "                            print('samples escaped box')\n",
    "                            loc_sample[i] = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    \n",
    "                    \n",
    "#                     if p_val > 0.99 and p_val_f12 > 0.99:\n",
    "#                         num_base_kernels = max(num_base_kernels - 1, 3)\n",
    "                        #iter_hp = iter_hp - 10\n",
    "                    chi_f_target = (Qf12 ).inv_quad(f_target - pf1)\n",
    "                    p_val_f_target = 1. - stats.chi2.cdf(chi_f_target, Qf12.shape[0])\n",
    "                    print('p_val_ftarget is '+str(p_val_f_target))\n",
    "                    if (p_val_f_target > .95):\n",
    "                        print('acquiring target point becuse p_val_ftarget is '+str(p_val_f_target))\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "        \n",
    "\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                    else:\n",
    "#                         x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "#                         g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                       \n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                        \n",
    "                    if x0_new.detach()[0,0] < -3. or x0_new.detach()[0,0] > 3. or x0_new.detach()[0,1] < -3. or x0_new.detach()[0,1] > 3.:\n",
    "#                         x0 = Tensor(np.array([0.0,-1.0])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = x0.reshape(1,2) \n",
    "                        #x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = x0.reshape(1,2)\n",
    "                        #x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                 \n",
    "             #       loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "                        loc_sample[-1] = x0 #(x0_new.detach()) \n",
    "                    print('new 2 points')\n",
    "                    print(loc_sample)\n",
    "                  \n",
    " #                    agg_data  = (agg_data  - agg_data.mean())/agg_data .std(dim=-1, keepdim=True)\n",
    "#                     g_theta1 = (g_theta1 - g_theta1.mean())/g_theta1.std(dim=-2, keepdim=True)\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            #clear_output(wait=False)\n",
    "           \n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "torch.save(model, PATH)\n",
    "print('current sol is'+str(x0_new.detach()))\n",
    "    \n",
    "print('Success is ' + str(SUCCESS) + ' and failure is ' + str(FAILURE)+' after '+ str(iter) + ' iterations')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target - 0.001)\n",
    "print(f_target + 0.001)\n",
    "print(pf1)\n",
    "print(num_base_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting model validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "#plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "p21_vec_plot = p21_vec.detach()[1:]\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "#print(p21_vec_plot)\n",
    "ax.plot(p21_vec_plot,'s',color = 'blue', markersize=6)\n",
    "ax.axhline(.01,linestyle = '--',color = 'red', markersize=12, alpha = 1.0)\n",
    "ax.set_xlim(-0.3, p21_vec_plot.shape[0])\n",
    "ax.set_ylim(-0.3, 1.)\n",
    "#ax.set_yscale('log')\n",
    "plt.xticks(np.arange(0, iter+7, step=5.))\n",
    "ax.tick_params(labelsize='small', width=3)\n",
    "ax.set_xlabel('# of Model Checks')\n",
    "ax.set_ylabel('p-value')\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.05, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(1.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.02, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(0.1,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.26, 0.34), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(6.2,        #x start point\n",
    "             0.14,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.07,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.29, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(7.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.57, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(14.15,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.69, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(17.18,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.32, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(10.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "# ############################\n",
    "\n",
    "# ############################\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.33, 0.34), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(11.25,        #x start point\n",
    "#              0.14,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.07,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# #################################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.38, 0.1), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(12.25,        #x start point\n",
    "#              -0.15,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.1,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.38, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(13.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.55, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(18.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')  \n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.49, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(17.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.55, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(19.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.94, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(33.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ################################\n",
    "\n",
    "ax.annotate('Threshold Line ($y = 0.01$)', xy=(1.0,0.01), xytext=(6,0), color='red', \n",
    "                xycoords = ax.get_yaxis_transform(), textcoords=\"offset points\",\n",
    "                size=14, va=\"center\")\n",
    "\n",
    "\n",
    "#ax.legend(['p-value'], loc = 'upper left', fontsize = 30)\n",
    "plt.savefig('figures_Carlo/qvalue_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting expected value and data fit term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(18, 8), tight_layout=True)\n",
    "\n",
    "ax1.plot(np.array(range(1,iter+2)), (entropy_vec_plot), '--o', color = 'blue', markersize=12)\n",
    "#ax1.set_yscale('log')\n",
    "# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\n",
    "#ax1.set_yscale('log')\n",
    "\n",
    "ax2.plot(np.array(range(1,iter+2)), data_fit_vec_plot, '--o', color = 'red', markersize=12)\n",
    "#ax2.set_ylim(-10, 600)\n",
    "ax1.set_xlabel('Iteration #', size=32)\n",
    "ax2.set_xlabel('Iteration #', size=32)\n",
    "ax1.set_ylabel('Expected Information', size = 32)\n",
    "ax2.set_ylabel('Log-Gaussian Term', size = 32)\n",
    "ax1.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "ax2.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "plt.savefig('figures_Carlo/exp_info/expectedinfo_vs_datafit_2_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving all data needed for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/vec_x_success_base.txt',vec_x.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta2_success_base.txt', g_theta2.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta1_success_base.txt', g_theta1.detach().numpy())\n",
    "np.savetxt('data_plots/x_train_ini_success_base.txt', x_train.detach().numpy())\n",
    "np.savetxt('data_plots/y_train_ini_success_base.txt', y_train.detach().numpy())\n",
    "np.savetxt('data_plots/entropy_vec_success_base.txt', entropy_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/datafit_success_base.txt', data_fit_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/p21_vec_success_base.txt',p21_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/loss_success_base.txt',loss.detach().numpy())\n",
    "np.savetxt('data_plots/pf1_success_base.txt',pf1.detach().numpy())\n",
    "np.savetxt('data_plots/Qf1_success_base.txt',Qf1.evaluate().detach().numpy())\n",
    "np.savetxt('data_plots/Qf12_success_base.txt', Qf12.evaluate().detach().numpy())\n",
    "np.savetxt( 'data_plots/Q21_success_base.txt', Q21.evaluate().detach().numpy())\n",
    "#np.savetxt('data_plots/iter_success.txt', iter+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2),2)\n",
    "torch.save(v2, 'data_plots/v2_success_base.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.set_xlim(-3.1, 3.2)\n",
    "ax.set_ylim(-3.1, 3.2)\n",
    "#ax.scatter(g_theta1[:, 0].detach(),g_theta1[:, 1].detach(), c=\"b\", alpha=0.8)\n",
    "ax.plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(vec_x[-1,0], vec_x[-1,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd', color = 'green',markersize=15)\n",
    "ax.set_title('Final TAD configuration', fontsize = 40)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.legend(['1-points', 'TAD solution', 'Target'])\n",
    "plt.savefig('figures_Carlo/strategies/tad_sol_all_2_base.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "#v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "ii = 0\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "########################\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "ax.plot(vec_x[ii,0], vec_x[ii,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o', color = 'blue', markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Initial Configuration', fontsize = 40)\n",
    "ax.legend(['Target', 'Initial Target Candidate', 'Initial 1-sample','Initial 2-sample'])\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_ini_2_base.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "#for iter_plt in range(1,iter):\n",
    "iter_plt = 13\n",
    "ii = 3 + (iter_plt - 1) * (loc_size + 1)\n",
    "#######################\n",
    "\n",
    " \n",
    "###########\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(14,14))\n",
    "\n",
    "#ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 'bv', markersize=8)\n",
    "for i in range(1, iter_plt):\n",
    "    ax.plot(vec_x[i-1:i,0], vec_x[i-1:i,1],'v', color = 'red', markersize=15, alpha=0.2, label = '_nolegend_')\n",
    "    \n",
    "ax.plot(vec_x[0:iter_plt,0], vec_x[0:iter_plt,1],'v', color = 'red', markersize=15, linewidth=15, alpha= 0.2)\n",
    "#ax.plot(g_theta1.detach()[:,0], g_theta.detach()[:,1], 'bv', markersize=8)\n",
    "ax.plot(v2.detach()[0:ii,0], v2.detach()[0:ii,1], 's', color = 'blue', markersize=15,alpha=0.2)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o',color = 'blue' , markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd',markersize=15)\n",
    "ax.plot(vec_x[iter_plt,0], vec_x[iter_plt,1],'v', color = 'red',markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "#ax.set_title('Iteration '+str(iter_plt), fontsize = 40)\n",
    "ax.set_title('Final Configuration')\n",
    "ax.legend([ 'Previous Target Candidates', 'Previous 2-samples', 'Initial 1-sample','Current 2-sample', 'Target', 'Current Target Candidate'], fontsize = 30)\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_base'+str(iter_plt)+'_final.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta2_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_pll(x0,f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    #g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': x_d, 'lr': 0.001}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        #print(g_theta)\n",
    "        loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood, noise_value)\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood ,noise_value)\n",
    "    #loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "   # print(optimizer.state_dict())\n",
    "    print(x_d)\n",
    "    return x_d, lower_bound, upper_bound\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(vf.low, vf.high, 15)\n",
    "y_plot = np.linspace(vf.low, vf.high, 15)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat = torch.zeros(n * n, 2)\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "\n",
    "g_theta_grid = x_concat\n",
    "agg_data1_grid = vfield_(g_theta_grid)\n",
    "agg_data1_grid = agg_data1_grid.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.,2.])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_grid = Tensor(np.array([0.0,0.0])) \n",
    "vec_x_grid = vec_x_grid.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "model_grid, likelihood_grid = hyper_opti(g_theta_grid,agg_data1_grid,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_grid.eval()\n",
    "likelihood_grid.eval()\n",
    "x0_new_grid,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_grid, agg_data1_grid, model_grid, likelihood_grid, iter_design, iter_param, lr_new,noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "#loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_grid)\n",
    "print(SUCCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_grid = x0_new.detach()\n",
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.plot(x_concat[:,0],x_concat[:,1], 's', color = 'blue', markersize=15, alpha = 0.2)\n",
    "ax.plot(x0_new_grid.detach()[0,0], x0_new_grid.detach()[0,1],'v',color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Grid Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/grid_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_hp = 30\n",
    "# iter_design = 40 \n",
    "# iter_param = 50\n",
    "# num_base_kernels = 3\n",
    "\n",
    "# f_target = Tensor(vf.tgt_vec) \n",
    "# f_target = f_target.reshape(f_target.shape[0],1) \n",
    "# tol_vector = 0.005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size_rdn = math.ceil(g_theta1.shape[0]) #(iter)*(loc_size+1) + sample_size\n",
    "\n",
    "loc_sample = high_minus_low  * np.random.random_sample((loc_size_rdn,2)) + vf.low #np.random.random_sample((loc_size_rdn,2))\n",
    "g_theta_ = (Tensor(loc_sample).clone())\n",
    "agg_data1 = vfield_(g_theta_)\n",
    "agg_data1 = agg_data1.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.0,2.0])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_rdn = Tensor(np.array([0.,0.])) \n",
    "vec_x_rdn = vec_x_rdn.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "\n",
    "model_rdn, likelihood_rdn = hyper_opti(g_theta_,agg_data1,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_rdn.eval()\n",
    "likelihood_rdn.eval()\n",
    "x0_new_rdn,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_, agg_data1, model_rdn, likelihood_rdn, iter_design, iter_param, lr_new, noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_rdn)\n",
    "print(SUCCESS)\n",
    "sol_rdn = x0_new_rdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/sol_rdn_success_base.txt', sol_rdn.detach().numpy())\n",
    "np.savetxt('data_plots/sol_grid_success_base.txt', x0_new_grid.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta_rdn_success_base.txt', g_theta_.detach().numpy())\n",
    "#np.savetxt('data_plots/g_theta_grid_success.txt', x_concat.detach().numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "\n",
    "ax.plot(g_theta_[:,0].detach(),g_theta_[:,1].detach(), 's', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(sol_rdn.detach()[0,0], sol_rdn.detach()[0,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$', fontsize = 32)\n",
    "ax.set_ylabel('$d_2$', fontsize = 32)\n",
    "ax.set_title('Uniformly Random Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/rdn_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizing Means and Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker\n",
    "class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "        self.oom = order\n",
    "        self.fformat = fformat\n",
    "        matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
    "    def _set_order_of_magnitude(self):\n",
    "        self.orderOfMagnitude = self.oom\n",
    "    def _set_format(self, vmin=None, vmax=None):\n",
    "        self.format = self.fformat\n",
    "        if self._useMathText:\n",
    "             self.format = r'$\\mathdefault{%s}$' % self.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "likelihood.eval()\n",
    "\n",
    "#noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "#print(x_concat_)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(True):\n",
    "    pred = GPprediction(model)\n",
    "    pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "    #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "    pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "    mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "    mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "    pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "    \n",
    "    var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "    var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "#     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "# #     print(pr.covariance_matrix.mean(axis=0))\n",
    "# #     print(AA)\n",
    "# #     print(pr.variance)\n",
    "# #     print((pr.covariance_matrix))\n",
    "# #     K = model.covar_module\n",
    "#     print((cov.diag()))\n",
    "#     print(pr_mean)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 2, figsize = (28, 24), tight_layout=True)\n",
    "diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "cs10 = ax1[0].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax1[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[0].set_title('$|v_1 - \\mu(v_1)|$', fontsize = 40)\n",
    "cbar10 = fig.colorbar(cs10, ax = ax1[0],format=OOMFormatter(0, mathText=False));\n",
    "\n",
    "ax1[0].set_xlabel('$d_1$')\n",
    "ax1[0].set_ylabel('$d_2$')\n",
    "diff_mean_v1 = torch.abs(v_1 - mean_v_1.detach())/torch.sqrt(var_v_1)\n",
    "#print(var_v_1)\n",
    "cs11 = ax1[1].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(diff_mean_v1.min(), diff_mean_v1.max(), 100), cmap = 'jet')\n",
    "ax1[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[1].set_title('$|v_1 - \\mu(v_1)|/\\sigma(v_1)$', fontsize = 40)\n",
    "# ax1[0].set_aspect('equal')\n",
    "# ax1[1].set_aspect('equal')\n",
    "cbar11 = fig.colorbar(cs11, ax = ax1[1],format=OOMFormatter(0, mathText=False));\n",
    "ax1[1].set_xlabel('$d_1$')\n",
    "ax1[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())\n",
    "cs20 = ax2[0].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax2[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[0].set_title('$|v_2 - \\mu(v_2)|$', fontsize = 40)\n",
    "cbar20 = fig.colorbar(cs20, ax = ax2[0],format=OOMFormatter(0, mathText=False));\n",
    "ax2[0].set_xlabel('$d_1$')\n",
    "ax2[0].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())/torch.sqrt(var_v_2)\n",
    "cs21 = ax2[1].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(diff_mean_v2.min(), diff_mean_v2.max(), 100), cmap = 'jet')\n",
    "ax2[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[1].set_title('$|v_2 - \\mu(v_2)|/\\sigma(v_2)$', fontsize = 40)\n",
    "cbar21 = fig.colorbar(cs21, ax = ax2[1],format=OOMFormatter(0, mathText=False));\n",
    "ax2[1].set_xlabel('$d_1$')\n",
    "ax2[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "# ax2[0].set_aspect('equal')\n",
    "# ax2[1].set_aspect('equal')\n",
    "\n",
    "plt.savefig('figures_Carlo/mean_var/mean_final_2.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "plot = [1, 6, 10, iter+1]\n",
    "\n",
    "for ii in plot:\n",
    "    try:\n",
    "        \n",
    "        PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "    except:\n",
    "        PATH = \".//model_Carlo/model_update/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "        \n",
    "    #model_16 = torch.load(PATH)\n",
    "    model_16.eval()\n",
    "\n",
    "    likelihood.eval()\n",
    "\n",
    "    #noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "    #print(x_concat_)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var(False):\n",
    "        pred = GPprediction(model_16)\n",
    "        pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "        #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "        pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "        mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "        mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "        pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "\n",
    "        var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "        var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "    #     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "    # #     print(pr.covariance_matrix.mean(axis=0))\n",
    "    # #     print(AA)\n",
    "    # #     print(pr.variance)\n",
    "    # #     print((pr.covariance_matrix))\n",
    "    # #     K = model.covar_module\n",
    "    #     print((cov.diag()))\n",
    "    #     print(pr_mean)\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (28, 12), tight_layout=True)\n",
    "    diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "    minn = torch.min(var_v_1.detach().min(), var_v_2.detach().min())\n",
    "    maxx = torch.min(var_v_1.detach().max(), var_v_2.detach().max())\n",
    "\n",
    "    cs11 = ax1.contourf(xv_plot, yv_plot, var_v_1.detach(), np.linspace(0.0,1.1, 100), cmap = 'jet')\n",
    "    ax1.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    \n",
    "    ax1.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "  \n",
    "    ax1.set_title('$\\sigma^2(v_1)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    # ax1[0].set_aspect('equal')\n",
    "    # ax1[1].set_aspect('equal')\n",
    "    cbar11 = fig.colorbar(cs11, ax = ax1,format=OOMFormatter(-0, mathText=False));\n",
    "    ax1.set_xlabel('$d_1$')\n",
    "    ax1.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    cs21 = ax2.contourf(xv_plot, yv_plot, var_v_2.detach(), np.linspace(0.0, 1.1, 100), cmap = 'jet')\n",
    "    ax2.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "    ax2.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    ax2.set_title('$\\sigma^2(v_2)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    cbar21 = fig.colorbar(cs21, ax = ax2,format=OOMFormatter(-0, mathText=False));\n",
    "    ax2.set_xlabel('$d_1$')\n",
    "    ax2.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "    # ax2[0].set_aspect('equal')\n",
    "    # ax2[1].set_aspect('equal')\n",
    "\n",
    "    plt.savefig('figures_Carlo/mean_var/var_iter_base'+str(ii)+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# # #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "# #                     #loc_size = 4\n",
    "# # loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# # loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# # g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# z = torch.zeros(n*n, 1)\n",
    "# for ii in range(n*n):\n",
    "#     print(ii)\n",
    "#     x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "#     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#     g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21 = conduct_2opt(agg_data,f_target,x0, g_theta1, model, likelihood, noise_value, g2, training_param_iter)\n",
    "#     z[ii] = loss2\n",
    "# z = z.reshape(n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "# loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# g2 = g_theta2.detach() #loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# plot = [1, 7, 17, 26]\n",
    "# zz = torch.zeros(n*n, 4)\n",
    "# kk = 0\n",
    "# for jj in plot:\n",
    "#     try:\n",
    "        \n",
    "#         PATH = \".//model_Carlo/model_goodmodel/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#     except:\n",
    "#         PATH = \".//model_Carlo/model_update/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#    # model_16 = torch.load(PATH)\n",
    "#     model_16.eval()\n",
    "\n",
    "#     likelihood.eval()\n",
    "#     g2 = v2.detach()[jj - 1 +loc_size+1 : jj - 1 +loc_size+1 +loc_size+1]\n",
    "#     g_theta1_cur = g_theta1[0:(4 + (jj - 1)* 3)]\n",
    "#     agg_data_cur = agg_data[0:2 * (4 + (jj - 1)* 3)]\n",
    "#     print(agg_data_cur.shape)\n",
    "#     for ii in range(n*n):\n",
    "#         print(ii)\n",
    "#         x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     #     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#     #                     #loc_size = 4\n",
    "#     #     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     #     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#         #g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "\n",
    "\n",
    "        \n",
    "#         loss2_, pf1_, Qf1_, Qf12_, data_fit_, Q21_ = likelihood.get_ell(agg_data_cur,f_target,x0, g_theta1_cur, model_16, likelihood, noise_value, g2)\n",
    "#         zz[ii, kk] = loss2_\n",
    "#     kk = kk+1\n",
    "# zz = zz.reshape(n,n, 4)\n",
    "# torch.save(zz.detach(), 'data_plots/zz_success.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot = [1, 5, 10, 20]\n",
    "\n",
    "# for jj in range(4):\n",
    "#     fig, ax = plt.subplots(figsize = (16,14))\n",
    "#     #\n",
    "#     cs = ax.contour(xv_plot, yv_plot,  zz[:,:,jj].detach(), np.linspace( zz[:,:,jj].detach().numpy().min(), zz[:,:,jj].detach().numpy().max(), 1000), cmap = 'jet')\n",
    "#     cbar = fig.colorbar(cs, ax = ax,format=OOMFormatter(0, mathText=False));\n",
    "#     ax.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "#     kk = plot[jj]\n",
    "#     if kk < plot[3]:\n",
    "#         ax.plot(vec_x[kk,0], vec_x[kk,1],'o', color = 'black',markersize=12)\n",
    "#     if kk == plot[3]:\n",
    "#         ax.plot(vec_x[- 1,0], vec_x[- 1,1],'o', color = 'black',markersize=12)\n",
    "#     ax.set_title('TAD Acquisition Function (Iteration '+str(kk)+')', fontsize = 40)\n",
    "#     ax.set_xlabel('$d_1$')\n",
    "#     ax.set_ylabel('$d_2$')\n",
    "    \n",
    "#     plt.savefig('figures_Carlo/tad_obj'+str(kk)+'.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p21_vec_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
