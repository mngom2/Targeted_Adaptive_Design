{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(\"..\")\n",
    "from LBFGS import FullBatchLBFGS\n",
    "from kernels import vvkernels as vvk, sep_vvkernels as svvk, vvk_rbfkernel as vvk_rbf\n",
    "from means import vvmeans as vvm\n",
    "from likelihood import vvlikelihood as vvll\n",
    "from mlikelihoods import MarginalLogLikelihood as exmll\n",
    "from predstrategies import GPprediction\n",
    "from utils import ObjFun, get_vertices, stopping_criteria\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=40)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=32)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=32)    # legend fontsize\n",
    "plt.rc('font', size=32)          # controls default text sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "\n",
    "We sample from $$V_1(x_1, x_2) = 3(1 - x_1)^2 e^{-x_1^2 - (x_2 +1)^2} - 10 (x_1/5 - x_1 ^3 - x_2^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- (x_1 + 2) ^2 - x_2^2} + 0.5(2x_1 + x_2)$$\n",
    "$$V_2(x_1, x_2) = 3(1 +x_2)^2 e^{-x_2^2 - (x_1 +1)^2} - 10 (-x_2/5 + x_2 ^3 + x_1^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- ( 2- x_2) ^2 - x_1^2} + 0.5(2x_1 + x_2)$$\n",
    "\n",
    "where $(x_1, x_2) \\in [-3, 3]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3380, 0.3502], dtype=torch.float32)\n",
      "tensor([[ 1.4946, -1.6349],\n",
      "        [ 1.6712, -1.4105],\n",
      "        [ 1.0285, -1.3352],\n",
      "        [ 1.6098, -1.2711]])\n"
     ]
    }
   ],
   "source": [
    "vf = ObjFun()\n",
    "f_target = vf.tgt_vec\n",
    "print(f_target)\n",
    "sample_size = 4\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -3.\n",
    "vf.high = 3.\n",
    "\n",
    "high_minus_low = vf.high- vf.low\n",
    "#high_minus_low = -\n",
    "def g_theta(sample_size, D):\n",
    "    loc_x = (2. - 1.0 )  * np.random.random_sample((sample_size,1)) + 1.0\n",
    "    \n",
    "    loc_y = (2.  -1.0)  * np.random.random_sample((sample_size,1)) - 2.\n",
    "    loc = np.concatenate((loc_x, loc_y), 1)\n",
    "    #loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "#train_x = Tensor([[-1.5, 1.5], [-1.5, 1.3]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    \n",
    "    out = vf(x[:,0], x[:,1]) + torch.randn(Tensor(vf(x[:,0], x[:,1])).size()) * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)\n",
    "\n",
    "# print(train_y)\n",
    "# train_y = (train_y - train_y.mean())/train_y.std(dim=-2, keepdim=True)\n",
    "# train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "# print(train_y)\n",
    "# print(train_y.std(dim=-2, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP model initialization\n",
    "We inialize the GP model following https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)  #vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)#vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)  # \n",
    "        base_kernels = [] #contain all the base kernels\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters oprimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels,noise_value, current_model = None, current_likelihood = None):\n",
    "    noises = torch.ones(agg_data.shape[0]) * (noise_value) #  torch.zeros(agg_data.shape[0]) # \n",
    "    noises = noises.reshape(g_theta1.shape[0], 2)\n",
    "    \n",
    "#     if (current_model is not None):\n",
    "#         likelihood = current_likelihood #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)  #\n",
    "\n",
    "#         model = current_model#.get_fantasy_model(g_theta1, agg_data) #MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#         model.set_train_data(g_theta1, agg_data,  strict=False)\n",
    "#     else:\n",
    "#         likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2)#vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #\n",
    "#         model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "        \n",
    "    likelihood =  vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2) #\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.double()\n",
    "    likelihood.double()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #, weight_decay=0.001)  # Includes GaussianLikelihood parameters\n",
    "    mll = exmll(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, chi_square = mll(agg_data,g_theta1, model, likelihood, noise_value)\n",
    "        loss = -1. * loss\n",
    "#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \n",
    "        #print('loss is %.3f' %loss)\n",
    "        df = agg_data.shape[0]\n",
    "        chi_square = chi_square.clone().detach()\n",
    "        \n",
    "        p_val = 1. - stats.chi2.cdf(chi_square, df)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "       # print(p_val)\n",
    "#         if (p_val > 0.99999):\n",
    "#             return model, likelihood\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    print('loss is %.3f' %loss)\n",
    "#     for params in model.named_parameters():\n",
    "#         print(params)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design parameters and sampling point optimization (where to explore?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': g_theta2, 'lr': 0.1},{'params': x_d, 'lr': 0.1}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss2)\n",
    "        scheduler.step()\n",
    "        \n",
    "    loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, noise_value, g_theta2)\n",
    "    loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    #print(x_d)\n",
    "    return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conducting the TAD experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_size = 2\n",
    "#loc_sample0 = Tensor((2. - 1.5)  * np.random.random_sample((loc_size,2)) + 1.5)\n",
    "x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "x0 = x0.reshape(1,2)\n",
    "\n",
    "dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(loc_size) )\n",
    "                    #loc_size = 4\n",
    "loc_sample = dis_2sample.sample((loc_size + 1,))\n",
    "\n",
    "loc_sample0 = loc_sample.reshape(loc_size + 1, 2)\n",
    "#loc_sample0[-1] = train_x[-1] + 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9775,  1.9504],\n",
      "        [-1.7812,  2.0668],\n",
      "        [-2.0650,  2.0864]])\n",
      "0\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /lus/theta-fs0/software/thetagpu/conda/2022-07-01/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -3.200\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 283.314\n",
      "expected info is tensor([[1.8475]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(570.6480, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1287,  2.9150]])\n",
      "new data istensor([[ 0.0120, -0.0372]])\n",
      "g_theta2 istensor([[-1.2726,  1.8766],\n",
      "        [-1.0627,  2.1588],\n",
      "        [-1.3791,  2.2182]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 272.176810833421314\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "0\n",
      "Loss design: 141.693\n",
      "expected info is tensor([[1.0704]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(292.8185, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1877,  2.8392]])\n",
      "new data istensor([[ 0.0253, -0.0221]])\n",
      "g_theta2 istensor([[-2.9178,  1.6951],\n",
      "        [-1.7676,  1.6179],\n",
      "        [-2.4797,  1.4930]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 537.880691463420590\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is3\n",
      "acquiring 2, new size is7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "1\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.261\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 20.237\n",
      "expected info is tensor([[0.0230]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(48.1439, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.3933,  2.8073]])\n",
      "new data istensor([[ 0.1891, -0.1276]])\n",
      "g_theta2 istensor([[-2.4615,  1.5151],\n",
      "        [-2.3728,  1.4647],\n",
      "        [-2.5926,  1.5382]])\n",
      "p21val is 0.863298893659420\n",
      "pf12val is 0.000033665422023\n",
      "chi_f12 is 20.598078600199308\n",
      "p_val_ftarget is 3.5129676945189203e-11\n",
      "new 2 points\n",
      "tensor([[-2.1917,  1.7069],\n",
      "        [-1.5129,  2.0560],\n",
      "        [-0.3933,  2.8073]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.306\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 46.208\n",
      "expected info is tensor([[0.0451]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(101.0863, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.4353, 2.8249]])\n",
      "new data istensor([[ 0.2652, -0.1563]])\n",
      "g_theta2 istensor([[-2.4981,  1.5208],\n",
      "        [-1.6683,  1.4900],\n",
      "        [-0.9546,  2.2113]])\n",
      "p21val is 0.000000289847724\n",
      "pf12val is 0.899533695340671\n",
      "chi_f12 is 0.211757532429187\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "Loss design: 6230.666\n",
      "expected info is tensor([[0.6653]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(179.9282, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1547,  2.9697]])\n",
      "new data istensor([[ 0.0361, -0.0398]])\n",
      "g_theta2 istensor([[ 0.7906,  3.2479],\n",
      "        [-1.4689,  1.6078],\n",
      "        [-0.1607,  2.2938]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.279790536461048\n",
      "chi_f12 is 2.547428079670173\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is4\n",
      "acquiring 2, new size is14\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "3\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.252\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 60.160\n",
      "expected info is tensor([[0.0493]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(127.4038, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.9493,  2.7991]])\n",
      "new data istensor([[-0.0513, -0.0283]])\n",
      "g_theta2 istensor([[-1.9740,  1.4931],\n",
      "        [-1.0526,  1.6301],\n",
      "        [-0.0329,  2.4216]])\n",
      "p21val is 0.026878436518449\n",
      "pf12val is 0.177061590449219\n",
      "chi_f12 is 3.462515276591394\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.7195, -1.7714],\n",
      "        [-2.1626,  1.3383],\n",
      "        [-1.9494,  2.7992]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "4\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.420\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 124.233\n",
      "expected info is tensor([[0.0453]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(256.4433, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.7564,  2.8647]])\n",
      "new data istensor([[-0.1627,  0.0249]])\n",
      "g_theta2 istensor([[-2.0886, -2.4996],\n",
      "        [-1.8237,  1.5541],\n",
      "        [-1.4557,  2.2660]])\n",
      "p21val is 0.429164737852665\n",
      "pf12val is 0.728439881294053\n",
      "chi_f12 is 0.633700361427319\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-1.2134, -1.1898],\n",
      "        [-1.6102, -2.5673],\n",
      "        [-2.7563,  2.8648]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "5\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.462\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 325.196\n",
      "expected info is tensor([[0.0630]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(658.8487, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0038,  2.9038]])\n",
      "new data istensor([[-0.1952, -0.0104]])\n",
      "g_theta2 istensor([[-0.4468, -2.0595],\n",
      "        [-1.7484, -2.5261],\n",
      "        [-2.2127,  2.2992]])\n",
      "p21val is 0.264947886144825\n",
      "pf12val is 0.375255583325761\n",
      "chi_f12 is 1.960295859259721\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3987, -0.8955],\n",
      "        [-0.6160,  2.5412],\n",
      "        [ 0.6437,  1.7134]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "6\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.433\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 25.155\n",
      "expected info is tensor([[0.0364]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(56.9776, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.2030, 1.1601]])\n",
      "new data istensor([[0.4281, 0.0919]])\n",
      "g_theta2 istensor([[-0.0074, -1.6491],\n",
      "        [-1.0843,  2.7350],\n",
      "        [ 0.0638,  2.3040]])\n",
      "p21val is 0.093074463833180\n",
      "pf12val is 0.000041926543177\n",
      "chi_f12 is 20.159182885769244\n",
      "samples escaped box\n",
      "p_val_ftarget is 4.241051954068098e-13\n",
      "new 2 points\n",
      "tensor([[-0.4531,  2.4290],\n",
      "        [-0.4754,  1.9188],\n",
      "        [ 1.2029,  1.1601]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.316\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 2.477\n",
      "expected info is tensor([[0.0205]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(12.1578, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8061, 0.6179]])\n",
      "new data istensor([[ 0.4356, -0.2041]])\n",
      "g_theta2 istensor([[-0.9450,  2.8452],\n",
      "        [-0.5996,  1.9209],\n",
      "        [ 1.0902,  1.1129]])\n",
      "p21val is 0.008288897364681\n",
      "pf12val is 0.000603858825670\n",
      "chi_f12 is 14.824340239365698\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "Loss design: 10.693\n",
      "expected info is tensor([[0.3798]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(16.6672, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.7330, 0.6763]])\n",
      "new data istensor([[ 0.3947, -0.2086]])\n",
      "g_theta2 istensor([[ 2.7754, -0.4377],\n",
      "        [-2.3033,  3.0078],\n",
      "        [ 0.9911,  1.3774]])\n",
      "p21val is 0.358486431518938\n",
      "pf12val is 0.000012176072837\n",
      "chi_f12 is 22.632075549631942\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.00024030785065809468\n",
      "new 2 points\n",
      "tensor([[ 0.0572,  0.2470],\n",
      "        [-2.4322, -1.0830],\n",
      "        [ 1.7330,  0.6763]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "8\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.198\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 45.928\n",
      "expected info is tensor([[0.0260]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(100.2335, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.4213, 0.1301]])\n",
      "new data istensor([[ 0.3107, -0.2031]])\n",
      "g_theta2 istensor([[-0.4719, -0.0353],\n",
      "        [-2.9371, -1.7146],\n",
      "        [ 1.3241,  1.1589]])\n",
      "p21val is 0.271202843591279\n",
      "pf12val is 0.655976833502655\n",
      "chi_f12 is 0.843259610888336\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.4703, -1.2945],\n",
      "        [ 2.0116, -0.2273],\n",
      "        [ 2.4213,  0.1302]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.170\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 10.492\n",
      "expected info is tensor([[0.2445]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(27.1386, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9629, 0.6865]])\n",
      "new data istensor([[ 0.3906, -0.0142]])\n",
      "g_theta2 istensor([[-2.6934, -1.6308],\n",
      "        [ 1.4644, -0.7286],\n",
      "        [ 2.0377, -0.2542]])\n",
      "p21val is 0.000452588432328\n",
      "pf12val is 0.744507109065010\n",
      "chi_f12 is 0.590065756251893\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: 1261871.955\n",
      "expected info is tensor([[0.2385]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(23.3908, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9971, 0.7272]])\n",
      "new data istensor([[0.3578, 0.0047]])\n",
      "g_theta2 istensor([[-1.5480, -1.8409],\n",
      "        [ 6.5523,  2.6134],\n",
      "        [ 2.3149, -0.0955]])\n",
      "p21val is 0.487060837045191\n",
      "pf12val is 0.921260407321505\n",
      "chi_f12 is 0.164025077171087\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 8.332027688129351e-06\n",
      "new 2 points\n",
      "tensor([[-2.2116,  1.0429],\n",
      "        [-0.2363, -2.6614],\n",
      "        [ 2.9972,  0.7272]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "10\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.140\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5.376\n",
      "expected info is tensor([[0.0013]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(17.7235, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.7657, 1.4305]])\n",
      "new data istensor([[0.3983, 0.0440]])\n",
      "g_theta2 istensor([[-2.7455,  1.7100],\n",
      "        [-0.7494, -2.8848],\n",
      "        [ 2.5345,  0.3696]])\n",
      "p21val is 0.943823443826118\n",
      "pf12val is 0.546187712066081\n",
      "chi_f12 is 1.209585134656732\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0001417094444081668\n",
      "new 2 points\n",
      "tensor([[ 1.4090,  1.7170],\n",
      "        [-0.8454,  0.7318],\n",
      "        [ 2.7656,  1.4304]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "11\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.192\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1.349\n",
      "expected info is tensor([[0.1295]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9.7105, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.2139, 1.9922]])\n",
      "new data istensor([[0.3776, 0.0127]])\n",
      "g_theta2 istensor([[ 0.7133,  2.4010],\n",
      "        [-1.2778,  0.3244],\n",
      "        [ 2.8304,  1.0944]])\n",
      "p21val is 0.000001524556577\n",
      "pf12val is 0.716005357215304\n",
      "chi_f12 is 0.668135259810405\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "11\n",
      "Loss design: 3171301.530\n",
      "expected info is tensor([[0.1286]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(9.9312, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.2289, 1.9843]])\n",
      "new data istensor([[ 0.3687, -0.0046]])\n",
      "g_theta2 istensor([[8.6314, 1.3984],\n",
      "        [1.4623, 0.3708],\n",
      "        [2.8695, 1.0979]])\n",
      "p21val is 0.214361802219057\n",
      "pf12val is 0.541926785840099\n",
      "chi_f12 is 1.225248736285296\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.006973838914874686\n",
      "new 2 points\n",
      "tensor([[2.5437, 2.2567],\n",
      "        [1.5910, 1.5274],\n",
      "        [2.2289, 1.9844]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "12\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.191\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 26.618\n",
      "expected info is tensor([[0.4114]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(63.1581, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.6750, 1.4617]])\n",
      "new data istensor([[0.3692, 0.0365]])\n",
      "g_theta2 istensor([[2.7954, 2.7384],\n",
      "        [1.1239, 1.0396],\n",
      "        [2.7433, 2.3697]])\n",
      "p21val is 0.039007535149228\n",
      "pf12val is 0.364322707006255\n",
      "chi_f12 is 2.019430492792142\n",
      "p_val_ftarget is 1.9317880628477724e-14\n",
      "new 2 points\n",
      "tensor([[1.1142, 0.6845],\n",
      "        [1.9043, 1.7269],\n",
      "        [1.6752, 1.4616]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "13\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.174\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 39.771\n",
      "expected info is tensor([[0.1241]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(90.3595, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.3320, 1.1605]])\n",
      "new data istensor([[ 0.3663, -0.0041]])\n",
      "g_theta2 istensor([[1.3071, 0.4835],\n",
      "        [2.1309, 2.1543],\n",
      "        [1.3618, 1.7587]])\n",
      "p21val is 0.020992183541823\n",
      "pf12val is 0.626201555894759\n",
      "chi_f12 is 0.936165970863708\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5640,  0.1108],\n",
      "        [-2.9232, -0.4147],\n",
      "        [ 2.3320,  1.1605]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "14\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.157\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 99.736\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(212.6762, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8309, 1.7861]])\n",
      "new data istensor([[0.3784, 0.0079]])\n",
      "g_theta2 istensor([[-0.1243, -0.6226],\n",
      "        [-2.7469, -1.3275],\n",
      "        [ 2.8444,  1.0688]])\n",
      "p21val is 0.000036283775114\n",
      "pf12val is 0.215110438175222\n",
      "chi_f12 is 3.073207433608787\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "14\n",
      "Loss design: 99.471\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(212.1407, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8289, 1.7888]])\n",
      "new data istensor([[0.3197, 0.0106]])\n",
      "g_theta2 istensor([[-0.3705, -0.6893],\n",
      "        [-2.7125, -1.8687],\n",
      "        [ 2.8458,  1.0749]])\n",
      "p21val is 0.000000023078550\n",
      "pf12val is 0.472175760282137\n",
      "chi_f12 is 1.500807978462552\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is5\n",
      "acquiring 2, new size is61\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "15\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.094\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7.378\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(22.1557, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.2358, 2.5408]])\n",
      "new data istensor([[ 0.3102, -0.0740]])\n",
      "g_theta2 istensor([[ 0.1157, -0.2918],\n",
      "        [-2.7461, -1.0650],\n",
      "        [ 2.7287,  0.8011]])\n",
      "p21val is 0.472040666294410\n",
      "pf12val is 0.628242120443229\n",
      "chi_f12 is 0.929659289418295\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 1.545056403129408e-05\n",
      "new 2 points\n",
      "tensor([[-1.5329, -1.4345],\n",
      "        [ 0.2298, -0.1507],\n",
      "        [ 1.2356,  2.5408]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "16\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.669\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 5.084\n",
      "expected info is tensor([[0.1338]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(16.3663, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.8056, 2.9451]])\n",
      "new data istensor([[ 0.3755, -0.0094]])\n",
      "g_theta2 istensor([[-0.8057, -1.9719],\n",
      "        [ 0.1893, -0.2390],\n",
      "        [ 1.0151,  2.1759]])\n",
      "p21val is 0.805811427266484\n",
      "pf12val is 0.723299435095673\n",
      "chi_f12 is 0.647863972273843\n",
      "p_val_ftarget is 0.00027932199063473995\n",
      "new 2 points\n",
      "tensor([[ 0.1361, -0.1531],\n",
      "        [ 1.7057,  2.7720],\n",
      "        [ 1.8055,  2.9451]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "17\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.122\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 24.404\n",
      "expected info is tensor([[0.0469]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(58.7478, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.3897, 2.8596]])\n",
      "new data istensor([[0.4134, 0.0252]])\n",
      "g_theta2 istensor([[ 0.1397, -0.2574],\n",
      "        [ 1.4421,  2.4263],\n",
      "        [ 1.4446,  2.5566]])\n",
      "p21val is 0.362711141220257\n",
      "pf12val is 0.877996833631740\n",
      "chi_f12 is 0.260224583391011\n",
      "samples escaped box\n",
      "p_val_ftarget is 1.7497114868092467e-13\n",
      "new 2 points\n",
      "tensor([[-0.0671, -1.1352],\n",
      "        [-1.7263,  0.3732],\n",
      "        [ 2.3898,  2.8596]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -1.457\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 16.502\n",
      "expected info is tensor([[0.0116]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(42.4092, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9578, 2.8750]])\n",
      "new data istensor([[ 0.5359, -0.0310]])\n",
      "g_theta2 istensor([[ 0.0799, -1.0197],\n",
      "        [-2.1118, -0.0784],\n",
      "        [ 1.8547,  2.4979]])\n",
      "p21val is 0.005359480230406\n",
      "pf12val is 0.553593835627169\n",
      "chi_f12 is 1.182648019614406\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "Loss design: 15.499\n",
      "expected info is tensor([[0.0140]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(40.2777, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.9667, 2.8753]])\n",
      "new data istensor([[ 0.5171, -0.0080]])\n",
      "g_theta2 istensor([[ 0.2165, -0.2376],\n",
      "        [ 2.7984,  2.3139],\n",
      "        [ 1.8458,  2.4780]])\n",
      "p21val is 0.428519328891980\n",
      "pf12val is 0.819706814488393\n",
      "chi_f12 is 0.397617091929047\n",
      "p_val_ftarget is 1.7939150165346973e-09\n",
      "new 2 points\n",
      "tensor([[2.9642, 2.7154],\n",
      "        [2.3239, 1.9862],\n",
      "        [2.9665, 2.8754]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "19\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.135\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 92.376\n",
      "expected info is tensor([[0.0118]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(194.7384, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.7846, 3.0036]])\n",
      "new data istensor([[ 0.4791, -0.0344]])\n",
      "g_theta2 istensor([[2.6174, 2.3740],\n",
      "        [2.3815, 2.2543],\n",
      "        [2.5878, 2.5156]])\n",
      "p21val is 0.708032728149311\n",
      "pf12val is 0.357535715775625\n",
      "chi_f12 is 2.057040035141384\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8633, -2.5057],\n",
      "        [ 2.4628, -2.9316],\n",
      "        [-1.7424, -0.9427]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "20\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.265\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 7.555\n",
      "expected info is tensor([[0.0087]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(21.8486, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.2364, -0.4588]])\n",
      "new data istensor([[-0.3636,  0.5801]])\n",
      "g_theta2 istensor([[ 1.3246, -2.7431],\n",
      "        [ 2.9242, -2.7552],\n",
      "        [-2.2607, -1.4504]])\n",
      "p21val is 0.959766541866555\n",
      "pf12val is 0.000005026529731\n",
      "chi_f12 is 24.401561452508098\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 1.8015088404155222e-05\n",
      "new 2 points\n",
      "tensor([[-1.4044, -2.3266],\n",
      "        [-0.4902, -2.9440],\n",
      "        [-1.2365, -0.4589]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "21\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.084\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 4.520\n",
      "expected info is tensor([[0.1899]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(18.6818, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.7463, -0.9022]])\n",
      "new data istensor([[0.1795, 0.2151]])\n",
      "g_theta2 istensor([[-1.3996, -2.7958],\n",
      "        [-0.1616, -2.4355],\n",
      "        [-1.1801, -0.2500]])\n",
      "p21val is 0.017438912464162\n",
      "pf12val is 0.631364314827565\n",
      "chi_f12 is 0.919744443983492\n",
      "samples escaped box\n",
      "p_val_ftarget is 8.77599737498791e-05\n",
      "new 2 points\n",
      "tensor([[-1.1873, -1.1613],\n",
      "        [ 1.7479,  0.6884],\n",
      "        [-0.7463, -0.9022]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "22\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.174\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 26.702\n",
      "expected info is tensor([[0.0792]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(64.8718, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-0.6968, -0.5320]])\n",
      "new data istensor([[0.2136, 0.1236]])\n",
      "g_theta2 istensor([[-1.5874, -1.7740],\n",
      "        [ 2.0367,  1.2787],\n",
      "        [-0.3492, -0.7954]])\n",
      "p21val is 0.025336947617441\n",
      "pf12val is 0.002475662307698\n",
      "chi_f12 is 12.002494639379462\n",
      "p_val_ftarget is 8.215650382226158e-15\n",
      "new 2 points\n",
      "tensor([[ 0.0845,  0.1611],\n",
      "        [ 2.0617,  1.9790],\n",
      "        [-0.6967, -0.5321]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "23\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.185\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 40.701\n",
      "expected info is tensor([[0.0019]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(90.9770, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.1989, -1.1366]])\n",
      "new data istensor([[-0.1317,  0.2458]])\n",
      "g_theta2 istensor([[ 0.3766, -0.3505],\n",
      "        [ 1.9042,  2.4723],\n",
      "        [-0.1938, -0.9714]])\n",
      "p21val is 0.549988058746650\n",
      "pf12val is 0.880919615263662\n",
      "chi_f12 is 0.253577799630661\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 2.4490,  2.1099],\n",
      "        [-1.5030, -1.9514],\n",
      "        [-1.1989, -1.1367]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "24\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.201\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 43.189\n",
      "expected info is tensor([[0.0097]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(94.0350, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-1.8783, -0.7922]])\n",
      "new data istensor([[-0.4670,  0.3974]])\n",
      "g_theta2 istensor([[ 2.5270,  2.1491],\n",
      "        [-1.3317, -1.8893],\n",
      "        [-0.7312, -0.6412]])\n",
      "p21val is 0.181762709129845\n",
      "pf12val is 0.810839122026795\n",
      "chi_f12 is 0.419371228850557\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.0242,  1.1325],\n",
      "        [-2.5904, -1.5065],\n",
      "        [-1.8781, -0.7922]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "25\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.233\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 27.405\n",
      "expected info is tensor([[0.0507]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(61.3488, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.4468, -0.1897]])\n",
      "new data istensor([[-0.5670,  0.2005]])\n",
      "g_theta2 istensor([[ 0.6821,  1.7896],\n",
      "        [-2.0574, -0.9896],\n",
      "        [-1.5762, -1.0835]])\n",
      "p21val is 0.709633760417697\n",
      "pf12val is 0.550183376480686\n",
      "chi_f12 is 1.195007289083825\n",
      "samples escaped box\n",
      "p_val_ftarget is 4.7628567756419216e-14\n",
      "new 2 points\n",
      "tensor([[-0.4527,  0.6774],\n",
      "        [-1.2969, -1.3052],\n",
      "        [-2.4468, -0.1897]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "26\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.232\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 39.752\n",
      "expected info is tensor([[0.1093]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(85.7082, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.9882,  0.3629]])\n",
      "new data istensor([[-0.4290,  0.0265]])\n",
      "g_theta2 istensor([[-0.1085,  1.1248],\n",
      "        [-1.1515, -1.4086],\n",
      "        [-2.1097, -0.5381]])\n",
      "p21val is 0.644008525007449\n",
      "pf12val is 0.987939984970502\n",
      "chi_f12 is 0.024266654072480\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.7790,  1.0861],\n",
      "        [-2.6042, -0.8182],\n",
      "        [-2.9884,  0.3629]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "27\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.258\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 171.021\n",
      "expected info is tensor([[0.2946]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(350.6522, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-2.8934, -0.4044]])\n",
      "new data istensor([[-0.4657,  0.0316]])\n",
      "g_theta2 istensor([[-2.5263,  1.6668],\n",
      "        [-2.2629, -1.0772],\n",
      "        [-2.6307,  0.4565]])\n",
      "p21val is 0.641707516884161\n",
      "pf12val is 0.887493315733499\n",
      "chi_f12 is 0.238708578457635\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.0478,  1.6022],\n",
      "        [ 0.1219, -1.7085],\n",
      "        [-2.8934, -0.4045]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "28\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.276\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 370.610\n",
      "expected info is tensor([[0.0045]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(728.6425, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0109, -1.0771]])\n",
      "new data istensor([[-0.4601,  0.0389]])\n",
      "g_theta2 istensor([[-1.8620,  1.4994],\n",
      "        [-0.0041, -1.8759],\n",
      "        [-2.4712,  0.0793]])\n",
      "p21val is 0.326671604265471\n",
      "pf12val is 0.307510844936713\n",
      "chi_f12 is 2.358489848541195\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-2.4812,  0.7312],\n",
      "        [-0.5013,  2.8455],\n",
      "        [-2.7611, -0.1744]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "29\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.236\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 474.158\n",
      "expected info is tensor([[0.0368]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(941.6037, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[-3.0096, -0.0777]])\n",
      "new data istensor([[-0.4456, -0.0281]])\n",
      "g_theta2 istensor([[-2.0999,  1.4176],\n",
      "        [-0.3227,  2.5802],\n",
      "        [-2.2546, -0.6247]])\n",
      "p21val is 0.961287940626230\n",
      "pf12val is 0.332366681412234\n",
      "chi_f12 is 2.203032914900276\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6049, -2.7796],\n",
      "        [-2.2802,  1.1245],\n",
      "        [ 2.3450,  0.7285]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "30\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.333\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 181.076\n",
      "expected info is tensor([[0.0021]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(374.8963, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.2729, 0.7742]])\n",
      "new data istensor([[ 0.3501, -0.1154]])\n",
      "g_theta2 istensor([[ 0.5099, -2.6776],\n",
      "        [-1.8530,  1.3830],\n",
      "        [ 2.8939,  1.3024]])\n",
      "p21val is 0.360286741011044\n",
      "pf12val is 0.852647716426679\n",
      "chi_f12 is 0.318817620867887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3047, -1.4525],\n",
      "        [ 1.8635, -1.2983],\n",
      "        [ 2.2729,  0.7741]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "31\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.342\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 135.847\n",
      "expected info is tensor([[0.0159]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(285.4424, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[2.4386, 1.6315]])\n",
      "new data istensor([[0.3321, 0.0032]])\n",
      "g_theta2 istensor([[-0.2391, -1.7139],\n",
      "        [ 1.4706, -1.7161],\n",
      "        [ 2.1717,  0.8200]])\n",
      "p21val is 0.040568381110166\n",
      "pf12val is 0.265225270132714\n",
      "chi_f12 is 2.654351476266710\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[2.5712, 1.7397],\n",
      "        [2.4760, 1.2625],\n",
      "        [2.4384, 1.6314]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "32\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.349\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 125.193\n",
      "expected info is tensor([[0.0054]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(263.8234, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.7011, 1.9182]])\n",
      "new data istensor([[0.3212, 0.0003]])\n",
      "g_theta2 istensor([[2.9886, 2.1241],\n",
      "        [2.8624, 0.9061],\n",
      "        [2.9763, 1.1865]])\n",
      "p21val is 0.878668426729898\n",
      "pf12val is 0.740805631922800\n",
      "chi_f12 is 0.600033986382972\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6287,  2.3243],\n",
      "        [-0.3528, -2.1261],\n",
      "        [ 1.7012,  1.9183]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "33\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.383\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 235.072\n",
      "expected info is tensor([[0.0061]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(484.7870, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.9903, 1.6324]])\n",
      "new data istensor([[0.3247, 0.0099]])\n",
      "g_theta2 istensor([[ 0.0080,  2.9271],\n",
      "        [-0.4744, -2.2136],\n",
      "        [ 1.2083,  1.4413]])\n",
      "p21val is 0.678233872781101\n",
      "pf12val is 0.970138550866143\n",
      "chi_f12 is 0.060632763480627\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 1.0850, -0.5341],\n",
      "        [ 2.3863,  2.9618],\n",
      "        [ 1.9901,  1.6323]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "34\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.401\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 169.682\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(354.1474, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[1.3458, 0.9344]])\n",
      "new data istensor([[0.4623, 0.0522]])\n",
      "g_theta2 istensor([[ 0.8639, -1.0886],\n",
      "        [ 2.0495,  2.9645],\n",
      "        [ 2.2700,  1.9150]])\n",
      "p21val is 0.029606974569604\n",
      "pf12val is 0.205690070603788\n",
      "chi_f12 is 3.162769509477481\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5801,  1.4785],\n",
      "        [-2.2685, -1.1373],\n",
      "        [ 1.3458,  0.9345]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "35\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.400\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: 1.212\n",
      "expected info is tensor([[0.0181]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11.8978, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7572, 0.5748]])\n",
      "new data istensor([[0.2776, 0.3782]])\n",
      "g_theta2 istensor([[ 0.2453,  1.8230],\n",
      "        [-2.3017, -1.3533],\n",
      "        [ 1.1908,  1.4229]])\n",
      "p21val is 0.018097191368801\n",
      "pf12val is 0.007672403129148\n",
      "chi_f12 is 9.740250794544206\n",
      "p_val_ftarget is 0.002608675701389007\n",
      "new 2 points\n",
      "tensor([[-1.0283,  0.4923],\n",
      "        [-1.7395,  1.2315],\n",
      "        [ 0.7571,  0.5746]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "36\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.390\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.249\n",
      "expected info is tensor([[0.0145]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(0.3558, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7093, 0.7523]])\n",
      "new data istensor([[0.3118, 0.3440]])\n",
      "g_theta2 istensor([[-1.5828, -0.0651],\n",
      "        [-2.0944,  1.3916],\n",
      "        [ 1.2712,  0.4579]])\n",
      "p21val is 0.166984042867521\n",
      "pf12val is 0.844126480819214\n",
      "chi_f12 is 0.338905873646697\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.8370435744124989\n",
      "new 2 points\n",
      "tensor([[ 0.7339,  1.5861],\n",
      "        [ 0.7652, -1.5091],\n",
      "        [ 0.7093,  0.7522]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "37\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.398\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -7.085\n",
      "expected info is tensor([[0.0063]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(0.8319, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7845, 0.7457]])\n",
      "new data istensor([[0.3524, 0.3026]])\n",
      "g_theta2 istensor([[ 0.7765,  1.7472],\n",
      "        [ 0.3916, -1.4737],\n",
      "        [ 1.0071,  0.3094]])\n",
      "p21val is 0.001995510394805\n",
      "pf12val is 0.225154171260988\n",
      "chi_f12 is 2.981939811639013\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "37\n",
      "Loss design: -7.075\n",
      "expected info is tensor([[0.0044]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(0.8250, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7763, 0.7479]])\n",
      "new data istensor([[0.3239, 0.3306]])\n",
      "g_theta2 istensor([[ 0.0994, -0.2245],\n",
      "        [-0.2798, -2.4653],\n",
      "        [ 1.2160,  0.3378]])\n",
      "p21val is 0.264160279788190\n",
      "pf12val is 0.894518068370233\n",
      "chi_f12 is 0.222940353659236\n",
      "p_val_ftarget is 0.661979296818467\n",
      "new 2 points\n",
      "tensor([[1.4973, 2.6258],\n",
      "        [0.8800, 1.3062],\n",
      "        [0.7763, 0.7480]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "38\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.429\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.837\n",
      "expected info is tensor([[0.0210]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.2290, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7896, 0.7389]])\n",
      "new data istensor([[0.3417, 0.3266]])\n",
      "g_theta2 istensor([[1.6523, 2.4853],\n",
      "        [1.0700, 1.5359],\n",
      "        [1.1200, 0.4757]])\n",
      "p21val is 0.495113391032752\n",
      "pf12val is 0.773614224179612\n",
      "chi_f12 is 0.513363896033458\n",
      "p_val_ftarget is 0.32807960979164585\n",
      "new 2 points\n",
      "tensor([[0.5682, 0.5080],\n",
      "        [0.4310, 0.0716],\n",
      "        [0.7896, 0.7390]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "39\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.449\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.926\n",
      "expected info is tensor([[0.0067]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.5248, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7665, 0.7453]])\n",
      "new data istensor([[0.3444, 0.2916]])\n",
      "g_theta2 istensor([[ 0.0708,  0.1053],\n",
      "        [ 1.0723, -0.1095],\n",
      "        [ 1.2409,  0.2759]])\n",
      "p21val is 0.290054638111513\n",
      "pf12val is 0.104730756400943\n",
      "chi_f12 is 4.512724893665593\n",
      "p_val_ftarget is 0.28297787272611896\n",
      "new 2 points\n",
      "tensor([[1.5906, 0.9006],\n",
      "        [0.4769, 0.9205],\n",
      "        [0.7662, 0.7453]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "40\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.427\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.876\n",
      "expected info is tensor([[0.0537]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.8869, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8223, 0.6555]])\n",
      "new data istensor([[0.3155, 0.3596]])\n",
      "g_theta2 istensor([[1.4237, 0.8766],\n",
      "        [0.5031, 1.3510],\n",
      "        [0.9922, 1.0473]])\n",
      "p21val is 0.854979740507021\n",
      "pf12val is 0.550874259223365\n",
      "chi_f12 is 1.192497401096320\n",
      "p_val_ftarget is 0.23611364056186113\n",
      "new 2 points\n",
      "tensor([[1.0844, 0.6026],\n",
      "        [0.8756, 0.7583],\n",
      "        [0.8222, 0.6554]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "41\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.489\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -2.239\n",
      "expected info is tensor([[0.0138]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11.3481, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8389, 0.5277]])\n",
      "new data istensor([[0.3066, 0.3771]])\n",
      "g_theta2 istensor([[1.4539, 0.5712],\n",
      "        [1.4419, 1.1985],\n",
      "        [0.7419, 0.8968]])\n",
      "p21val is 0.153389870916908\n",
      "pf12val is 0.076447703717844\n",
      "chi_f12 is 5.142296766900010\n",
      "p_val_ftarget is 0.0034339055219810843\n",
      "new 2 points\n",
      "tensor([[1.6540, 0.9668],\n",
      "        [0.8293, 0.8873],\n",
      "        [0.8387, 0.5277]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "42\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.479\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.205\n",
      "expected info is tensor([[0.0027]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5.4238, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8116, 0.7071]])\n",
      "new data istensor([[0.3340, 0.3194]])\n",
      "g_theta2 istensor([[1.4168, 0.5178],\n",
      "        [0.6195, 1.1660],\n",
      "        [0.6699, 1.2117]])\n",
      "p21val is 0.534474794140525\n",
      "pf12val is 0.604422206213392\n",
      "chi_f12 is 1.006964616672521\n",
      "p_val_ftarget is 0.066411678368552\n",
      "new 2 points\n",
      "tensor([[0.4958, 0.5945],\n",
      "        [0.8030, 0.4015],\n",
      "        [0.8116, 0.7071]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "43\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.474\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.002\n",
      "expected info is tensor([[0.0194]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5.9065, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8362, 0.6639]])\n",
      "new data istensor([[0.3611, 0.3156]])\n",
      "g_theta2 istensor([[ 0.0644,  0.0814],\n",
      "        [ 1.0243, -0.0045],\n",
      "        [ 0.8416,  0.9685]])\n",
      "p21val is 0.032617540070011\n",
      "pf12val is 0.123498143997409\n",
      "chi_f12 is 4.183058302776189\n",
      "p_val_ftarget is 0.05216860466259976\n",
      "new 2 points\n",
      "tensor([[0.9058, 1.1621],\n",
      "        [0.6619, 2.0476],\n",
      "        [0.8361, 0.6639]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "44\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.513\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -5.013\n",
      "expected info is tensor([[0.0007]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(7.3522, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8441, 0.5803]])\n",
      "new data istensor([[0.3105, 0.3760]])\n",
      "g_theta2 istensor([[0.5710, 1.3981],\n",
      "        [1.0751, 2.6870],\n",
      "        [1.0221, 1.0058]])\n",
      "p21val is 0.711991090438503\n",
      "pf12val is 0.193393519060544\n",
      "chi_f12 is 3.286056414139420\n",
      "p_val_ftarget is 0.025321754469662672\n",
      "new 2 points\n",
      "tensor([[ 0.6313,  0.8375],\n",
      "        [ 0.4970, -0.5563],\n",
      "        [ 0.8441,  0.5804]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "45\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.541\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -2.727\n",
      "expected info is tensor([[0.0135]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(10.3667, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8288, 0.4370]])\n",
      "new data istensor([[0.3143, 0.3920]])\n",
      "g_theta2 istensor([[ 0.8299,  1.2859],\n",
      "        [ 0.2230, -0.3056],\n",
      "        [ 0.6430,  0.9112]])\n",
      "p21val is 0.234135565155742\n",
      "pf12val is 0.044313402068121\n",
      "chi_f12 is 6.232936235859029\n",
      "p_val_ftarget is 0.0056091117100214305\n",
      "new 2 points\n",
      "tensor([[1.2567, 1.7227],\n",
      "        [0.1844, 0.0299],\n",
      "        [0.8288, 0.4370]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "46\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.556\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -6.427\n",
      "expected info is tensor([[0.0013]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(3.7809, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8480, 0.4534]])\n",
      "new data istensor([[0.3134, 0.3331]])\n",
      "g_theta2 istensor([[ 1.4040,  1.5218],\n",
      "        [-0.3261, -0.5439],\n",
      "        [ 1.2776,  0.8426]])\n",
      "p21val is 0.690069291636649\n",
      "pf12val is 0.847641852509409\n",
      "chi_f12 is 0.330594152282751\n",
      "p_val_ftarget is 0.15100369075573106\n",
      "new 2 points\n",
      "tensor([[-0.9635, -1.7607],\n",
      "        [ 1.5110,  1.4972],\n",
      "        [ 0.8480,  0.4535]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "47\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.573\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -3.786\n",
      "expected info is tensor([[0.0015]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(11.1119, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.7897, 0.7158]])\n",
      "new data istensor([[0.3193, 0.3533]])\n",
      "g_theta2 istensor([[-1.0478, -1.8670],\n",
      "        [ 1.4221,  1.3992],\n",
      "        [ 0.8171,  1.1712]])\n",
      "p21val is 0.352046712222290\n",
      "pf12val is 0.768871577865410\n",
      "chi_f12 is 0.525662644593973\n",
      "p_val_ftarget is 0.003864481179693735\n",
      "new 2 points\n",
      "tensor([[-0.4567, -0.9002],\n",
      "        [ 0.7859,  0.6131],\n",
      "        [ 0.7898,  0.7160]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "48\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.587\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -7.937\n",
      "expected info is tensor([[0.0020]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(2.5711, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8614, 0.6031]])\n",
      "new data istensor([[0.3389, 0.3405]])\n",
      "g_theta2 istensor([[-0.4050, -0.8348],\n",
      "        [ 0.5619,  0.7089],\n",
      "        [ 0.6588,  0.9013]])\n",
      "p21val is 0.565486893410468\n",
      "pf12val is 0.914246313450213\n",
      "chi_f12 is 0.179310508564237\n",
      "p_val_ftarget is 0.2765045952521795\n",
      "new 2 points\n",
      "tensor([[1.6972, 1.9647],\n",
      "        [1.2183, 1.2039],\n",
      "        [0.8614, 0.6032]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "49\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.607\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -5.876\n",
      "expected info is tensor([[0.0026]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(5.4564, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8485, 0.4585]])\n",
      "new data istensor([[0.3368, 0.3217]])\n",
      "g_theta2 istensor([[1.7391, 2.0186],\n",
      "        [1.3933, 1.3755],\n",
      "        [1.0405, 0.9584]])\n",
      "p21val is 0.730186323227556\n",
      "pf12val is 0.547226170529400\n",
      "chi_f12 is 1.205786175115134\n",
      "p_val_ftarget is 0.0653362600563141\n",
      "new 2 points\n",
      "tensor([[1.6790, 1.9038],\n",
      "        [1.0164, 0.7236],\n",
      "        [0.8483, 0.4585]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "50\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.632\n",
      "END HYPERPARAMETERS optimization\n",
      "Loss design: -7.334\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "mohabb disatance istensor(4.2261, grad_fn=<SumBackward1>)\n",
      "current sol istensor([[0.8566, 0.6078]])\n",
      "new data istensor([[0.3121, 0.3432]])\n",
      "g_theta2 istensor([[2.0950, 2.2929],\n",
      "        [0.7226, 1.0786],\n",
      "        [1.2898, 1.0306]])\n",
      "p21val is 0.268381556183809\n",
      "pf12val is 0.704960422346460\n",
      "chi_f12 is 0.699227232522708\n",
      "p_val_ftarget is 0.12086837945898643\n",
      "new 2 points\n",
      "tensor([[ 1.8042,  1.8633],\n",
      "        [ 0.1006, -0.6194],\n",
      "        [ 0.8566,  0.6078]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "51\n",
      "START HYPERPARAMETERS optimization\n",
      "loss is -2.648\n",
      "END HYPERPARAMETERS optimization\n"
     ]
    }
   ],
   "source": [
    "loc_sample = loc_sample0.clone()\n",
    "iter_hp = 50\n",
    "iter_design = 200\n",
    "iter_param = 200\n",
    "num_base_kernels = 2\n",
    "max_iter = 50\n",
    "\n",
    "\n",
    "f_target = f_target.reshape(2,1) \n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    " #np.random.random_sample((loc_size,2))\n",
    "#loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "#train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "\n",
    "#loc_sample = Tensor([[0.0, 0.1], [0.0, -0.1]]) #T\n",
    "# loc_x = (-1.5 + 2.)  * np.random.random_sample((loc_size,1)) +2.\n",
    "\n",
    "# # loc_y = (2. - 1.5)  * np.random.random_sample((loc_size,1)) - 1.5\n",
    "# # loc = np.concatenate((loc_x, loc_y), 1)\n",
    "print(loc_sample)\n",
    "\n",
    "\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten()\n",
    "\n",
    "data_fit_vec = torch.empty((1,1))\n",
    "entropy_vec = torch.empty((1,1))\n",
    "loss_vec = torch.empty((1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "vec_x = x0.clone() #Tensor(np.array([0.0,0.0])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "var_vec = torch.zeros([max_iter, 1])\n",
    "p21_vec = torch.empty((1,1))\n",
    "\n",
    "lr_new = .01\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "show_TTRBox = False\n",
    "iter = 0    \n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "patience = 0.0\n",
    "patience_f = 0.0\n",
    "patience_2 = 0.0\n",
    "checking_model = False\n",
    "model_double_check = False\n",
    "while(SUCCESS == False and FAILURE == False):\n",
    "    print(iter)\n",
    "    model_double_check = False\n",
    "    if (checking_model == False):\n",
    "        print('START HYPERPARAMETERS optimization')\n",
    "        if (iter == 0):\n",
    "            cur_model = None\n",
    "            cur_likelihood = None\n",
    "\n",
    "\n",
    "        loc_sample_old = loc_sample.clone()\n",
    "        x0_old = x0.clone()\n",
    "        model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "\n",
    "\n",
    "        print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "   \n",
    "    \n",
    "    x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new,noise_value)\n",
    "  \n",
    "    cur_model = model\n",
    "    cur_likelihood = likelihood\n",
    "    \n",
    "  \n",
    "    lower_bound = torch.zeros(pf1.shape)\n",
    "    upper_bound = torch.zeros(pf1.shape)\n",
    "        \n",
    "    for i in range(pf1.shape[0]):\n",
    "        lower_bound[i] = pf1[i] -  torch.sqrt(Qf12[i,i])\n",
    "        upper_bound[i] = pf1[i] +  torch.sqrt(Qf12[i,i])\n",
    "\n",
    "    SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    \n",
    "    \n",
    "    entropy = ( 0.5 * torch.log( torch.det(Qf1.evaluate()) / torch.det(Qf12.evaluate()) ) ).reshape(1,1)\n",
    "    \n",
    "    print('expected info is '+str(entropy))\n",
    "    print('mohabb disatance is' + str(Qf12.inv_quad(f_target - pf1)))\n",
    "    if not SUCCESS:\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        new_data = vfield_(g_theta2.detach())  \n",
    "        agg_data12 = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "        g_theta12= torch.cat([g_theta1, g_theta2.detach()], 0)\n",
    "        new_data_x = vfield_(x0_new.detach() )  \n",
    "        print('current sol is'+str(x0_new.detach()))\n",
    "        print('new data is' + str(new_data_x))\n",
    "        print('g_theta2 is' + str(g_theta2.detach()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            if iter >= 0:\n",
    "                \n",
    "                \n",
    "                p21 = likelihood.get_p21(g_theta1, g_theta2.detach(), agg_data, model, noise_value)\n",
    "                \n",
    "#                 Q21 = Q21 + noise_value*torch.eye(Q21.shape[0])\n",
    "                chi_21 = (Q21).inv_quad(new_data.flatten() - p21.reshape(new_data.flatten().shape))\n",
    "                p_val = 1. - stats.chi2.cdf(chi_21, Q21.shape[0])\n",
    "                pf12 = likelihood.get_pf12(Q21,g_theta1, g_theta2.detach(), x0_new.detach(), new_data.flatten(), pf1, p21, model, noise_value)\n",
    "               \n",
    "                #Qf12 = Qf12 + \n",
    "                chi_f12 = (Qf12 + noise_value*torch.eye(Qf12.shape[0])).inv_quad(new_data_x.flatten() - pf12.reshape(new_data_x.flatten().shape))\n",
    "                p_val_f12 = 1. - stats.chi2.cdf(chi_f12, Qf12.shape[0])\n",
    "                print('p21val is %.15f' %p_val)\n",
    "                p21_vec = torch.cat([p21_vec, Tensor([p_val]).reshape(1,1)], 0)\n",
    "                print('pf12val is %.15f' %p_val_f12)\n",
    "                print('chi_f12 is %.15f' %chi_f12 )\n",
    "                \n",
    "                if (p_val < 0.01):# or p_val_f12 < 0.001:\n",
    "                    model_double_check = True\n",
    "                    checking_model = True\n",
    "                    patience = patience+1\n",
    "                    print('patience is %.3f' %patience)\n",
    "\n",
    "                if (model_double_check == True):\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_sample.shape[0],2)) + vf.low)\n",
    "                    sum = torch.zeros(2, 2) #replace with num_tasks\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_old).t(), (g_theta2.detach()[i] - x0_old) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum #+ torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_old, covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_old],0)\n",
    "                    \n",
    "                    x0 = x0_old #Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    if (patience >= 2):# or patience_2 >= 2 or patience_f >= 2):\n",
    "                        PATH = \".//model_Carlo/model_update/model_base_\"+str(iter)+\".pt\"\n",
    "                        torch.save(model, PATH)\n",
    "                        entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                        data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                        iter = iter + 1\n",
    "                        patience = 0\n",
    "#                         patience_2 = 0\n",
    "#                         patience_f = 0\n",
    "                        model_double_check = False\n",
    "                        checking_model = False\n",
    "                        num_base_kernels = num_base_kernels + 1\n",
    "                        print('adding complexity to model')\n",
    "                        print('num base is' + str(num_base_kernels))\n",
    "#     #                         \n",
    "                        loc_sample = loc_sample_old\n",
    "                        #x0 = x0_old\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                        g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                        print('acquiring 2, new size is' + str(g_theta1.shape[0]))\n",
    "                 \n",
    "                    #iter_hp = iter_hp + 10\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "                    torch.save(model, PATH)\n",
    "                    vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "                    loss_vec = torch.cat([loss_vec, -loss])\n",
    "                    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                    entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                    data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                    model_double_check = False\n",
    "                    iter = iter + 1\n",
    "                    patience = 0\n",
    "                    patience_2 = 0\n",
    "                    patience_f = 0\n",
    "                    checking_model = False\n",
    "                    if (entropy < 1e-4 * tol_vector[0,0]):\n",
    "                        FAILURE = True\n",
    "                    \n",
    "                    x0 = (x0_new.detach())# + torch.randn(x0_new.detach().size()) * .001)#/torch.norm(x0_new.detach())\n",
    "                    sum = torch.zeros(2, 2)\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum# + torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_new.detach(), covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = loc_sample#/torch.norm(loc_sample)\n",
    "                    #loc_sample = 2. *  (loc_sample - torch.min(loc_sample)) / (torch.max(loc_sample) - torch.min(loc_sample)) - 1.\n",
    "                    #loc_sample[0] = x0_new.detach() #+ torch.randn(x0_new.detach().size()) * .001 #g_theta2.detach() #loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_size,2)) + vf.low)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_new.detach()],0)\n",
    "                    for i in range(loc_sample.shape[0]):\n",
    "                        if loc_sample[i,0] < -3. or loc_sample[i,0] > 3. or loc_sample[i,1] < -3. or loc_sample[i,1] > 3.:\n",
    "                            print('samples escaped box')\n",
    "                            loc_sample[i] = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    \n",
    "                    \n",
    "#                     if p_val > 0.99 and p_val_f12 > 0.99:\n",
    "#                         num_base_kernels = max(num_base_kernels - 1, 3)\n",
    "                        #iter_hp = iter_hp - 10\n",
    "                    chi_f_target = (Qf12 ).inv_quad(f_target - pf1)\n",
    "                    p_val_f_target = 1. - stats.chi2.cdf(chi_f_target, Qf12.shape[0])\n",
    "                    print('p_val_ftarget is '+str(p_val_f_target))\n",
    "                    if (p_val_f_target > .95):\n",
    "                        print('acquiring target point becuse p_val_ftarget is '+str(p_val_f_target))\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "        \n",
    "\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                    else:\n",
    "#                         x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "#                         g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                       \n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .0001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                        \n",
    "                    if x0_new.detach()[0,0] < -3. or x0_new.detach()[0,0] > 3. or x0_new.detach()[0,1] < -3. or x0_new.detach()[0,1] > 3.:\n",
    "#                         x0 = Tensor(np.array([0.0,-1.0])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = x0.reshape(1,2) \n",
    "                        #x0 = Tensor(np.array([-2. , 2.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = x0.reshape(1,2)\n",
    "                        #x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                 \n",
    "             #       loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "                        loc_sample[-1] = x0 #(x0_new.detach()) \n",
    "                    print('new 2 points')\n",
    "                    print(loc_sample)\n",
    "                  \n",
    " #                    agg_data  = (agg_data  - agg_data.mean())/agg_data .std(dim=-1, keepdim=True)\n",
    "#                     g_theta1 = (g_theta1 - g_theta1.mean())/g_theta1.std(dim=-2, keepdim=True)\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            #clear_output(wait=False)\n",
    "           \n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "torch.save(model, PATH)\n",
    "print('current sol is'+str(x0_new.detach()))\n",
    "    \n",
    "print('Success is ' + str(SUCCESS) + ' and failure is ' + str(FAILURE)+' after '+ str(iter) + ' iterations')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target - 0.001)\n",
    "print(f_target + 0.001)\n",
    "print(pf1)\n",
    "print(num_base_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting model validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "#plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "p21_vec_plot = p21_vec.detach()[1:]\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "#print(p21_vec_plot)\n",
    "ax.plot(p21_vec_plot,'s',color = 'blue', markersize=6)\n",
    "ax.axhline(.01,linestyle = '--',color = 'red', markersize=12, alpha = 1.0)\n",
    "ax.set_xlim(-0.3, p21_vec_plot.shape[0])\n",
    "ax.set_ylim(-0.3, 1.)\n",
    "#ax.set_yscale('log')\n",
    "plt.xticks(np.arange(0, iter+7, step=5.))\n",
    "ax.tick_params(labelsize='small', width=3)\n",
    "ax.set_xlabel('# of Model Checks')\n",
    "ax.set_ylabel('p-value')\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.05, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(1.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.02, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(0.1,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.26, 0.34), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(6.2,        #x start point\n",
    "             0.14,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.07,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.29, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(7.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.57, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(14.15,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.69, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(17.18,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.32, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(10.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "# ############################\n",
    "\n",
    "# ############################\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.33, 0.34), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(11.25,        #x start point\n",
    "#              0.14,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.07,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# #################################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.38, 0.1), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(12.25,        #x start point\n",
    "#              -0.15,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.1,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.38, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(13.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.55, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(18.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')  \n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.49, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(17.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.55, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(19.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.94, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(33.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ################################\n",
    "\n",
    "ax.annotate('Threshold Line ($y = 0.01$)', xy=(1.0,0.01), xytext=(6,0), color='red', \n",
    "                xycoords = ax.get_yaxis_transform(), textcoords=\"offset points\",\n",
    "                size=14, va=\"center\")\n",
    "\n",
    "\n",
    "#ax.legend(['p-value'], loc = 'upper left', fontsize = 30)\n",
    "plt.savefig('figures_Carlo/qvalue_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting expected value and data fit term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(18, 8), tight_layout=True)\n",
    "\n",
    "ax1.plot(np.array(range(1,iter+2)), (entropy_vec_plot), '--o', color = 'blue', markersize=12)\n",
    "#ax1.set_yscale('log')\n",
    "# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\n",
    "#ax1.set_yscale('log')\n",
    "\n",
    "ax2.plot(np.array(range(1,iter+2)), data_fit_vec_plot, '--o', color = 'red', markersize=12)\n",
    "#ax2.set_ylim(-10, 600)\n",
    "ax1.set_xlabel('Iteration #', size=32)\n",
    "ax2.set_xlabel('Iteration #', size=32)\n",
    "ax1.set_ylabel('Expected Information', size = 32)\n",
    "ax2.set_ylabel('Log-Gaussian Term', size = 32)\n",
    "ax1.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "ax2.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "plt.savefig('figures_Carlo/exp_info/expectedinfo_vs_datafit_2_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving all data needed for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/vec_x_success_base.txt',vec_x.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta2_success_base.txt', g_theta2.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta1_success_base.txt', g_theta1.detach().numpy())\n",
    "np.savetxt('data_plots/x_train_ini_success_base.txt', x_train.detach().numpy())\n",
    "np.savetxt('data_plots/y_train_ini_success_base.txt', y_train.detach().numpy())\n",
    "np.savetxt('data_plots/entropy_vec_success_base.txt', entropy_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/datafit_success_base.txt', data_fit_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/p21_vec_success_base.txt',p21_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/loss_success_base.txt',loss.detach().numpy())\n",
    "np.savetxt('data_plots/pf1_success_base.txt',pf1.detach().numpy())\n",
    "np.savetxt('data_plots/Qf1_success_base.txt',Qf1.evaluate().detach().numpy())\n",
    "np.savetxt('data_plots/Qf12_success_base.txt', Qf12.evaluate().detach().numpy())\n",
    "np.savetxt( 'data_plots/Q21_success_base.txt', Q21.evaluate().detach().numpy())\n",
    "#np.savetxt('data_plots/iter_success.txt', iter+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2),2)\n",
    "torch.save(v2, 'data_plots/v2_success_base.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.set_xlim(-3.1, 3.2)\n",
    "ax.set_ylim(-3.1, 3.2)\n",
    "#ax.scatter(g_theta1[:, 0].detach(),g_theta1[:, 1].detach(), c=\"b\", alpha=0.8)\n",
    "ax.plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(vec_x[-1,0], vec_x[-1,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd', color = 'green',markersize=15)\n",
    "ax.set_title('Final TAD configuration', fontsize = 40)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.legend(['1-points', 'TAD solution', 'Target'])\n",
    "plt.savefig('figures_Carlo/strategies/tad_sol_all_2_base.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "#v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "ii = 0\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "########################\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "ax.plot(vec_x[ii,0], vec_x[ii,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o', color = 'blue', markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Initial Configuration', fontsize = 40)\n",
    "ax.legend(['Target', 'Initial Target Candidate', 'Initial 1-sample','Initial 2-sample'])\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_ini_2_base.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "#for iter_plt in range(1,iter):\n",
    "iter_plt = 13\n",
    "ii = 3 + (iter_plt - 1) * (loc_size + 1)\n",
    "#######################\n",
    "\n",
    " \n",
    "###########\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(14,14))\n",
    "\n",
    "#ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 'bv', markersize=8)\n",
    "for i in range(1, iter_plt):\n",
    "    ax.plot(vec_x[i-1:i,0], vec_x[i-1:i,1],'v', color = 'red', markersize=15, alpha=0.2, label = '_nolegend_')\n",
    "    \n",
    "ax.plot(vec_x[0:iter_plt,0], vec_x[0:iter_plt,1],'v', color = 'red', markersize=15, linewidth=15, alpha= 0.2)\n",
    "#ax.plot(g_theta1.detach()[:,0], g_theta.detach()[:,1], 'bv', markersize=8)\n",
    "ax.plot(v2.detach()[0:ii,0], v2.detach()[0:ii,1], 's', color = 'blue', markersize=15,alpha=0.2)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o',color = 'blue' , markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd',markersize=15)\n",
    "ax.plot(vec_x[iter_plt,0], vec_x[iter_plt,1],'v', color = 'red',markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "#ax.set_title('Iteration '+str(iter_plt), fontsize = 40)\n",
    "ax.set_title('Final Configuration')\n",
    "ax.legend([ 'Previous Target Candidates', 'Previous 2-samples', 'Initial 1-sample','Current 2-sample', 'Target', 'Current Target Candidate'], fontsize = 30)\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_base'+str(iter_plt)+'_final.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta2_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_pll(x0,f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    #g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': x_d, 'lr': 0.001}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        #print(g_theta)\n",
    "        loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood, noise_value)\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood ,noise_value)\n",
    "    #loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "   # print(optimizer.state_dict())\n",
    "    print(x_d)\n",
    "    return x_d, lower_bound, upper_bound\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(vf.low, vf.high, 15)\n",
    "y_plot = np.linspace(vf.low, vf.high, 15)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat = torch.zeros(n * n, 2)\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "\n",
    "g_theta_grid = x_concat\n",
    "agg_data1_grid = vfield_(g_theta_grid)\n",
    "agg_data1_grid = agg_data1_grid.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.,2.])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_grid = Tensor(np.array([0.0,0.0])) \n",
    "vec_x_grid = vec_x_grid.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "model_grid, likelihood_grid = hyper_opti(g_theta_grid,agg_data1_grid,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_grid.eval()\n",
    "likelihood_grid.eval()\n",
    "x0_new_grid,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_grid, agg_data1_grid, model_grid, likelihood_grid, iter_design, iter_param, lr_new,noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "#loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_grid)\n",
    "print(SUCCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_grid = x0_new.detach()\n",
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.plot(x_concat[:,0],x_concat[:,1], 's', color = 'blue', markersize=15, alpha = 0.2)\n",
    "ax.plot(x0_new_grid.detach()[0,0], x0_new_grid.detach()[0,1],'v',color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Grid Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/grid_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_hp = 30\n",
    "# iter_design = 40 \n",
    "# iter_param = 50\n",
    "# num_base_kernels = 3\n",
    "\n",
    "# f_target = Tensor(vf.tgt_vec) \n",
    "# f_target = f_target.reshape(f_target.shape[0],1) \n",
    "# tol_vector = 0.005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size_rdn = math.ceil(g_theta1.shape[0]) #(iter)*(loc_size+1) + sample_size\n",
    "\n",
    "loc_sample = high_minus_low  * np.random.random_sample((loc_size_rdn,2)) + vf.low #np.random.random_sample((loc_size_rdn,2))\n",
    "g_theta_ = (Tensor(loc_sample).clone())\n",
    "agg_data1 = vfield_(g_theta_)\n",
    "agg_data1 = agg_data1.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.0,2.0])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_rdn = Tensor(np.array([0.,0.])) \n",
    "vec_x_rdn = vec_x_rdn.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "\n",
    "model_rdn, likelihood_rdn = hyper_opti(g_theta_,agg_data1,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_rdn.eval()\n",
    "likelihood_rdn.eval()\n",
    "x0_new_rdn,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_, agg_data1, model_rdn, likelihood_rdn, iter_design, iter_param, lr_new, noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_rdn)\n",
    "print(SUCCESS)\n",
    "sol_rdn = x0_new_rdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/sol_rdn_success_base.txt', sol_rdn.detach().numpy())\n",
    "np.savetxt('data_plots/sol_grid_success_base.txt', x0_new_grid.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta_rdn_success_base.txt', g_theta_.detach().numpy())\n",
    "#np.savetxt('data_plots/g_theta_grid_success.txt', x_concat.detach().numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "\n",
    "ax.plot(g_theta_[:,0].detach(),g_theta_[:,1].detach(), 's', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(sol_rdn.detach()[0,0], sol_rdn.detach()[0,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$', fontsize = 32)\n",
    "ax.set_ylabel('$d_2$', fontsize = 32)\n",
    "ax.set_title('Uniformly Random Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/rdn_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizing Means and Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker\n",
    "class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "        self.oom = order\n",
    "        self.fformat = fformat\n",
    "        matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
    "    def _set_order_of_magnitude(self):\n",
    "        self.orderOfMagnitude = self.oom\n",
    "    def _set_format(self, vmin=None, vmax=None):\n",
    "        self.format = self.fformat\n",
    "        if self._useMathText:\n",
    "             self.format = r'$\\mathdefault{%s}$' % self.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "likelihood.eval()\n",
    "\n",
    "#noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "#print(x_concat_)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(True):\n",
    "    pred = GPprediction(model)\n",
    "    pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "    #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "    pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "    mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "    mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "    pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "    \n",
    "    var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "    var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "#     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "# #     print(pr.covariance_matrix.mean(axis=0))\n",
    "# #     print(AA)\n",
    "# #     print(pr.variance)\n",
    "# #     print((pr.covariance_matrix))\n",
    "# #     K = model.covar_module\n",
    "#     print((cov.diag()))\n",
    "#     print(pr_mean)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 2, figsize = (28, 24), tight_layout=True)\n",
    "diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "cs10 = ax1[0].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax1[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[0].set_title('$|v_1 - \\mu(v_1)|$', fontsize = 40)\n",
    "cbar10 = fig.colorbar(cs10, ax = ax1[0],format=OOMFormatter(0, mathText=False));\n",
    "\n",
    "ax1[0].set_xlabel('$d_1$')\n",
    "ax1[0].set_ylabel('$d_2$')\n",
    "diff_mean_v1 = torch.abs(v_1 - mean_v_1.detach())/torch.sqrt(var_v_1)\n",
    "#print(var_v_1)\n",
    "cs11 = ax1[1].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(diff_mean_v1.min(), diff_mean_v1.max(), 100), cmap = 'jet')\n",
    "ax1[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[1].set_title('$|v_1 - \\mu(v_1)|/\\sigma(v_1)$', fontsize = 40)\n",
    "# ax1[0].set_aspect('equal')\n",
    "# ax1[1].set_aspect('equal')\n",
    "cbar11 = fig.colorbar(cs11, ax = ax1[1],format=OOMFormatter(0, mathText=False));\n",
    "ax1[1].set_xlabel('$d_1$')\n",
    "ax1[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())\n",
    "cs20 = ax2[0].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax2[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[0].set_title('$|v_2 - \\mu(v_2)|$', fontsize = 40)\n",
    "cbar20 = fig.colorbar(cs20, ax = ax2[0],format=OOMFormatter(0, mathText=False));\n",
    "ax2[0].set_xlabel('$d_1$')\n",
    "ax2[0].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())/torch.sqrt(var_v_2)\n",
    "cs21 = ax2[1].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(diff_mean_v2.min(), diff_mean_v2.max(), 100), cmap = 'jet')\n",
    "ax2[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[1].set_title('$|v_2 - \\mu(v_2)|/\\sigma(v_2)$', fontsize = 40)\n",
    "cbar21 = fig.colorbar(cs21, ax = ax2[1],format=OOMFormatter(0, mathText=False));\n",
    "ax2[1].set_xlabel('$d_1$')\n",
    "ax2[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "# ax2[0].set_aspect('equal')\n",
    "# ax2[1].set_aspect('equal')\n",
    "\n",
    "plt.savefig('figures_Carlo/mean_var/mean_final_2.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "plot = [1, 6, 10, iter+1]\n",
    "\n",
    "for ii in plot:\n",
    "    try:\n",
    "        \n",
    "        PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "    except:\n",
    "        PATH = \".//model_Carlo/model_update/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "        \n",
    "    #model_16 = torch.load(PATH)\n",
    "    model_16.eval()\n",
    "\n",
    "    likelihood.eval()\n",
    "\n",
    "    #noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "    #print(x_concat_)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var(False):\n",
    "        pred = GPprediction(model_16)\n",
    "        pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "        #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "        pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "        mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "        mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "        pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "\n",
    "        var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "        var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "    #     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "    # #     print(pr.covariance_matrix.mean(axis=0))\n",
    "    # #     print(AA)\n",
    "    # #     print(pr.variance)\n",
    "    # #     print((pr.covariance_matrix))\n",
    "    # #     K = model.covar_module\n",
    "    #     print((cov.diag()))\n",
    "    #     print(pr_mean)\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (28, 12), tight_layout=True)\n",
    "    diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "    minn = torch.min(var_v_1.detach().min(), var_v_2.detach().min())\n",
    "    maxx = torch.min(var_v_1.detach().max(), var_v_2.detach().max())\n",
    "\n",
    "    cs11 = ax1.contourf(xv_plot, yv_plot, var_v_1.detach(), np.linspace(0.0,1.1, 100), cmap = 'jet')\n",
    "    ax1.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    \n",
    "    ax1.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "  \n",
    "    ax1.set_title('$\\sigma^2(v_1)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    # ax1[0].set_aspect('equal')\n",
    "    # ax1[1].set_aspect('equal')\n",
    "    cbar11 = fig.colorbar(cs11, ax = ax1,format=OOMFormatter(-0, mathText=False));\n",
    "    ax1.set_xlabel('$d_1$')\n",
    "    ax1.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    cs21 = ax2.contourf(xv_plot, yv_plot, var_v_2.detach(), np.linspace(0.0, 1.1, 100), cmap = 'jet')\n",
    "    ax2.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "    ax2.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    ax2.set_title('$\\sigma^2(v_2)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    cbar21 = fig.colorbar(cs21, ax = ax2,format=OOMFormatter(-0, mathText=False));\n",
    "    ax2.set_xlabel('$d_1$')\n",
    "    ax2.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "    # ax2[0].set_aspect('equal')\n",
    "    # ax2[1].set_aspect('equal')\n",
    "\n",
    "    plt.savefig('figures_Carlo/mean_var/var_iter_base'+str(ii)+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# # #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "# #                     #loc_size = 4\n",
    "# # loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# # loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# # g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# z = torch.zeros(n*n, 1)\n",
    "# for ii in range(n*n):\n",
    "#     print(ii)\n",
    "#     x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "#     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#     g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21 = conduct_2opt(agg_data,f_target,x0, g_theta1, model, likelihood, noise_value, g2, training_param_iter)\n",
    "#     z[ii] = loss2\n",
    "# z = z.reshape(n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "# loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# g2 = g_theta2.detach() #loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# plot = [1, 7, 17, 26]\n",
    "# zz = torch.zeros(n*n, 4)\n",
    "# kk = 0\n",
    "# for jj in plot:\n",
    "#     try:\n",
    "        \n",
    "#         PATH = \".//model_Carlo/model_goodmodel/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#     except:\n",
    "#         PATH = \".//model_Carlo/model_update/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#    # model_16 = torch.load(PATH)\n",
    "#     model_16.eval()\n",
    "\n",
    "#     likelihood.eval()\n",
    "#     g2 = v2.detach()[jj - 1 +loc_size+1 : jj - 1 +loc_size+1 +loc_size+1]\n",
    "#     g_theta1_cur = g_theta1[0:(4 + (jj - 1)* 3)]\n",
    "#     agg_data_cur = agg_data[0:2 * (4 + (jj - 1)* 3)]\n",
    "#     print(agg_data_cur.shape)\n",
    "#     for ii in range(n*n):\n",
    "#         print(ii)\n",
    "#         x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     #     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#     #                     #loc_size = 4\n",
    "#     #     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     #     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#         #g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "\n",
    "\n",
    "        \n",
    "#         loss2_, pf1_, Qf1_, Qf12_, data_fit_, Q21_ = likelihood.get_ell(agg_data_cur,f_target,x0, g_theta1_cur, model_16, likelihood, noise_value, g2)\n",
    "#         zz[ii, kk] = loss2_\n",
    "#     kk = kk+1\n",
    "# zz = zz.reshape(n,n, 4)\n",
    "# torch.save(zz.detach(), 'data_plots/zz_success.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot = [1, 5, 10, 20]\n",
    "\n",
    "# for jj in range(4):\n",
    "#     fig, ax = plt.subplots(figsize = (16,14))\n",
    "#     #\n",
    "#     cs = ax.contour(xv_plot, yv_plot,  zz[:,:,jj].detach(), np.linspace( zz[:,:,jj].detach().numpy().min(), zz[:,:,jj].detach().numpy().max(), 1000), cmap = 'jet')\n",
    "#     cbar = fig.colorbar(cs, ax = ax,format=OOMFormatter(0, mathText=False));\n",
    "#     ax.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "#     kk = plot[jj]\n",
    "#     if kk < plot[3]:\n",
    "#         ax.plot(vec_x[kk,0], vec_x[kk,1],'o', color = 'black',markersize=12)\n",
    "#     if kk == plot[3]:\n",
    "#         ax.plot(vec_x[- 1,0], vec_x[- 1,1],'o', color = 'black',markersize=12)\n",
    "#     ax.set_title('TAD Acquisition Function (Iteration '+str(kk)+')', fontsize = 40)\n",
    "#     ax.set_xlabel('$d_1$')\n",
    "#     ax.set_ylabel('$d_2$')\n",
    "    \n",
    "#     plt.savefig('figures_Carlo/tad_obj'+str(kk)+'.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p21_vec_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
