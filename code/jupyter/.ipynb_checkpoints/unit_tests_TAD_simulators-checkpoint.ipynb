{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import sep_vvkernels as svvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = VField()\n",
    "f_target = vf.tgt_vec\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "def g_theta(sample_size, D):\n",
    "    loc = np.random.random_sample((sample_size,D))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "train_y = torch.zeros([sample_size, N])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    train_y[i] = Tensor(vf(train_x[i]))# + torch.randn(Tensor(vf(train_x[i])).size()) * 0.005\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = Tensor(vf(x[i])) #+ torch.randn(Tensor(vf(x[i])).size()) * 0.005\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criteria(tol_vector, f_target, lower_bound, upper_bound):\n",
    "    lower_tol_vector = f_target - tol_vector\n",
    "    upper_tol_vector = f_target + tol_vector\n",
    "    SUCCESS = True\n",
    "    FAILURE = False\n",
    "    for i in range(f_target.shape[0]):\n",
    "            if (lower_bound[i] < lower_tol_vector[i]) or  (upper_bound[i] > upper_tol_vector[i]):\n",
    "                SUCCESS = False  \n",
    "            if ((lower_bound[i] > upper_tol_vector[i]) or  (upper_bound[i] < lower_tol_vector[i])):\n",
    "                FAILURE = True\n",
    "    return SUCCESS, FAILURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(vvk_rbf.vvkRBFKernel())\n",
    "\n",
    "\n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "#\\         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "       # self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##hyperparameters optimization###\n",
    "# def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "#     likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "#     model = MultitaskGPModel(g_theta1, agg_data, likelihood,3)\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "    \n",
    "    \n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),  lr=0.07)  # Includes GaussianLikelihood parameters\n",
    "#     #mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "#     for i in range(training_iter):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(g_theta1)\n",
    "#         output_ll = likelihood(output)\n",
    "\n",
    "#         loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "#         #loss = -mll(output, agg_data)\n",
    "#         loss.backward(retain_graph=True)\n",
    "\n",
    "#         print('Iter %d/%d - Loss hyperparam: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "#         optimizer.step()\n",
    "\n",
    "#     return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    def closure():\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "        #output_ll = likelihood(output)\n",
    "\n",
    "        #loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss = -mll(output, agg_data)\n",
    "        print('Loss gp: %.3f' % ( loss))\n",
    "        \n",
    "        return loss\n",
    "    optimizer = gpytorch.LBFGS.FullBatchLBFGS(model.parameters(), lr=.1)\n",
    "    optimizer.step(closure)\n",
    "\n",
    "\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class param_opti(nn.Module):\n",
    "    def __init__(self, sample):\n",
    "        super(param_opti, self).__init__()\n",
    "        #loc = np.random.random_sample((loc_size,2))\n",
    "        self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return (self.g_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_param_opti(x,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_iter):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    _par = param_opti(loc_sample)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        g_theta2 = _par.forward()\n",
    "\n",
    "        loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "        loss1 = -1 * loss1\n",
    "        \n",
    "        return loss1\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(_par.parameters(), lr=1., history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    g_theta2 = _par.forward()\n",
    "\n",
    "    loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "    loss1 = -1 * loss1\n",
    "    \n",
    "    print('Loss theta: %.3f' % ( loss1))\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(_par.parameters(), lr=0.007)\n",
    "\n",
    "# #     for j in range(training_iter):\n",
    "# #         optimizer.zero_grad()\n",
    "# #         g_theta2 = _par.forward()\n",
    "\n",
    "# #         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "# #         loss1 = -1 * loss1\n",
    "# #         loss1.backward(retain_graph=True)\n",
    "# #         print('Iter %d/%d - Loss theta2: %.3f' % (j + 1, training_iter, loss1.item()))\n",
    "# #         optimizer.step()\n",
    "          \n",
    "    return loss1, g_theta2, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class design_opti(nn.Module):\n",
    "    def __init__(self, _x):\n",
    "        super(design_opti, self).__init__()\n",
    "#         a = _x[0,0]\n",
    "#         b = _x[0,1]\n",
    "#         self.x_design_1 = nn.Parameter(a)\n",
    "#         self.x_design_2 = nn.Parameter(b)\n",
    "        self.x_design = nn.Parameter(_x)\n",
    "    def forward(self):\n",
    "#         x_design = torch.zeros(1,2)\n",
    "#         x_design[0,0] = self.x_design_1\n",
    "#         x_design[0,1] = self.x_design_2\n",
    "        return (self.x_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new):\n",
    "    design = design_opti(x0)\n",
    "    loc_sample0 = loc_sample\n",
    "    x_d = design.forward()\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss2, g_theta2_out,lower_bound, upper_bound = conduct_param_opti(x_d,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_param_iter)\n",
    "        loss2.backward(retain_graph=True)\n",
    "#         print(x_d)\n",
    "#         print(lower_bound)\n",
    "#         print(upper_bound)\n",
    "       \n",
    "        return loss2\n",
    "        \n",
    "        \n",
    "        \n",
    "    optimizer = torch.optim.LBFGS(design.parameters(), lr=lr_new, history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    x_d = design.forward()\n",
    "    loss2, g_theta2_out,lower_bound, upper_bound = conduct_param_opti(x_d,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_param_iter) ##takethis out\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    print(optimizer.state)\n",
    "    return x_d, g_theta2_out, lower_bound, upper_bound\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_hp = 50\n",
    "iter_design = 40 \n",
    "iter_param = 50\n",
    "\n",
    "\n",
    "f_target = Tensor(vf.tgt_vec) \n",
    "f_target = f_target.reshape(f_target.shape[0],1) \n",
    "tol_vector = 0.005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size = 10\n",
    "loc_sample = np.random.random_sample((loc_size,2))\n",
    "g_theta2_vec = (Tensor(loc_sample)).flatten()\n",
    "\n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([0.5,0.7])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x = x00 \n",
    "\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "iter = 0 \n",
    "tol = 0.009 \n",
    "while(SUCCESS == False and iter < 50):\n",
    "    print('START HYPERPARAMETERS optimization')\n",
    "    model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp)\n",
    "\n",
    "    print('END HYPERPARAMETERS optimization')\n",
    "    print(iter)\n",
    "    \n",
    "    x0_new,g_theta2,lower_bound, upper_bound = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new)\n",
    "    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.flatten()], 0)\n",
    "    print(torch.norm(upper_bound - tol_vector))\n",
    "#     if (torch.norm(upper_bound - tol_vector) <= 0.1 ):\n",
    "#         print('bkhjghf')\n",
    "#         lr_new = lr_new * 0.1\n",
    "    print(lower_bound)\n",
    "    print(upper_bound)\n",
    "    print(f_target-tol_vector)\n",
    "    print(f_target+tol_vector)\n",
    "    loc_sample = np.random.random_sample((loc_size,2))\n",
    "    x0 = x0_new #Tensor(np.random.random_sample((1,2))) #x0_new\n",
    "    vec_x = torch.cat([vec_x, x0_new])\n",
    "    g_theta2_detach = g_theta2.detach()\n",
    "    new_data = vfield_(g_theta2_detach)\n",
    "    agg_data = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "    g_theta1= torch.cat([g_theta1, g_theta2], 0)\n",
    "\n",
    "    SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    iter = iter + 1\n",
    "    print(x0_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "\n",
    "x0 = Tensor(np.array([0.1566, 0.1873])) # lr = .2, samp_size = 5\n",
    "print(vf(x0))\n",
    "x0 = x0.reshape(1,2)\n",
    "print(x0)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "pr = likelihood(model(x0))\n",
    "print(pr.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "res = res + torch.ones(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta2_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(70,2)\n",
    "print(vec_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v2)\n",
    "np.savetxt('g_theta2_005.txt', v2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt('vec_x_005.txt', vec_x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
