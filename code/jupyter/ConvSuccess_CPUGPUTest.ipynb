{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAD CPU vs. GPU Numerical Tests\n",
    "In this notebook, we test the running speed of various components of TAD on CPU vs. GPU. TAD is an example application of Gaussian Process (GP) and GPytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(\"..\")\n",
    "from LBFGS import FullBatchLBFGS\n",
    "from kernels import vvkernels as vvk, sep_vvkernels as svvk, vvk_rbfkernel as vvk_rbf\n",
    "from means import vvmeans as vvm\n",
    "from likelihood import vvlikelihood as vvll\n",
    "from mlikelihoods import MarginalLogLikelihood as exmll\n",
    "from predstrategies import GPprediction\n",
    "from utils import ObjFun, get_vertices, stopping_criteria\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=40)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=32)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=32)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=32)    # legend fontsize\n",
    "plt.rc('font', size=32)          # controls default text sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "\n",
    "We sample from $$V_1(x_1, x_2) = 3(1 - x_1)^2 e^{-x_1^2 - (x_2 +1)^2} - 10 (x_1/5 - x_1 ^3 - x_2^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- (x_1 + 2) ^2 - x_2^2} + 0.5(2x_1 + x_2)$$\n",
    "$$V_2(x_1, x_2) = 3(1 +x_2)^2 e^{-x_2^2 - (x_1 +1)^2} - 10 (-x_2/5 + x_2 ^3 + x_1^5) e^{-x_1^2 - x_2 ^2} - 3 e^{- ( 2- x_2) ^2 - x_1^2} + 0.5(2x_1 + x_2)$$\n",
    "\n",
    "where $(x_1, x_2) \\in [-3, 3]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3380, 0.3502], dtype=torch.float32)\n",
      "tensor([[ 0.8829, -0.6673],\n",
      "        [-0.8336,  0.2361],\n",
      "        [-0.8384,  0.7408],\n",
      "        [ 0.0818, -0.8991]])\n"
     ]
    }
   ],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "vf = ObjFun() # need to write a function, input + output (training points), if switching to higher dimension. Google scalable Bayesian Optimization and use what they are using\n",
    "# Need target point. \n",
    "f_target = vf.tgt_vec\n",
    "print(f_target)\n",
    "sample_size = 4\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -1.\n",
    "vf.high = 1.\n",
    "\n",
    "high_minus_low = vf.high - vf.low\n",
    "def g_theta(sample_size, D):\n",
    "#     loc_x = (2. - 1.0 )  * np.random.random_sample((sample_size,1)) + 1.0\n",
    "    \n",
    "#     loc_y = (2.  -1.0)  * np.random.random_sample((sample_size,1)) - 2.\n",
    "#     loc = np.concatenate((loc_x, loc_y), 1)\n",
    "    loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "if use_cuda:\n",
    "    train_x = train_x.cuda()\n",
    "#train_x = Tensor([[-1.5, 1.5], [-1.5, 1.3]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    randn = torch.randn(Tensor(vf(x[:,0], x[:,1])).size())\n",
    "    randn = randn.to(x.device)\n",
    "    out = vf(x[:,0], x[:,1]) + randn * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)\n",
    "\n",
    "# print(train_y)\n",
    "# train_y = (train_y - train_y.mean())/train_y.std(dim=-2, keepdim=True)\n",
    "# train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "# print(train_y)\n",
    "# print(train_y.std(dim=-2, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP model initialization\n",
    "We inialize the GP model following https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)  #vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)#vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)  # \n",
    "        base_kernels = [] #contain all the base kernels\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters oprimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels,noise_value, current_model = None, current_likelihood = None):\n",
    "    noises = torch.ones(agg_data.shape[0]) * (noise_value) #  torch.zeros(agg_data.shape[0]) # \n",
    "    noises = noises.reshape(g_theta1.shape[0], 2)\n",
    "    \n",
    "#     if (current_model is not None):\n",
    "#         likelihood = current_likelihood #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)  #\n",
    "\n",
    "#         model = current_model#.get_fantasy_model(g_theta1, agg_data) #MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#         model.set_train_data(g_theta1, agg_data,  strict=False)\n",
    "#     else:\n",
    "#         likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2)#vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #\n",
    "#         model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "        \n",
    "    cov_noise1 =  noise_value * torch.eye(agg_data.shape[0])\n",
    "    likelihood =  vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2) #\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "    model.double()\n",
    "    likelihood.double()\n",
    "\n",
    "    \"\"\"Put related things on GPU\"\"\"\n",
    "    if use_cuda:\n",
    "        print(\"Using CUDA\")\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "#         g_theta1 = g_theta1.cuda()\n",
    "#         agg_data = agg_data.cuda()\n",
    "        cov_noise1 = cov_noise1.cuda()\n",
    "        \n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    \n",
    "    \"\"\"end for GPU\"\"\"\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #, weight_decay=0.001)  # Includes GaussianLikelihood parameters\n",
    "    mll = exmll(likelihood, model)\n",
    "    # Is this a likelihood?\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, chi_square = mll(agg_data,g_theta1, model, likelihood, cov_noise1)\n",
    "        loss = -1. * loss\n",
    "#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \n",
    "        #print('loss is %.3f' %loss)\n",
    "#         df = agg_data.shape[0]\n",
    "#         chi_square = chi_square.clone().detach()\n",
    "        \n",
    "#         p_val = 1. - stats.chi2.cdf(chi_square, df)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "       # print(p_val)\n",
    "#         if (p_val > 0.99999):\n",
    "#             return model, likelihood\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    print('loss is %.3f' %loss)\n",
    "#     for params in model.named_parameters():\n",
    "#         print(params)\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design parameters and sampling point optimization (where to explore?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': g_theta2, 'lr': 0.01},{'params': x_d, 'lr': 0.01}])\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    cov_noise1 =  noise_value * torch.eye(agg_data.shape[0])\n",
    "    cov_noise2 =  noise_value * torch.eye(2 * g_theta2.shape[0])\n",
    "    \n",
    "    \"\"\"Put related things on GPU for conduct_design_opti\"\"\"\n",
    "    if use_cuda:\n",
    "        print(\"Using CUDA for conduct_design_opti()\")\n",
    "        \n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        # agg_data = agg_data.cuda()\n",
    "        \n",
    "        cov_noise1 = cov_noise1.cuda()\n",
    "        cov_noise2 = cov_noise2.cuda()\n",
    "#         g_theta1 = g_theta1.cuda()\n",
    "#         g_theta2 = g_theta2.cuda()\n",
    "#         agg_data = agg_data.cuda()\n",
    "        x_d = x_d.cuda()\n",
    "#         f_target = f_target.cuda()\n",
    "        \n",
    "    else:\n",
    "        print(\"Using CPU for conduct_design_opti()\")\n",
    "    \n",
    "    \"\"\"end for GPU\"\"\"\n",
    "    \n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        loss2, pf1, Qf1, Qf12, data_fit, Q21 = likelihood.get_ell(agg_data,f_target,x_d, g_theta1, model, likelihood, g_theta2, cov_noise1, cov_noise2)\n",
    "\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step(loss2)\n",
    "        #scheduler.step()\n",
    "\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    #print(x_d)\n",
    "    return x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting the TAD experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_size = 2\n",
    "#loc_sample0 = Tensor((2. - 1.5)  * np.random.random_sample((loc_size,2)) + 1.5)\n",
    "x0 = Tensor(np.array([0. , 0.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "x0 = x0.reshape(1,2)\n",
    "\n",
    "dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(loc_size) )\n",
    "                    #loc_size = 4\n",
    "loc_sample = dis_2sample.sample((loc_size + 1,))\n",
    "\n",
    "loc_sample0 = loc_sample.reshape(loc_size + 1, 2)\n",
    "#loc_sample0[-1] = train_x[-1] + 0.01\n",
    "\n",
    "# This will make TAD fail to find a solution\n",
    "f_target = Tensor([[-1., -1.]]).reshape(2,1)\n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "#####\n",
    "\n",
    "# Failing criterion\n",
    "thresh_EI = 0.0007 # This also\n",
    "count_EI = 0\n",
    "max_count_EI = 5 # This affects the number of iterations required to fail\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Saving x0, loc_sample0, x_train, and y_train for running (basically) the same thing on GPU again later\"\"\"\n",
    "x0_copy = x0\n",
    "loc_sample0_copy = loc_sample0\n",
    "x_train_copy = x_train\n",
    "y_train_copy = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_hyper_opti = []\n",
    "cpu_conduct_design = []\n",
    "cpu_total = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0178,  0.0281],\n",
      "        [ 0.0041, -0.0184],\n",
      "        [ 0.0507,  0.1586]])\n",
      "0\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /lus/theta-fs0/software/thetagpu/conda/2022-07-01/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.862\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 3023.305\n",
      "expected info is tensor([[0.0578]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.7372, 0.7429]])\n",
      "new data istensor([[0.3137, 0.3327]])\n",
      "g_theta2 istensor([[-0.5392, -0.5352],\n",
      "        [-0.5348, -0.5592],\n",
      "        [-0.5512, -0.4628]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 1935.722855923002953\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "0\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 2970.047\n",
      "expected info is tensor([[0.0591]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.7534, 0.7563]])\n",
      "new data istensor([[0.3209, 0.3533]])\n",
      "g_theta2 istensor([[-0.4614, -0.4452],\n",
      "        [-0.7337, -0.7368],\n",
      "        [-0.5748, -0.5762]])\n",
      "p21val is 0.000000000000000\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 2023.576785158172470\n",
      "patience is 2.000\n",
      "adding complexity to model\n",
      "num base is 3\n",
      "acquiring 2, new size is 7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "1\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.411\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 618.908\n",
      "expected info is tensor([[0.0021]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[1.4498, 1.4823]])\n",
      "new data istensor([[0.4336, 0.0133]])\n",
      "g_theta2 istensor([[-0.4317, -0.4258],\n",
      "        [-0.4289, -0.4636],\n",
      "        [-0.4341, -0.3123]])\n",
      "p21val is 0.524320752673150\n",
      "pf12val is 0.000000000000000\n",
      "chi_f12 is 657.646569676728063\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6676, -0.0099],\n",
      "        [ 0.2088,  0.2084],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.171\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 427.458\n",
      "expected info is tensor([[0.3296]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[ 0.2522, -0.2004]])\n",
      "new data istensor([[0.0828, 0.0805]])\n",
      "g_theta2 istensor([[1.0178, 0.3972],\n",
      "        [0.3602, 0.4540],\n",
      "        [0.3490, 0.4290]])\n",
      "p21val is 0.024708875209806\n",
      "pf12val is 0.224743304997826\n",
      "chi_f12 is 2.985592789475664\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.4134, -0.9803],\n",
      "        [ 0.6172,  0.8624],\n",
      "        [ 0.2533, -0.2012]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "3\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.005\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 395.489\n",
      "expected info is tensor([[0.1092]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[ 0.5162, -0.4670]])\n",
      "new data istensor([[0.0986, 0.0098]])\n",
      "g_theta2 istensor([[ 0.5266, -1.0840],\n",
      "        [ 0.7358,  1.0350],\n",
      "        [ 0.2168, -0.1569]])\n",
      "p21val is 0.104902322347540\n",
      "pf12val is 0.964025254092699\n",
      "chi_f12 is 0.073275575046272\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3829,  0.2971],\n",
      "        [ 0.8117, -0.3453],\n",
      "        [ 0.5136, -0.4679]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "4\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -1.999\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 2094.150\n",
      "expected info is tensor([[0.2916]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[ 0.3188, -0.6819]])\n",
      "new data istensor([[0.0013, 0.1003]])\n",
      "g_theta2 istensor([[ 0.5349,  0.4867],\n",
      "        [ 0.7540, -0.4634],\n",
      "        [ 0.6472, -0.3265]])\n",
      "p21val is 0.231489947676324\n",
      "pf12val is 0.961672819685399\n",
      "chi_f12 is 0.078161980884278\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3984, -0.1110],\n",
      "        [ 0.6554, -0.9474],\n",
      "        [ 0.3177, -0.6817]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "5\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.153\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 2224.222\n",
      "expected info is tensor([[0.0624]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.1877, -0.8629]])\n",
      "new data istensor([[0.1051, 0.2248]])\n",
      "g_theta2 istensor([[ 0.3151, -0.1679],\n",
      "        [ 0.7117, -1.0190],\n",
      "        [ 0.5559, -0.8882]])\n",
      "p21val is 0.720937005162225\n",
      "pf12val is 0.572777949101756\n",
      "chi_f12 is 1.114514321513225\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5139, -0.4121],\n",
      "        [ 0.2806,  0.8231],\n",
      "        [-0.1877, -0.8641]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "6\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.254\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 425.932\n",
      "expected info is tensor([[0.0524]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.5479, -1.2697]])\n",
      "new data istensor([[-0.1583,  0.3140]])\n",
      "g_theta2 istensor([[-0.3960, -0.6047],\n",
      "        [ 0.2013,  0.7737],\n",
      "        [-0.0053, -0.6841]])\n",
      "p21val is 0.049857959733621\n",
      "pf12val is 0.050986533378706\n",
      "chi_f12 is 5.952387465046897\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[0.1822, 0.4975],\n",
      "        [0.4633, 0.7897],\n",
      "        [0.0000, 0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.271\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 467.239\n",
      "expected info is tensor([[0.0108]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.3052,  0.3728]])\n",
      "new data istensor([[0.0919, 0.1103]])\n",
      "g_theta2 istensor([[ 0.3431,  0.6487],\n",
      "        [ 0.3465,  0.6398],\n",
      "        [ 0.2334, -0.2444]])\n",
      "p21val is 0.001517484243275\n",
      "pf12val is 0.052960251640513\n",
      "chi_f12 is 5.876427231688806\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "7\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 501.442\n",
      "expected info is tensor([[0.0102]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.2814,  0.3354]])\n",
      "new data istensor([[0.0759, 0.1277]])\n",
      "g_theta2 istensor([[ 0.3210,  0.6884],\n",
      "        [ 0.2723, -0.1755],\n",
      "        [ 0.2279, -0.2374]])\n",
      "p21val is 0.764713879062223\n",
      "pf12val is 0.067056034241830\n",
      "chi_f12 is 5.404453354308542\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2905,  0.5812],\n",
      "        [-0.6689,  0.5991],\n",
      "        [-0.2808,  0.3359]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "8\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.395\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 3192.851\n",
      "expected info is tensor([[0.0862]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0913,  0.1055]])\n",
      "new data istensor([[0.1110, 0.1047]])\n",
      "g_theta2 istensor([[ 0.3508,  0.6194],\n",
      "        [-0.8995,  0.6383],\n",
      "        [-0.4927,  0.5490]])\n",
      "p21val is 0.013913675446503\n",
      "pf12val is 0.262145858770331\n",
      "chi_f12 is 2.677708434528599\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.2002, -0.5040],\n",
      "        [ 0.5282,  0.3593],\n",
      "        [-0.0903,  0.1047]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "9\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.367\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 521.505\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.9962, -0.2105]])\n",
      "new data istensor([[-0.2813,  0.3624]])\n",
      "g_theta2 istensor([[-0.4578, -0.4808],\n",
      "        [ 0.3834,  0.4709],\n",
      "        [ 0.2009, -0.1753]])\n",
      "p21val is 0.022398127297805\n",
      "pf12val is 0.034602282210926\n",
      "chi_f12 is 6.727671278480739\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9973, -0.5776],\n",
      "        [ 0.5540,  0.0265],\n",
      "        [-0.9965, -0.2106]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "10\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.373\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss design: 221.024\n",
      "expected info is tensor([[0.0645]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-1.3524,  0.1059]])\n",
      "new data istensor([[-0.6324,  0.8019]])\n",
      "g_theta2 istensor([[-0.8413, -0.7079],\n",
      "        [ 0.9180, -0.0766],\n",
      "        [-0.8329, -0.3997]])\n",
      "p21val is 0.663911938315921\n",
      "pf12val is 0.000808239460030\n",
      "chi_f12 is 14.241304363853899\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4061,  0.5302],\n",
      "        [ 0.3887,  0.2851],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "11\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.361\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 4286.946\n",
      "expected info is tensor([[0.0025]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.5927,  0.0915]])\n",
      "new data istensor([[0.0090, 0.0569]])\n",
      "g_theta2 istensor([[-0.5170,  0.6103],\n",
      "        [ 0.4182,  0.4373],\n",
      "        [ 0.2426, -0.2563]])\n",
      "p21val is 0.043852063410861\n",
      "pf12val is 0.266463078412745\n",
      "chi_f12 is 2.645039175030870\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5338,  0.0325],\n",
      "        [-0.8979, -0.3539],\n",
      "        [-0.5918,  0.0914]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "12\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.405\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 340.720\n",
      "expected info is tensor([[0.0022]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-1.2755,  0.6627]])\n",
      "new data istensor([[-0.4520,  0.4575]])\n",
      "g_theta2 istensor([[-0.3268, -0.1774],\n",
      "        [-0.7836, -0.1536],\n",
      "        [-0.3506, -0.1532]])\n",
      "p21val is 0.972691478099314\n",
      "pf12val is 0.926694283864005\n",
      "chi_f12 is 0.152263117357432\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.8819,  0.2739],\n",
      "        [-0.6135,  0.5021],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "13\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.493\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 2357.533\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.1524,  0.6692]])\n",
      "new data istensor([[ 0.1856, -0.0032]])\n",
      "g_theta2 istensor([[-0.9750,  0.2057],\n",
      "        [-1.0467,  0.3175],\n",
      "        [ 0.2593, -0.2557]])\n",
      "p21val is 0.421757546620849\n",
      "pf12val is 0.222018112484005\n",
      "chi_f12 is 3.009992625344343\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.1992,  0.9164],\n",
      "        [ 0.1665,  0.8220],\n",
      "        [-0.1515,  0.6700]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "14\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.507\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 257.944\n",
      "expected info is tensor([[0.0323]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.4106,  1.0514]])\n",
      "new data istensor([[ 0.4132, -0.3368]])\n",
      "g_theta2 istensor([[0.4364, 0.7960],\n",
      "        [0.3582, 0.7493],\n",
      "        [0.0346, 0.4952]])\n",
      "p21val is 0.835315747022923\n",
      "pf12val is 0.865631096711228\n",
      "chi_f12 is 0.288592892998264\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.1182, -0.9485],\n",
      "        [ 0.3232, -0.1615],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "15\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.569\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 7977.914\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0132,  0.2153]])\n",
      "new data istensor([[0.0678, 0.2081]])\n",
      "g_theta2 istensor([[ 0.0207, -0.8306],\n",
      "        [ 0.3499, -0.1857],\n",
      "        [ 0.2599, -0.2532]])\n",
      "p21val is 0.192336746045946\n",
      "pf12val is 0.421900354465313\n",
      "chi_f12 is 1.725972239375819\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.6661,  0.1886],\n",
      "        [-0.2461,  0.7916],\n",
      "        [-0.0155,  0.2157]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "16\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.587\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 357.061\n",
      "expected info is tensor([[0.1673]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.9919,  1.1083]])\n",
      "new data istensor([[ 0.0529, -0.0828]])\n",
      "g_theta2 istensor([[-0.6307,  0.6510],\n",
      "        [-0.5269,  1.6830],\n",
      "        [-0.3313, -0.1175]])\n",
      "p21val is 0.948115292097807\n",
      "pf12val is 0.556792821835643\n",
      "chi_f12 is 1.171124123726396\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.6207,  0.5379],\n",
      "        [ 0.4726,  0.5608],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "17\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.604\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 8962.169\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.6298,  0.2266]])\n",
      "new data istensor([[-0.0518,  0.1151]])\n",
      "g_theta2 istensor([[-0.9246,  0.6568],\n",
      "        [ 0.3804,  0.3954],\n",
      "        [ 0.2866, -0.2702]])\n",
      "p21val is 0.480459935215902\n",
      "pf12val is 0.559196451101536\n",
      "chi_f12 is 1.162508868935250\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8411, -0.8951],\n",
      "        [-0.7141,  0.1311],\n",
      "        [-0.6295,  0.2273]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.667\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 9440.521\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.5450,  0.3028]])\n",
      "new data istensor([[-0.0529,  0.1001]])\n",
      "g_theta2 istensor([[ 0.7840, -0.8474],\n",
      "        [-0.8983, -0.0507],\n",
      "        [-0.9011, -0.0404]])\n",
      "p21val is 0.000234911704736\n",
      "pf12val is 0.369981102351241\n",
      "chi_f12 is 1.988606698749201\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "18\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 9445.029\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.5418,  0.2966]])\n",
      "new data istensor([[-0.0228,  0.0848]])\n",
      "g_theta2 istensor([[-2.3229,  1.6485],\n",
      "        [-2.5140,  1.1560],\n",
      "        [-0.9060, -0.0407]])\n",
      "p21val is 0.537959659310890\n",
      "pf12val is 0.997570198190675\n",
      "chi_f12 is 0.004865517136541\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8527,  0.5112],\n",
      "        [ 0.1779, -0.3434],\n",
      "        [-0.5405,  0.2951]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "19\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.634\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 288.879\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-1.4464, -0.4417]])\n",
      "new data istensor([[-0.5394,  0.7093]])\n",
      "g_theta2 istensor([[ 1.0237,  0.5015],\n",
      "        [ 0.3041, -0.4961],\n",
      "        [-0.4680,  0.6116]])\n",
      "p21val is 0.811723892889423\n",
      "pf12val is 0.941790006002368\n",
      "chi_f12 is 0.119945905642581\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[0.5598, 0.5024],\n",
      "        [0.8690, 0.2590],\n",
      "        [0.0000, 0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "20\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.673\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 12430.406\n",
      "expected info is tensor([[0.0006]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0358,  0.2936]])\n",
      "new data istensor([[0.0619, 0.1724]])\n",
      "g_theta2 istensor([[ 0.3900,  0.5846],\n",
      "        [ 1.0321,  0.3668],\n",
      "        [ 0.2715, -0.2716]])\n",
      "p21val is 0.103857950134395\n",
      "pf12val is 0.809366756089183\n",
      "chi_f12 is 0.423006239345972\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5045,  0.6092],\n",
      "        [-0.0972,  0.3624],\n",
      "        [-0.0349,  0.2935]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "21\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.695\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 5240.910\n",
      "expected info is tensor([[0.0021]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.7305, 0.0869]])\n",
      "new data istensor([[0.2391, 0.2412]])\n",
      "g_theta2 istensor([[ 0.4307,  0.5902],\n",
      "        [-0.2975,  0.5593],\n",
      "        [-0.2951,  0.5521]])\n",
      "p21val is 0.759548839392120\n",
      "pf12val is 0.547871407947943\n",
      "chi_f12 is 1.203429353199384\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.9634, -0.1837],\n",
      "        [ 0.6834, -0.3717],\n",
      "        [ 0.7301,  0.0862]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "22\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.743\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 7751.396\n",
      "expected info is tensor([[0.0100]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.6364, 0.0478]])\n",
      "new data istensor([[0.1626, 0.2280]])\n",
      "g_theta2 istensor([[ 0.9654, -0.2020],\n",
      "        [ 0.6420, -0.5296],\n",
      "        [ 0.9820, -0.1575]])\n",
      "p21val is 0.014851955868319\n",
      "pf12val is 0.726293527136122\n",
      "chi_f12 is 0.639602077035009\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3158,  0.6731],\n",
      "        [ 0.4844, -0.0213],\n",
      "        [ 0.6366,  0.0482]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "23\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.713\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 1605.591\n",
      "expected info is tensor([[0.0009]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[1.0576, 0.9205]])\n",
      "new data istensor([[0.4062, 0.1798]])\n",
      "g_theta2 istensor([[ 0.3512,  0.6443],\n",
      "        [ 0.3003, -0.2253],\n",
      "        [ 0.3642, -0.2303]])\n",
      "p21val is 0.093009264382761\n",
      "pf12val is 0.261502428536052\n",
      "chi_f12 is 2.682623417151068\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5580,  0.7797],\n",
      "        [-0.8193, -0.8198],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "24\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.748\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 7301.472\n",
      "expected info is tensor([[0.0025]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0280, -0.5035]])\n",
      "new data istensor([[0.2204, 0.0279]])\n",
      "g_theta2 istensor([[ 0.2938,  0.5171],\n",
      "        [-1.6280, -1.4505],\n",
      "        [ 0.3464, -0.2741]])\n",
      "p21val is 0.223178033281197\n",
      "pf12val is 0.282682853252565\n",
      "chi_f12 is 2.526859339265302\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4353, -0.4500],\n",
      "        [ 0.1342, -0.5905],\n",
      "        [-0.0274, -0.5036]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "25\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.740\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 11454.609\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0350, -0.4709]])\n",
      "new data istensor([[0.2078, 0.0716]])\n",
      "g_theta2 istensor([[-0.4921, -0.3260],\n",
      "        [ 0.3200, -0.8042],\n",
      "        [ 0.2771, -0.8294]])\n",
      "p21val is 0.289169077550655\n",
      "pf12val is 0.263233612443523\n",
      "chi_f12 is 2.669426761665024\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.0431, -0.4319],\n",
      "        [ 0.2051, -0.5425],\n",
      "        [-0.0353, -0.4705]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "26\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.740\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 14085.795\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.1422,  0.0669]])\n",
      "new data istensor([[0.1401, 0.1064]])\n",
      "g_theta2 istensor([[ 0.3512, -0.1760],\n",
      "        [ 0.3542, -0.7829],\n",
      "        [ 0.3534, -0.1827]])\n",
      "p21val is 0.024538897614747\n",
      "pf12val is 0.750912047008213\n",
      "chi_f12 is 0.572933497158972\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5623,  0.4366],\n",
      "        [ 0.7338, -0.8150],\n",
      "        [-0.1427,  0.0668]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "27\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.739\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 10334.317\n",
      "expected info is tensor([[0.0018]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.9620,  0.4457]])\n",
      "new data istensor([[-0.3648,  0.2901]])\n",
      "g_theta2 istensor([[-0.7464,  0.8484],\n",
      "        [ 0.7402, -0.8189],\n",
      "        [ 0.1779,  0.3955]])\n",
      "p21val is 0.800084931508397\n",
      "pf12val is 0.887483387040508\n",
      "chi_f12 is 0.238730953270725\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5717,  0.8988],\n",
      "        [-0.3461, -0.4454],\n",
      "        [-0.9641,  0.4458]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "28\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.760\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 299.987\n",
      "expected info is tensor([[0.0061]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-1.4564,  1.0512]])\n",
      "new data istensor([[-0.2620,  0.2431]])\n",
      "g_theta2 istensor([[-0.9321,  0.7069],\n",
      "        [-0.4868, -0.3374],\n",
      "        [-0.7370,  0.2741]])\n",
      "p21val is 0.844098317141559\n",
      "pf12val is 0.885218812151462\n",
      "chi_f12 is 0.243840838330847\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9841, -0.7324],\n",
      "        [ 0.8185, -0.3279],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "29\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.806\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 14225.593\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0341, -0.4667]])\n",
      "new data istensor([[0.2078, 0.0373]])\n",
      "g_theta2 istensor([[-1.6432, -1.1689],\n",
      "        [ 0.8842, -0.3085],\n",
      "        [ 0.3512, -0.2756]])\n",
      "p21val is 0.893079019902180\n",
      "pf12val is 0.949469860312870\n",
      "chi_f12 is 0.103702983853480\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.7735, -0.9667],\n",
      "        [-0.3112, -0.6165],\n",
      "        [-0.0331, -0.4671]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "30\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.791\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 20541.995\n",
      "expected info is tensor([[0.0033]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0015, -0.5439]])\n",
      "new data istensor([[0.2298, 0.0807]])\n",
      "g_theta2 istensor([[-0.6888, -0.8862],\n",
      "        [-0.5407, -0.8229],\n",
      "        [ 0.3089, -0.1559]])\n",
      "p21val is 0.115353280801422\n",
      "pf12val is 0.074231452525724\n",
      "chi_f12 is 5.201134660274605\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.2429, -0.6654],\n",
      "        [-0.6189, -0.8620],\n",
      "        [-0.0026, -0.5450]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "31\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.813\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 17741.766\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.2384,  0.0662]])\n",
      "new data istensor([[0.0989, 0.1034]])\n",
      "g_theta2 istensor([[-0.3308, -0.8339],\n",
      "        [-0.6337, -0.8144],\n",
      "        [ 0.2837, -0.8259]])\n",
      "p21val is 0.835668824845741\n",
      "pf12val is 0.047133751605151\n",
      "chi_f12 is 6.109531880003781\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4928, -0.5503],\n",
      "        [-0.7779,  0.2014],\n",
      "        [-0.2396,  0.0672]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "32\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.820\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 13111.103\n",
      "expected info is tensor([[0.0026]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.8565, -0.1378]])\n",
      "new data istensor([[-0.1947,  0.2158]])\n",
      "g_theta2 istensor([[-0.3438, -0.3203],\n",
      "        [-1.0025,  0.3760],\n",
      "        [-0.1547, -0.1534]])\n",
      "p21val is 0.536556809948037\n",
      "pf12val is 0.041918897874851\n",
      "chi_f12 is 6.344037060936665\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.7850, -0.1049],\n",
      "        [-0.7318, -0.3746],\n",
      "        [-0.8555, -0.1373]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "33\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.818\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 892.396\n",
      "expected info is tensor([[0.0092]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-1.2263, -0.8178]])\n",
      "new data istensor([[-0.1699,  0.4132]])\n",
      "g_theta2 istensor([[-0.6177,  0.0738],\n",
      "        [-0.5330, -0.5182],\n",
      "        [-0.6598,  0.0696]])\n",
      "p21val is 0.860325403157220\n",
      "pf12val is 0.512285916357561\n",
      "chi_f12 is 1.337744758841986\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5511,  0.6756],\n",
      "        [-0.5780, -0.4692],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "34\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.856\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 11863.202\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.7841, 0.1901]])\n",
      "new data istensor([[0.3029, 0.2297]])\n",
      "g_theta2 istensor([[ 0.3657,  0.9045],\n",
      "        [-0.5749, -0.5443],\n",
      "        [-0.2629, -0.2290]])\n",
      "p21val is 0.009129248838208\n",
      "pf12val is 0.134195887022832\n",
      "chi_f12 is 4.016909406060452\n",
      "patience is 1.000\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "34\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 11895.595\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.7860, 0.1846]])\n",
      "new data istensor([[0.2966, 0.2899]])\n",
      "g_theta2 istensor([[ 0.5495,  0.9309],\n",
      "        [-0.5001, -0.2522],\n",
      "        [-0.2627, -0.2264]])\n",
      "p21val is 0.325978224174315\n",
      "pf12val is 0.427528375365700\n",
      "chi_f12 is 1.699469235367217\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[0.5432, 0.7914],\n",
      "        [0.2058, 0.3871],\n",
      "        [0.7866, 0.1837]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "35\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.853\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 6740.647\n",
      "expected info is tensor([[0.0031]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.8240, 0.7425]])\n",
      "new data istensor([[0.3189, 0.3351]])\n",
      "g_theta2 istensor([[ 0.4484,  0.8815],\n",
      "        [ 0.4139,  0.3491],\n",
      "        [ 0.8821, -0.0852]])\n",
      "p21val is 0.522329402266083\n",
      "pf12val is 0.805503064681757\n",
      "chi_f12 is 0.432576543354853\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5104, -0.3962],\n",
      "        [ 0.2417,  0.6160],\n",
      "        [ 0.8256,  0.7429]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "36\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.888\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 10703.485\n",
      "expected info is tensor([[0.0077]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.8100, 0.7358]])\n",
      "new data istensor([[0.3042, 0.3484]])\n",
      "g_theta2 istensor([[ 0.3953, -0.2568],\n",
      "        [ 0.3303,  0.5540],\n",
      "        [ 0.5607,  1.0136]])\n",
      "p21val is 0.548636005416208\n",
      "pf12val is 0.671090139667309\n",
      "chi_f12 is 0.797703629435708\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.6348, -0.3648],\n",
      "        [ 0.8960,  0.7259],\n",
      "        [ 0.8096,  0.7361]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "37\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.879\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 578.325\n",
      "expected info is tensor([[0.0085]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[1.0038, 1.4037]])\n",
      "new data istensor([[0.5792, 0.0133]])\n",
      "g_theta2 istensor([[-0.7099, -0.4395],\n",
      "        [ 1.0918,  0.5178],\n",
      "        [ 1.0839,  0.4912]])\n",
      "p21val is 0.649269059016489\n",
      "pf12val is 0.866830687658804\n",
      "chi_f12 is 0.285823213112425\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.1528, -0.1010],\n",
      "        [ 0.8193,  0.0376],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "38\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.915\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 18332.234\n",
      "expected info is tensor([[0.0010]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.2188, 0.1623]])\n",
      "new data istensor([[0.0756, 0.2233]])\n",
      "g_theta2 istensor([[ 0.3262, -0.2983],\n",
      "        [ 1.0118,  0.0316],\n",
      "        [-0.2235, -0.2243]])\n",
      "p21val is 0.266482800212326\n",
      "pf12val is 0.049382613529143\n",
      "chi_f12 is 6.016313739179100\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5604, -0.8702],\n",
      "        [ 0.0990,  0.0664],\n",
      "        [ 0.2208,  0.1619]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "39\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.902\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 15511.716\n",
      "expected info is tensor([[0.0010]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.7849, 0.6606]])\n",
      "new data istensor([[0.3410, 0.3805]])\n",
      "g_theta2 istensor([[-0.5564, -0.8726],\n",
      "        [-0.1215, -0.1223],\n",
      "        [-0.0674, -0.1110]])\n",
      "p21val is 0.068664005933833\n",
      "pf12val is 0.035171922947115\n",
      "chi_f12 is 6.695014316425687\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.8803, -0.0226],\n",
      "        [-0.6730,  0.2488],\n",
      "        [ 0.7841,  0.6600]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "40\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.896\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 17586.531\n",
      "expected info is tensor([[3.1548e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.7337, 0.1927]])\n",
      "new data istensor([[0.2711, 0.3314]])\n",
      "g_theta2 istensor([[-1.0570,  0.1904],\n",
      "        [-0.7338,  0.2444],\n",
      "        [ 0.5057,  0.9473]])\n",
      "p21val is 0.018533527710198\n",
      "pf12val is 0.266502098772970\n",
      "chi_f12 is 2.644746320173072\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.6250,  0.2453],\n",
      "        [ 0.6966,  0.3934],\n",
      "        [ 0.7342,  0.1932]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "41\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.896\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 410.810\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[ 1.1892, -0.7260]])\n",
      "new data istensor([[ 0.2903, -0.3178]])\n",
      "g_theta2 istensor([[-0.5454,  0.2717],\n",
      "        [ 0.4963,  0.5928],\n",
      "        [ 0.4517,  0.5169]])\n",
      "p21val is 0.036005158930460\n",
      "pf12val is 0.549922217928375\n",
      "chi_f12 is 1.195956865410113\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.4112, -0.1383],\n",
      "        [-0.7396, -0.8509],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "42\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.895\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 19431.947\n",
      "expected info is tensor([[0.0010]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.4266, 0.1081]])\n",
      "new data istensor([[0.0656, 0.3179]])\n",
      "g_theta2 istensor([[-0.1653,  0.0453],\n",
      "        [-0.5694, -0.7664],\n",
      "        [-0.2417, -0.2095]])\n",
      "p21val is 0.953653545575591\n",
      "pf12val is 0.421786292086348\n",
      "chi_f12 is 1.726513020150888\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8334,  0.6107],\n",
      "        [-0.1001,  0.0078],\n",
      "        [ 0.4272,  0.1081]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "43\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.898\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 524.157\n",
      "expected info is tensor([[0.0011]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[ 1.1171, -1.0204]])\n",
      "new data istensor([[ 0.0728, -0.1136]])\n",
      "g_theta2 istensor([[ 0.7136,  0.5340],\n",
      "        [-0.1654,  0.0806],\n",
      "        [ 0.1817, -0.1625]])\n",
      "p21val is 0.732446204307186\n",
      "pf12val is 0.689020702815425\n",
      "chi_f12 is 0.744967921584572\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.8726,  0.3064],\n",
      "        [ 0.1008, -0.3892],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "44\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -2.921\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 25793.663\n",
      "expected info is tensor([[0.0009]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.1788, 0.2035]])\n",
      "new data istensor([[0.0394, 0.2191]])\n",
      "g_theta2 istensor([[-0.7754,  0.1906],\n",
      "        [ 0.0219, -0.2437],\n",
      "        [-0.2171, -0.2151]])\n",
      "p21val is 0.534939647700224\n",
      "pf12val is 0.254959594676980\n",
      "chi_f12 is 2.733300397251462\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6082,  0.8462],\n",
      "        [-0.4574,  0.0308],\n",
      "        [ 0.1789,  0.2027]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "45\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.937\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 9032.096\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[ 0.9103, -0.5959]])\n",
      "new data istensor([[ 0.2638, -0.1455]])\n",
      "g_theta2 istensor([[ 0.6953,  0.9666],\n",
      "        [-0.1457,  0.0082],\n",
      "        [-0.1205,  0.4805]])\n",
      "p21val is 0.601016276098719\n",
      "pf12val is 0.267168060855746\n",
      "chi_f12 is 2.639754754379509\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.7011, -0.0840],\n",
      "        [ 0.9498,  0.0501],\n",
      "        [ 0.9090, -0.5961]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "46\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.927\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 262.671\n",
      "expected info is tensor([[0.0005]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[ 1.4930, -0.7409]])\n",
      "new data istensor([[ 0.3339, -0.4793]])\n",
      "g_theta2 istensor([[ 0.9329, -0.2730],\n",
      "        [ 0.9155, -0.1332],\n",
      "        [ 0.6895, -0.3712]])\n",
      "p21val is 0.641757190613300\n",
      "pf12val is 0.991149289446447\n",
      "chi_f12 is 0.017780221488550\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.5541,  0.1661],\n",
      "        [ 0.7004, -0.0056],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "47\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.938\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 32521.379\n",
      "expected info is tensor([[0.0012]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.1887, 0.2064]])\n",
      "new data istensor([[0.0715, 0.2687]])\n",
      "g_theta2 istensor([[ 0.7239,  0.0144],\n",
      "        [ 0.7611,  0.0282],\n",
      "        [-0.2212, -0.2181]])\n",
      "p21val is 0.967765928674453\n",
      "pf12val is 0.253885467140064\n",
      "chi_f12 is 2.741744060857084\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.9037,  0.5706],\n",
      "        [ 0.5192,  0.0829],\n",
      "        [ 0.1885,  0.2056]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "48\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.953\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 1735.265\n",
      "expected info is tensor([[0.0017]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0157,  1.0693]])\n",
      "new data istensor([[ 0.4991, -0.3271]])\n",
      "g_theta2 istensor([[-0.8419,  0.5265],\n",
      "        [ 0.6604, -0.0929],\n",
      "        [ 0.4768,  0.4867]])\n",
      "p21val is 0.197991698345217\n",
      "pf12val is 0.723569924740226\n",
      "chi_f12 is 0.647116179404046\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5467,  0.9998],\n",
      "        [-0.2681, -0.1929],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "49\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.958\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 36038.800\n",
      "expected info is tensor([[0.0018]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.1790, 0.2024]])\n",
      "new data istensor([[0.0734, 0.2733]])\n",
      "g_theta2 istensor([[-0.5177,  0.9531],\n",
      "        [-0.3561, -0.2726],\n",
      "        [-0.2229, -0.2201]])\n",
      "p21val is 0.979364213393171\n",
      "pf12val is 0.206912504615534\n",
      "chi_f12 is 3.150918516178314\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.0968, -0.1102],\n",
      "        [ 0.0196, -0.2626],\n",
      "        [ 0.1787,  0.2034]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "50\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.969\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 26885.813\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.8596, 0.4841]])\n",
      "new data istensor([[0.3456, 0.3470]])\n",
      "g_theta2 istensor([[-0.2660, -0.2907],\n",
      "        [ 0.1738, -0.1776],\n",
      "        [-0.1933,  0.4820]])\n",
      "p21val is 0.129310815536121\n",
      "pf12val is 0.802375096826348\n",
      "chi_f12 is 0.440358157351039\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8994, -0.1646],\n",
      "        [ 0.2051, -0.6680],\n",
      "        [ 0.8589,  0.4844]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "51\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.939\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 444.512\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[1.3882, 1.0155]])\n",
      "new data istensor([[0.4233, 0.0911]])\n",
      "g_theta2 istensor([[ 0.7880,  0.0143],\n",
      "        [ 0.1340, -0.7666],\n",
      "        [ 1.0681,  0.3043]])\n",
      "p21val is 0.989107150870586\n",
      "pf12val is 0.161697623046637\n",
      "chi_f12 is 3.644054424561086\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.4372, -0.5739],\n",
      "        [ 0.6280,  0.4142],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "52\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -2.982\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 36966.133\n",
      "expected info is tensor([[0.0011]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.2260, 0.1836]])\n",
      "new data istensor([[0.0389, 0.2311]])\n",
      "g_theta2 istensor([[ 0.6246, -0.7588],\n",
      "        [ 0.7049,  0.3807],\n",
      "        [-0.2177, -0.2161]])\n",
      "p21val is 0.997547598007324\n",
      "pf12val is 0.275317790379080\n",
      "chi_f12 is 2.579658494260818\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2905,  0.3732],\n",
      "        [ 0.8647, -0.1753],\n",
      "        [ 0.2263,  0.1819]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "53\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.001\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 34227.451\n",
      "expected info is tensor([[0.0008]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.5286, 0.0605]])\n",
      "new data istensor([[0.0929, 0.2738]])\n",
      "g_theta2 istensor([[ 0.4621,  0.5664],\n",
      "        [ 1.4511, -0.6400],\n",
      "        [ 0.0696,  0.3255]])\n",
      "p21val is 0.704185534991876\n",
      "pf12val is 0.570008423782589\n",
      "chi_f12 is 1.124208279393596\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.8507,  0.8612],\n",
      "        [ 0.7335, -0.0058],\n",
      "        [ 0.5276,  0.0604]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "54\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.017\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 32914.709\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.4838, 0.0726]])\n",
      "new data istensor([[0.1100, 0.3075]])\n",
      "g_theta2 istensor([[-1.0671,  0.5157],\n",
      "        [ 0.8709, -0.1498],\n",
      "        [ 0.8348, -0.2402]])\n",
      "p21val is 0.111649352352086\n",
      "pf12val is 0.319770569947784\n",
      "chi_f12 is 2.280303018493336\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6944,  0.0737],\n",
      "        [-0.5720,  0.3279],\n",
      "        [ 0.4822,  0.0729]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "55\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.004\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 43106.379\n",
      "expected info is tensor([[0.0017]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.6088, 0.0930]])\n",
      "new data istensor([[0.1409, 0.2897]])\n",
      "g_theta2 istensor([[ 0.9253, -0.2106],\n",
      "        [-0.3058,  0.3701],\n",
      "        [ 0.1469, -0.2267]])\n",
      "p21val is 0.931342819969552\n",
      "pf12val is 0.691180307680961\n",
      "chi_f12 is 0.738709103886093\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5733,  0.4467],\n",
      "        [ 0.3073,  0.0387],\n",
      "        [ 0.6107,  0.0924]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "56\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -3.027\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 38983.196\n",
      "expected info is tensor([[8.4325e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.0718, 0.1511]])\n",
      "new data istensor([[0.0527, 0.2328]])\n",
      "g_theta2 istensor([[-0.9577,  0.7727],\n",
      "        [ 0.0594, -0.3220],\n",
      "        [ 0.8756, -0.1971]])\n",
      "p21val is 0.866641538091866\n",
      "pf12val is 0.113424163270473\n",
      "chi_f12 is 4.353241660800528\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.9121, -0.1438],\n",
      "        [ 0.5836, -0.7507],\n",
      "        [ 0.0732,  0.1516]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "57\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.011\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 7140.104\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0927,  0.9665]])\n",
      "new data istensor([[ 0.4306, -0.3200]])\n",
      "g_theta2 istensor([[ 0.5691,  0.2846],\n",
      "        [ 0.5768, -0.7835],\n",
      "        [ 0.2830, -0.0899]])\n",
      "p21val is 0.378820805468333\n",
      "pf12val is 0.007211030556103\n",
      "chi_f12 is 9.864286807254429\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.7813,  0.7684],\n",
      "        [-0.4195, -0.3685],\n",
      "        [-0.0934,  0.9663]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "58\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.021\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 590.764\n",
      "expected info is tensor([[0.0095]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.2419, 1.4326]])\n",
      "new data istensor([[ 0.9195, -0.4517]])\n",
      "g_theta2 istensor([[ 0.9637,  0.6186],\n",
      "        [-0.3839, -0.3423],\n",
      "        [-0.3030,  0.7705]])\n",
      "p21val is 0.982002665662367\n",
      "pf12val is 0.731564262405593\n",
      "chi_f12 is 0.625140424404063\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.3524, -0.7595],\n",
      "        [ 0.9005,  0.8421],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "59\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.037\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 43356.459\n",
      "expected info is tensor([[6.3381e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.0619, 0.1722]])\n",
      "new data istensor([[0.0723, 0.1870]])\n",
      "g_theta2 istensor([[ 0.3592, -0.7642],\n",
      "        [ 1.1806,  1.1222],\n",
      "        [-0.2173, -0.2182]])\n",
      "p21val is 0.690623262930078\n",
      "pf12val is 0.798812750264882\n",
      "chi_f12 is 0.449257431588204\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.2834, -0.5309],\n",
      "        [-0.4351, -0.6413],\n",
      "        [ 0.0607,  0.1725]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "60\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.044\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 10567.646\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.0040,  0.9706]])\n",
      "new data istensor([[ 0.4546, -0.2372]])\n",
      "g_theta2 istensor([[-0.2184, -0.2808],\n",
      "        [-0.1978, -0.2065],\n",
      "        [ 0.2615, -0.0633]])\n",
      "p21val is 0.984541180576086\n",
      "pf12val is 0.139963811884195\n",
      "chi_f12 is 3.932742752655405\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.3837, -0.0804],\n",
      "        [-0.0805,  0.5907],\n",
      "        [-0.0030,  0.9702]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "61\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.056\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 1878.121\n",
      "expected info is tensor([[0.0037]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.2228,  1.5331]])\n",
      "new data istensor([[ 0.9581, -0.5641]])\n",
      "g_theta2 istensor([[-0.2045, -0.0434],\n",
      "        [-0.2328,  0.4315],\n",
      "        [-0.2195,  0.7822]])\n",
      "p21val is 0.638193479357441\n",
      "pf12val is 0.034578787113181\n",
      "chi_f12 is 6.729029748297835\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.1954,  0.0316],\n",
      "        [ 0.3561, -0.8752],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "62\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.046\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 20252.587\n",
      "expected info is tensor([[4.9275e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.6882,  0.6124]])\n",
      "new data istensor([[-0.0930,  0.0207]])\n",
      "g_theta2 istensor([[ 0.3752, -0.1775],\n",
      "        [-0.0508, -0.6671],\n",
      "        [ 0.2470, -0.2397]])\n",
      "p21val is 0.879677667466329\n",
      "pf12val is 0.780608251508026\n",
      "chi_f12 is 0.495363707132751\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.6862,  0.8147],\n",
      "        [-0.9218, -0.1746],\n",
      "        [-0.6887,  0.6113]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "63\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.065\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 947.028\n",
      "expected info is tensor([[0.0009]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-1.0141,  1.3247]])\n",
      "new data istensor([[ 0.1575, -0.1396]])\n",
      "g_theta2 istensor([[ 0.8286,  0.4256],\n",
      "        [-0.8744, -0.3212],\n",
      "        [-0.9118,  0.3807]])\n",
      "p21val is 0.989688506567324\n",
      "pf12val is 0.905637637928056\n",
      "chi_f12 is 0.198232022161100\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.8738,  0.4910],\n",
      "        [-0.7858, -0.6112],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "64\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.072\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 48301.175\n",
      "expected info is tensor([[0.0002]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.0780, 0.2273]])\n",
      "new data istensor([[0.0726, 0.2429]])\n",
      "g_theta2 istensor([[-0.8220,  0.3551],\n",
      "        [-0.5960, -0.3276],\n",
      "        [-0.2180, -0.2192]])\n",
      "p21val is 0.788359923377395\n",
      "pf12val is 0.330624287211124\n",
      "chi_f12 is 2.213545264510155\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.2216,  0.4461],\n",
      "        [ 0.0064, -0.3206],\n",
      "        [ 0.0785,  0.2288]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "65\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.067\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 13872.715\n",
      "expected info is tensor([[0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[0.0495, 1.0067]])\n",
      "new data istensor([[ 0.4447, -0.2807]])\n",
      "g_theta2 istensor([[ 0.4649,  0.6251],\n",
      "        [ 0.1109, -0.1513],\n",
      "        [ 0.3060, -0.0171]])\n",
      "p21val is 0.041867790336924\n",
      "pf12val is 0.456930144212563\n",
      "chi_f12 is 1.566449514149454\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.6604, -0.0679],\n",
      "        [ 0.7058,  0.2176],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "66\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.073\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 26112.686\n",
      "expected info is tensor([[5.0155e-05]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.5979,  0.4674]])\n",
      "new data istensor([[-0.0772,  0.0643]])\n",
      "g_theta2 istensor([[-0.7234, -0.0009],\n",
      "        [ 0.8220,  0.1954],\n",
      "        [ 0.2451, -0.2372]])\n",
      "p21val is 0.089582966390763\n",
      "pf12val is 0.690086208066690\n",
      "chi_f12 is 0.741877499936000\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.4194,  0.1055],\n",
      "        [ 0.2633,  0.0227],\n",
      "        [-0.5999,  0.4655]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "67\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n",
      "loss is -3.072\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 2899.148\n",
      "expected info is tensor([[0.0003]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.6566,  1.2437]])\n",
      "new data istensor([[ 0.3863, -0.3734]])\n",
      "g_theta2 istensor([[ 0.4885, -0.1208],\n",
      "        [ 0.1144,  0.2226],\n",
      "        [-0.8959,  0.1959]])\n",
      "p21val is 0.595842741955128\n",
      "pf12val is 0.412245459887503\n",
      "chi_f12 is 1.772272661221897\n",
      "samples escaped box\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[ 0.8776,  0.5519],\n",
      "        [ 0.6727, -0.3769],\n",
      "        [ 0.0000,  0.0000]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "68\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -3.081\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CPU for conduct_design_opti()\n",
      "Loss design: 27590.828\n",
      "expected info is tensor([[0.0001]], grad_fn=<ReshapeAliasBackward0>)\n",
      "current sol istensor([[-0.4732,  0.2422]])\n",
      "new data istensor([[0.0316, 0.0708]])\n",
      "g_theta2 istensor([[ 0.9404,  0.4909],\n",
      "        [ 0.7843, -0.4731],\n",
      "        [ 0.2545, -0.2423]])\n",
      "p21val is 0.051832422730436\n",
      "pf12val is 0.664810017787291\n",
      "chi_f12 is 0.816507933361299\n",
      "samples escaped box\n",
      "p_val_ftarget is 0.0\n",
      "new 2 points\n",
      "tensor([[-0.5843, -0.7653],\n",
      "        [-0.8587,  0.2340],\n",
      "        [-0.4734,  0.2431]])\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "current sol istensor([[-0.4732,  0.2422]])\n",
      "Success is False and failure is True after 69 iterations\n"
     ]
    }
   ],
   "source": [
    "loc_sample = loc_sample0.clone()\n",
    "iter_hp = 50\n",
    "iter_design = 100\n",
    "iter_param = 100\n",
    "num_base_kernels = 2\n",
    "max_iter = 50\n",
    "\n",
    "# f_target = f_target.reshape(2,1) \n",
    "# tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    " #np.random.random_sample((loc_size,2))\n",
    "#loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "#train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "\n",
    "#loc_sample = Tensor([[0.0, 0.1], [0.0, -0.1]]) #T\n",
    "# loc_x = (-1.5 + 2.)  * np.random.random_sample((loc_size,1)) +2.\n",
    "\n",
    "# # loc_y = (2. - 1.5)  * np.random.random_sample((loc_size,1)) - 1.5\n",
    "# # loc = np.concatenate((loc_x, loc_y), 1)\n",
    "print(loc_sample)\n",
    "\n",
    "\n",
    "data_fit_vec = torch.empty((1,1))\n",
    "entropy_vec = torch.empty((1,1))\n",
    "loss_vec = torch.empty((1,1))\n",
    "\n",
    "\n",
    "#\n",
    "vec_x = x0.clone() #Tensor(np.array([0.0,0.0])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "var_vec = torch.zeros([max_iter, 1])\n",
    "p21_vec = torch.empty((1,1))\n",
    "\n",
    "lr_new = .01\n",
    "\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten()  \n",
    "    \n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "show_TTRBox = False\n",
    "iter = 0    \n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "patience = 0.0\n",
    "patience_f = 0.0\n",
    "patience_2 = 0.0\n",
    "checking_model = False\n",
    "model_double_check = False\n",
    "\n",
    "\n",
    "### Copy as many variables to CUDA as we can before entering the while loop\n",
    "if use_cuda:\n",
    "    g_theta2_vec = g_theta2_vec.to(device)\n",
    "    tol_vector = tol_vector.cuda()\n",
    "    data_fit_vec = data_fit_vec.to(device)\n",
    "    entropy_vec = entropy_vec.to(device)\n",
    "    loss_vec = loss_vec.to(device)\n",
    "\n",
    "    g_theta1 = g_theta1.cuda()\n",
    "    agg_data = agg_data.cuda()\n",
    "    f_target = f_target.cuda()\n",
    "    x0 = x0.cuda()\n",
    "    loc_sample = loc_sample.cuda()\n",
    "##########################\n",
    "\n",
    "\n",
    "while(SUCCESS == False and FAILURE == False):\n",
    "    total_time_start = time.time()\n",
    "    \n",
    "    print(iter)\n",
    "    model_double_check = False\n",
    "    if (checking_model == False):\n",
    "        print('START HYPERPARAMETERS optimization')\n",
    "        if (iter == 0):\n",
    "            cur_model = None\n",
    "            cur_likelihood = None\n",
    "\n",
    "\n",
    "        loc_sample_old = loc_sample.clone()\n",
    "        x0_old = x0.clone()\n",
    "        hyper_opti_start = time.time()\n",
    "        \n",
    "        model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "        \n",
    "        hyper_opti_end = time.time()\n",
    "        cpu_hyper_opti.append(hyper_opti_end - hyper_opti_start)\n",
    "        \n",
    "# Before this is hyper_opti\n",
    "\n",
    "        print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "   \n",
    "    conduct_design_start = time.time()\n",
    "    x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new,noise_value)\n",
    "    conduct_design_end = time.time()\n",
    "    cpu_conduct_design.append(conduct_design_end - conduct_design_start)\n",
    "\n",
    "    \"\"\"Everything after here might be independent\"\"\"\n",
    "    # torch.inv_quad, torch.det, everything with likelihood.something is expensive\n",
    "    cur_model = model\n",
    "    cur_likelihood = likelihood\n",
    "    \n",
    "# Check tolerance to see if we succeeded\n",
    "    lower_bound = torch.zeros(pf1.shape)\n",
    "    upper_bound = torch.zeros(pf1.shape)\n",
    "    if use_cuda:\n",
    "        lower_bound = lower_bound.cuda()\n",
    "        upper_bound = upper_bound.cuda()\n",
    "        \n",
    "    for i in range(pf1.shape[0]):\n",
    "        lower_bound[i] = pf1[i] -  torch.sqrt(Qf12[i,i])\n",
    "        upper_bound[i] = pf1[i] +  torch.sqrt(Qf12[i,i])\n",
    "\n",
    "    SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "# End of checking\n",
    "    entropy = ( 0.5 * torch.log( torch.det(Qf1.evaluate()) / torch.det(Qf12.evaluate()) ) ).reshape(1,1)\n",
    "    # Computing entropy can be expensive to compute\n",
    "    \n",
    "    print('expected info is '+str(entropy))\n",
    "    \n",
    "    if not SUCCESS:\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        new_data = vfield_(g_theta2.detach())  \n",
    "        agg_data12 = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "        g_theta12= torch.cat([g_theta1, g_theta2.detach()], 0)\n",
    "        new_data_x = vfield_(x0_new.detach() )  \n",
    "        print('current sol is'+str(x0_new.detach()))\n",
    "        print('new data is' + str(new_data_x))\n",
    "        print('g_theta2 is' + str(g_theta2.detach()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            if iter >= 0:\n",
    "                \n",
    "                \n",
    "                p21 = likelihood.get_p21(g_theta1, g_theta2.detach(), agg_data, model, noise_value)\n",
    "                \n",
    "#                 Q21 = Q21 + noise_value*torch.eye(Q21.shape[0])\n",
    "                chi_21 = (Q21).inv_quad(new_data.flatten() - p21.reshape(new_data.flatten().shape)).cpu()\n",
    "                \n",
    "                p_val = 1. - stats.chi2.cdf(chi_21, Q21.shape[0])\n",
    "                pf12 = likelihood.get_pf12(Q21,g_theta1, g_theta2.detach(), x0_new.detach(), new_data.flatten(), pf1, p21, model, noise_value)\n",
    "               \n",
    "                eye = torch.eye(Qf12.shape[0]).to(device)\n",
    "                chi_f12 = (Qf12 + noise_value*eye).inv_quad(new_data_x.flatten() - pf12.reshape(new_data_x.flatten().shape)).cpu()\n",
    "                p_val_f12 = 1. - stats.chi2.cdf(chi_f12, Qf12.shape[0])\n",
    "                print('p21val is %.15f' %p_val)\n",
    "                p21_vec = torch.cat([p21_vec, Tensor([p_val]).reshape(1,1)], 0)\n",
    "                print('pf12val is %.15f' %p_val_f12)\n",
    "                print('chi_f12 is %.15f' %chi_f12 )\n",
    "                \n",
    "                if (p_val < 0.01):# or p_val_f12 < 0.001:\n",
    "                    model_double_check = True\n",
    "                    checking_model = True\n",
    "                    patience = patience+1\n",
    "                    print('patience is %.3f' %patience)\n",
    "\n",
    "                if (model_double_check == True):\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_sample.shape[0],2)) + vf.low)\n",
    "                    sum = torch.zeros(2, 2).to(device) #replace with num_tasks\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "                    x0_old = x0_old.to(device)\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_old).t(), (g_theta2.detach()[i] - x0_old) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum #+ torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_old, covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_old],0)\n",
    "                    \n",
    "                    x0 = x0_old #Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    if (patience >= 2):# or patience_2 >= 2 or patience_f >= 2):\n",
    "                        PATH = \".//model_Carlo/model_update/model_base_\"+str(iter)+\".pt\"\n",
    "                        torch.save(model, PATH)\n",
    "                        \n",
    "                        entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                        data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                        iter = iter + 1\n",
    "                        patience = 0\n",
    "#                         patience_2 = 0\n",
    "#                         patience_f = 0\n",
    "                        model_double_check = False\n",
    "                        checking_model = False\n",
    "                        num_base_kernels = num_base_kernels + 1\n",
    "                        print('adding complexity to model')\n",
    "                        print('num base is ' + str(num_base_kernels))\n",
    "#     #                         \n",
    "                        loc_sample = loc_sample_old\n",
    "                        #x0 = x0_old\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        vec_x = torch.cat([vec_x.to(device), x0_new.detach()])\n",
    "                        g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                        print('acquiring 2, new size is ' + str(g_theta1.shape[0]))\n",
    "                 \n",
    "                    #iter_hp = iter_hp + 10\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "                    torch.save(model, PATH)\n",
    "                    vec_x = torch.cat([vec_x.to(device), x0_new.detach()])\n",
    "                    loss_vec = torch.cat([loss_vec, -loss])\n",
    "                    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                    entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                    data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                    model_double_check = False\n",
    "                    iter = iter + 1\n",
    "                    patience = 0\n",
    "                    patience_2 = 0\n",
    "                    patience_f = 0\n",
    "                    checking_model = False\n",
    "                    if (entropy < thresh_EI):\n",
    "                        count_EI = count_EI + 1\n",
    "                    else:\n",
    "                        count_EI = 0\n",
    "                    if (count_EI >= max_count_EI):\n",
    "                        FAILURE = True\n",
    "                    \n",
    "                    x0 = (x0_new.detach())# + torch.randn(x0_new.detach().size()) * .001)#/torch.norm(x0_new.detach())\n",
    "                    sum = torch.zeros(2, 2).to(device)\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum# + torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_new.detach(), covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = loc_sample#/torch.norm(loc_sample)\n",
    "                    #loc_sample = 2. *  (loc_sample - torch.min(loc_sample)) / (torch.max(loc_sample) - torch.min(loc_sample)) - 1.\n",
    "                    #loc_sample[0] = x0_new.detach() #+ torch.randn(x0_new.detach().size()) * .001 #g_theta2.detach() #loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_size,2)) + vf.low)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_new.detach()],0)\n",
    "                    for i in range(loc_sample.shape[0]):\n",
    "                        if loc_sample[i,0] < -1. or loc_sample[i,0] > 1. or loc_sample[i,1] < -1. or loc_sample[i,1] > 1.:\n",
    "                            print('samples escaped box')\n",
    "                            loc_sample[i] = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    \n",
    "                    \n",
    "#                     if p_val > 0.99 and p_val_f12 > 0.99:\n",
    "#                         num_base_kernels = max(num_base_kernels - 1, 3)\n",
    "                        #iter_hp = iter_hp - 10\n",
    "                    chi_f_target = (Qf12 ).inv_quad(f_target.to(device) - pf1).cpu()\n",
    "                    p_val_f_target = 1. - stats.chi2.cdf(chi_f_target, Qf12.shape[0])\n",
    "                    print('p_val_ftarget is '+str(p_val_f_target))\n",
    "                    if (p_val_f_target > .95):\n",
    "                        print('acquiring target point becuse p_val_ftarget is '+str(p_val_f_target))\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "        \n",
    "                        rand = torch.randn(x0_new.detach().size()).to(device)\n",
    "                        x0 = (x0_new.detach()) + rand * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + rand * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                    else:\n",
    "#                         x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "#                         g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                       \n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()).to(device) * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()).to(device) * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                        \n",
    "                    if x0_new.detach()[0,0] < -1. or x0_new.detach()[0,0] > 1. or x0_new.detach()[0,1] < -1. or x0_new.detach()[0,1] > 1.:\n",
    "#                         x0 = Tensor(np.array([0.0,-1.0])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = x0.reshape(1,2) \n",
    "                        x0 = Tensor(np.array([0. , 0.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = x0.reshape(1,2)\n",
    "                        #x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                 \n",
    "             #       loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "                        loc_sample[-1] = x0 #(x0_new.detach()) \n",
    "                    print('new 2 points')\n",
    "                    print(loc_sample)\n",
    "                  \n",
    " #                    agg_data  = (agg_data  - agg_data.mean())/agg_data .std(dim=-1, keepdim=True)\n",
    "#                     g_theta1 = (g_theta1 - g_theta1.mean())/g_theta1.std(dim=-2, keepdim=True)\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            #clear_output(wait=False)\n",
    "        total_time_end = time.time()\n",
    "        cpu_total.append(total_time_end - total_time_start)\n",
    "        \n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "torch.save(model, PATH)\n",
    "print('current sol is'+str(x0_new.detach()))\n",
    "    \n",
    "print('Success is ' + str(SUCCESS) + ' and failure is ' + str(FAILURE)+' after '+ str(iter) + ' iterations')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3481, -0.7225],\n",
      "        [ 0.1554,  0.8078],\n",
      "        [ 0.0350,  0.5990],\n",
      "        [-0.0240, -0.9202]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "vf = ObjFun()\n",
    "# f_target = vf.tgt_vec\n",
    "# print(f_target)\n",
    "sample_size = 4\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -1.\n",
    "vf.high = 1.\n",
    "\n",
    "high_minus_low = vf.high- vf.low\n",
    "#high_minus_low = -\n",
    "def g_theta(sample_size, D):\n",
    "#     loc_x = (2. - 1.0 )  * np.random.random_sample((sample_size,1)) + 1.0\n",
    "    \n",
    "#     loc_y = (2.  -1.0)  * np.random.random_sample((sample_size,1)) - 2.\n",
    "#     loc = np.concatenate((loc_x, loc_y), 1)\n",
    "    loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "if use_cuda:\n",
    "    train_x = train_x.cuda()\n",
    "#train_x = Tensor([[-1.5, 1.5], [-1.5, 1.3]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    randn = torch.randn(Tensor(vf(x[:,0], x[:,1])).size())\n",
    "    randn = randn.to(x.device)\n",
    "    out = vf(x[:,0], x[:,1]) + randn * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)\n",
    "\n",
    "# print(train_y)\n",
    "# train_y = (train_y - train_y.mean())/train_y.std(dim=-2, keepdim=True)\n",
    "# train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "# print(train_y)\n",
    "# print(train_y.std(dim=-2, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_hyper_opti = []\n",
    "gpu_conduct_design = []\n",
    "gpu_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x0_copy\n",
    "loc_sample0 = loc_sample0_copy\n",
    "x_train = x_train_copy\n",
    "y_train = y_train_copy\n",
    "\n",
    "# This will make TAD fail to find a solution\n",
    "f_target = Tensor([[-1., -1.]]).reshape(2,1)\n",
    "tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0178,  0.0281],\n",
      "        [ 0.0041, -0.0184],\n",
      "        [ 0.0507,  0.1586]])\n",
      "0\n",
      "START HYPERPARAMETERS optimization\n",
      "Using CUDA\n",
      "loss is -2.884\n",
      "END HYPERPARAMETERS optimization\n",
      "Using CUDA for conduct_design_opti()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m likelihood\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     98\u001b[0m conduct_design_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 99\u001b[0m x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_design_opti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_theta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_design\u001b[49m\u001b[43m,\u001b[49m\u001b[43miter_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnoise_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m conduct_design_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    101\u001b[0m gpu_conduct_design\u001b[38;5;241m.\u001b[39mappend(conduct_design_end \u001b[38;5;241m-\u001b[39m conduct_design_start)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mconduct_design_opti\u001b[0;34m(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new, noise_value)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( training_param_iter ):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m         loss2, pf1, Qf1, Qf12, data_fit, Q21 \u001b[38;5;241m=\u001b[39m \u001b[43mlikelihood\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ell\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_theta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_theta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_noise1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_noise2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         loss2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m*\u001b[39m loss2\n\u001b[1;32m     45\u001b[0m         loss2\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/tiany/Targeted_Adaptive_Design/code/jupyter/../likelihood/vvlikelihood.py:307\u001b[0m, in \u001b[0;36mFixedNoiseMultitaskGaussianLikelihood.get_ell\u001b[0;34m(self, agg_data, f_target, x_, g_theta1, model, likelihood, g_theta2, cov_noise1, cov_noise2)\u001b[0m\n\u001b[1;32m    302\u001b[0m Cf1 \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mforward(x_,g_theta1)\n\u001b[1;32m    303\u001b[0m C11 \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mforward(g_theta1, g_theta1, add_jitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m cov_noise1\n\u001b[0;32m--> 307\u001b[0m Cf2 \u001b[38;5;241m=\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_theta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m C12 \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mforward(g_theta1, g_theta2)\n\u001b[1;32m    311\u001b[0m C22 \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mforward(g_theta2, g_theta2, add_jitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m cov_noise2\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/tiany/Targeted_Adaptive_Design/code/jupyter/../kernels/sep_vvkernels.py:53\u001b[0m, in \u001b[0;36mSepTensorProductKernel.forward\u001b[0;34m(self, x1, x2, **params)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 53\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovar_module_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module_list[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     55\u001b[0m         res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward(x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/tiany/Targeted_Adaptive_Design/code/jupyter/../kernels/vvkernels.py:30\u001b[0m, in \u001b[0;36mTensorProductKernel.forward\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, add_jitter, **params)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]):\n\u001b[1;32m     29\u001b[0m     covar_i \u001b[38;5;241m=\u001b[39m covar_i\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m*\u001b[39mx1\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m covar_x \u001b[38;5;241m=\u001b[39m gpytorch\u001b[38;5;241m.\u001b[39mlazy\u001b[38;5;241m.\u001b[39mlazify(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_covar_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m#(self.data_covar_module.forward(x1, x2, **params))#\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (add_jitter \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     covar_x \u001b[38;5;241m=\u001b[39m covar_x \u001b[38;5;66;03m#+ (1e-6) * torch.eye(covar_x.shape[0])\u001b[39;00m\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[0;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 109\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputscale\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_dim_is_batch:\n",
      "File \u001b[0;32m/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/kernels/rbf_kernel.py:82\u001b[0m, in \u001b[0;36mRBFKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     74\u001b[0m     x1\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m x2\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m trace_mode\u001b[38;5;241m.\u001b[39mon()\n\u001b[1;32m     80\u001b[0m ):\n\u001b[1;32m     81\u001b[0m     x1_ \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale)\n\u001b[0;32m---> 82\u001b[0m     x2_ \u001b[38;5;241m=\u001b[39m \u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlengthscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_dist(\n\u001b[1;32m     84\u001b[0m         x1_, x2_, square_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39mdiag, dist_postprocess_func\u001b[38;5;241m=\u001b[39mpostprocess_rbf, postprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RBFCovariance\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     87\u001b[0m     x1,\n\u001b[1;32m     88\u001b[0m     x2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     ),\n\u001b[1;32m     93\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "loc_sample = loc_sample0.clone()\n",
    "iter_hp = 50\n",
    "iter_design = 100\n",
    "iter_param = 100\n",
    "num_base_kernels = 2\n",
    "max_iter = 50\n",
    "\n",
    "# f_target = f_target.reshape(2,1) \n",
    "# tol_vector = 0.01 * torch.ones(f_target.shape)\n",
    "\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    " #np.random.random_sample((loc_size,2))\n",
    "#loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "#train_x = (train_x - train_x.mean())/train_x.std(dim=-2, keepdim=True)\n",
    "\n",
    "#loc_sample = Tensor([[0.0, 0.1], [0.0, -0.1]]) #T\n",
    "# loc_x = (-1.5 + 2.)  * np.random.random_sample((loc_size,1)) +2.\n",
    "\n",
    "# # loc_y = (2. - 1.5)  * np.random.random_sample((loc_size,1)) - 1.5\n",
    "# # loc = np.concatenate((loc_x, loc_y), 1)\n",
    "print(loc_sample)\n",
    "\n",
    "\n",
    "g_theta2_vec = (Tensor(loc_sample).clone()).flatten().to(device)\n",
    "\n",
    "data_fit_vec = torch.empty((1,1)).to(device)\n",
    "entropy_vec = torch.empty((1,1)).to(device)\n",
    "loss_vec = torch.empty((1,1)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "vec_x = x0.clone() #Tensor(np.array([0.0,0.0])) \n",
    "vec_x = vec_x.reshape(1,2)\n",
    "var_vec = torch.zeros([max_iter, 1])\n",
    "p21_vec = torch.empty((1,1))\n",
    "\n",
    "lr_new = .01\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "show_TTRBox = False\n",
    "iter = 0    \n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "patience = 0.0\n",
    "patience_f = 0.0\n",
    "patience_2 = 0.0\n",
    "checking_model = False\n",
    "model_double_check = False\n",
    "\n",
    "\n",
    "g_theta1 = g_theta1.to(device)\n",
    "agg_data = agg_data.to(device)\n",
    "\n",
    "\n",
    "while(SUCCESS == False and FAILURE == False):\n",
    "    total_time_start = time.time()\n",
    "    \n",
    "    print(iter)\n",
    "    model_double_check = False\n",
    "    if (checking_model == False):\n",
    "        print('START HYPERPARAMETERS optimization')\n",
    "        if (iter == 0):\n",
    "            cur_model = None\n",
    "            cur_likelihood = None\n",
    "\n",
    "\n",
    "        loc_sample_old = loc_sample.clone()\n",
    "        x0_old = x0.clone()\n",
    "        hyper_opti_start = time.time()\n",
    "        \n",
    "        model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "        \n",
    "        hyper_opti_end = time.time()\n",
    "        gpu_hyper_opti.append(hyper_opti_end - hyper_opti_start)\n",
    "        \n",
    "# Before this is hyper_opti\n",
    "\n",
    "        print('END HYPERPARAMETERS optimization')\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "   \n",
    "    conduct_design_start = time.time()\n",
    "    x0_new,g_theta2, loss, pf1, Qf1, Qf12, data_fit, Q21 = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new,noise_value)\n",
    "    conduct_design_end = time.time()\n",
    "    gpu_conduct_design.append(conduct_design_end - conduct_design_start)\n",
    "\n",
    "\n",
    "    cur_model = model\n",
    "    cur_likelihood = likelihood\n",
    "    \n",
    "  \n",
    "    lower_bound = torch.zeros(pf1.shape)\n",
    "    upper_bound = torch.zeros(pf1.shape)\n",
    "        \n",
    "    for i in range(pf1.shape[0]):\n",
    "        lower_bound[i] = pf1[i] -  torch.sqrt(Qf12[i,i])\n",
    "        upper_bound[i] = pf1[i] +  torch.sqrt(Qf12[i,i])\n",
    "\n",
    "    SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    \n",
    "    entropy = ( 0.5 * torch.log( torch.det(Qf1.evaluate()) / torch.det(Qf12.evaluate()) ) ).reshape(1,1)\n",
    "    \n",
    "    print('expected info is '+str(entropy))\n",
    "    \n",
    "    if not SUCCESS:\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        new_data = vfield_(g_theta2.detach())  \n",
    "        agg_data12 = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "        g_theta12= torch.cat([g_theta1, g_theta2.detach()], 0)\n",
    "        new_data_x = vfield_(x0_new.detach() )  \n",
    "        print('current sol is'+str(x0_new.detach()))\n",
    "        print('new data is' + str(new_data_x))\n",
    "        print('g_theta2 is' + str(g_theta2.detach()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            if iter >= 0:\n",
    "                \n",
    "                \n",
    "                p21 = likelihood.get_p21(g_theta1, g_theta2.detach(), agg_data, model, noise_value)\n",
    "                \n",
    "#                 Q21 = Q21 + noise_value*torch.eye(Q21.shape[0])\n",
    "                chi_21 = (Q21).inv_quad(new_data.flatten() - p21.reshape(new_data.flatten().shape)).cpu()\n",
    "                \n",
    "                p_val = 1. - stats.chi2.cdf(chi_21, Q21.shape[0])\n",
    "                pf12 = likelihood.get_pf12(Q21,g_theta1, g_theta2.detach(), x0_new.detach(), new_data.flatten(), pf1, p21, model, noise_value)\n",
    "               \n",
    "                eye = torch.eye(Qf12.shape[0]).to(device)\n",
    "                chi_f12 = (Qf12 + noise_value*eye).inv_quad(new_data_x.flatten() - pf12.reshape(new_data_x.flatten().shape)).cpu()\n",
    "                p_val_f12 = 1. - stats.chi2.cdf(chi_f12, Qf12.shape[0])\n",
    "                print('p21val is %.15f' %p_val)\n",
    "                p21_vec = torch.cat([p21_vec, Tensor([p_val]).reshape(1,1)], 0)\n",
    "                print('pf12val is %.15f' %p_val_f12)\n",
    "                print('chi_f12 is %.15f' %chi_f12 )\n",
    "                \n",
    "                if (p_val < 0.01):# or p_val_f12 < 0.001:\n",
    "                    model_double_check = True\n",
    "                    checking_model = True\n",
    "                    patience = patience+1\n",
    "                    print('patience is %.3f' %patience)\n",
    "\n",
    "                if (model_double_check == True):\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_sample.shape[0],2)) + vf.low)\n",
    "                    sum = torch.zeros(2, 2).to(device) #replace with num_tasks\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "                    x0_old = x0_old.to(device)\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_old).t(), (g_theta2.detach()[i] - x0_old) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum #+ torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_old, covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_old],0)\n",
    "                    \n",
    "                    x0 = x0_old #Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    if (patience >= 2):# or patience_2 >= 2 or patience_f >= 2):\n",
    "                        PATH = \".//model_Carlo/model_update/model_base_\"+str(iter)+\".pt\"\n",
    "                        torch.save(model, PATH)\n",
    "                        \n",
    "                        entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                        data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                        iter = iter + 1\n",
    "                        patience = 0\n",
    "#                         patience_2 = 0\n",
    "#                         patience_f = 0\n",
    "                        model_double_check = False\n",
    "                        checking_model = False\n",
    "                        num_base_kernels = num_base_kernels + 1\n",
    "                        print('adding complexity to model')\n",
    "                        print('num base is ' + str(num_base_kernels))\n",
    "#     #                         \n",
    "                        loc_sample = loc_sample_old\n",
    "                        #x0 = x0_old\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        vec_x = torch.cat([vec_x.to(device), x0_new.detach()])\n",
    "                        g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                        print('acquiring 2, new size is ' + str(g_theta1.shape[0]))\n",
    "                 \n",
    "                    #iter_hp = iter_hp + 10\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "                    torch.save(model, PATH)\n",
    "                    vec_x = torch.cat([vec_x.to(device), x0_new.detach()])\n",
    "                    loss_vec = torch.cat([loss_vec, -loss])\n",
    "                    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "                    entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "                    data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "                    model_double_check = False\n",
    "                    iter = iter + 1\n",
    "                    patience = 0\n",
    "                    patience_2 = 0\n",
    "                    patience_f = 0\n",
    "                    checking_model = False\n",
    "                    if (entropy < thresh_EI):\n",
    "                        count_EI = count_EI + 1\n",
    "                    else:\n",
    "                        count_EI = 0\n",
    "                    if (count_EI >= max_count_EI):\n",
    "                        FAILURE = True\n",
    "                    \n",
    "                    x0 = (x0_new.detach())# + torch.randn(x0_new.detach().size()) * .001)#/torch.norm(x0_new.detach())\n",
    "                    sum = torch.zeros(2, 2).to(device)\n",
    "                    mean_2 = torch.mean(g_theta2.detach(), 0, True)\n",
    "\n",
    "                    for i in range(loc_size):\n",
    "                        #sum =sum + torch.matmul((g_theta2.detach()[i] -mean_2).t(), ( g_theta2.detach()[i] - mean_2 ) )# sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) # \n",
    "                        sum =sum + torch.matmul((g_theta2.detach()[i] -x0_new.detach()).t(), (g_theta2.detach()[i] - x0_new.detach()) ) #sum + torch.matmul((g_theta2.detach()[i] -\n",
    "                    emp_cov = 1./loc_size * sum# + torch.eye(sum.shape[0]) * 1e-8\n",
    "\n",
    "                    dis_2sample = MultivariateNormal( loc = x0_new.detach(), covariance_matrix=emp_cov )\n",
    "                    #loc_size = 4\n",
    "                    loc_sample = dis_2sample.sample((loc_size,))\n",
    "\n",
    "                    loc_sample = loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = loc_sample#/torch.norm(loc_sample)\n",
    "                    #loc_sample = 2. *  (loc_sample - torch.min(loc_sample)) / (torch.max(loc_sample) - torch.min(loc_sample)) - 1.\n",
    "                    #loc_sample[0] = x0_new.detach() #+ torch.randn(x0_new.detach().size()) * .001 #g_theta2.detach() #loc_sample.reshape(loc_size, 2)\n",
    "                    #loc_sample = Tensor(high_minus_low  * np.random.random_sample((loc_size,2)) + vf.low)\n",
    "                    loc_sample = torch.cat([loc_sample, x0_new.detach()],0)\n",
    "                    for i in range(loc_sample.shape[0]):\n",
    "                        if loc_sample[i,0] < -1. or loc_sample[i,0] > 1. or loc_sample[i,1] < -1. or loc_sample[i,1] > 1.:\n",
    "                            print('samples escaped box')\n",
    "                            loc_sample[i] = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low)\n",
    "                    \n",
    "                    \n",
    "#                     if p_val > 0.99 and p_val_f12 > 0.99:\n",
    "#                         num_base_kernels = max(num_base_kernels - 1, 3)\n",
    "                        #iter_hp = iter_hp - 10\n",
    "                    chi_f_target = (Qf12 ).inv_quad(f_target.to(device) - pf1).cpu()\n",
    "                    p_val_f_target = 1. - stats.chi2.cdf(chi_f_target, Qf12.shape[0])\n",
    "                    print('p_val_ftarget is '+str(p_val_f_target))\n",
    "                    if (p_val_f_target > .95):\n",
    "                        print('acquiring target point becuse p_val_ftarget is '+str(p_val_f_target))\n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "        \n",
    "                        rand = torch.randn(x0_new.detach().size()).to(device)\n",
    "                        x0 = (x0_new.detach()) + rand * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + rand * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                    else:\n",
    "#                         x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()) * .001\n",
    "#                         agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "#                         g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                       \n",
    "                        agg_data = agg_data12.clone()\n",
    "                        g_theta1 = g_theta12.clone()\n",
    "                        x0 = (x0_new.detach()) + torch.randn(x0_new.detach().size()).to(device) * .001\n",
    "                        loc_sample[-1] = (x0_new.detach()) + torch.randn(x0_new.detach().size()).to(device) * .001\n",
    "                        agg_data = torch.cat([agg_data12, new_data_x.flatten()], 0)\n",
    "                        g_theta1= torch.cat([g_theta12, x0_new.detach()], 0)\n",
    "                        \n",
    "                    if x0_new.detach()[0,0] < -1. or x0_new.detach()[0,0] > 1. or x0_new.detach()[0,1] < -1. or x0_new.detach()[0,1] > 1.:\n",
    "#                         x0 = Tensor(np.array([0.0,-1.0])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = x0.reshape(1,2) \n",
    "                        x0 = Tensor(np.array([0. , 0.]))\n",
    " # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "#                         x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                        x0 = x0.reshape(1,2)\n",
    "                        #x0 = Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "                 \n",
    "             #       loc_sample = (loc_sample - loc_sample.mean())/loc_sample.std(dim=-2, keepdim=True)\n",
    "                        loc_sample[-1] = x0 #(x0_new.detach()) \n",
    "                    print('new 2 points')\n",
    "                    print(loc_sample)\n",
    "                  \n",
    " #                    agg_data  = (agg_data  - agg_data.mean())/agg_data .std(dim=-1, keepdim=True)\n",
    "#                     g_theta1 = (g_theta1 - g_theta1.mean())/g_theta1.std(dim=-2, keepdim=True)\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            #clear_output(wait=False)\n",
    "        total_time_end = time.time()\n",
    "        gpu_total.append(total_time_end - total_time_start)\n",
    "        \n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "vec_x = torch.cat([vec_x, x0_new.detach()])\n",
    "g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.detach().flatten()], 0)\n",
    "entropy_vec = torch.cat([entropy_vec, entropy], 0)\n",
    "data_fit_vec = torch.cat([data_fit_vec, data_fit], 0)\n",
    "PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(iter)+\".pt\"\n",
    "torch.save(model, PATH)\n",
    "print('current sol is'+str(x0_new.detach()))\n",
    "    \n",
    "print('Success is ' + str(SUCCESS) + ' and failure is ' + str(FAILURE)+' after '+ str(iter) + ' iterations')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cpu_hyper_opti)\n",
    "print(cpu_conduct_design)\n",
    "print(cpu_total)\n",
    "print()\n",
    "print(gpu_hyper_opti)\n",
    "print(gpu_conduct_design)\n",
    "print(gpu_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_total_x = list(range(1, len(cpu_total)+1))\n",
    "gpu_total_x = list(range(1, len(gpu_total)+1))\n",
    "\n",
    "cpu_conduct_x = list(range(1, len(cpu_conduct_design)+1))\n",
    "gpu_conduct_x = list(range(1, len(gpu_conduct_design)+1))\n",
    "\n",
    "cpu_hyper_x = list(range(1, len(cpu_hyper_opti)+1))\n",
    "gpu_hyper_x = list(range(1, len(gpu_hyper_opti)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting CPU vs. GPU time\n",
    "\n",
    "# # Initialize plots\n",
    "plt.style.use('default')\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 6))\n",
    "\n",
    "plt.suptitle('CPU vs. GPU TAD Running Time Per Iteration', fontweight='bold')\n",
    "\n",
    "ax1.plot(cpu_hyper_x[1:], cpu_hyper_opti[1:], 'r-', label='CPU hyper_opti')\n",
    "ax1.plot(gpu_hyper_x[1:], gpu_hyper_opti[1:], 'b-', label='GPU hyper_opti')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "ax2.plot(cpu_conduct_x[1:], cpu_conduct_design[1:], 'g-', label='CPU conduct_design_opti')\n",
    "ax2.plot(gpu_conduct_x[1:], gpu_conduct_design[1:], 'c-', label='GPU conduct_design_opti')\n",
    "ax2.legend()\n",
    "# plt.ylabel('Time')\n",
    "# plt.xlabel('Input Size')\n",
    "\n",
    "plt.subplot(133)\n",
    "ax3.plot(cpu_total_x[1:], cpu_total[1:], 'y-', label='CPU whole iteration')\n",
    "ax3.plot(gpu_total_x[1:], gpu_total[1:], 'm-', label='GPU whole iteration')\n",
    "ax3.legend()\n",
    "\n",
    "plt.setp([ax1,ax2,ax3], xlabel='Iteration')\n",
    "plt.setp(ax1, ylabel='Time (Second)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore the part below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target - 0.001)\n",
    "print(f_target + 0.001)\n",
    "print(pf1)\n",
    "print(num_base_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting model validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "#plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "p21_vec_plot = p21_vec.detach()[1:]\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "#print(p21_vec_plot)\n",
    "ax.plot(p21_vec_plot,'s',color = 'blue', markersize=6)\n",
    "ax.axhline(.01,linestyle = '--',color = 'red', markersize=12, alpha = 1.0)\n",
    "ax.set_xlim(-0.3, p21_vec_plot.shape[0])\n",
    "ax.set_ylim(-0.3, 1.)\n",
    "#ax.set_yscale('log')\n",
    "plt.xticks(np.arange(0, iter+7, step=5.))\n",
    "ax.tick_params(labelsize='small', width=3)\n",
    "ax.set_xlabel('# of Model Checks')\n",
    "ax.set_ylabel('p-value')\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.05, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(1.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.02, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(0.1,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.26, 0.34), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(6.2,        #x start point\n",
    "             0.14,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.07,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.29, 0.03), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(7.2,        #x start point\n",
    "             -0.25,                      #y start point\n",
    "             0,       #change in x \n",
    "             0.2,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black')             #arrow edge color\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.57, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(14.15,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "############################\n",
    "ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.69, 0.4), xycoords='axes fraction', \n",
    "            fontsize=9*1.5, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'))\n",
    "ax.arrow(17.18,        #x start point\n",
    "             0.23,                      #y start point\n",
    "             0,       #change in x \n",
    "             -0.16,                      #change in y\n",
    "             head_width=0.2,         #arrow head width\n",
    "             head_length=0.06,        #arrow head length\n",
    "             width=0.1,              #arrow stem width\n",
    "             fc='black',             #arrow fill color\n",
    "             ec='black') \n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.32, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(10.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "# ############################\n",
    "\n",
    "# ############################\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.33, 0.34), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(11.25,        #x start point\n",
    "#              0.14,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.07,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# #################################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.38, 0.1), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(12.25,        #x start point\n",
    "#              -0.15,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.1,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')             #arrow edge color\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.38, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(13.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ax.annotate('Update model', xy=(0.55, 0.2), xytext=(0.55, 0.03), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(18.25,        #x start point\n",
    "#              -0.25,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              0.2,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black')  \n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.49, 0.4), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(17.25,        #x start point\n",
    "#              0.23,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.16,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.55, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(19.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "# ax.annotate('Restart', xy=(0.55, 0.2), xytext=(0.94, 0.43), xycoords='axes fraction', \n",
    "#             fontsize=9*1.5, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'))\n",
    "# ax.arrow(33.25,        #x start point\n",
    "#              0.26,                      #y start point\n",
    "#              0,       #change in x \n",
    "#              -0.19,                      #change in y\n",
    "#              head_width=0.2,         #arrow head width\n",
    "#              head_length=0.06,        #arrow head length\n",
    "#              width=0.1,              #arrow stem width\n",
    "#              fc='black',             #arrow fill color\n",
    "#              ec='black') \n",
    "# ################################\n",
    "\n",
    "\n",
    "# ################################\n",
    "\n",
    "ax.annotate('Threshold Line ($y = 0.01$)', xy=(1.0,0.01), xytext=(6,0), color='red', \n",
    "                xycoords = ax.get_yaxis_transform(), textcoords=\"offset points\",\n",
    "                size=14, va=\"center\")\n",
    "\n",
    "\n",
    "#ax.legend(['p-value'], loc = 'upper left', fontsize = 30)\n",
    "plt.savefig('figures_Carlo/qvalue_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the data back to CPU for plotting and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    vec_x = vec_x.cpu()\n",
    "    g_theta1 = g_theta1.cpu()\n",
    "    g_theta2 = g_theta2.cpu()\n",
    "    x_train, y_train = x_train.cpu(), y_train.cpu()\n",
    "    loss = loss.cpu()\n",
    "    pf1 = pf1.cpu()\n",
    "    Qf1 = Qf1.cpu()\n",
    "    Qf12 = Qf12.cpu()\n",
    "    Q21 = Q21.cpu()\n",
    "    data_fit_vec = data_fit_vec.cpu()\n",
    "    entropy_vec = entropy_vec.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting expected value and data fit term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "data_fit_vec_plot = 0.5* data_fit_vec.detach()[1:]\n",
    "entropy_vec_plot = entropy_vec.detach()[1:]\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(18, 8), tight_layout=True)\n",
    "\n",
    "ax1.plot(np.array(range(1,iter+2)), (entropy_vec_plot), '--o', color = 'blue', markersize=12)\n",
    "#ax1.set_yscale('log')\n",
    "# ax.plot(np.array(data_fit_vec_plot), (entropy_vec_plot), 'o')\n",
    "#ax1.set_yscale('log')\n",
    "\n",
    "ax2.plot(np.array(range(1,iter+2)), data_fit_vec_plot, '--o', color = 'red', markersize=12)\n",
    "#ax2.set_ylim(-10, 600)\n",
    "ax1.set_xlabel('Iteration #', size=32)\n",
    "ax2.set_xlabel('Iteration #', size=32)\n",
    "ax1.set_ylabel('Expected Information', size = 32)\n",
    "ax2.set_ylabel('Log-Gaussian Term', size = 32)\n",
    "ax1.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "ax2.set_xticks(np.arange(0, iter+3, step=3.))\n",
    "plt.savefig('figures_Carlo/exp_info/expectedinfo_vs_datafit_2_base.pdf',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving all data needed for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/vec_x_success_base.txt',vec_x.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta2_success_base.txt', g_theta2.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta1_success_base.txt', g_theta1.detach().numpy())\n",
    "np.savetxt('data_plots/x_train_ini_success_base.txt', x_train.detach().numpy())\n",
    "np.savetxt('data_plots/y_train_ini_success_base.txt', y_train.detach().numpy())\n",
    "np.savetxt('data_plots/entropy_vec_success_base.txt', entropy_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/datafit_success_base.txt', data_fit_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/p21_vec_success_base.txt',p21_vec_plot.detach().numpy())\n",
    "np.savetxt('data_plots/loss_success_base.txt',loss.detach().numpy())\n",
    "np.savetxt('data_plots/pf1_success_base.txt',pf1.detach().numpy())\n",
    "np.savetxt('data_plots/Qf1_success_base.txt',Qf1.evaluate().detach().numpy())\n",
    "np.savetxt('data_plots/Qf12_success_base.txt', Qf12.evaluate().detach().numpy())\n",
    "np.savetxt( 'data_plots/Q21_success_base.txt', Q21.evaluate().detach().numpy())\n",
    "#np.savetxt('data_plots/iter_success.txt', iter+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2),2)\n",
    "torch.save(v2, 'data_plots/v2_success_base.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.set_xlim(-3.1, 3.2)\n",
    "ax.set_ylim(-3.1, 3.2)\n",
    "#ax.scatter(g_theta1[:, 0].detach(),g_theta1[:, 1].detach(), c=\"b\", alpha=0.8)\n",
    "ax.plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(vec_x[-1,0], vec_x[-1,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd', color = 'green',markersize=15)\n",
    "ax.set_title('Final TAD configuration', fontsize = 40)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.legend(['1-points', 'TAD solution', 'Target'])\n",
    "plt.savefig('figures_Carlo/strategies/tad_sol_all_2_base.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "#v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2)\n",
    "ii = 0\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "v2 = v2.cpu()\n",
    "########################\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "ax.plot(vec_x[ii,0], vec_x[ii,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o', color = 'blue', markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Initial Configuration', fontsize = 40)\n",
    "ax.legend(['Target', 'Initial Target Candidate', 'Initial 1-sample','Initial 2-sample'])\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_ini_2_base.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x = vec_x.detach()\n",
    "v2 = g_theta2_vec.reshape(math.ceil(g_theta2_vec.shape[0]/2), 2).cpu()\n",
    "\n",
    "low = -3.2\n",
    "high = 3.2\n",
    "#for iter_plt in range(1,iter):\n",
    "iter_plt = 13\n",
    "ii = 3 + (iter_plt - 1) * (loc_size + 1)\n",
    "#######################\n",
    "\n",
    " \n",
    "###########\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(14,14))\n",
    "\n",
    "#ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 'bv', markersize=8)\n",
    "for i in range(1, iter_plt):\n",
    "    ax.plot(vec_x[i-1:i,0], vec_x[i-1:i,1],'v', color = 'red', markersize=15, alpha=0.2, label = '_nolegend_')\n",
    "    \n",
    "ax.plot(vec_x[0:iter_plt,0], vec_x[0:iter_plt,1],'v', color = 'red', markersize=15, linewidth=15, alpha= 0.2)\n",
    "#ax.plot(g_theta1.detach()[:,0], g_theta.detach()[:,1], 'bv', markersize=8)\n",
    "ax.plot(v2.detach()[0:ii,0], v2.detach()[0:ii,1], 's', color = 'blue', markersize=15,alpha=0.2)\n",
    "ax.plot(x_train.detach()[:,0], x_train.detach()[:,1], 's', color = 'black', markersize=15, alpha = 0.2)\n",
    "ax.plot(v2.detach()[ii:ii+loc_size+1,0], v2.detach()[ii:ii+loc_size+1,1], 'o',color = 'blue' , markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'gd',markersize=15)\n",
    "ax.plot(vec_x[iter_plt,0], vec_x[iter_plt,1],'v', color = 'red',markersize=15)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "#ax.set_title('Iteration '+str(iter_plt), fontsize = 40)\n",
    "ax.set_title('Final Configuration')\n",
    "ax.legend([ 'Previous Target Candidates', 'Previous 2-samples', 'Initial 1-sample','Current 2-sample', 'Target', 'Current Target Candidate'], fontsize = 30)\n",
    "\n",
    "ax.set_xlim(low, high)\n",
    "ax.set_ylim(low, high)\n",
    "plt.savefig('figures_Carlo/evol_solTAD/evol_sol_base'+str(iter_plt)+'_final.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta2_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_pll(x0,f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new,noise_value):\n",
    "\n",
    "    #g_theta2 = nn.Parameter(Tensor(loc_sample))\n",
    "\n",
    "    x_d= nn.Parameter(Tensor(x0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params': x_d, 'lr': 0.001}])\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    for ii in range( training_param_iter ):\n",
    "#         x_d = torch.cat([x_d_0, x_d_1]).reshape(1,2)\n",
    "#         g_theta2 = torch.cat([g_theta20, g_theta21],1)\n",
    "        optimizer.zero_grad()\n",
    "        #print(g_theta)\n",
    "        loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood, noise_value)\n",
    "        loss2 = -1. * loss2\n",
    "        \n",
    "        loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    loss2,lower_bound, upper_bound = likelihood.get_pll(f_target,x_d, g_theta1, agg_data, model, likelihood ,noise_value)\n",
    "    #loss2 = -1. * loss2\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "   # print(optimizer.state_dict())\n",
    "    print(x_d)\n",
    "    return x_d, lower_bound, upper_bound\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure everything is on CPU\n",
    "use_cuda = False\n",
    "#\n",
    "\n",
    "x_plot = np.linspace(vf.low, vf.high, 15)\n",
    "y_plot = np.linspace(vf.low, vf.high, 15)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat = torch.zeros(n * n, 2)\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "\n",
    "g_theta_grid = x_concat\n",
    "agg_data1_grid = vfield_(g_theta_grid)\n",
    "agg_data1_grid = agg_data1_grid.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.,2.])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_grid = Tensor(np.array([0.0,0.0])) \n",
    "vec_x_grid = vec_x_grid.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "model_grid, likelihood_grid = hyper_opti(g_theta_grid,agg_data1_grid,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_grid.eval()\n",
    "likelihood_grid.eval()\n",
    "x0_new_grid,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_grid, agg_data1_grid, model_grid, likelihood_grid, iter_design, iter_param, lr_new,noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "#loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_grid)\n",
    "print(SUCCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_grid = x0_new.detach()\n",
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "ax.plot(x_concat[:,0],x_concat[:,1], 's', color = 'blue', markersize=15, alpha = 0.2)\n",
    "ax.plot(x0_new_grid.detach()[0,0], x0_new_grid.detach()[0,1],'v',color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('Grid Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/grid_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_hp = 30\n",
    "# iter_design = 40 \n",
    "# iter_param = 50\n",
    "# num_base_kernels = 3\n",
    "\n",
    "# f_target = Tensor(vf.tgt_vec) \n",
    "# f_target = f_target.reshape(f_target.shape[0],1) \n",
    "# tol_vector = 0.005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size_rdn = math.ceil(g_theta1.shape[0]) #(iter)*(loc_size+1) + sample_size\n",
    "\n",
    "loc_sample = high_minus_low  * np.random.random_sample((loc_size_rdn,2)) + vf.low #np.random.random_sample((loc_size_rdn,2))\n",
    "g_theta_ = (Tensor(loc_sample).clone())\n",
    "agg_data1 = vfield_(g_theta_)\n",
    "agg_data1 = agg_data1.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([-2.0,2.0])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x_rdn = Tensor(np.array([0.,0.])) \n",
    "vec_x_rdn = vec_x_rdn.reshape(1,2)\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    " \n",
    "tol = 0.009 \n",
    "print('START HYPERPARAMETERS optimization')\n",
    "\n",
    "model_rdn, likelihood_rdn = hyper_opti(g_theta_,agg_data1,iter_hp,num_base_kernels,noise_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('END HYPERPARAMETERS optimization')\n",
    "model_rdn.eval()\n",
    "likelihood_rdn.eval()\n",
    "x0_new_rdn,lower_bound, upper_bound = conduct_design_pll(x0,f_target, g_theta_, agg_data1, model_rdn, likelihood_rdn, iter_design, iter_param, lr_new, noise_value)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(f_target-tol_vector)\n",
    "print(f_target+tol_vector)\n",
    "loc_sample = np.random.random_sample((loc_size_rdn,2))\n",
    "\n",
    "\n",
    "SUCCESS = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "print(x0_new_rdn)\n",
    "print(SUCCESS)\n",
    "sol_rdn = x0_new_rdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_plots/sol_rdn_success_base.txt', sol_rdn.detach().numpy())\n",
    "np.savetxt('data_plots/sol_grid_success_base.txt', x0_new_grid.detach().numpy())\n",
    "np.savetxt('data_plots/g_theta_rdn_success_base.txt', g_theta_.detach().numpy())\n",
    "#np.savetxt('data_plots/g_theta_grid_success.txt', x_concat.detach().numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,14))\n",
    "\n",
    "ax.plot(g_theta_[:,0].detach(),g_theta_[:,1].detach(), 's', color = 'blue',markersize=15, alpha = 0.2)\n",
    "ax.plot(sol_rdn.detach()[0,0], sol_rdn.detach()[0,1],'v', color = 'red',markersize=15)\n",
    "ax.plot(0.8731, 0.5664,'d', color = 'green',markersize=15)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(-3.1, 3.1)\n",
    "ax.set_xlabel('$d_1$', fontsize = 32)\n",
    "ax.set_ylabel('$d_2$', fontsize = 32)\n",
    "ax.set_title('Uniformly Random Solution', fontsize = 40)\n",
    "\n",
    "plt.savefig('figures_Carlo/strategies/rdn_sol_2_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizing Means and Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker\n",
    "class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "        self.oom = order\n",
    "        self.fformat = fformat\n",
    "        matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
    "    def _set_order_of_magnitude(self):\n",
    "        self.orderOfMagnitude = self.oom\n",
    "    def _set_format(self, vmin=None, vmax=None):\n",
    "        self.format = self.fformat\n",
    "        if self._useMathText:\n",
    "             self.format = r'$\\mathdefault{%s}$' % self.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "likelihood.eval()\n",
    "\n",
    "#noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "#print(x_concat_)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(True):\n",
    "    model = model.cpu()\n",
    "    pred = GPprediction(model)\n",
    "    agg_data = agg_data.cpu()\n",
    "    pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "    #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "    pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "    mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "    mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "    pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "    \n",
    "    var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "    var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "#     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "# #     print(pr.covariance_matrix.mean(axis=0))\n",
    "# #     print(AA)\n",
    "# #     print(pr.variance)\n",
    "# #     print((pr.covariance_matrix))\n",
    "# #     K = model.covar_module\n",
    "#     print((cov.diag()))\n",
    "#     print(pr_mean)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 2, figsize = (28, 24), tight_layout=True)\n",
    "diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "cs10 = ax1[0].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax1[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[0].set_title('$|v_1 - \\mu(v_1)|$', fontsize = 40)\n",
    "cbar10 = fig.colorbar(cs10, ax = ax1[0],format=OOMFormatter(0, mathText=False));\n",
    "\n",
    "ax1[0].set_xlabel('$d_1$')\n",
    "ax1[0].set_ylabel('$d_2$')\n",
    "diff_mean_v1 = torch.abs(v_1 - mean_v_1.detach())/torch.sqrt(var_v_1)\n",
    "#print(var_v_1)\n",
    "cs11 = ax1[1].contourf(xv_plot, yv_plot,diff_mean_v1 ,np.linspace(diff_mean_v1.min(), diff_mean_v1.max(), 100), cmap = 'jet')\n",
    "ax1[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax1[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax1[1].set_title('$|v_1 - \\mu(v_1)|/\\sigma(v_1)$', fontsize = 40)\n",
    "# ax1[0].set_aspect('equal')\n",
    "# ax1[1].set_aspect('equal')\n",
    "cbar11 = fig.colorbar(cs11, ax = ax1[1],format=OOMFormatter(0, mathText=False));\n",
    "ax1[1].set_xlabel('$d_1$')\n",
    "ax1[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())\n",
    "cs20 = ax2[0].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(0, 1.1, 100), cmap = 'jet')\n",
    "ax2[0].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[0].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[0].set_title('$|v_2 - \\mu(v_2)|$', fontsize = 40)\n",
    "cbar20 = fig.colorbar(cs20, ax = ax2[0],format=OOMFormatter(0, mathText=False));\n",
    "ax2[0].set_xlabel('$d_1$')\n",
    "ax2[0].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "diff_mean_v2= torch.abs(v_2 - mean_v_2.detach())/torch.sqrt(var_v_2)\n",
    "cs21 = ax2[1].contourf(xv_plot, yv_plot, diff_mean_v2,np.linspace(diff_mean_v2.min(), diff_mean_v2.max(), 100), cmap = 'jet')\n",
    "ax2[1].plot(g_theta1[:, 0].detach(),g_theta1[:, 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "ax2[1].plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "ax2[1].set_title('$|v_2 - \\mu(v_2)|/\\sigma(v_2)$', fontsize = 40)\n",
    "cbar21 = fig.colorbar(cs21, ax = ax2[1],format=OOMFormatter(0, mathText=False));\n",
    "ax2[1].set_xlabel('$d_1$')\n",
    "ax2[1].set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "# ax2[0].set_aspect('equal')\n",
    "# ax2[1].set_aspect('equal')\n",
    "\n",
    "plt.savefig('figures_Edward/mean_var/mean_final_2.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target = vf.tgt_vec\n",
    "f_target = f_target.reshape(f_target.shape[0],1)\n",
    "vf.tgt_loc = vf.tgt_loc.reshape(2,1)\n",
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "x_plot = np.linspace(-3.5, 3.5, 100)\n",
    "y_plot = np.linspace(-3.5, 3.5, 100)\n",
    "xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "n = x_plot.shape[0]\n",
    "x_concat_ = torch.zeros(n * n, 2)\n",
    "\n",
    "# n_sample = x_concat_.shape[0]\n",
    "num_tasks = 2\n",
    "i = 0\n",
    "k = 0\n",
    "while i < n*n:\n",
    "    x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "    x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "    k = k+1\n",
    "    i = i+n\n",
    "    \n",
    "\n",
    "tgt_plot = vfield_(x_concat_)\n",
    "\n",
    "\n",
    "\n",
    "v_1 = tgt_plot[:,0].reshape(n,n)\n",
    "v_2 = tgt_plot[:,1].reshape(n,n)\n",
    "plot = [1, 6, 10, iter+1]\n",
    "\n",
    "for ii in plot:\n",
    "    try:\n",
    "        \n",
    "        PATH = \".//model_Carlo/model_goodmodel/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "    except:\n",
    "        PATH = \".//model_Carlo/model_update/model_base_\"+str(ii - 1)+\".pt\"\n",
    "        model_16 = torch.load(PATH)\n",
    "        \n",
    "    #model_16 = torch.load(PATH)\n",
    "    model_16.eval()\n",
    "\n",
    "    likelihood.eval()\n",
    "\n",
    "    #noise = torch.eye(2 * g_theta1.detach().shape[0]) * noise_value\n",
    "    #print(x_concat_)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var(False):\n",
    "        pred = GPprediction(model_16)\n",
    "        pr_mean, cov = pred.GPpred(g_theta1.detach(), agg_data, x_concat_, noise_value)\n",
    "        #pr = (model(g_theta1.detach())) # likelihood(model(x_concat_), noise = torch.ones(x_concat_.shape) * noise_value)#\n",
    "        pr_mean = pr_mean.reshape(x_concat_.shape[0], num_tasks)\n",
    "        mean_v_1 = pr_mean[:,0].reshape(n,n)\n",
    "        mean_v_2 = pr_mean[:,1].reshape(n,n)\n",
    "        pred_var = cov.diag().reshape(num_tasks, x_concat_.shape[0]).T\n",
    "\n",
    "        var_v_1 = pred_var[:,0].reshape(n,n)\n",
    "        var_v_2 = pred_var[:,1].reshape(n,n)\n",
    "    #     AA = pr.covariance_matrix.mean(axis=0).reshape(num_tasks, x_concat_.shape[0]).T #.diag() #.reshape(num_tasks, num_tasks * g_theta1.shape[0]).T\n",
    "\n",
    "    # #     print(pr.covariance_matrix.mean(axis=0))\n",
    "    # #     print(AA)\n",
    "    # #     print(pr.variance)\n",
    "    # #     print((pr.covariance_matrix))\n",
    "    # #     K = model.covar_module\n",
    "    #     print((cov.diag()))\n",
    "    #     print(pr_mean)\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (28, 12), tight_layout=True)\n",
    "    diff_mean_v1= torch.abs(v_1 - mean_v_1.detach())#/torch.abs(v_1)\n",
    "    minn = torch.min(var_v_1.detach().min(), var_v_2.detach().min())\n",
    "    maxx = torch.min(var_v_1.detach().max(), var_v_2.detach().max())\n",
    "\n",
    "    cs11 = ax1.contourf(xv_plot, yv_plot, var_v_1.detach(), np.linspace(0.0,1.1, 100), cmap = 'jet')\n",
    "    ax1.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    \n",
    "    ax1.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "  \n",
    "    ax1.set_title('$\\sigma^2(v_1)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    # ax1[0].set_aspect('equal')\n",
    "    # ax1[1].set_aspect('equal')\n",
    "    cbar11 = fig.colorbar(cs11, ax = ax1,format=OOMFormatter(-0, mathText=False));\n",
    "    ax1.set_xlabel('$d_1$')\n",
    "    ax1.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    cs21 = ax2.contourf(xv_plot, yv_plot, var_v_2.detach(), np.linspace(0.0, 1.1, 100), cmap = 'jet')\n",
    "    ax2.plot(g_theta1[0:(4 + (ii - 1)* 3), 0].detach(),g_theta1[0:(4 + (ii - 1)* 3), 1].detach() , 'o', color = 'black',markersize=12, alpha = 1.0)\n",
    "    ax2.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "    ax2.set_title('$\\sigma^2(v_2)$ (Iteration '+str(ii)+')', fontsize = 40)\n",
    "    cbar21 = fig.colorbar(cs21, ax = ax2,format=OOMFormatter(-0, mathText=False));\n",
    "    ax2.set_xlabel('$d_1$')\n",
    "    ax2.set_ylabel('$d_2$')\n",
    "\n",
    "\n",
    "    # ax2[0].set_aspect('equal')\n",
    "    # ax2[1].set_aspect('equal')\n",
    "\n",
    "    plt.savefig('figures_Carlo/mean_var/var_iter_base'+str(ii)+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# # #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "# #                     #loc_size = 4\n",
    "# # loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# # loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# # g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# z = torch.zeros(n*n, 1)\n",
    "# for ii in range(n*n):\n",
    "#     print(ii)\n",
    "#     x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "#     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#     g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     x_d, g_theta2, loss2, pf1, Qf1, Qf12, data_fit, Q21 = conduct_2opt(agg_data,f_target,x0, g_theta1, model, likelihood, noise_value, g2, training_param_iter)\n",
    "#     z[ii] = loss2\n",
    "# z = z.reshape(n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot = np.linspace(-3., 3., 30)\n",
    "# y_plot = np.linspace(-3., 3., 30)\n",
    "# xv_plot, yv_plot = np.meshgrid(x_plot, y_plot)\n",
    "# n = x_plot.shape[0]\n",
    "# x_concat_ = torch.zeros(n * n, 2)\n",
    "# training_param_iter = 200\n",
    "\n",
    "# # n_sample = x_concat_.shape[0]\n",
    "# num_tasks = 2\n",
    "# i = 0\n",
    "# k = 0\n",
    "# while i < n*n:\n",
    "#     x_concat_[i:i+n,0] = Tensor(xv_plot[:,k])\n",
    "#     x_concat_[i:i+n,1] = Tensor(y_plot)\n",
    "#     k = k+1\n",
    "#     i = i+n\n",
    "\n",
    "# #dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .01 * torch.eye(2) )\n",
    "#                     #loc_size = 4\n",
    "# loc_sample = 1./3. * Tensor(high_minus_low  * np.random.random_sample((3,2)) + vf.low) # #dis_2sample.sample((2 + 1,))\n",
    "# loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "# g2 = g_theta2.detach() #loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "# likelihood.eval()\n",
    "# model.eval()\n",
    "# plot = [1, 7, 17, 26]\n",
    "# zz = torch.zeros(n*n, 4)\n",
    "# kk = 0\n",
    "# for jj in plot:\n",
    "#     try:\n",
    "        \n",
    "#         PATH = \".//model_Carlo/model_goodmodel/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#     except:\n",
    "#         PATH = \".//model_Carlo/model_update/model_\"+str(jj - 1)+\".pt\"\n",
    "#         model_16 = torch.load(PATH)\n",
    "#    # model_16 = torch.load(PATH)\n",
    "#     model_16.eval()\n",
    "\n",
    "#     likelihood.eval()\n",
    "#     g2 = v2.detach()[jj - 1 +loc_size+1 : jj - 1 +loc_size+1 +loc_size+1]\n",
    "#     g_theta1_cur = g_theta1[0:(4 + (jj - 1)* 3)]\n",
    "#     agg_data_cur = agg_data[0:2 * (4 + (jj - 1)* 3)]\n",
    "#     print(agg_data_cur.shape)\n",
    "#     for ii in range(n*n):\n",
    "#         print(ii)\n",
    "#         x0 = x_concat_[ii,:].reshape(1,2)\n",
    "#     #     dis_2sample = MultivariateNormal( loc = x0, covariance_matrix= .001 * torch.eye(2) )\n",
    "#     #                     #loc_size = 4\n",
    "#     #     loc_sample = dis_2sample.sample((2 + 1,))\n",
    "#     #     loc_sample0 = loc_sample.reshape(2 + 1, 2)\n",
    "#         #g2 = loc_sample0 #Tensor(loc_sample) #.detach()\n",
    "\n",
    "\n",
    "        \n",
    "#         loss2_, pf1_, Qf1_, Qf12_, data_fit_, Q21_ = likelihood.get_ell(agg_data_cur,f_target,x0, g_theta1_cur, model_16, likelihood, noise_value, g2)\n",
    "#         zz[ii, kk] = loss2_\n",
    "#     kk = kk+1\n",
    "# zz = zz.reshape(n,n, 4)\n",
    "# torch.save(zz.detach(), 'data_plots/zz_success.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot = [1, 5, 10, 20]\n",
    "\n",
    "# for jj in range(4):\n",
    "#     fig, ax = plt.subplots(figsize = (16,14))\n",
    "#     #\n",
    "#     cs = ax.contour(xv_plot, yv_plot,  zz[:,:,jj].detach(), np.linspace( zz[:,:,jj].detach().numpy().min(), zz[:,:,jj].detach().numpy().max(), 1000), cmap = 'jet')\n",
    "#     cbar = fig.colorbar(cs, ax = ax,format=OOMFormatter(0, mathText=False));\n",
    "#     ax.plot(vf.tgt_loc[0],vf.tgt_loc[1], 'o', color = 'magenta', markersize=12)\n",
    "#     kk = plot[jj]\n",
    "#     if kk < plot[3]:\n",
    "#         ax.plot(vec_x[kk,0], vec_x[kk,1],'o', color = 'black',markersize=12)\n",
    "#     if kk == plot[3]:\n",
    "#         ax.plot(vec_x[- 1,0], vec_x[- 1,1],'o', color = 'black',markersize=12)\n",
    "#     ax.set_title('TAD Acquisition Function (Iteration '+str(kk)+')', fontsize = 40)\n",
    "#     ax.set_xlabel('$d_1$')\n",
    "#     ax.set_ylabel('$d_2$')\n",
    "    \n",
    "#     plt.savefig('figures_Carlo/tad_obj'+str(kk)+'.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p21_vec_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
