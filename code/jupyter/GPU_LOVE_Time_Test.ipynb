{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU vs. CPU Running Time Test\n",
    "\n",
    "We investigate the running time of GPytorch on CPU and GPU for training (150 iterations) and computing predictive distributions. For predictive distribution we also investigate the effect of LOVE approximation, so the scenarios are:\n",
    "- CPU Exact\n",
    "- GPU Exact\n",
    "- CPU with LOVE without cache\n",
    "- GPU with LOVE without cache\n",
    "- CPU with LOVE with cache\n",
    "- GPU with LOVE with cache\n",
    "\n",
    "We use a customized Multitask Kernel with variable number of Kronecker Product, the same Kernel as used in Targeted Adaptive Design. We use 4 dimensional input and output.\n",
    "\n",
    "The training and testing data are generated using sin and cos functions, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "# from Data_Gen_Script import VField\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from gpytorch.kernels import MultitaskKernel\n",
    "from gpytorch.constraints import Positive\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TensorProductKernel(MultitaskKernel):\n",
    "    \"\"\"\n",
    "    Class to get the tensorproduct kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_covar_module,  num_tasks, rank=1, pos_constraint = None, tri_constaint = None, task_covar_prior=None, **kwargs):\n",
    "        super().__init__(data_covar_module, num_tasks, rank, task_covar_prior = None, **kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, add_jitter = False, **params):\n",
    "        if last_dim_is_batch:\n",
    "            raise RuntimeError(\"MultitaskKernel does not accept the last_dim_is_batch argument.\")\n",
    "        covar_i = self.task_covar_module.covar_matrix #.evaluate()\n",
    "            \n",
    "        covar_i = covar_i.evaluate()\n",
    "        if len(x1.shape[:-2]):\n",
    "            covar_i = covar_i.repeat(*x1.shape[:-2], 1, 1)\n",
    "        covar_x = gpytorch.lazy.lazify(self.data_covar_module.forward(x1, x2, **params))#(self.data_covar_module.forward(x1, x2, **params))#\n",
    "        if (add_jitter == True):\n",
    "            covar_x = covar_x #+ (1e-6) * torch.eye(covar_x.shape[0])\n",
    "        res=gpytorch.lazy.KroneckerProductLazyTensor(covar_x, covar_i) #gpytorch.lazy.lazify(torch.kron(covar_x, covar_i))\n",
    "\n",
    "        return res.diag() if diag else res\n",
    "        \n",
    "        \n",
    "from copy import deepcopy\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from gpytorch.priors import Prior\n",
    "from gpytorch.kernels import Kernel\n",
    "from gpytorch.kernels import IndexKernel\n",
    "from gpytorch.constraints import Positive\n",
    "\n",
    "# This is the main Kernel to use\n",
    "\n",
    "class SepTensorProductKernel(Kernel):\n",
    "    \"\"\"\n",
    "    Class to get the tensorproduct kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_kernels: List, num_tasks: int, rank: Union[int, List] = 1, \n",
    "        task_covar_prior: Optional[Prior] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_kernels (:type: list of `Kernel` objects): A list of base kernels.\n",
    "            num_tasks (int): The number of output tasks to fit.\n",
    "            rank (int): Rank of index kernel to use for task covariance matrix for each\n",
    "                        of the base kernels.\n",
    "            task_covar_prior (:obj:`gpytorch.priors.Prior`): Prior to use for each\n",
    "                task kernel. See :class:`gpytorch.kernels.IndexKernel` for details.\n",
    "        \"\"\"\n",
    "        if len(base_kernels) < 1:\n",
    "            raise ValueError(\"At least one base kernel must be provided.\")\n",
    "        for k in base_kernels:\n",
    "            if not isinstance(k, Kernel):\n",
    "                raise ValueError(\"base_kernels must only contain Kernel objects\")\n",
    "        if not isinstance(rank, list):\n",
    "            rank = [rank] * len(base_kernels)\n",
    "\n",
    "        super(SepTensorProductKernel, self).__init__()\n",
    "        self.covar_module_list = ModuleList(\n",
    "            [\n",
    "                TensorProductKernel(base_kernel, num_tasks=num_tasks, rank=r, task_covar_prior=task_covar_prior)\n",
    "                for base_kernel, r in zip(base_kernels, rank)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        res = self.covar_module_list[0].forward(x1, x2, **params)\n",
    "        for m in self.covar_module_list[1:]:\n",
    "            res += m.forward(x1, x2, **params)\n",
    "        return res\n",
    "\n",
    "    def num_outputs_per_input(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Given `n` data points `x1` and `m` datapoints `x2`, this multitask kernel\n",
    "        returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.\n",
    "        \"\"\"\n",
    "        return self.covar_module_list[0].num_outputs_per_input(x1, x2)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        new_kernel = deepcopy(self)\n",
    "        new_kernel.covar_module_list = ModuleList(\n",
    "            [base_kernel.__getitem__(index) for base_kernel in self.covar_module_list]\n",
    "        )\n",
    "        return new_kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Initialization\n",
    "\"\"\"\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "              gpytorch.means.ConstantMean(), num_tasks=Dval\n",
    "        )\n",
    "        \n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) \n",
    "            #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = SepTensorProductKernel(base_kernels,num_tasks = Dval)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "num_base_kernels = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_training_time = []\n",
    "cpu_exact_meancovar = []\n",
    "cpu_love_meancovar = []\n",
    "cpu_love_meancovar_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 100\n",
      "Iter 1/150 - Loss: 1.217\n",
      "Iter 2/150 - Loss: 1.194\n",
      "Iter 3/150 - Loss: 1.172\n",
      "Iter 4/150 - Loss: 1.150\n",
      "Iter 5/150 - Loss: 1.128\n",
      "Iter 6/150 - Loss: 1.106\n",
      "Iter 7/150 - Loss: 1.084\n",
      "Iter 8/150 - Loss: 1.063\n",
      "Iter 9/150 - Loss: 1.042\n",
      "Iter 10/150 - Loss: 1.021\n",
      "Iter 11/150 - Loss: 1.000\n",
      "Iter 12/150 - Loss: 0.980\n",
      "Iter 13/150 - Loss: 0.959\n",
      "Iter 14/150 - Loss: 0.938\n",
      "Iter 15/150 - Loss: 0.917\n",
      "Iter 16/150 - Loss: 0.896\n",
      "Iter 17/150 - Loss: 0.875\n",
      "Iter 18/150 - Loss: 0.853\n",
      "Iter 19/150 - Loss: 0.831\n",
      "Iter 20/150 - Loss: 0.808\n",
      "Iter 21/150 - Loss: 0.786\n",
      "Iter 22/150 - Loss: 0.763\n",
      "Iter 23/150 - Loss: 0.741\n",
      "Iter 24/150 - Loss: 0.718\n",
      "Iter 25/150 - Loss: 0.696\n",
      "Iter 26/150 - Loss: 0.674\n",
      "Iter 27/150 - Loss: 0.652\n",
      "Iter 28/150 - Loss: 0.629\n",
      "Iter 29/150 - Loss: 0.607\n",
      "Iter 30/150 - Loss: 0.584\n",
      "Iter 31/150 - Loss: 0.561\n",
      "Iter 32/150 - Loss: 0.538\n",
      "Iter 33/150 - Loss: 0.515\n",
      "Iter 34/150 - Loss: 0.491\n",
      "Iter 35/150 - Loss: 0.467\n",
      "Iter 36/150 - Loss: 0.444\n",
      "Iter 37/150 - Loss: 0.420\n",
      "Iter 38/150 - Loss: 0.396\n",
      "Iter 39/150 - Loss: 0.372\n",
      "Iter 40/150 - Loss: 0.348\n",
      "Iter 41/150 - Loss: 0.324\n",
      "Iter 42/150 - Loss: 0.300\n",
      "Iter 43/150 - Loss: 0.275\n",
      "Iter 44/150 - Loss: 0.251\n",
      "Iter 45/150 - Loss: 0.226\n",
      "Iter 46/150 - Loss: 0.202\n",
      "Iter 47/150 - Loss: 0.177\n",
      "Iter 48/150 - Loss: 0.152\n",
      "Iter 49/150 - Loss: 0.128\n",
      "Iter 50/150 - Loss: 0.103\n",
      "Iter 51/150 - Loss: 0.078\n",
      "Iter 52/150 - Loss: 0.053\n",
      "Iter 53/150 - Loss: 0.028\n",
      "Iter 54/150 - Loss: 0.003\n",
      "Iter 55/150 - Loss: -0.022\n",
      "Iter 56/150 - Loss: -0.047\n",
      "Iter 57/150 - Loss: -0.072\n",
      "Iter 58/150 - Loss: -0.097\n",
      "Iter 59/150 - Loss: -0.122\n",
      "Iter 60/150 - Loss: -0.147\n",
      "Iter 61/150 - Loss: -0.172\n",
      "Iter 62/150 - Loss: -0.197\n",
      "Iter 63/150 - Loss: -0.223\n",
      "Iter 64/150 - Loss: -0.248\n",
      "Iter 65/150 - Loss: -0.273\n",
      "Iter 66/150 - Loss: -0.298\n",
      "Iter 67/150 - Loss: -0.323\n",
      "Iter 68/150 - Loss: -0.348\n",
      "Iter 69/150 - Loss: -0.374\n",
      "Iter 70/150 - Loss: -0.399\n",
      "Iter 71/150 - Loss: -0.424\n",
      "Iter 72/150 - Loss: -0.449\n",
      "Iter 73/150 - Loss: -0.474\n",
      "Iter 74/150 - Loss: -0.499\n",
      "Iter 75/150 - Loss: -0.524\n",
      "Iter 76/150 - Loss: -0.550\n",
      "Iter 77/150 - Loss: -0.575\n",
      "Iter 78/150 - Loss: -0.600\n",
      "Iter 79/150 - Loss: -0.625\n",
      "Iter 80/150 - Loss: -0.650\n",
      "Iter 81/150 - Loss: -0.675\n",
      "Iter 82/150 - Loss: -0.700\n",
      "Iter 83/150 - Loss: -0.724\n",
      "Iter 84/150 - Loss: -0.749\n",
      "Iter 85/150 - Loss: -0.774\n",
      "Iter 86/150 - Loss: -0.799\n",
      "Iter 87/150 - Loss: -0.824\n",
      "Iter 88/150 - Loss: -0.849\n",
      "Iter 89/150 - Loss: -0.873\n",
      "Iter 90/150 - Loss: -0.898\n",
      "Iter 91/150 - Loss: -0.922\n",
      "Iter 92/150 - Loss: -0.947\n",
      "Iter 93/150 - Loss: -0.971\n",
      "Iter 94/150 - Loss: -0.996\n",
      "Iter 95/150 - Loss: -1.020\n",
      "Iter 96/150 - Loss: -1.045\n",
      "Iter 97/150 - Loss: -1.069\n",
      "Iter 98/150 - Loss: -1.093\n",
      "Iter 99/150 - Loss: -1.117\n",
      "Iter 100/150 - Loss: -1.141\n",
      "Iter 101/150 - Loss: -1.165\n",
      "Iter 102/150 - Loss: -1.189\n",
      "Iter 103/150 - Loss: -1.213\n",
      "Iter 104/150 - Loss: -1.236\n",
      "Iter 105/150 - Loss: -1.260\n",
      "Iter 106/150 - Loss: -1.284\n",
      "Iter 107/150 - Loss: -1.307\n",
      "Iter 108/150 - Loss: -1.331\n",
      "Iter 109/150 - Loss: -1.354\n",
      "Iter 110/150 - Loss: -1.377\n",
      "Iter 111/150 - Loss: -1.400\n",
      "Iter 112/150 - Loss: -1.423\n",
      "Iter 113/150 - Loss: -1.446\n",
      "Iter 114/150 - Loss: -1.469\n",
      "Iter 115/150 - Loss: -1.491\n",
      "Iter 116/150 - Loss: -1.514\n",
      "Iter 117/150 - Loss: -1.536\n",
      "Iter 118/150 - Loss: -1.558\n",
      "Iter 119/150 - Loss: -1.580\n",
      "Iter 120/150 - Loss: -1.602\n",
      "Iter 121/150 - Loss: -1.624\n",
      "Iter 122/150 - Loss: -1.645\n",
      "Iter 123/150 - Loss: -1.667\n",
      "Iter 124/150 - Loss: -1.688\n",
      "Iter 125/150 - Loss: -1.709\n",
      "Iter 126/150 - Loss: -1.730\n",
      "Iter 127/150 - Loss: -1.751\n",
      "Iter 128/150 - Loss: -1.771\n",
      "Iter 129/150 - Loss: -1.792\n",
      "Iter 130/150 - Loss: -1.812\n",
      "Iter 131/150 - Loss: -1.832\n",
      "Iter 132/150 - Loss: -1.852\n",
      "Iter 133/150 - Loss: -1.871\n",
      "Iter 134/150 - Loss: -1.891\n",
      "Iter 135/150 - Loss: -1.910\n",
      "Iter 136/150 - Loss: -1.929\n",
      "Iter 137/150 - Loss: -1.948\n",
      "Iter 138/150 - Loss: -1.966\n",
      "Iter 139/150 - Loss: -1.985\n",
      "Iter 140/150 - Loss: -2.003\n",
      "Iter 141/150 - Loss: -2.021\n",
      "Iter 142/150 - Loss: -2.038\n",
      "Iter 143/150 - Loss: -2.055\n",
      "Iter 144/150 - Loss: -2.073\n",
      "Iter 145/150 - Loss: -2.090\n",
      "Iter 146/150 - Loss: -2.107\n",
      "Iter 147/150 - Loss: -2.123\n",
      "Iter 148/150 - Loss: -2.139\n",
      "Iter 149/150 - Loss: -2.154\n",
      "Iter 150/150 - Loss: -2.170\n",
      "\n",
      "data size: 300\n",
      "Iter 1/150 - Loss: 1.142\n",
      "Iter 2/150 - Loss: 1.119\n",
      "Iter 3/150 - Loss: 1.110\n",
      "Iter 4/150 - Loss: 1.083\n",
      "Iter 5/150 - Loss: 1.063\n",
      "Iter 6/150 - Loss: 1.048\n",
      "Iter 7/150 - Loss: 1.022\n",
      "Iter 8/150 - Loss: 1.002\n",
      "Iter 9/150 - Loss: 0.989\n",
      "Iter 10/150 - Loss: 0.959\n",
      "Iter 11/150 - Loss: 0.937\n",
      "Iter 12/150 - Loss: 0.925\n",
      "Iter 13/150 - Loss: 0.905\n",
      "Iter 14/150 - Loss: 0.881\n",
      "Iter 15/150 - Loss: 0.879\n",
      "Iter 16/150 - Loss: 0.845\n",
      "Iter 17/150 - Loss: 0.817\n",
      "Iter 18/150 - Loss: 0.795\n",
      "Iter 19/150 - Loss: 0.774\n",
      "Iter 20/150 - Loss: 0.756\n",
      "Iter 21/150 - Loss: 0.726\n",
      "Iter 22/150 - Loss: 0.710\n",
      "Iter 23/150 - Loss: 0.687\n",
      "Iter 24/150 - Loss: 0.669\n",
      "Iter 25/150 - Loss: 0.644\n",
      "Iter 26/150 - Loss: 0.616\n",
      "Iter 27/150 - Loss: 0.593\n",
      "Iter 28/150 - Loss: 0.576\n",
      "Iter 29/150 - Loss: 0.555\n",
      "Iter 30/150 - Loss: 0.521\n",
      "Iter 31/150 - Loss: 0.492\n",
      "Iter 32/150 - Loss: 0.471\n",
      "Iter 33/150 - Loss: 0.457\n",
      "Iter 34/150 - Loss: 0.426\n",
      "Iter 35/150 - Loss: 0.402\n",
      "Iter 36/150 - Loss: 0.381\n",
      "Iter 37/150 - Loss: 0.366\n",
      "Iter 38/150 - Loss: 0.344\n",
      "Iter 39/150 - Loss: 0.320\n",
      "Iter 40/150 - Loss: 0.285\n",
      "Iter 41/150 - Loss: 0.260\n",
      "Iter 42/150 - Loss: 0.234\n",
      "Iter 43/150 - Loss: 0.214\n",
      "Iter 44/150 - Loss: 0.180\n",
      "Iter 45/150 - Loss: 0.162\n",
      "Iter 46/150 - Loss: 0.134\n",
      "Iter 47/150 - Loss: 0.109\n",
      "Iter 48/150 - Loss: 0.086\n",
      "Iter 49/150 - Loss: 0.059\n",
      "Iter 50/150 - Loss: 0.026\n",
      "Iter 51/150 - Loss: 0.011\n",
      "Iter 52/150 - Loss: -0.022\n",
      "Iter 53/150 - Loss: -0.044\n",
      "Iter 54/150 - Loss: -0.063\n",
      "Iter 55/150 - Loss: -0.101\n",
      "Iter 56/150 - Loss: -0.120\n",
      "Iter 57/150 - Loss: -0.146\n",
      "Iter 58/150 - Loss: -0.165\n",
      "Iter 59/150 - Loss: -0.200\n",
      "Iter 60/150 - Loss: -0.211\n",
      "Iter 61/150 - Loss: -0.241\n",
      "Iter 62/150 - Loss: -0.269\n",
      "Iter 63/150 - Loss: -0.295\n",
      "Iter 64/150 - Loss: -0.320\n",
      "Iter 65/150 - Loss: -0.353\n",
      "Iter 66/150 - Loss: -0.374\n",
      "Iter 67/150 - Loss: -0.404\n",
      "Iter 68/150 - Loss: -0.428\n",
      "Iter 69/150 - Loss: -0.454\n",
      "Iter 70/150 - Loss: -0.481\n",
      "Iter 71/150 - Loss: -0.510\n",
      "Iter 72/150 - Loss: -0.532\n",
      "Iter 73/150 - Loss: -0.544\n",
      "Iter 74/150 - Loss: -0.575\n",
      "Iter 75/150 - Loss: -0.611\n",
      "Iter 76/150 - Loss: -0.631\n",
      "Iter 77/150 - Loss: -0.668\n",
      "Iter 78/150 - Loss: -0.692\n",
      "Iter 79/150 - Loss: -0.706\n",
      "Iter 80/150 - Loss: -0.747\n",
      "Iter 81/150 - Loss: -0.763\n",
      "Iter 82/150 - Loss: -0.793\n",
      "Iter 83/150 - Loss: -0.799\n",
      "Iter 84/150 - Loss: -0.844\n",
      "Iter 85/150 - Loss: -0.864\n",
      "Iter 86/150 - Loss: -0.899\n",
      "Iter 87/150 - Loss: -0.911\n",
      "Iter 88/150 - Loss: -0.935\n",
      "Iter 89/150 - Loss: -0.949\n",
      "Iter 90/150 - Loss: -0.984\n",
      "Iter 91/150 - Loss: -1.011\n",
      "Iter 92/150 - Loss: -1.039\n",
      "Iter 93/150 - Loss: -1.065\n",
      "Iter 94/150 - Loss: -1.087\n",
      "Iter 95/150 - Loss: -1.115\n",
      "Iter 96/150 - Loss: -1.137\n",
      "Iter 97/150 - Loss: -1.153\n",
      "Iter 98/150 - Loss: -1.178\n",
      "Iter 99/150 - Loss: -1.212\n",
      "Iter 100/150 - Loss: -1.234\n",
      "Iter 101/150 - Loss: -1.262\n",
      "Iter 102/150 - Loss: -1.281\n",
      "Iter 103/150 - Loss: -1.288\n",
      "Iter 104/150 - Loss: -1.329\n",
      "Iter 105/150 - Loss: -1.339\n",
      "Iter 106/150 - Loss: -1.376\n",
      "Iter 107/150 - Loss: -1.401\n",
      "Iter 108/150 - Loss: -1.410\n",
      "Iter 109/150 - Loss: -1.444\n",
      "Iter 110/150 - Loss: -1.463\n",
      "Iter 111/150 - Loss: -1.490\n",
      "Iter 112/150 - Loss: -1.511\n",
      "Iter 113/150 - Loss: -1.544\n",
      "Iter 114/150 - Loss: -1.552\n",
      "Iter 115/150 - Loss: -1.591\n",
      "Iter 116/150 - Loss: -1.612\n",
      "Iter 117/150 - Loss: -1.623\n",
      "Iter 118/150 - Loss: -1.663\n",
      "Iter 119/150 - Loss: -1.652\n",
      "Iter 120/150 - Loss: -1.697\n",
      "Iter 121/150 - Loss: -1.702\n",
      "Iter 122/150 - Loss: -1.715\n",
      "Iter 123/150 - Loss: -1.732\n",
      "Iter 124/150 - Loss: -1.778\n",
      "Iter 125/150 - Loss: -1.767\n",
      "Iter 126/150 - Loss: -1.760\n",
      "Iter 127/150 - Loss: -1.785\n",
      "Iter 128/150 - Loss: -1.800\n",
      "Iter 129/150 - Loss: -1.817\n",
      "Iter 130/150 - Loss: -1.813\n",
      "Iter 131/150 - Loss: -1.802\n",
      "Iter 132/150 - Loss: -1.861\n",
      "Iter 133/150 - Loss: -1.860\n",
      "Iter 134/150 - Loss: -1.885\n",
      "Iter 135/150 - Loss: -1.915\n",
      "Iter 136/150 - Loss: -1.916\n",
      "Iter 137/150 - Loss: -1.932\n",
      "Iter 138/150 - Loss: -1.945\n",
      "Iter 139/150 - Loss: -1.958\n",
      "Iter 140/150 - Loss: -1.954\n",
      "Iter 141/150 - Loss: -1.959\n",
      "Iter 142/150 - Loss: -2.010\n",
      "Iter 143/150 - Loss: -2.034\n",
      "Iter 144/150 - Loss: -2.020\n",
      "Iter 145/150 - Loss: -1.993\n",
      "Iter 146/150 - Loss: -2.041\n",
      "Iter 147/150 - Loss: -2.060\n",
      "Iter 148/150 - Loss: -2.095\n",
      "Iter 149/150 - Loss: -2.097\n",
      "Iter 150/150 - Loss: -2.115\n",
      "\n",
      "data size: 500\n",
      "Iter 1/150 - Loss: 1.135\n",
      "Iter 2/150 - Loss: 1.106\n",
      "Iter 3/150 - Loss: 1.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4/150 - Loss: 1.066\n",
      "Iter 5/150 - Loss: 1.045\n",
      "Iter 6/150 - Loss: 1.022\n",
      "Iter 7/150 - Loss: 1.006\n",
      "Iter 8/150 - Loss: 0.991\n",
      "Iter 9/150 - Loss: 0.966\n",
      "Iter 10/150 - Loss: 0.951\n",
      "Iter 11/150 - Loss: 0.929\n",
      "Iter 12/150 - Loss: 0.904\n",
      "Iter 13/150 - Loss: 0.885\n",
      "Iter 14/150 - Loss: 0.861\n",
      "Iter 15/150 - Loss: 0.850\n",
      "Iter 16/150 - Loss: 0.822\n",
      "Iter 17/150 - Loss: 0.805\n",
      "Iter 18/150 - Loss: 0.781\n",
      "Iter 19/150 - Loss: 0.760\n",
      "Iter 20/150 - Loss: 0.735\n",
      "Iter 21/150 - Loss: 0.716\n",
      "Iter 22/150 - Loss: 0.689\n",
      "Iter 23/150 - Loss: 0.669\n",
      "Iter 24/150 - Loss: 0.653\n",
      "Iter 25/150 - Loss: 0.624\n",
      "Iter 26/150 - Loss: 0.600\n",
      "Iter 27/150 - Loss: 0.579\n",
      "Iter 28/150 - Loss: 0.558\n",
      "Iter 29/150 - Loss: 0.529\n",
      "Iter 30/150 - Loss: 0.506\n",
      "Iter 31/150 - Loss: 0.483\n",
      "Iter 32/150 - Loss: 0.460\n",
      "Iter 33/150 - Loss: 0.438\n",
      "Iter 34/150 - Loss: 0.408\n",
      "Iter 35/150 - Loss: 0.388\n",
      "Iter 36/150 - Loss: 0.364\n",
      "Iter 37/150 - Loss: 0.340\n",
      "Iter 38/150 - Loss: 0.313\n",
      "Iter 39/150 - Loss: 0.295\n",
      "Iter 40/150 - Loss: 0.263\n",
      "Iter 41/150 - Loss: 0.241\n",
      "Iter 42/150 - Loss: 0.215\n",
      "Iter 43/150 - Loss: 0.190\n",
      "Iter 44/150 - Loss: 0.167\n",
      "Iter 45/150 - Loss: 0.146\n",
      "Iter 46/150 - Loss: 0.114\n",
      "Iter 47/150 - Loss: 0.093\n",
      "Iter 48/150 - Loss: 0.072\n",
      "Iter 49/150 - Loss: 0.037\n",
      "Iter 50/150 - Loss: 0.013\n",
      "Iter 51/150 - Loss: -0.008\n",
      "Iter 52/150 - Loss: -0.036\n",
      "Iter 53/150 - Loss: -0.063\n",
      "Iter 54/150 - Loss: -0.086\n",
      "Iter 55/150 - Loss: -0.115\n",
      "Iter 56/150 - Loss: -0.140\n",
      "Iter 57/150 - Loss: -0.167\n",
      "Iter 58/150 - Loss: -0.188\n",
      "Iter 59/150 - Loss: -0.215\n",
      "Iter 60/150 - Loss: -0.244\n",
      "Iter 61/150 - Loss: -0.266\n",
      "Iter 62/150 - Loss: -0.294\n",
      "Iter 63/150 - Loss: -0.325\n",
      "Iter 64/150 - Loss: -0.346\n",
      "Iter 65/150 - Loss: -0.378\n",
      "Iter 66/150 - Loss: -0.403\n",
      "Iter 67/150 - Loss: -0.427\n",
      "Iter 68/150 - Loss: -0.453\n",
      "Iter 69/150 - Loss: -0.471\n",
      "Iter 70/150 - Loss: -0.508\n",
      "Iter 71/150 - Loss: -0.530\n",
      "Iter 72/150 - Loss: -0.547\n",
      "Iter 73/150 - Loss: -0.579\n",
      "Iter 74/150 - Loss: -0.598\n",
      "Iter 75/150 - Loss: -0.631\n",
      "Iter 76/150 - Loss: -0.659\n",
      "Iter 77/150 - Loss: -0.684\n",
      "Iter 78/150 - Loss: -0.702\n",
      "Iter 79/150 - Loss: -0.731\n",
      "Iter 80/150 - Loss: -0.760\n",
      "Iter 81/150 - Loss: -0.780\n",
      "Iter 82/150 - Loss: -0.815\n",
      "Iter 83/150 - Loss: -0.835\n",
      "Iter 84/150 - Loss: -0.865\n",
      "Iter 85/150 - Loss: -0.883\n",
      "Iter 86/150 - Loss: -0.908\n",
      "Iter 87/150 - Loss: -0.928\n",
      "Iter 88/150 - Loss: -0.946\n",
      "Iter 89/150 - Loss: -0.979\n",
      "Iter 90/150 - Loss: -0.999\n",
      "Iter 91/150 - Loss: -1.024\n",
      "Iter 92/150 - Loss: -1.053\n",
      "Iter 93/150 - Loss: -1.063\n",
      "Iter 94/150 - Loss: -1.079\n",
      "Iter 95/150 - Loss: -1.113\n",
      "Iter 96/150 - Loss: -1.130\n",
      "Iter 97/150 - Loss: -1.165\n",
      "Iter 98/150 - Loss: -1.182\n",
      "Iter 99/150 - Loss: -1.200\n",
      "Iter 100/150 - Loss: -1.228\n",
      "Iter 101/150 - Loss: -1.244\n",
      "Iter 102/150 - Loss: -1.263\n",
      "Iter 103/150 - Loss: -1.273\n",
      "Iter 104/150 - Loss: -1.284\n",
      "Iter 105/150 - Loss: -1.309\n",
      "Iter 106/150 - Loss: -1.337\n",
      "Iter 107/150 - Loss: -1.326\n",
      "Iter 108/150 - Loss: -1.383\n",
      "Iter 109/150 - Loss: -1.404\n",
      "Iter 110/150 - Loss: -1.396\n",
      "Iter 111/150 - Loss: -1.449\n",
      "Iter 112/150 - Loss: -1.456\n",
      "Iter 113/150 - Loss: -1.469\n",
      "Iter 114/150 - Loss: -1.500\n",
      "Iter 115/150 - Loss: -1.480\n",
      "Iter 116/150 - Loss: -1.512\n",
      "Iter 117/150 - Loss: -1.533\n",
      "Iter 118/150 - Loss: -1.532\n",
      "Iter 119/150 - Loss: -1.617\n",
      "Iter 120/150 - Loss: -1.621\n",
      "Iter 121/150 - Loss: -1.644\n",
      "Iter 122/150 - Loss: -1.666\n",
      "Iter 123/150 - Loss: -1.660\n",
      "Iter 124/150 - Loss: -1.696\n",
      "Iter 125/150 - Loss: -1.685\n",
      "Iter 126/150 - Loss: -1.786\n",
      "Iter 127/150 - Loss: -1.726\n",
      "Iter 128/150 - Loss: -1.784\n",
      "Iter 129/150 - Loss: -1.759\n",
      "Iter 130/150 - Loss: -1.782\n",
      "Iter 131/150 - Loss: -1.841\n",
      "Iter 132/150 - Loss: -1.870\n",
      "Iter 133/150 - Loss: -1.814\n",
      "Iter 134/150 - Loss: -1.896\n",
      "Iter 135/150 - Loss: -1.917\n",
      "Iter 136/150 - Loss: -1.978\n",
      "Iter 137/150 - Loss: -1.953\n",
      "Iter 138/150 - Loss: -2.015\n",
      "Iter 139/150 - Loss: -1.987\n",
      "Iter 140/150 - Loss: -1.991\n",
      "Iter 141/150 - Loss: -1.972\n",
      "Iter 142/150 - Loss: -2.002\n",
      "Iter 143/150 - Loss: -1.988\n",
      "Iter 144/150 - Loss: -2.069\n",
      "Iter 145/150 - Loss: -2.100\n",
      "Iter 146/150 - Loss: -2.096\n",
      "Iter 147/150 - Loss: -2.135\n",
      "Iter 148/150 - Loss: -2.165\n",
      "Iter 149/150 - Loss: -2.151\n",
      "Iter 150/150 - Loss: -2.176\n",
      "\n",
      "data size: 700\n",
      "Iter 1/150 - Loss: 1.114\n",
      "Iter 2/150 - Loss: 1.107\n",
      "Iter 3/150 - Loss: 1.089\n",
      "Iter 4/150 - Loss: 1.072\n",
      "Iter 5/150 - Loss: 1.055\n",
      "Iter 6/150 - Loss: 1.038\n",
      "Iter 7/150 - Loss: 1.018\n",
      "Iter 8/150 - Loss: 1.000\n",
      "Iter 9/150 - Loss: 0.981\n",
      "Iter 10/150 - Loss: 0.963\n",
      "Iter 11/150 - Loss: 0.943\n",
      "Iter 12/150 - Loss: 0.923\n",
      "Iter 13/150 - Loss: 0.904\n",
      "Iter 14/150 - Loss: 0.884\n",
      "Iter 15/150 - Loss: 0.864\n",
      "Iter 16/150 - Loss: 0.844\n",
      "Iter 17/150 - Loss: 0.824\n",
      "Iter 18/150 - Loss: 0.803\n",
      "Iter 19/150 - Loss: 0.782\n",
      "Iter 20/150 - Loss: 0.761\n",
      "Iter 21/150 - Loss: 0.740\n",
      "Iter 22/150 - Loss: 0.718\n",
      "Iter 23/150 - Loss: 0.696\n",
      "Iter 24/150 - Loss: 0.674\n",
      "Iter 25/150 - Loss: 0.652\n",
      "Iter 26/150 - Loss: 0.630\n",
      "Iter 27/150 - Loss: 0.607\n",
      "Iter 28/150 - Loss: 0.586\n",
      "Iter 29/150 - Loss: 0.563\n",
      "Iter 30/150 - Loss: 0.540\n",
      "Iter 31/150 - Loss: 0.517\n",
      "Iter 32/150 - Loss: 0.494\n",
      "Iter 33/150 - Loss: 0.470\n",
      "Iter 34/150 - Loss: 0.447\n",
      "Iter 35/150 - Loss: 0.422\n",
      "Iter 36/150 - Loss: 0.399\n",
      "Iter 37/150 - Loss: 0.375\n",
      "Iter 38/150 - Loss: 0.352\n",
      "Iter 39/150 - Loss: 0.328\n",
      "Iter 40/150 - Loss: 0.302\n",
      "Iter 41/150 - Loss: 0.277\n",
      "Iter 42/150 - Loss: 0.254\n",
      "Iter 43/150 - Loss: 0.229\n",
      "Iter 44/150 - Loss: 0.204\n",
      "Iter 45/150 - Loss: 0.178\n",
      "Iter 46/150 - Loss: 0.154\n",
      "Iter 47/150 - Loss: 0.128\n",
      "Iter 48/150 - Loss: 0.104\n",
      "Iter 49/150 - Loss: 0.079\n",
      "Iter 50/150 - Loss: 0.053\n",
      "Iter 51/150 - Loss: 0.028\n",
      "Iter 52/150 - Loss: 0.003\n",
      "Iter 53/150 - Loss: -0.024\n",
      "Iter 54/150 - Loss: -0.049\n",
      "Iter 55/150 - Loss: -0.075\n",
      "Iter 56/150 - Loss: -0.101\n",
      "Iter 57/150 - Loss: -0.127\n",
      "Iter 58/150 - Loss: -0.151\n",
      "Iter 59/150 - Loss: -0.177\n",
      "Iter 60/150 - Loss: -0.203\n",
      "Iter 61/150 - Loss: -0.229\n",
      "Iter 62/150 - Loss: -0.256\n",
      "Iter 63/150 - Loss: -0.282\n",
      "Iter 64/150 - Loss: -0.309\n",
      "Iter 65/150 - Loss: -0.335\n",
      "Iter 66/150 - Loss: -0.360\n",
      "Iter 67/150 - Loss: -0.386\n",
      "Iter 68/150 - Loss: -0.412\n",
      "Iter 69/150 - Loss: -0.439\n",
      "Iter 70/150 - Loss: -0.464\n",
      "Iter 71/150 - Loss: -0.492\n",
      "Iter 72/150 - Loss: -0.518\n",
      "Iter 73/150 - Loss: -0.543\n",
      "Iter 74/150 - Loss: -0.569\n",
      "Iter 75/150 - Loss: -0.596\n",
      "Iter 76/150 - Loss: -0.621\n",
      "Iter 77/150 - Loss: -0.649\n",
      "Iter 78/150 - Loss: -0.674\n",
      "Iter 79/150 - Loss: -0.700\n",
      "Iter 80/150 - Loss: -0.726\n",
      "Iter 81/150 - Loss: -0.752\n",
      "Iter 82/150 - Loss: -0.778\n",
      "Iter 83/150 - Loss: -0.805\n",
      "Iter 84/150 - Loss: -0.830\n",
      "Iter 85/150 - Loss: -0.858\n",
      "Iter 86/150 - Loss: -0.884\n",
      "Iter 87/150 - Loss: -0.907\n",
      "Iter 88/150 - Loss: -0.935\n",
      "Iter 89/150 - Loss: -0.960\n",
      "Iter 90/150 - Loss: -0.987\n",
      "Iter 91/150 - Loss: -1.011\n",
      "Iter 92/150 - Loss: -1.037\n",
      "Iter 93/150 - Loss: -1.063\n",
      "Iter 94/150 - Loss: -1.089\n",
      "Iter 95/150 - Loss: -1.114\n",
      "Iter 96/150 - Loss: -1.140\n",
      "Iter 97/150 - Loss: -1.165\n",
      "Iter 98/150 - Loss: -1.192\n",
      "Iter 99/150 - Loss: -1.217\n",
      "Iter 100/150 - Loss: -1.242\n",
      "Iter 101/150 - Loss: -1.267\n",
      "Iter 102/150 - Loss: -1.293\n",
      "Iter 103/150 - Loss: -1.317\n",
      "Iter 104/150 - Loss: -1.343\n",
      "Iter 105/150 - Loss: -1.369\n",
      "Iter 106/150 - Loss: -1.394\n",
      "Iter 107/150 - Loss: -1.418\n",
      "Iter 108/150 - Loss: -1.442\n",
      "Iter 109/150 - Loss: -1.468\n",
      "Iter 110/150 - Loss: -1.493\n",
      "Iter 111/150 - Loss: -1.515\n",
      "Iter 112/150 - Loss: -1.542\n",
      "Iter 113/150 - Loss: -1.565\n",
      "Iter 114/150 - Loss: -1.589\n",
      "Iter 115/150 - Loss: -1.615\n",
      "Iter 116/150 - Loss: -1.637\n",
      "Iter 117/150 - Loss: -1.662\n",
      "Iter 118/150 - Loss: -1.684\n",
      "Iter 119/150 - Loss: -1.709\n",
      "Iter 120/150 - Loss: -1.732\n",
      "Iter 121/150 - Loss: -1.757\n",
      "Iter 122/150 - Loss: -1.779\n",
      "Iter 123/150 - Loss: -1.801\n",
      "Iter 124/150 - Loss: -1.826\n",
      "Iter 125/150 - Loss: -1.851\n",
      "Iter 126/150 - Loss: -1.871\n",
      "Iter 127/150 - Loss: -1.894\n",
      "Iter 128/150 - Loss: -1.917\n",
      "Iter 129/150 - Loss: -1.940\n",
      "Iter 130/150 - Loss: -1.962\n",
      "Iter 131/150 - Loss: -1.985\n",
      "Iter 132/150 - Loss: -2.006\n",
      "Iter 133/150 - Loss: -2.025\n",
      "Iter 134/150 - Loss: -2.046\n",
      "Iter 135/150 - Loss: -2.067\n",
      "Iter 136/150 - Loss: -2.090\n",
      "Iter 137/150 - Loss: -2.111\n",
      "Iter 138/150 - Loss: -2.129\n",
      "Iter 139/150 - Loss: -2.150\n",
      "Iter 140/150 - Loss: -2.167\n",
      "Iter 141/150 - Loss: -2.188\n",
      "Iter 142/150 - Loss: -2.207\n",
      "Iter 143/150 - Loss: -2.228\n",
      "Iter 144/150 - Loss: -2.243\n",
      "Iter 145/150 - Loss: -2.263\n",
      "Iter 146/150 - Loss: -2.286\n",
      "Iter 147/150 - Loss: -2.302\n",
      "Iter 148/150 - Loss: -2.320\n",
      "Iter 149/150 - Loss: -2.336\n",
      "Iter 150/150 - Loss: -2.354\n",
      "\n",
      "data size: 1000\n",
      "Iter 1/150 - Loss: 1.105\n",
      "Iter 2/150 - Loss: 1.100\n",
      "Iter 3/150 - Loss: 1.084\n",
      "Iter 4/150 - Loss: 1.067\n",
      "Iter 5/150 - Loss: 1.048\n",
      "Iter 6/150 - Loss: 1.029\n",
      "Iter 7/150 - Loss: 1.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8/150 - Loss: 0.995\n",
      "Iter 9/150 - Loss: 0.975\n",
      "Iter 10/150 - Loss: 0.956\n",
      "Iter 11/150 - Loss: 0.937\n",
      "Iter 12/150 - Loss: 0.917\n",
      "Iter 13/150 - Loss: 0.898\n",
      "Iter 14/150 - Loss: 0.877\n",
      "Iter 15/150 - Loss: 0.857\n",
      "Iter 16/150 - Loss: 0.837\n",
      "Iter 17/150 - Loss: 0.818\n",
      "Iter 18/150 - Loss: 0.796\n",
      "Iter 19/150 - Loss: 0.775\n",
      "Iter 20/150 - Loss: 0.754\n",
      "Iter 21/150 - Loss: 0.733\n",
      "Iter 22/150 - Loss: 0.712\n",
      "Iter 23/150 - Loss: 0.689\n",
      "Iter 24/150 - Loss: 0.668\n",
      "Iter 25/150 - Loss: 0.646\n",
      "Iter 26/150 - Loss: 0.623\n",
      "Iter 27/150 - Loss: 0.601\n",
      "Iter 28/150 - Loss: 0.578\n",
      "Iter 29/150 - Loss: 0.555\n",
      "Iter 30/150 - Loss: 0.533\n",
      "Iter 31/150 - Loss: 0.509\n",
      "Iter 32/150 - Loss: 0.486\n",
      "Iter 33/150 - Loss: 0.463\n",
      "Iter 34/150 - Loss: 0.439\n",
      "Iter 35/150 - Loss: 0.415\n",
      "Iter 36/150 - Loss: 0.392\n",
      "Iter 37/150 - Loss: 0.368\n",
      "Iter 38/150 - Loss: 0.343\n",
      "Iter 39/150 - Loss: 0.319\n",
      "Iter 40/150 - Loss: 0.295\n",
      "Iter 41/150 - Loss: 0.271\n",
      "Iter 42/150 - Loss: 0.246\n",
      "Iter 43/150 - Loss: 0.221\n",
      "Iter 44/150 - Loss: 0.196\n",
      "Iter 45/150 - Loss: 0.171\n",
      "Iter 46/150 - Loss: 0.146\n",
      "Iter 47/150 - Loss: 0.121\n",
      "Iter 48/150 - Loss: 0.096\n",
      "Iter 49/150 - Loss: 0.071\n",
      "Iter 50/150 - Loss: 0.045\n",
      "Iter 51/150 - Loss: 0.020\n",
      "Iter 52/150 - Loss: -0.006\n",
      "Iter 53/150 - Loss: -0.032\n",
      "Iter 54/150 - Loss: -0.057\n",
      "Iter 55/150 - Loss: -0.083\n",
      "Iter 56/150 - Loss: -0.108\n",
      "Iter 57/150 - Loss: -0.135\n",
      "Iter 58/150 - Loss: -0.161\n",
      "Iter 59/150 - Loss: -0.187\n",
      "Iter 60/150 - Loss: -0.213\n",
      "Iter 61/150 - Loss: -0.238\n",
      "Iter 62/150 - Loss: -0.264\n",
      "Iter 63/150 - Loss: -0.291\n",
      "Iter 64/150 - Loss: -0.316\n",
      "Iter 65/150 - Loss: -0.343\n",
      "Iter 66/150 - Loss: -0.369\n",
      "Iter 67/150 - Loss: -0.396\n",
      "Iter 68/150 - Loss: -0.421\n",
      "Iter 69/150 - Loss: -0.447\n",
      "Iter 70/150 - Loss: -0.473\n",
      "Iter 71/150 - Loss: -0.500\n",
      "Iter 72/150 - Loss: -0.525\n",
      "Iter 73/150 - Loss: -0.551\n",
      "Iter 74/150 - Loss: -0.578\n",
      "Iter 75/150 - Loss: -0.604\n",
      "Iter 76/150 - Loss: -0.630\n",
      "Iter 77/150 - Loss: -0.656\n",
      "Iter 78/150 - Loss: -0.683\n",
      "Iter 79/150 - Loss: -0.709\n",
      "Iter 80/150 - Loss: -0.735\n",
      "Iter 81/150 - Loss: -0.761\n",
      "Iter 82/150 - Loss: -0.787\n",
      "Iter 83/150 - Loss: -0.813\n",
      "Iter 84/150 - Loss: -0.840\n",
      "Iter 85/150 - Loss: -0.866\n",
      "Iter 86/150 - Loss: -0.891\n",
      "Iter 87/150 - Loss: -0.918\n",
      "Iter 88/150 - Loss: -0.944\n",
      "Iter 89/150 - Loss: -0.970\n",
      "Iter 90/150 - Loss: -0.996\n",
      "Iter 91/150 - Loss: -1.020\n",
      "Iter 92/150 - Loss: -1.046\n",
      "Iter 93/150 - Loss: -1.072\n",
      "Iter 94/150 - Loss: -1.098\n",
      "Iter 95/150 - Loss: -1.125\n",
      "Iter 96/150 - Loss: -1.149\n",
      "Iter 97/150 - Loss: -1.175\n",
      "Iter 98/150 - Loss: -1.199\n",
      "Iter 99/150 - Loss: -1.224\n",
      "Iter 100/150 - Loss: -1.252\n",
      "Iter 101/150 - Loss: -1.278\n",
      "Iter 102/150 - Loss: -1.303\n",
      "Iter 103/150 - Loss: -1.327\n",
      "Iter 104/150 - Loss: -1.353\n",
      "Iter 105/150 - Loss: -1.379\n",
      "Iter 106/150 - Loss: -1.403\n",
      "Iter 107/150 - Loss: -1.429\n",
      "Iter 108/150 - Loss: -1.454\n",
      "Iter 109/150 - Loss: -1.478\n",
      "Iter 110/150 - Loss: -1.504\n",
      "Iter 111/150 - Loss: -1.528\n",
      "Iter 112/150 - Loss: -1.552\n",
      "Iter 113/150 - Loss: -1.577\n",
      "Iter 114/150 - Loss: -1.600\n",
      "Iter 115/150 - Loss: -1.626\n",
      "Iter 116/150 - Loss: -1.651\n",
      "Iter 117/150 - Loss: -1.675\n",
      "Iter 118/150 - Loss: -1.698\n",
      "Iter 119/150 - Loss: -1.721\n",
      "Iter 120/150 - Loss: -1.744\n",
      "Iter 121/150 - Loss: -1.768\n",
      "Iter 122/150 - Loss: -1.790\n",
      "Iter 123/150 - Loss: -1.816\n",
      "Iter 124/150 - Loss: -1.838\n",
      "Iter 125/150 - Loss: -1.861\n",
      "Iter 126/150 - Loss: -1.883\n",
      "Iter 127/150 - Loss: -1.906\n",
      "Iter 128/150 - Loss: -1.928\n",
      "Iter 129/150 - Loss: -1.950\n",
      "Iter 130/150 - Loss: -1.973\n",
      "Iter 131/150 - Loss: -1.994\n",
      "Iter 132/150 - Loss: -2.016\n",
      "Iter 133/150 - Loss: -2.039\n",
      "Iter 134/150 - Loss: -2.059\n",
      "Iter 135/150 - Loss: -2.081\n",
      "Iter 136/150 - Loss: -2.103\n",
      "Iter 137/150 - Loss: -2.124\n",
      "Iter 138/150 - Loss: -2.142\n",
      "Iter 139/150 - Loss: -2.162\n",
      "Iter 140/150 - Loss: -2.182\n",
      "Iter 141/150 - Loss: -2.200\n",
      "Iter 142/150 - Loss: -2.221\n",
      "Iter 143/150 - Loss: -2.239\n",
      "Iter 144/150 - Loss: -2.259\n",
      "Iter 145/150 - Loss: -2.275\n",
      "Iter 146/150 - Loss: -2.296\n",
      "Iter 147/150 - Loss: -2.313\n",
      "Iter 148/150 - Loss: -2.330\n",
      "Iter 149/150 - Loss: -2.346\n",
      "Iter 150/150 - Loss: -2.364\n",
      "\n",
      "data size: 1500\n",
      "Iter 1/150 - Loss: 1.099\n",
      "Iter 2/150 - Loss: 1.094\n",
      "Iter 3/150 - Loss: 1.077\n",
      "Iter 4/150 - Loss: 1.061\n",
      "Iter 5/150 - Loss: 1.042\n",
      "Iter 6/150 - Loss: 1.024\n",
      "Iter 7/150 - Loss: 1.006\n",
      "Iter 8/150 - Loss: 0.988\n",
      "Iter 9/150 - Loss: 0.969\n",
      "Iter 10/150 - Loss: 0.950\n",
      "Iter 11/150 - Loss: 0.932\n",
      "Iter 12/150 - Loss: 0.912\n",
      "Iter 13/150 - Loss: 0.893\n",
      "Iter 14/150 - Loss: 0.872\n",
      "Iter 15/150 - Loss: 0.852\n",
      "Iter 16/150 - Loss: 0.832\n",
      "Iter 17/150 - Loss: 0.811\n",
      "Iter 18/150 - Loss: 0.791\n",
      "Iter 19/150 - Loss: 0.770\n",
      "Iter 20/150 - Loss: 0.749\n",
      "Iter 21/150 - Loss: 0.727\n",
      "Iter 22/150 - Loss: 0.706\n",
      "Iter 23/150 - Loss: 0.685\n",
      "Iter 24/150 - Loss: 0.663\n",
      "Iter 25/150 - Loss: 0.641\n",
      "Iter 26/150 - Loss: 0.618\n",
      "Iter 27/150 - Loss: 0.596\n",
      "Iter 28/150 - Loss: 0.573\n",
      "Iter 29/150 - Loss: 0.551\n",
      "Iter 30/150 - Loss: 0.527\n",
      "Iter 31/150 - Loss: 0.504\n",
      "Iter 32/150 - Loss: 0.481\n",
      "Iter 33/150 - Loss: 0.458\n",
      "Iter 34/150 - Loss: 0.434\n",
      "Iter 35/150 - Loss: 0.410\n",
      "Iter 36/150 - Loss: 0.386\n",
      "Iter 37/150 - Loss: 0.362\n",
      "Iter 38/150 - Loss: 0.339\n",
      "Iter 39/150 - Loss: 0.314\n",
      "Iter 40/150 - Loss: 0.290\n",
      "Iter 41/150 - Loss: 0.265\n",
      "Iter 42/150 - Loss: 0.240\n",
      "Iter 43/150 - Loss: 0.216\n",
      "Iter 44/150 - Loss: 0.190\n",
      "Iter 45/150 - Loss: 0.166\n",
      "Iter 46/150 - Loss: 0.140\n",
      "Iter 47/150 - Loss: 0.116\n",
      "Iter 48/150 - Loss: 0.090\n",
      "Iter 49/150 - Loss: 0.065\n",
      "Iter 50/150 - Loss: 0.039\n",
      "Iter 51/150 - Loss: 0.014\n",
      "Iter 52/150 - Loss: -0.012\n",
      "Iter 53/150 - Loss: -0.037\n",
      "Iter 54/150 - Loss: -0.063\n",
      "Iter 55/150 - Loss: -0.090\n",
      "Iter 56/150 - Loss: -0.115\n",
      "Iter 57/150 - Loss: -0.140\n",
      "Iter 58/150 - Loss: -0.166\n",
      "Iter 59/150 - Loss: -0.193\n",
      "Iter 60/150 - Loss: -0.219\n",
      "Iter 61/150 - Loss: -0.245\n",
      "Iter 62/150 - Loss: -0.271\n",
      "Iter 63/150 - Loss: -0.297\n",
      "Iter 64/150 - Loss: -0.323\n",
      "Iter 65/150 - Loss: -0.349\n",
      "Iter 66/150 - Loss: -0.375\n",
      "Iter 67/150 - Loss: -0.402\n",
      "Iter 68/150 - Loss: -0.428\n",
      "Iter 69/150 - Loss: -0.453\n",
      "Iter 70/150 - Loss: -0.480\n",
      "Iter 71/150 - Loss: -0.506\n",
      "Iter 72/150 - Loss: -0.532\n",
      "Iter 73/150 - Loss: -0.558\n",
      "Iter 74/150 - Loss: -0.585\n",
      "Iter 75/150 - Loss: -0.611\n",
      "Iter 76/150 - Loss: -0.637\n",
      "Iter 77/150 - Loss: -0.664\n",
      "Iter 78/150 - Loss: -0.689\n",
      "Iter 79/150 - Loss: -0.716\n",
      "Iter 80/150 - Loss: -0.742\n",
      "Iter 81/150 - Loss: -0.768\n",
      "Iter 82/150 - Loss: -0.794\n",
      "Iter 83/150 - Loss: -0.821\n",
      "Iter 84/150 - Loss: -0.846\n",
      "Iter 85/150 - Loss: -0.873\n",
      "Iter 86/150 - Loss: -0.897\n",
      "Iter 87/150 - Loss: -0.925\n",
      "Iter 88/150 - Loss: -0.950\n",
      "Iter 89/150 - Loss: -0.976\n",
      "Iter 90/150 - Loss: -1.003\n",
      "Iter 91/150 - Loss: -1.028\n",
      "Iter 92/150 - Loss: -1.054\n",
      "Iter 93/150 - Loss: -1.080\n",
      "Iter 94/150 - Loss: -1.106\n",
      "Iter 95/150 - Loss: -1.133\n",
      "Iter 96/150 - Loss: -1.158\n",
      "Iter 97/150 - Loss: -1.183\n",
      "Iter 98/150 - Loss: -1.209\n",
      "Iter 99/150 - Loss: -1.234\n",
      "Iter 100/150 - Loss: -1.260\n",
      "Iter 101/150 - Loss: -1.286\n",
      "Iter 102/150 - Loss: -1.311\n",
      "Iter 103/150 - Loss: -1.337\n",
      "Iter 104/150 - Loss: -1.362\n",
      "Iter 105/150 - Loss: -1.388\n",
      "Iter 106/150 - Loss: -1.412\n",
      "Iter 107/150 - Loss: -1.435\n",
      "Iter 108/150 - Loss: -1.462\n",
      "Iter 109/150 - Loss: -1.488\n",
      "Iter 110/150 - Loss: -1.511\n",
      "Iter 111/150 - Loss: -1.536\n",
      "Iter 112/150 - Loss: -1.560\n",
      "Iter 113/150 - Loss: -1.584\n",
      "Iter 114/150 - Loss: -1.610\n",
      "Iter 115/150 - Loss: -1.634\n",
      "Iter 116/150 - Loss: -1.657\n",
      "Iter 117/150 - Loss: -1.683\n",
      "Iter 118/150 - Loss: nan\n",
      "Iter 119/150 - Loss: -1.730\n",
      "Iter 120/150 - Loss: -1.753\n",
      "Iter 121/150 - Loss: nan\n",
      "Iter 122/150 - Loss: -1.801\n",
      "Iter 123/150 - Loss: -1.823\n",
      "Iter 124/150 - Loss: -1.846\n",
      "Iter 125/150 - Loss: -1.870\n",
      "Iter 126/150 - Loss: -1.892\n",
      "Iter 127/150 - Loss: -1.916\n",
      "Iter 128/150 - Loss: -1.939\n",
      "Iter 129/150 - Loss: -1.960\n",
      "Iter 130/150 - Loss: -1.983\n",
      "Iter 131/150 - Loss: -2.006\n",
      "Iter 132/150 - Loss: -2.027\n",
      "Iter 133/150 - Loss: -2.048\n",
      "Iter 134/150 - Loss: -2.070\n",
      "Iter 135/150 - Loss: -2.091\n",
      "Iter 136/150 - Loss: -2.113\n",
      "Iter 137/150 - Loss: nan\n",
      "Iter 138/150 - Loss: nan\n",
      "Iter 139/150 - Loss: -2.172\n",
      "Iter 140/150 - Loss: nan\n",
      "Iter 141/150 - Loss: nan\n",
      "Iter 142/150 - Loss: -2.231\n",
      "Iter 143/150 - Loss: -2.251\n",
      "Iter 144/150 - Loss: nan\n",
      "Iter 145/150 - Loss: -2.285\n",
      "Iter 146/150 - Loss: nan\n",
      "Iter 147/150 - Loss: nan\n",
      "Iter 148/150 - Loss: nan\n",
      "Iter 149/150 - Loss: -2.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0702104568481445 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150/150 - Loss: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.156534194946289 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.6978683471679688 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 2000\n",
      "Iter 1/150 - Loss: 1.096\n",
      "Iter 2/150 - Loss: 1.091\n",
      "Iter 3/150 - Loss: 1.075\n",
      "Iter 4/150 - Loss: 1.057\n",
      "Iter 5/150 - Loss: 1.041\n",
      "Iter 6/150 - Loss: 1.021\n",
      "Iter 7/150 - Loss: 1.003\n",
      "Iter 8/150 - Loss: 0.985\n",
      "Iter 9/150 - Loss: 0.966\n",
      "Iter 10/150 - Loss: 0.948\n",
      "Iter 11/150 - Loss: 0.929\n",
      "Iter 12/150 - Loss: 0.910\n",
      "Iter 13/150 - Loss: 0.890\n",
      "Iter 14/150 - Loss: 0.870\n",
      "Iter 15/150 - Loss: 0.850\n",
      "Iter 16/150 - Loss: 0.830\n",
      "Iter 17/150 - Loss: 0.810\n",
      "Iter 18/150 - Loss: 0.789\n",
      "Iter 19/150 - Loss: 0.768\n",
      "Iter 20/150 - Loss: 0.746\n",
      "Iter 21/150 - Loss: 0.725\n",
      "Iter 22/150 - Loss: 0.704\n",
      "Iter 23/150 - Loss: 0.682\n",
      "Iter 24/150 - Loss: 0.661\n",
      "Iter 25/150 - Loss: 0.638\n",
      "Iter 26/150 - Loss: 0.615\n",
      "Iter 27/150 - Loss: 0.594\n",
      "Iter 28/150 - Loss: 0.570\n",
      "Iter 29/150 - Loss: 0.548\n",
      "Iter 30/150 - Loss: 0.525\n",
      "Iter 31/150 - Loss: 0.502\n",
      "Iter 32/150 - Loss: 0.478\n",
      "Iter 33/150 - Loss: 0.455\n",
      "Iter 34/150 - Loss: 0.431\n",
      "Iter 35/150 - Loss: 0.408\n",
      "Iter 36/150 - Loss: 0.383\n",
      "Iter 37/150 - Loss: 0.359\n",
      "Iter 38/150 - Loss: 0.335\n",
      "Iter 39/150 - Loss: 0.311\n",
      "Iter 40/150 - Loss: 0.287\n",
      "Iter 41/150 - Loss: 0.262\n",
      "Iter 42/150 - Loss: 0.237\n",
      "Iter 43/150 - Loss: 0.212\n",
      "Iter 44/150 - Loss: 0.188\n",
      "Iter 45/150 - Loss: 0.163\n",
      "Iter 46/150 - Loss: 0.137\n",
      "Iter 47/150 - Loss: 0.112\n",
      "Iter 48/150 - Loss: 0.087\n",
      "Iter 49/150 - Loss: 0.062\n",
      "Iter 50/150 - Loss: 0.037\n",
      "Iter 51/150 - Loss: 0.011\n",
      "Iter 52/150 - Loss: -0.015\n",
      "Iter 53/150 - Loss: -0.041\n",
      "Iter 54/150 - Loss: -0.067\n",
      "Iter 55/150 - Loss: -0.092\n",
      "Iter 56/150 - Loss: -0.118\n",
      "Iter 57/150 - Loss: -0.144\n",
      "Iter 58/150 - Loss: -0.170\n",
      "Iter 59/150 - Loss: -0.196\n",
      "Iter 60/150 - Loss: -0.222\n",
      "Iter 61/150 - Loss: -0.248\n",
      "Iter 62/150 - Loss: -0.274\n",
      "Iter 63/150 - Loss: -0.299\n",
      "Iter 64/150 - Loss: -0.327\n",
      "Iter 65/150 - Loss: -0.352\n",
      "Iter 66/150 - Loss: -0.378\n",
      "Iter 67/150 - Loss: -0.405\n",
      "Iter 68/150 - Loss: -0.431\n",
      "Iter 69/150 - Loss: -0.457\n",
      "Iter 70/150 - Loss: -0.483\n",
      "Iter 71/150 - Loss: -0.510\n",
      "Iter 72/150 - Loss: -0.536\n",
      "Iter 73/150 - Loss: -0.562\n",
      "Iter 74/150 - Loss: -0.589\n",
      "Iter 75/150 - Loss: -0.615\n",
      "Iter 76/150 - Loss: -0.641\n",
      "Iter 77/150 - Loss: -0.668\n",
      "Iter 78/150 - Loss: -0.693\n",
      "Iter 79/150 - Loss: -0.720\n",
      "Iter 80/150 - Loss: -0.746\n",
      "Iter 81/150 - Loss: -0.772\n",
      "Iter 82/150 - Loss: -0.798\n",
      "Iter 83/150 - Loss: -0.824\n",
      "Iter 84/150 - Loss: -0.851\n",
      "Iter 85/150 - Loss: -0.877\n",
      "Iter 86/150 - Loss: -0.904\n",
      "Iter 87/150 - Loss: -0.929\n",
      "Iter 88/150 - Loss: -0.955\n",
      "Iter 89/150 - Loss: -0.982\n",
      "Iter 90/150 - Loss: -1.007\n",
      "Iter 91/150 - Loss: -1.033\n",
      "Iter 92/150 - Loss: -1.060\n",
      "Iter 93/150 - Loss: -1.086\n",
      "Iter 94/150 - Loss: -1.111\n",
      "Iter 95/150 - Loss: -1.136\n",
      "Iter 96/150 - Loss: -1.162\n",
      "Iter 97/150 - Loss: -1.188\n",
      "Iter 98/150 - Loss: -1.213\n",
      "Iter 99/150 - Loss: -1.239\n",
      "Iter 100/150 - Loss: -1.265\n",
      "Iter 101/150 - Loss: -1.290\n",
      "Iter 102/150 - Loss: -1.316\n",
      "Iter 103/150 - Loss: -1.342\n",
      "Iter 104/150 - Loss: -1.367\n",
      "Iter 105/150 - Loss: -1.392\n",
      "Iter 106/150 - Loss: -1.417\n",
      "Iter 107/150 - Loss: -1.442\n",
      "Iter 108/150 - Loss: -1.467\n",
      "Iter 109/150 - Loss: -1.492\n",
      "Iter 110/150 - Loss: -1.517\n",
      "Iter 111/150 - Loss: nan\n",
      "Iter 112/150 - Loss: -1.567\n",
      "Iter 113/150 - Loss: nan\n",
      "Iter 114/150 - Loss: -1.616\n",
      "Iter 115/150 - Loss: nan\n",
      "Iter 116/150 - Loss: nan\n",
      "Iter 117/150 - Loss: -1.689\n",
      "Iter 118/150 - Loss: -1.712\n",
      "Iter 119/150 - Loss: nan\n",
      "Iter 120/150 - Loss: -1.760\n",
      "Iter 121/150 - Loss: -1.785\n",
      "Iter 122/150 - Loss: nan\n",
      "Iter 123/150 - Loss: -1.830\n",
      "Iter 124/150 - Loss: -1.854\n",
      "Iter 125/150 - Loss: -1.876\n",
      "Iter 126/150 - Loss: -1.899\n",
      "Iter 127/150 - Loss: -1.923\n",
      "Iter 128/150 - Loss: nan\n",
      "Iter 129/150 - Loss: -1.968\n",
      "Iter 130/150 - Loss: nan\n",
      "Iter 131/150 - Loss: nan\n",
      "Iter 132/150 - Loss: nan\n",
      "Iter 133/150 - Loss: nan\n",
      "Iter 134/150 - Loss: nan\n",
      "Iter 135/150 - Loss: nan\n",
      "Iter 136/150 - Loss: nan\n",
      "Iter 137/150 - Loss: nan\n",
      "Iter 138/150 - Loss: nan\n",
      "Iter 139/150 - Loss: -2.178\n",
      "Iter 140/150 - Loss: nan\n",
      "Iter 141/150 - Loss: nan\n",
      "Iter 142/150 - Loss: nan\n",
      "Iter 143/150 - Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6248997449874878 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 144/150 - Loss: nan\n",
      "Iter 145/150 - Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6809377670288086 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 146/150 - Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4910030364990234 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 147/150 - Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.67107355594635 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 148/150 - Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.013857841491699 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 149/150 - Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.777685284614563 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150/150 - Loss: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.137876987457275 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.4686453640460968 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cpu_size_vec = [100,300,500,700,1000,1500,2000]\n",
    "Nval = 4\n",
    "Dval = 4\n",
    "\n",
    "for size in cpu_size_vec:\n",
    "    print(f\"data size: {size}\")\n",
    "    \"\"\"Set up the training and testing data\"\"\"\n",
    "    n = size # input size\n",
    "\n",
    "#     x = 5 * torch.rand(n, Dval)\n",
    "\n",
    "#     y = torch.stack([\n",
    "#         torch.sin(x[:, 0] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         torch.cos(x[:, 0] * (2 * math.pi)) + torch.cos(x[:, 2] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         torch.sin(x[:, 2] * (2 * math.pi)) + torch.cos(x[:, 1] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         (torch.cos(x[:, 3] * (2 * math.pi)))* (torch.sin(x[:, 0] * (2 * math.pi))) + torch.randn(n) * 0.02,\n",
    "#     ], -1)\n",
    "\n",
    "    x = 3 * torch.rand(n)\n",
    "    \n",
    "    y = torch.stack([\n",
    "        torch.sin(3 * x) + torch.randn(n) * 0.01,\n",
    "        torch.cos(x) + torch.cos(2 * x) + torch.randn(n) * 0.01,\n",
    "        torch.sin(x) + torch.cos(x) + torch.randn(n) * 0.01,\n",
    "        torch.cos(x) * torch.cos(x) + torch.randn(n) * 0.01,\n",
    "    ], -1)\n",
    "\n",
    "    train_x = x[:int(0.8*n)]\n",
    "    train_y = y[:int(0.8*n)]\n",
    "\n",
    "    test_x = x[int(0.8*n): ]\n",
    "\n",
    "    test_y = y[int(0.8*n): ]\n",
    "\n",
    "#     # normalize features\n",
    "#     mean = train_x.mean()\n",
    "#     std = train_x.std() + 1e-6 # prevent dividing by 0\n",
    "#     train_x = (train_x - mean) / std\n",
    "#     test_x = (test_x - mean) / std\n",
    "\n",
    "#     # normalize labels\n",
    "#     mean, std = train_y.mean(),train_y.std()\n",
    "#     train_y = (train_y - mean) / std\n",
    "#     test_y = (test_y - mean) / std\n",
    "\n",
    "    \n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=Dval)\n",
    "    model = MultitaskGPModel(train_x, train_y, likelihood, num_base_kernels)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \"\"\"train the model hyperparameters\"\"\"\n",
    "    training_iterations = 150\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "#           if(i > training_iterations*0.8):\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "    \n",
    "    cpu_training_time.append(time.time() - start_time)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \"\"\" Making predictions with the model\"\"\"\n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Exact predictions\n",
    "    with torch.no_grad(): #, gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x) # no noise\n",
    "        covar = preds.covariance_matrix\n",
    "        cpu_exact_meancovar.append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # LOVE without cache\n",
    "        # Clear the cache from the previous computations\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x)\n",
    "        fast_covar = preds.covariance_matrix\n",
    "        cpu_love_meancovar.append(time.time() - start_time)\n",
    "    \n",
    "    # LOVE with cache\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x)\n",
    "        fast_covar = preds.covariance_matrix\n",
    "        cpu_love_meancovar_cache.append(time.time() - start_time)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.7813191413879395, 20.281359434127808, 29.221570014953613, 29.987565994262695, 34.28912806510925, 111.40788269042969, 270.16520977020264]\n",
      "[0.2011868953704834, 0.47016167640686035, 0.6492369174957275, 1.2938873767852783, 4.657888650894165, 205.26769304275513, 727.7417969703674]\n",
      "[0.05069756507873535, 0.44669389724731445, 0.5175209045410156, 0.4238770008087158, 0.6530847549438477, 5.922862768173218, 14.493090867996216]\n",
      "[0.016495704650878906, 0.056517839431762695, 0.07649540901184082, 0.06994819641113281, 0.11183714866638184, 0.21816301345825195, 0.37868762016296387]\n"
     ]
    }
   ],
   "source": [
    "print(cpu_training_time)\n",
    "print(cpu_exact_meancovar)\n",
    "print(cpu_love_meancovar)\n",
    "print(cpu_love_meancovar_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize plots\n",
    "# f, (y1_ax, y2_ax) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# # This contains predictions for both tasks, flattened out\n",
    "# # The first half of the predictions is for the first task\n",
    "# # The second half is for the second task\n",
    "\n",
    "# # Plot training data as black stars\n",
    "# y1_ax.plot(train_x[:, 0].detach().numpy(), train_y[:, 0].detach().numpy(), 'k*')\n",
    "# # Predictive mean as blue line\n",
    "# y1_ax.plot(test_x[:, 0].numpy(), preds.mean[:, 0].numpy(), 'b')\n",
    "# # Shade in confidence\n",
    "# # y1_ax.fill_between(test_x[:, 0].numpy(), lower[:, 0].numpy(), upper[:, 0].numpy(), alpha=0.5)\n",
    "# # y1_ax.set_ylim([-3, 3])\n",
    "# y1_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "# y1_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# # Plot training data as black stars\n",
    "# y2_ax.plot(train_x[:, 1].detach().numpy(), train_y[:, 1].detach().numpy(), 'k*')\n",
    "# # Predictive mean as blue line\n",
    "# y2_ax.plot(test_x[:, 1].numpy(), preds.mean[:, 1].numpy(), 'b')\n",
    "# # Shade in confidence\n",
    "# # y2_ax.fill_between(test_x[:, 1].numpy(), lower[:, 1].numpy(), upper[:, 1].numpy(), alpha=0.5)\n",
    "# # y2_ax.set_ylim([-3, 3])\n",
    "# y2_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "# y2_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_training_time = []\n",
    "gpu_exact_meancovar = []\n",
    "gpu_love_meancovar = []\n",
    "gpu_love_meancovar_cache = []\n",
    "love_covar_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 100\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.263\n",
      "Iter 2/90 - Loss: 1.220\n",
      "Iter 3/90 - Loss: 1.178\n",
      "Iter 4/90 - Loss: 1.137\n",
      "Iter 5/90 - Loss: 1.098\n",
      "Iter 6/90 - Loss: 1.060\n",
      "Iter 7/90 - Loss: 1.023\n",
      "Iter 8/90 - Loss: 0.986\n",
      "Iter 9/90 - Loss: 0.950\n",
      "Iter 10/90 - Loss: 0.913\n",
      "Iter 11/90 - Loss: 0.876\n",
      "Iter 12/90 - Loss: 0.839\n",
      "Iter 13/90 - Loss: 0.800\n",
      "Iter 14/90 - Loss: 0.761\n",
      "Iter 15/90 - Loss: 0.720\n",
      "Iter 16/90 - Loss: 0.680\n",
      "Iter 17/90 - Loss: 0.640\n",
      "Iter 18/90 - Loss: 0.601\n",
      "Iter 19/90 - Loss: 0.562\n",
      "Iter 20/90 - Loss: 0.523\n",
      "Iter 21/90 - Loss: 0.483\n",
      "Iter 22/90 - Loss: 0.442\n",
      "Iter 23/90 - Loss: 0.400\n",
      "Iter 24/90 - Loss: 0.358\n",
      "Iter 25/90 - Loss: 0.316\n",
      "Iter 26/90 - Loss: 0.273\n",
      "Iter 27/90 - Loss: 0.231\n",
      "Iter 28/90 - Loss: 0.188\n",
      "Iter 29/90 - Loss: 0.145\n",
      "Iter 30/90 - Loss: 0.103\n",
      "Iter 31/90 - Loss: 0.060\n",
      "Iter 32/90 - Loss: 0.017\n",
      "Iter 33/90 - Loss: -0.026\n",
      "Iter 34/90 - Loss: -0.069\n",
      "Iter 35/90 - Loss: -0.112\n",
      "Iter 36/90 - Loss: -0.156\n",
      "Iter 37/90 - Loss: -0.199\n",
      "Iter 38/90 - Loss: -0.242\n",
      "Iter 39/90 - Loss: -0.285\n",
      "Iter 40/90 - Loss: -0.328\n",
      "Iter 41/90 - Loss: -0.371\n",
      "Iter 42/90 - Loss: -0.414\n",
      "Iter 43/90 - Loss: -0.457\n",
      "Iter 44/90 - Loss: -0.500\n",
      "Iter 45/90 - Loss: -0.542\n",
      "Iter 46/90 - Loss: -0.585\n",
      "Iter 47/90 - Loss: -0.627\n",
      "Iter 48/90 - Loss: -0.669\n",
      "Iter 49/90 - Loss: -0.712\n",
      "Iter 50/90 - Loss: -0.754\n",
      "Iter 51/90 - Loss: -0.796\n",
      "Iter 52/90 - Loss: -0.837\n",
      "Iter 53/90 - Loss: -0.879\n",
      "Iter 54/90 - Loss: -0.920\n",
      "Iter 55/90 - Loss: -0.962\n",
      "Iter 56/90 - Loss: -1.002\n",
      "Iter 57/90 - Loss: -1.043\n",
      "Iter 58/90 - Loss: -1.083\n",
      "Iter 59/90 - Loss: -1.123\n",
      "Iter 60/90 - Loss: -1.162\n",
      "Iter 61/90 - Loss: -1.201\n",
      "Iter 62/90 - Loss: -1.239\n",
      "Iter 63/90 - Loss: -1.278\n",
      "Iter 64/90 - Loss: -1.315\n",
      "Iter 65/90 - Loss: -1.352\n",
      "Iter 66/90 - Loss: -1.389\n",
      "Iter 67/90 - Loss: -1.425\n",
      "Iter 68/90 - Loss: -1.460\n",
      "Iter 69/90 - Loss: -1.495\n",
      "Iter 70/90 - Loss: -1.529\n",
      "Iter 71/90 - Loss: -1.562\n",
      "Iter 72/90 - Loss: -1.594\n",
      "Iter 73/90 - Loss: -1.626\n",
      "Iter 74/90 - Loss: -1.656\n",
      "Iter 75/90 - Loss: -1.686\n",
      "Iter 76/90 - Loss: -1.715\n",
      "Iter 77/90 - Loss: -1.743\n",
      "Iter 78/90 - Loss: -1.769\n",
      "Iter 79/90 - Loss: -1.794\n",
      "Iter 80/90 - Loss: -1.817\n",
      "Iter 81/90 - Loss: -1.842\n",
      "Iter 82/90 - Loss: -1.864\n",
      "Iter 83/90 - Loss: -1.885\n",
      "Iter 84/90 - Loss: -1.905\n",
      "Iter 85/90 - Loss: -1.923\n",
      "Iter 86/90 - Loss: -1.942\n",
      "Iter 87/90 - Loss: -1.957\n",
      "Iter 88/90 - Loss: -1.973\n",
      "Iter 89/90 - Loss: -1.988\n",
      "Iter 90/90 - Loss: -2.000\n",
      "\n",
      "data size: 300\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.157\n",
      "Iter 2/90 - Loss: 1.129\n",
      "Iter 3/90 - Loss: 1.093\n",
      "Iter 4/90 - Loss: 1.050\n",
      "Iter 5/90 - Loss: 1.024\n",
      "Iter 6/90 - Loss: 0.972\n",
      "Iter 7/90 - Loss: 0.940\n",
      "Iter 8/90 - Loss: 0.900\n",
      "Iter 9/90 - Loss: 0.872\n",
      "Iter 10/90 - Loss: 0.832\n",
      "Iter 11/90 - Loss: 0.796\n",
      "Iter 12/90 - Loss: 0.756\n",
      "Iter 13/90 - Loss: 0.723\n",
      "Iter 14/90 - Loss: 0.670\n",
      "Iter 15/90 - Loss: 0.631\n",
      "Iter 16/90 - Loss: 0.603\n",
      "Iter 17/90 - Loss: 0.548\n",
      "Iter 18/90 - Loss: 0.502\n",
      "Iter 19/90 - Loss: 0.467\n",
      "Iter 20/90 - Loss: 0.439\n",
      "Iter 21/90 - Loss: 0.385\n",
      "Iter 22/90 - Loss: 0.346\n",
      "Iter 23/90 - Loss: 0.304\n",
      "Iter 24/90 - Loss: 0.266\n",
      "Iter 25/90 - Loss: 0.219\n",
      "Iter 26/90 - Loss: 0.170\n",
      "Iter 27/90 - Loss: 0.121\n",
      "Iter 28/90 - Loss: 0.087\n",
      "Iter 29/90 - Loss: 0.046\n",
      "Iter 30/90 - Loss: -0.013\n",
      "Iter 31/90 - Loss: -0.045\n",
      "Iter 32/90 - Loss: -0.099\n",
      "Iter 33/90 - Loss: -0.128\n",
      "Iter 34/90 - Loss: -0.180\n",
      "Iter 35/90 - Loss: -0.212\n",
      "Iter 36/90 - Loss: -0.286\n",
      "Iter 37/90 - Loss: -0.319\n",
      "Iter 38/90 - Loss: -0.373\n",
      "Iter 39/90 - Loss: -0.412\n",
      "Iter 40/90 - Loss: -0.465\n",
      "Iter 41/90 - Loss: -0.497\n",
      "Iter 42/90 - Loss: -0.537\n",
      "Iter 43/90 - Loss: -0.594\n",
      "Iter 44/90 - Loss: -0.639\n",
      "Iter 45/90 - Loss: -0.678\n",
      "Iter 46/90 - Loss: -0.722\n",
      "Iter 47/90 - Loss: -0.771\n",
      "Iter 48/90 - Loss: -0.811\n",
      "Iter 49/90 - Loss: -0.873\n",
      "Iter 50/90 - Loss: -0.897\n",
      "Iter 51/90 - Loss: -0.940\n",
      "Iter 52/90 - Loss: -0.984\n",
      "Iter 53/90 - Loss: -1.047\n",
      "Iter 54/90 - Loss: -1.055\n",
      "Iter 55/90 - Loss: -1.105\n",
      "Iter 56/90 - Loss: -1.129\n",
      "Iter 57/90 - Loss: -1.200\n",
      "Iter 58/90 - Loss: -1.215\n",
      "Iter 59/90 - Loss: -1.251\n",
      "Iter 60/90 - Loss: -1.314\n",
      "Iter 61/90 - Loss: -1.310\n",
      "Iter 62/90 - Loss: -1.334\n",
      "Iter 63/90 - Loss: -1.363\n",
      "Iter 64/90 - Loss: -1.418\n",
      "Iter 65/90 - Loss: -1.404\n",
      "Iter 66/90 - Loss: -1.429\n",
      "Iter 67/90 - Loss: -1.420\n",
      "Iter 68/90 - Loss: -1.412\n",
      "Iter 69/90 - Loss: -1.429\n",
      "Iter 70/90 - Loss: -1.487\n",
      "Iter 71/90 - Loss: -1.464\n",
      "Iter 72/90 - Loss: -1.507\n",
      "Iter 73/90 - Loss: -1.570\n",
      "Iter 74/90 - Loss: -1.571\n",
      "Iter 75/90 - Loss: -1.529\n",
      "Iter 76/90 - Loss: -1.577\n",
      "Iter 77/90 - Loss: -1.573\n",
      "Iter 78/90 - Loss: -1.613\n",
      "Iter 79/90 - Loss: -1.605\n",
      "Iter 80/90 - Loss: -1.592\n",
      "Iter 81/90 - Loss: -1.538\n",
      "Iter 82/90 - Loss: -1.569\n",
      "Iter 83/90 - Loss: -1.598\n",
      "Iter 84/90 - Loss: -1.538\n",
      "Iter 85/90 - Loss: -1.542\n",
      "Iter 86/90 - Loss: -1.598\n",
      "Iter 87/90 - Loss: -1.559\n",
      "Iter 88/90 - Loss: -1.715\n",
      "Iter 89/90 - Loss: -1.588\n",
      "Iter 90/90 - Loss: -1.685\n",
      "\n",
      "data size: 500\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.146\n",
      "Iter 2/90 - Loss: 1.106\n",
      "Iter 3/90 - Loss: 1.063\n",
      "Iter 4/90 - Loss: 1.029\n",
      "Iter 5/90 - Loss: 0.990\n",
      "Iter 6/90 - Loss: 0.964\n",
      "Iter 7/90 - Loss: 0.921\n",
      "Iter 8/90 - Loss: 0.881\n",
      "Iter 9/90 - Loss: 0.851\n",
      "Iter 10/90 - Loss: 0.806\n",
      "Iter 11/90 - Loss: 0.772\n",
      "Iter 12/90 - Loss: 0.737\n",
      "Iter 13/90 - Loss: 0.699\n",
      "Iter 14/90 - Loss: 0.651\n",
      "Iter 15/90 - Loss: 0.611\n",
      "Iter 16/90 - Loss: 0.567\n",
      "Iter 17/90 - Loss: 0.531\n",
      "Iter 18/90 - Loss: 0.487\n",
      "Iter 19/90 - Loss: 0.448\n",
      "Iter 20/90 - Loss: 0.402\n",
      "Iter 21/90 - Loss: 0.361\n",
      "Iter 22/90 - Loss: 0.317\n",
      "Iter 23/90 - Loss: 0.275\n",
      "Iter 24/90 - Loss: 0.232\n",
      "Iter 25/90 - Loss: 0.192\n",
      "Iter 26/90 - Loss: 0.146\n",
      "Iter 27/90 - Loss: 0.103\n",
      "Iter 28/90 - Loss: 0.058\n",
      "Iter 29/90 - Loss: 0.009\n",
      "Iter 30/90 - Loss: -0.037\n",
      "Iter 31/90 - Loss: -0.079\n",
      "Iter 32/90 - Loss: -0.119\n",
      "Iter 33/90 - Loss: -0.170\n",
      "Iter 34/90 - Loss: -0.211\n",
      "Iter 35/90 - Loss: -0.260\n",
      "Iter 36/90 - Loss: -0.298\n",
      "Iter 37/90 - Loss: -0.355\n",
      "Iter 38/90 - Loss: -0.396\n",
      "Iter 39/90 - Loss: -0.433\n",
      "Iter 40/90 - Loss: -0.480\n",
      "Iter 41/90 - Loss: -0.537\n",
      "Iter 42/90 - Loss: -0.565\n",
      "Iter 43/90 - Loss: -0.607\n",
      "Iter 44/90 - Loss: -0.661\n",
      "Iter 45/90 - Loss: -0.701\n",
      "Iter 46/90 - Loss: -0.756\n",
      "Iter 47/90 - Loss: -0.791\n",
      "Iter 48/90 - Loss: -0.833\n",
      "Iter 49/90 - Loss: -0.882\n",
      "Iter 50/90 - Loss: -0.929\n",
      "Iter 51/90 - Loss: -0.952\n",
      "Iter 52/90 - Loss: -0.999\n",
      "Iter 53/90 - Loss: -1.037\n",
      "Iter 54/90 - Loss: -1.088\n",
      "Iter 55/90 - Loss: -1.130\n",
      "Iter 56/90 - Loss: -1.134\n",
      "Iter 57/90 - Loss: -1.206\n",
      "Iter 58/90 - Loss: -1.222\n",
      "Iter 59/90 - Loss: -1.258\n",
      "Iter 60/90 - Loss: -1.295\n",
      "Iter 61/90 - Loss: -1.328\n",
      "Iter 62/90 - Loss: -1.388\n",
      "Iter 63/90 - Loss: -1.394\n",
      "Iter 64/90 - Loss: -1.425\n",
      "Iter 65/90 - Loss: -1.445\n",
      "Iter 66/90 - Loss: -1.507\n",
      "Iter 67/90 - Loss: -1.537\n",
      "Iter 68/90 - Loss: -1.530\n",
      "Iter 69/90 - Loss: -1.548\n",
      "Iter 70/90 - Loss: -1.588\n",
      "Iter 71/90 - Loss: -1.624\n",
      "Iter 72/90 - Loss: -1.612\n",
      "Iter 73/90 - Loss: -1.607\n",
      "Iter 74/90 - Loss: -1.669\n",
      "Iter 75/90 - Loss: -1.664\n",
      "Iter 76/90 - Loss: -1.709\n",
      "Iter 77/90 - Loss: -1.629\n",
      "Iter 78/90 - Loss: -1.714\n",
      "Iter 79/90 - Loss: -1.691\n",
      "Iter 80/90 - Loss: -1.707\n",
      "Iter 81/90 - Loss: -1.733\n",
      "Iter 82/90 - Loss: -1.788\n",
      "Iter 83/90 - Loss: -1.763\n",
      "Iter 84/90 - Loss: -1.771\n",
      "Iter 85/90 - Loss: -1.694\n",
      "Iter 86/90 - Loss: -1.764\n",
      "Iter 87/90 - Loss: -1.720\n",
      "Iter 88/90 - Loss: -1.789\n",
      "Iter 89/90 - Loss: -1.864\n",
      "Iter 90/90 - Loss: -1.770\n",
      "\n",
      "data size: 700\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.123\n",
      "Iter 2/90 - Loss: 1.117\n",
      "Iter 3/90 - Loss: 1.082\n",
      "Iter 4/90 - Loss: 1.054\n",
      "Iter 5/90 - Loss: 1.019\n",
      "Iter 6/90 - Loss: 0.984\n",
      "Iter 7/90 - Loss: 0.953\n",
      "Iter 8/90 - Loss: 0.920\n",
      "Iter 9/90 - Loss: 0.884\n",
      "Iter 10/90 - Loss: 0.847\n",
      "Iter 11/90 - Loss: 0.812\n",
      "Iter 12/90 - Loss: 0.773\n",
      "Iter 13/90 - Loss: 0.735\n",
      "Iter 14/90 - Loss: 0.699\n",
      "Iter 15/90 - Loss: 0.659\n",
      "Iter 16/90 - Loss: 0.618\n",
      "Iter 17/90 - Loss: 0.580\n",
      "Iter 18/90 - Loss: 0.544\n",
      "Iter 19/90 - Loss: 0.501\n",
      "Iter 20/90 - Loss: 0.460\n",
      "Iter 21/90 - Loss: 0.418\n",
      "Iter 22/90 - Loss: 0.378\n",
      "Iter 23/90 - Loss: 0.333\n",
      "Iter 24/90 - Loss: 0.289\n",
      "Iter 25/90 - Loss: 0.247\n",
      "Iter 26/90 - Loss: 0.205\n",
      "Iter 27/90 - Loss: 0.161\n",
      "Iter 28/90 - Loss: 0.118\n",
      "Iter 29/90 - Loss: 0.074\n",
      "Iter 30/90 - Loss: 0.031\n",
      "Iter 31/90 - Loss: -0.016\n",
      "Iter 32/90 - Loss: -0.062\n",
      "Iter 33/90 - Loss: -0.104\n",
      "Iter 34/90 - Loss: -0.150\n",
      "Iter 35/90 - Loss: -0.198\n",
      "Iter 36/90 - Loss: -0.240\n",
      "Iter 37/90 - Loss: -0.287\n",
      "Iter 38/90 - Loss: -0.333\n",
      "Iter 39/90 - Loss: -0.379\n",
      "Iter 40/90 - Loss: -0.422\n",
      "Iter 41/90 - Loss: -0.472\n",
      "Iter 42/90 - Loss: -0.516\n",
      "Iter 43/90 - Loss: -0.561\n",
      "Iter 44/90 - Loss: -0.606\n",
      "Iter 45/90 - Loss: -0.652\n",
      "Iter 46/90 - Loss: -0.699\n",
      "Iter 47/90 - Loss: -0.744\n",
      "Iter 48/90 - Loss: -0.790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 49/90 - Loss: -0.835\n",
      "Iter 50/90 - Loss: -0.880\n",
      "Iter 51/90 - Loss: -0.921\n",
      "Iter 52/90 - Loss: -0.967\n",
      "Iter 53/90 - Loss: -1.015\n",
      "Iter 54/90 - Loss: -1.060\n",
      "Iter 55/90 - Loss: -1.101\n",
      "Iter 56/90 - Loss: -1.148\n",
      "Iter 57/90 - Loss: -1.193\n",
      "Iter 58/90 - Loss: -1.237\n",
      "Iter 59/90 - Loss: -1.277\n",
      "Iter 60/90 - Loss: -1.324\n",
      "Iter 61/90 - Loss: -1.367\n",
      "Iter 62/90 - Loss: -1.409\n",
      "Iter 63/90 - Loss: -1.447\n",
      "Iter 64/90 - Loss: -1.486\n",
      "Iter 65/90 - Loss: -1.528\n",
      "Iter 66/90 - Loss: -1.575\n",
      "Iter 67/90 - Loss: -1.614\n",
      "Iter 68/90 - Loss: -1.659\n",
      "Iter 69/90 - Loss: -1.703\n",
      "Iter 70/90 - Loss: -1.731\n",
      "Iter 71/90 - Loss: -1.768\n",
      "Iter 72/90 - Loss: -1.809\n",
      "Iter 73/90 - Loss: -1.845\n",
      "Iter 74/90 - Loss: -1.886\n",
      "Iter 75/90 - Loss: -1.916\n",
      "Iter 76/90 - Loss: -1.965\n",
      "Iter 77/90 - Loss: -1.996\n",
      "Iter 78/90 - Loss: -2.034\n",
      "Iter 79/90 - Loss: -2.075\n",
      "Iter 80/90 - Loss: -2.089\n",
      "Iter 81/90 - Loss: -2.138\n",
      "Iter 82/90 - Loss: -2.183\n",
      "Iter 83/90 - Loss: -2.214\n",
      "Iter 84/90 - Loss: -2.261\n",
      "Iter 85/90 - Loss: -2.297\n",
      "Iter 86/90 - Loss: -2.311\n",
      "Iter 87/90 - Loss: -2.358\n",
      "Iter 88/90 - Loss: -2.398\n",
      "Iter 89/90 - Loss: -2.414\n",
      "Iter 90/90 - Loss: -2.422\n",
      "\n",
      "data size: 1000\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.115\n",
      "Iter 2/90 - Loss: 1.106\n",
      "Iter 3/90 - Loss: 1.075\n",
      "Iter 4/90 - Loss: 1.041\n",
      "Iter 5/90 - Loss: 1.009\n",
      "Iter 6/90 - Loss: 0.977\n",
      "Iter 7/90 - Loss: 0.945\n",
      "Iter 8/90 - Loss: 0.911\n",
      "Iter 9/90 - Loss: 0.875\n",
      "Iter 10/90 - Loss: 0.839\n",
      "Iter 11/90 - Loss: 0.803\n",
      "Iter 12/90 - Loss: 0.765\n",
      "Iter 13/90 - Loss: 0.728\n",
      "Iter 14/90 - Loss: 0.690\n",
      "Iter 15/90 - Loss: 0.652\n",
      "Iter 16/90 - Loss: 0.612\n",
      "Iter 17/90 - Loss: 0.575\n",
      "Iter 18/90 - Loss: 0.531\n",
      "Iter 19/90 - Loss: 0.491\n",
      "Iter 20/90 - Loss: 0.451\n",
      "Iter 21/90 - Loss: 0.409\n",
      "Iter 22/90 - Loss: 0.366\n",
      "Iter 23/90 - Loss: 0.326\n",
      "Iter 24/90 - Loss: 0.283\n",
      "Iter 25/90 - Loss: 0.238\n",
      "Iter 26/90 - Loss: 0.197\n",
      "Iter 27/90 - Loss: 0.155\n",
      "Iter 28/90 - Loss: 0.107\n",
      "Iter 29/90 - Loss: 0.064\n",
      "Iter 30/90 - Loss: 0.018\n",
      "Iter 31/90 - Loss: -0.026\n",
      "Iter 32/90 - Loss: -0.073\n",
      "Iter 33/90 - Loss: -0.114\n",
      "Iter 34/90 - Loss: -0.161\n",
      "Iter 35/90 - Loss: -0.205\n",
      "Iter 36/90 - Loss: -0.252\n",
      "Iter 37/90 - Loss: -0.298\n",
      "Iter 38/90 - Loss: -0.342\n",
      "Iter 39/90 - Loss: -0.390\n",
      "Iter 40/90 - Loss: -0.434\n",
      "Iter 41/90 - Loss: -0.480\n",
      "Iter 42/90 - Loss: -0.526\n",
      "Iter 43/90 - Loss: -0.574\n",
      "Iter 44/90 - Loss: -0.619\n",
      "Iter 45/90 - Loss: -0.664\n",
      "Iter 46/90 - Loss: -0.710\n",
      "Iter 47/90 - Loss: -0.755\n",
      "Iter 48/90 - Loss: -0.803\n",
      "Iter 49/90 - Loss: -0.850\n",
      "Iter 50/90 - Loss: -0.892\n",
      "Iter 51/90 - Loss: -0.940\n",
      "Iter 52/90 - Loss: -0.983\n",
      "Iter 53/90 - Loss: -1.028\n",
      "Iter 54/90 - Loss: -1.074\n",
      "Iter 55/90 - Loss: -1.120\n",
      "Iter 56/90 - Loss: -1.166\n",
      "Iter 57/90 - Loss: -1.208\n",
      "Iter 58/90 - Loss: -1.255\n",
      "Iter 59/90 - Loss: -1.293\n",
      "Iter 60/90 - Loss: -1.337\n",
      "Iter 61/90 - Loss: -1.384\n",
      "Iter 62/90 - Loss: -1.421\n",
      "Iter 63/90 - Loss: -1.465\n",
      "Iter 64/90 - Loss: -1.510\n",
      "Iter 65/90 - Loss: -1.552\n",
      "Iter 66/90 - Loss: -1.591\n",
      "Iter 67/90 - Loss: -1.632\n",
      "Iter 68/90 - Loss: -1.669\n",
      "Iter 69/90 - Loss: -1.716\n",
      "Iter 70/90 - Loss: -1.749\n",
      "Iter 71/90 - Loss: -1.791\n",
      "Iter 72/90 - Loss: -1.825\n",
      "Iter 73/90 - Loss: -1.862\n",
      "Iter 74/90 - Loss: -1.920\n",
      "Iter 75/90 - Loss: -1.946\n",
      "Iter 76/90 - Loss: -1.990\n",
      "Iter 77/90 - Loss: -2.007\n",
      "Iter 78/90 - Loss: -2.038\n",
      "Iter 79/90 - Loss: -2.063\n",
      "Iter 80/90 - Loss: -2.125\n",
      "Iter 81/90 - Loss: -2.128\n",
      "Iter 82/90 - Loss: -2.181\n",
      "Iter 83/90 - Loss: -2.204\n",
      "Iter 84/90 - Loss: -2.219\n",
      "Iter 85/90 - Loss: -2.240\n",
      "Iter 86/90 - Loss: -2.269\n",
      "Iter 87/90 - Loss: -2.318\n",
      "Iter 88/90 - Loss: -2.356\n",
      "Iter 89/90 - Loss: -2.395\n",
      "Iter 90/90 - Loss: -2.438\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.16609950363636017 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.12796521186828613 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 1500\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.107\n",
      "Iter 2/90 - Loss: 1.096\n",
      "Iter 3/90 - Loss: 1.068\n",
      "Iter 4/90 - Loss: 1.035\n",
      "Iter 5/90 - Loss: 1.003\n",
      "Iter 6/90 - Loss: 0.971\n",
      "Iter 7/90 - Loss: 0.936\n",
      "Iter 8/90 - Loss: 0.902\n",
      "Iter 9/90 - Loss: 0.867\n",
      "Iter 10/90 - Loss: 0.833\n",
      "Iter 11/90 - Loss: 0.795\n",
      "Iter 12/90 - Loss: 0.758\n",
      "Iter 13/90 - Loss: 0.721\n",
      "Iter 14/90 - Loss: 0.683\n",
      "Iter 15/90 - Loss: 0.644\n",
      "Iter 16/90 - Loss: 0.604\n",
      "Iter 17/90 - Loss: 0.565\n",
      "Iter 18/90 - Loss: 0.525\n",
      "Iter 19/90 - Loss: 0.484\n",
      "Iter 20/90 - Loss: 0.442\n",
      "Iter 21/90 - Loss: 0.403\n",
      "Iter 22/90 - Loss: 0.360\n",
      "Iter 23/90 - Loss: 0.316\n",
      "Iter 24/90 - Loss: 0.275\n",
      "Iter 25/90 - Loss: 0.230\n",
      "Iter 26/90 - Loss: 0.187\n",
      "Iter 27/90 - Loss: 0.144\n",
      "Iter 28/90 - Loss: 0.099\n",
      "Iter 29/90 - Loss: 0.055\n",
      "Iter 30/90 - Loss: 0.010\n",
      "Iter 31/90 - Loss: -0.034\n",
      "Iter 32/90 - Loss: -0.081\n",
      "Iter 33/90 - Loss: -0.125\n",
      "Iter 34/90 - Loss: -0.170\n",
      "Iter 35/90 - Loss: -0.217\n",
      "Iter 36/90 - Loss: -0.262\n",
      "Iter 37/90 - Loss: -0.307\n",
      "Iter 38/90 - Loss: -0.353\n",
      "Iter 39/90 - Loss: -0.398\n",
      "Iter 40/90 - Loss: -0.446\n",
      "Iter 41/90 - Loss: -0.493\n",
      "Iter 42/90 - Loss: -0.538\n",
      "Iter 43/90 - Loss: -0.582\n",
      "Iter 44/90 - Loss: -0.630\n",
      "Iter 45/90 - Loss: -0.676\n",
      "Iter 46/90 - Loss: -0.721\n",
      "Iter 47/90 - Loss: -0.768\n",
      "Iter 48/90 - Loss: -0.814\n",
      "Iter 49/90 - Loss: -0.860\n",
      "Iter 50/90 - Loss: -0.905\n",
      "Iter 51/90 - Loss: -0.951\n",
      "Iter 52/90 - Loss: -0.996\n",
      "Iter 53/90 - Loss: -1.043\n",
      "Iter 54/90 - Loss: -1.085\n",
      "Iter 55/90 - Loss: -1.131\n",
      "Iter 56/90 - Loss: -1.177\n",
      "Iter 57/90 - Loss: -1.220\n",
      "Iter 58/90 - Loss: -1.265\n",
      "Iter 59/90 - Loss: -1.307\n",
      "Iter 60/90 - Loss: -1.352\n",
      "Iter 61/90 - Loss: -1.396\n",
      "Iter 62/90 - Loss: -1.437\n",
      "Iter 63/90 - Loss: -1.483\n",
      "Iter 64/90 - Loss: -1.522\n",
      "Iter 65/90 - Loss: -1.563\n",
      "Iter 66/90 - Loss: -1.605\n",
      "Iter 67/90 - Loss: -1.650\n",
      "Iter 68/90 - Loss: -1.690\n",
      "Iter 69/90 - Loss: -1.728\n",
      "Iter 70/90 - Loss: -1.762\n",
      "Iter 71/90 - Loss: -1.805\n",
      "Iter 72/90 - Loss: -1.846\n",
      "Iter 73/90 - Loss: -1.883\n",
      "Iter 74/90 - Loss: -1.922\n",
      "Iter 75/90 - Loss: -1.967\n",
      "Iter 76/90 - Loss: -2.004\n",
      "Iter 77/90 - Loss: -2.035\n",
      "Iter 78/90 - Loss: -2.081\n",
      "Iter 79/90 - Loss: -2.120\n",
      "Iter 80/90 - Loss: -2.157\n",
      "Iter 81/90 - Loss: -2.209\n",
      "Iter 82/90 - Loss: -2.235\n",
      "Iter 83/90 - Loss: -2.274\n",
      "Iter 84/90 - Loss: -2.326\n",
      "Iter 85/90 - Loss: -2.351\n",
      "Iter 86/90 - Loss: -2.392\n",
      "Iter 87/90 - Loss: -2.415\n",
      "Iter 88/90 - Loss: -2.462\n",
      "Iter 89/90 - Loss: -2.491\n",
      "Iter 90/90 - Loss: -2.523\n",
      "\n",
      "data size: 2000\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.101\n",
      "Iter 2/90 - Loss: 1.093\n",
      "Iter 3/90 - Loss: 1.063\n",
      "Iter 4/90 - Loss: 1.032\n",
      "Iter 5/90 - Loss: 1.000\n",
      "Iter 6/90 - Loss: 0.966\n",
      "Iter 7/90 - Loss: 0.933\n",
      "Iter 8/90 - Loss: 0.899\n",
      "Iter 9/90 - Loss: 0.864\n",
      "Iter 10/90 - Loss: 0.828\n",
      "Iter 11/90 - Loss: 0.791\n",
      "Iter 12/90 - Loss: 0.755\n",
      "Iter 13/90 - Loss: 0.716\n",
      "Iter 14/90 - Loss: 0.679\n",
      "Iter 15/90 - Loss: 0.640\n",
      "Iter 16/90 - Loss: 0.600\n",
      "Iter 17/90 - Loss: 0.561\n",
      "Iter 18/90 - Loss: 0.520\n",
      "Iter 19/90 - Loss: 0.480\n",
      "Iter 20/90 - Loss: 0.438\n",
      "Iter 21/90 - Loss: 0.396\n",
      "Iter 22/90 - Loss: 0.355\n",
      "Iter 23/90 - Loss: 0.311\n",
      "Iter 24/90 - Loss: 0.269\n",
      "Iter 25/90 - Loss: 0.226\n",
      "Iter 26/90 - Loss: 0.182\n",
      "Iter 27/90 - Loss: 0.139\n",
      "Iter 28/90 - Loss: 0.095\n",
      "Iter 29/90 - Loss: 0.050\n",
      "Iter 30/90 - Loss: 0.005\n",
      "Iter 31/90 - Loss: -0.040\n",
      "Iter 32/90 - Loss: -0.085\n",
      "Iter 33/90 - Loss: -0.130\n",
      "Iter 34/90 - Loss: -0.176\n",
      "Iter 35/90 - Loss: -0.220\n",
      "Iter 36/90 - Loss: -0.267\n",
      "Iter 37/90 - Loss: -0.313\n",
      "Iter 38/90 - Loss: -0.360\n",
      "Iter 39/90 - Loss: -0.404\n",
      "Iter 40/90 - Loss: -0.449\n",
      "Iter 41/90 - Loss: -0.496\n",
      "Iter 42/90 - Loss: -0.543\n",
      "Iter 43/90 - Loss: -0.589\n",
      "Iter 44/90 - Loss: -0.636\n",
      "Iter 45/90 - Loss: -0.683\n",
      "Iter 46/90 - Loss: -0.727\n",
      "Iter 47/90 - Loss: -0.774\n",
      "Iter 48/90 - Loss: -0.819\n",
      "Iter 49/90 - Loss: -0.866\n",
      "Iter 50/90 - Loss: -0.911\n",
      "Iter 51/90 - Loss: -0.957\n",
      "Iter 52/90 - Loss: -1.003\n",
      "Iter 53/90 - Loss: -1.050\n",
      "Iter 54/90 - Loss: -1.094\n",
      "Iter 55/90 - Loss: -1.138\n",
      "Iter 56/90 - Loss: -1.183\n",
      "Iter 57/90 - Loss: -1.224\n",
      "Iter 58/90 - Loss: -1.272\n",
      "Iter 59/90 - Loss: -1.313\n",
      "Iter 60/90 - Loss: -1.361\n",
      "Iter 61/90 - Loss: -1.406\n",
      "Iter 62/90 - Loss: -1.451\n",
      "Iter 63/90 - Loss: -1.495\n",
      "Iter 64/90 - Loss: -1.542\n",
      "Iter 65/90 - Loss: -1.579\n",
      "Iter 66/90 - Loss: -1.622\n",
      "Iter 67/90 - Loss: -1.661\n",
      "Iter 68/90 - Loss: -1.713\n",
      "Iter 69/90 - Loss: -1.746\n",
      "Iter 70/90 - Loss: -1.780\n",
      "Iter 71/90 - Loss: -1.822\n",
      "Iter 72/90 - Loss: -1.879\n",
      "Iter 73/90 - Loss: -1.926\n",
      "Iter 74/90 - Loss: -1.960\n",
      "Iter 75/90 - Loss: -2.005\n",
      "Iter 76/90 - Loss: -2.040\n",
      "Iter 77/90 - Loss: -2.081\n",
      "Iter 78/90 - Loss: -2.134\n",
      "Iter 79/90 - Loss: -2.168\n",
      "Iter 80/90 - Loss: -2.208\n",
      "Iter 81/90 - Loss: -2.249\n",
      "Iter 82/90 - Loss: -2.282\n",
      "Iter 83/90 - Loss: -2.316\n",
      "Iter 84/90 - Loss: -2.338\n",
      "Iter 85/90 - Loss: -2.369\n",
      "Iter 86/90 - Loss: -2.418\n",
      "Iter 87/90 - Loss: -2.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5061639547348022 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/90 - Loss: -2.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8025588989257812 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/90 - Loss: -2.300\n",
      "Iter 90/90 - Loss: -2.544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2306578159332275 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.789131760597229 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 3000\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.095\n",
      "Iter 2/90 - Loss: 1.087\n",
      "Iter 3/90 - Loss: 1.058\n",
      "Iter 4/90 - Loss: 1.026\n",
      "Iter 5/90 - Loss: 0.995\n",
      "Iter 6/90 - Loss: 0.962\n",
      "Iter 7/90 - Loss: 0.928\n",
      "Iter 8/90 - Loss: 0.895\n",
      "Iter 9/90 - Loss: 0.859\n",
      "Iter 10/90 - Loss: 0.823\n",
      "Iter 11/90 - Loss: 0.787\n",
      "Iter 12/90 - Loss: 0.750\n",
      "Iter 13/90 - Loss: 0.713\n",
      "Iter 14/90 - Loss: 0.676\n",
      "Iter 15/90 - Loss: 0.635\n",
      "Iter 16/90 - Loss: 0.596\n",
      "Iter 17/90 - Loss: 0.556\n",
      "Iter 18/90 - Loss: 0.516\n",
      "Iter 19/90 - Loss: 0.475\n",
      "Iter 20/90 - Loss: 0.434\n",
      "Iter 21/90 - Loss: 0.392\n",
      "Iter 22/90 - Loss: 0.350\n",
      "Iter 23/90 - Loss: 0.308\n",
      "Iter 24/90 - Loss: 0.265\n",
      "Iter 25/90 - Loss: 0.222\n",
      "Iter 26/90 - Loss: 0.179\n",
      "Iter 27/90 - Loss: 0.133\n",
      "Iter 28/90 - Loss: 0.090\n",
      "Iter 29/90 - Loss: 0.046\n",
      "Iter 30/90 - Loss: 0.000\n",
      "Iter 31/90 - Loss: -0.044\n",
      "Iter 32/90 - Loss: -0.090\n",
      "Iter 33/90 - Loss: -0.135\n",
      "Iter 34/90 - Loss: -0.180\n",
      "Iter 35/90 - Loss: -0.227\n",
      "Iter 36/90 - Loss: -0.272\n",
      "Iter 37/90 - Loss: -0.317\n",
      "Iter 38/90 - Loss: -0.365\n",
      "Iter 39/90 - Loss: -0.410\n",
      "Iter 40/90 - Loss: -0.456\n",
      "Iter 41/90 - Loss: -0.503\n",
      "Iter 42/90 - Loss: -0.549\n",
      "Iter 43/90 - Loss: -0.595\n",
      "Iter 44/90 - Loss: -0.641\n",
      "Iter 45/90 - Loss: -0.688\n",
      "Iter 46/90 - Loss: -0.734\n",
      "Iter 47/90 - Loss: -0.778\n",
      "Iter 48/90 - Loss: -0.825\n",
      "Iter 49/90 - Loss: -0.872\n",
      "Iter 50/90 - Loss: -0.918\n",
      "Iter 51/90 - Loss: -0.963\n",
      "Iter 52/90 - Loss: -1.010\n",
      "Iter 53/90 - Loss: -1.055\n",
      "Iter 54/90 - Loss: -1.101\n",
      "Iter 55/90 - Loss: -1.145\n",
      "Iter 56/90 - Loss: -1.191\n",
      "Iter 57/90 - Loss: -1.236\n",
      "Iter 58/90 - Loss: -1.282\n",
      "Iter 59/90 - Loss: -1.323\n",
      "Iter 60/90 - Loss: -1.369\n",
      "Iter 61/90 - Loss: -1.414\n",
      "Iter 62/90 - Loss: -1.459\n",
      "Iter 63/90 - Loss: -1.503\n",
      "Iter 64/90 - Loss: -1.550\n",
      "Iter 65/90 - Loss: -1.591\n",
      "Iter 66/90 - Loss: -1.634\n",
      "Iter 67/90 - Loss: -1.675\n",
      "Iter 68/90 - Loss: -1.729\n",
      "Iter 69/90 - Loss: -1.776\n",
      "Iter 70/90 - Loss: -1.820\n",
      "Iter 71/90 - Loss: -1.852\n",
      "Iter 72/90 - Loss: -1.908\n",
      "Iter 73/90 - Loss: -1.953\n",
      "Iter 74/90 - Loss: -1.987\n",
      "Iter 75/90 - Loss: -2.020\n",
      "Iter 76/90 - Loss: -2.077\n",
      "Iter 77/90 - Loss: -2.118\n",
      "Iter 78/90 - Loss: -2.156\n",
      "Iter 79/90 - Loss: -2.196\n",
      "Iter 80/90 - Loss: -2.233\n",
      "Iter 81/90 - Loss: -2.270\n",
      "Iter 82/90 - Loss: -2.295\n",
      "Iter 83/90 - Loss: -2.336\n",
      "Iter 84/90 - Loss: -2.369\n",
      "Iter 85/90 - Loss: -2.411\n",
      "Iter 86/90 - Loss: -2.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0235408544540405 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 87/90 - Loss: -2.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0811233520507812 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/90 - Loss: -2.294\n",
      "Iter 89/90 - Loss: -2.302\n",
      "Iter 90/90 - Loss: -2.420\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8309599161148071 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.7946535348892212 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 4000\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.093\n",
      "Iter 2/90 - Loss: 1.085\n",
      "Iter 3/90 - Loss: 1.055\n",
      "Iter 4/90 - Loss: 1.024\n",
      "Iter 5/90 - Loss: 0.992\n",
      "Iter 6/90 - Loss: 0.959\n",
      "Iter 7/90 - Loss: 0.926\n",
      "Iter 8/90 - Loss: 0.891\n",
      "Iter 9/90 - Loss: 0.857\n",
      "Iter 10/90 - Loss: 0.821\n",
      "Iter 11/90 - Loss: 0.785\n",
      "Iter 12/90 - Loss: 0.748\n",
      "Iter 13/90 - Loss: 0.710\n",
      "Iter 14/90 - Loss: 0.672\n",
      "Iter 15/90 - Loss: 0.633\n",
      "Iter 16/90 - Loss: 0.594\n",
      "Iter 17/90 - Loss: 0.555\n",
      "Iter 18/90 - Loss: 0.514\n",
      "Iter 19/90 - Loss: 0.474\n",
      "Iter 20/90 - Loss: 0.432\n",
      "Iter 21/90 - Loss: 0.390\n",
      "Iter 22/90 - Loss: 0.348\n",
      "Iter 23/90 - Loss: 0.306\n",
      "Iter 24/90 - Loss: 0.263\n",
      "Iter 25/90 - Loss: 0.219\n",
      "Iter 26/90 - Loss: 0.176\n",
      "Iter 27/90 - Loss: 0.132\n",
      "Iter 28/90 - Loss: 0.088\n",
      "Iter 29/90 - Loss: 0.043\n",
      "Iter 30/90 - Loss: -0.002\n",
      "Iter 31/90 - Loss: -0.047\n",
      "Iter 32/90 - Loss: -0.092\n",
      "Iter 33/90 - Loss: -0.138\n",
      "Iter 34/90 - Loss: -0.183\n",
      "Iter 35/90 - Loss: -0.229\n",
      "Iter 36/90 - Loss: -0.275\n",
      "Iter 37/90 - Loss: -0.320\n",
      "Iter 38/90 - Loss: -0.367\n",
      "Iter 39/90 - Loss: -0.413\n",
      "Iter 40/90 - Loss: -0.459\n",
      "Iter 41/90 - Loss: -0.505\n",
      "Iter 42/90 - Loss: -0.551\n",
      "Iter 43/90 - Loss: -0.597\n",
      "Iter 44/90 - Loss: -0.644\n",
      "Iter 45/90 - Loss: -0.689\n",
      "Iter 46/90 - Loss: -0.737\n",
      "Iter 47/90 - Loss: -0.782\n",
      "Iter 48/90 - Loss: -0.829\n",
      "Iter 49/90 - Loss: -0.876\n",
      "Iter 50/90 - Loss: -0.921\n",
      "Iter 51/90 - Loss: -0.967\n",
      "Iter 52/90 - Loss: -1.014\n",
      "Iter 53/90 - Loss: -1.061\n",
      "Iter 54/90 - Loss: -1.105\n",
      "Iter 55/90 - Loss: -1.152\n",
      "Iter 56/90 - Loss: -1.199\n",
      "Iter 57/90 - Loss: -1.245\n",
      "Iter 58/90 - Loss: -1.291\n",
      "Iter 59/90 - Loss: -1.333\n",
      "Iter 60/90 - Loss: -1.380\n",
      "Iter 61/90 - Loss: -1.428\n",
      "Iter 62/90 - Loss: -1.471\n",
      "Iter 63/90 - Loss: -1.519\n",
      "Iter 64/90 - Loss: -1.560\n",
      "Iter 65/90 - Loss: -1.599\n",
      "Iter 66/90 - Loss: -1.649\n",
      "Iter 67/90 - Loss: -1.681\n",
      "Iter 68/90 - Loss: -1.735\n",
      "Iter 69/90 - Loss: -1.773\n",
      "Iter 70/90 - Loss: -1.823\n",
      "Iter 71/90 - Loss: -1.868\n",
      "Iter 72/90 - Loss: -1.907\n",
      "Iter 73/90 - Loss: -1.956\n",
      "Iter 74/90 - Loss: -1.996\n",
      "Iter 75/90 - Loss: -2.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.201503038406372 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 76/90 - Loss: -2.001\n",
      "Iter 77/90 - Loss: -2.033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5351202487945557 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 78/90 - Loss: -2.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5805938243865967 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 79/90 - Loss: -2.092\n",
      "Iter 80/90 - Loss: -2.122\n",
      "Iter 81/90 - Loss: -2.152\n",
      "Iter 82/90 - Loss: -2.174\n",
      "Iter 83/90 - Loss: -2.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.407294988632202 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 84/90 - Loss: -2.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.207353115081787 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 85/90 - Loss: -2.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1956214904785156 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 86/90 - Loss: -2.255\n",
      "Iter 87/90 - Loss: -2.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0348119735717773 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/90 - Loss: -2.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5683619976043701 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/90 - Loss: -2.302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0413354635238647 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 90/90 - Loss: -2.310\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4667816162109375 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.6418375968933105 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 5000\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.091\n",
      "Iter 2/90 - Loss: 1.083\n",
      "Iter 3/90 - Loss: 1.053\n",
      "Iter 4/90 - Loss: 1.023\n",
      "Iter 5/90 - Loss: 0.991\n",
      "Iter 6/90 - Loss: 0.958\n",
      "Iter 7/90 - Loss: 0.925\n",
      "Iter 8/90 - Loss: 0.890\n",
      "Iter 9/90 - Loss: 0.856\n",
      "Iter 10/90 - Loss: 0.820\n",
      "Iter 11/90 - Loss: 0.783\n",
      "Iter 12/90 - Loss: 0.747\n",
      "Iter 13/90 - Loss: 0.709\n",
      "Iter 14/90 - Loss: 0.671\n",
      "Iter 15/90 - Loss: 0.631\n",
      "Iter 16/90 - Loss: 0.593\n",
      "Iter 17/90 - Loss: 0.553\n",
      "Iter 18/90 - Loss: 0.512\n",
      "Iter 19/90 - Loss: 0.472\n",
      "Iter 20/90 - Loss: 0.430\n",
      "Iter 21/90 - Loss: 0.389\n",
      "Iter 22/90 - Loss: 0.347\n",
      "Iter 23/90 - Loss: 0.304\n",
      "Iter 24/90 - Loss: 0.261\n",
      "Iter 25/90 - Loss: 0.218\n",
      "Iter 26/90 - Loss: 0.174\n",
      "Iter 27/90 - Loss: 0.130\n",
      "Iter 28/90 - Loss: 0.086\n",
      "Iter 29/90 - Loss: 0.041\n",
      "Iter 30/90 - Loss: -0.004\n",
      "Iter 31/90 - Loss: -0.049\n",
      "Iter 32/90 - Loss: -0.094\n",
      "Iter 33/90 - Loss: -0.139\n",
      "Iter 34/90 - Loss: -0.185\n",
      "Iter 35/90 - Loss: -0.231\n",
      "Iter 36/90 - Loss: -0.277\n",
      "Iter 37/90 - Loss: -0.323\n",
      "Iter 38/90 - Loss: -0.368\n",
      "Iter 39/90 - Loss: -0.415\n",
      "Iter 40/90 - Loss: -0.461\n",
      "Iter 41/90 - Loss: -0.507\n",
      "Iter 42/90 - Loss: -0.553\n",
      "Iter 43/90 - Loss: -0.600\n",
      "Iter 44/90 - Loss: -0.646\n",
      "Iter 45/90 - Loss: -0.692\n",
      "Iter 46/90 - Loss: -0.739\n",
      "Iter 47/90 - Loss: -0.784\n",
      "Iter 48/90 - Loss: -0.830\n",
      "Iter 49/90 - Loss: -0.876\n",
      "Iter 50/90 - Loss: -0.923\n",
      "Iter 51/90 - Loss: -0.970\n",
      "Iter 52/90 - Loss: -1.016\n",
      "Iter 53/90 - Loss: -1.063\n",
      "Iter 54/90 - Loss: -1.109\n",
      "Iter 55/90 - Loss: -1.155\n",
      "Iter 56/90 - Loss: -1.200\n",
      "Iter 57/90 - Loss: -1.246\n",
      "Iter 58/90 - Loss: -1.294\n",
      "Iter 59/90 - Loss: -1.339\n",
      "Iter 60/90 - Loss: -1.383\n",
      "Iter 61/90 - Loss: -1.430\n",
      "Iter 62/90 - Loss: -1.474\n",
      "Iter 63/90 - Loss: -1.519\n",
      "Iter 64/90 - Loss: -1.567\n",
      "Iter 65/90 - Loss: -1.615\n",
      "Iter 66/90 - Loss: -1.659\n",
      "Iter 67/90 - Loss: -1.705\n",
      "Iter 68/90 - Loss: -1.750\n",
      "Iter 69/90 - Loss: -1.794\n",
      "Iter 70/90 - Loss: -1.836\n",
      "Iter 71/90 - Loss: -1.880\n",
      "Iter 72/90 - Loss: -1.922\n",
      "Iter 73/90 - Loss: -1.960\n",
      "Iter 74/90 - Loss: -2.001\n",
      "Iter 75/90 - Loss: -2.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.964000940322876 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 76/90 - Loss: -2.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5689201354980469 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 77/90 - Loss: -2.037\n",
      "Iter 78/90 - Loss: -2.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4567846059799194 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 79/90 - Loss: -2.093\n",
      "Iter 80/90 - Loss: -2.113\n",
      "Iter 81/90 - Loss: -2.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1717650890350342 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 82/90 - Loss: -2.162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7791874408721924 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 83/90 - Loss: -2.180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.471986770629883 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 84/90 - Loss: -2.195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5697788000106812 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 85/90 - Loss: -2.223\n",
      "Iter 86/90 - Loss: -2.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0402166843414307 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 87/90 - Loss: -2.236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7209057807922363 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/90 - Loss: -2.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.492553234100342 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/90 - Loss: -2.284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.018269538879395 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 90/90 - Loss: -2.271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.530604124069214 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.763795793056488 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 6000\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.090\n",
      "Iter 2/90 - Loss: 1.083\n",
      "Iter 3/90 - Loss: 1.053\n",
      "Iter 4/90 - Loss: 1.022\n",
      "Iter 5/90 - Loss: 0.990\n",
      "Iter 6/90 - Loss: 0.957\n",
      "Iter 7/90 - Loss: 0.924\n",
      "Iter 8/90 - Loss: 0.889\n",
      "Iter 9/90 - Loss: 0.855\n",
      "Iter 10/90 - Loss: 0.819\n",
      "Iter 11/90 - Loss: 0.783\n",
      "Iter 12/90 - Loss: 0.746\n",
      "Iter 13/90 - Loss: 0.708\n",
      "Iter 14/90 - Loss: 0.670\n",
      "Iter 15/90 - Loss: 0.631\n",
      "Iter 16/90 - Loss: 0.592\n",
      "Iter 17/90 - Loss: 0.552\n",
      "Iter 18/90 - Loss: 0.512\n",
      "Iter 19/90 - Loss: 0.471\n",
      "Iter 20/90 - Loss: 0.430\n",
      "Iter 21/90 - Loss: 0.388\n",
      "Iter 22/90 - Loss: 0.346\n",
      "Iter 23/90 - Loss: 0.303\n",
      "Iter 24/90 - Loss: 0.260\n",
      "Iter 25/90 - Loss: 0.217\n",
      "Iter 26/90 - Loss: 0.173\n",
      "Iter 27/90 - Loss: 0.129\n",
      "Iter 28/90 - Loss: 0.085\n",
      "Iter 29/90 - Loss: 0.040\n",
      "Iter 30/90 - Loss: -0.005\n",
      "Iter 31/90 - Loss: -0.049\n",
      "Iter 32/90 - Loss: -0.095\n",
      "Iter 33/90 - Loss: -0.141\n",
      "Iter 34/90 - Loss: -0.185\n",
      "Iter 35/90 - Loss: -0.232\n",
      "Iter 36/90 - Loss: -0.278\n",
      "Iter 37/90 - Loss: -0.323\n",
      "Iter 38/90 - Loss: -0.370\n",
      "Iter 39/90 - Loss: -0.416\n",
      "Iter 40/90 - Loss: -0.462\n",
      "Iter 41/90 - Loss: -0.508\n",
      "Iter 42/90 - Loss: -0.555\n",
      "Iter 43/90 - Loss: -0.601\n",
      "Iter 44/90 - Loss: -0.647\n",
      "Iter 45/90 - Loss: -0.694\n",
      "Iter 46/90 - Loss: -0.740\n",
      "Iter 47/90 - Loss: -0.787\n",
      "Iter 48/90 - Loss: -0.832\n",
      "Iter 49/90 - Loss: -0.878\n",
      "Iter 50/90 - Loss: -0.925\n",
      "Iter 51/90 - Loss: -0.972\n",
      "Iter 52/90 - Loss: -1.018\n",
      "Iter 53/90 - Loss: -1.063\n",
      "Iter 54/90 - Loss: -1.108\n",
      "Iter 55/90 - Loss: -1.155\n",
      "Iter 56/90 - Loss: -1.198\n",
      "Iter 57/90 - Loss: -1.244\n",
      "Iter 58/90 - Loss: -1.289\n",
      "Iter 59/90 - Loss: -1.332\n",
      "Iter 60/90 - Loss: -1.379\n",
      "Iter 61/90 - Loss: -1.428\n",
      "Iter 62/90 - Loss: -1.474\n",
      "Iter 63/90 - Loss: -1.519\n",
      "Iter 64/90 - Loss: -1.562\n",
      "Iter 65/90 - Loss: -1.603\n",
      "Iter 66/90 - Loss: -1.649\n",
      "Iter 67/90 - Loss: -1.693\n",
      "Iter 68/90 - Loss: -1.736\n",
      "Iter 69/90 - Loss: -1.779\n",
      "Iter 70/90 - Loss: -1.819\n",
      "Iter 71/90 - Loss: -1.856\n",
      "Iter 72/90 - Loss: -1.900\n",
      "Iter 73/90 - Loss: -1.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3948787450790405 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 74/90 - Loss: -1.924\n",
      "Iter 75/90 - Loss: -2.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5570186376571655 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 76/90 - Loss: -1.995\n",
      "Iter 77/90 - Loss: -2.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4278020858764648 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 78/90 - Loss: -2.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1239588260650635 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 79/90 - Loss: -2.096\n",
      "Iter 80/90 - Loss: -2.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.832900047302246 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 81/90 - Loss: -2.153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.53220796585083 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 82/90 - Loss: -2.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.368093490600586 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 83/90 - Loss: -2.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.819913387298584 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 84/90 - Loss: -2.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9450128078460693 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 85/90 - Loss: -2.245\n",
      "Iter 86/90 - Loss: -2.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1007262468338013 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 87/90 - Loss: -2.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6532351970672607 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/90 - Loss: -2.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1406118869781494 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/90 - Loss: -2.308\n",
      "Iter 90/90 - Loss: -2.301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2126699686050415 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.5359829068183899 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 7000\n",
      "Use Cuda: True\n",
      "Iter 1/90 - Loss: 1.089\n",
      "Iter 2/90 - Loss: 1.081\n",
      "Iter 3/90 - Loss: 1.052\n",
      "Iter 4/90 - Loss: 1.021\n",
      "Iter 5/90 - Loss: 0.989\n",
      "Iter 6/90 - Loss: 0.957\n",
      "Iter 7/90 - Loss: 0.923\n",
      "Iter 8/90 - Loss: 0.889\n",
      "Iter 9/90 - Loss: 0.853\n",
      "Iter 10/90 - Loss: 0.818\n",
      "Iter 11/90 - Loss: 0.782\n",
      "Iter 12/90 - Loss: 0.745\n",
      "Iter 13/90 - Loss: 0.707\n",
      "Iter 14/90 - Loss: 0.669\n",
      "Iter 15/90 - Loss: 0.630\n",
      "Iter 16/90 - Loss: 0.590\n",
      "Iter 17/90 - Loss: 0.551\n",
      "Iter 18/90 - Loss: 0.510\n",
      "Iter 19/90 - Loss: 0.470\n",
      "Iter 20/90 - Loss: 0.429\n",
      "Iter 21/90 - Loss: 0.387\n",
      "Iter 22/90 - Loss: 0.344\n",
      "Iter 23/90 - Loss: 0.302\n",
      "Iter 24/90 - Loss: 0.259\n",
      "Iter 25/90 - Loss: 0.215\n",
      "Iter 26/90 - Loss: 0.172\n",
      "Iter 27/90 - Loss: 0.128\n",
      "Iter 28/90 - Loss: 0.083\n",
      "Iter 29/90 - Loss: 0.039\n",
      "Iter 30/90 - Loss: -0.006\n",
      "Iter 31/90 - Loss: -0.051\n",
      "Iter 32/90 - Loss: -0.096\n",
      "Iter 33/90 - Loss: -0.141\n",
      "Iter 34/90 - Loss: -0.187\n",
      "Iter 35/90 - Loss: -0.232\n",
      "Iter 36/90 - Loss: -0.278\n",
      "Iter 37/90 - Loss: -0.325\n",
      "Iter 38/90 - Loss: -0.370\n",
      "Iter 39/90 - Loss: -0.417\n",
      "Iter 40/90 - Loss: -0.464\n",
      "Iter 41/90 - Loss: -0.510\n",
      "Iter 42/90 - Loss: -0.556\n",
      "Iter 43/90 - Loss: -0.603\n",
      "Iter 44/90 - Loss: -0.648\n",
      "Iter 45/90 - Loss: -0.695\n",
      "Iter 46/90 - Loss: -0.742\n",
      "Iter 47/90 - Loss: -0.788\n",
      "Iter 48/90 - Loss: -0.835\n",
      "Iter 49/90 - Loss: -0.880\n",
      "Iter 50/90 - Loss: -0.926\n",
      "Iter 51/90 - Loss: -0.971\n",
      "Iter 52/90 - Loss: -1.020\n",
      "Iter 53/90 - Loss: -1.068\n",
      "Iter 54/90 - Loss: -1.112\n",
      "Iter 55/90 - Loss: -1.158\n",
      "Iter 56/90 - Loss: -1.207\n",
      "Iter 57/90 - Loss: -1.251\n",
      "Iter 58/90 - Loss: -1.298\n",
      "Iter 59/90 - Loss: -1.347\n",
      "Iter 60/90 - Loss: -1.391\n",
      "Iter 61/90 - Loss: -1.428\n",
      "Iter 62/90 - Loss: -1.475\n",
      "Iter 63/90 - Loss: -1.521\n",
      "Iter 64/90 - Loss: -1.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6612063646316528 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 65/90 - Loss: -1.582\n",
      "Iter 66/90 - Loss: -1.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4039632081985474 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 67/90 - Loss: -1.662\n",
      "Iter 68/90 - Loss: -1.744\n",
      "Iter 69/90 - Loss: -1.775\n",
      "Iter 70/90 - Loss: -1.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2077322006225586 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 71/90 - Loss: -1.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8120429515838623 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 72/90 - Loss: -1.857\n",
      "Iter 73/90 - Loss: -1.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.846545696258545 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 74/90 - Loss: -1.933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6949286460876465 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 75/90 - Loss: -1.971\n",
      "Iter 76/90 - Loss: -2.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1487199068069458 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 77/90 - Loss: -2.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8353792428970337 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 78/90 - Loss: -2.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2152278423309326 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 79/90 - Loss: -2.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.088009834289551 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 80/90 - Loss: -2.123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.475698947906494 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 81/90 - Loss: -2.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.598480224609375 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 82/90 - Loss: -2.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.173841953277588 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 83/90 - Loss: -2.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.192974805831909 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 84/90 - Loss: -2.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.085235595703125 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 85/90 - Loss: -2.230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.670097351074219 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 86/90 - Loss: -2.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.456928253173828 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 87/90 - Loss: -2.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.157390594482422 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/90 - Loss: -2.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.4286208152771 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/90 - Loss: -2.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.047003746032715 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 90/90 - Loss: -2.312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7538414001464844 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.35445261001586914 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gpu_size_vec = [100,300,500,700,1000,1500,2000,3000,4000,5000,6000,7000]\n",
    "Nval = 4\n",
    "Dval = 4\n",
    "\n",
    "for size in gpu_size_vec:\n",
    "    print(f\"data size: {size}\")\n",
    "    \"\"\"Set up the training and testing data\"\"\"\n",
    "    n = size # input size\n",
    "\n",
    "#     x = 5 * torch.rand(n, Dval)\n",
    "\n",
    "#     y = torch.stack([\n",
    "#         torch.sin(x[:, 0] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         torch.cos(x[:, 0] * (2 * math.pi)) + torch.cos(x[:, 2] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         torch.sin(x[:, 2] * (2 * math.pi)) + torch.cos(x[:, 1] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         (torch.cos(x[:, 3] * (2 * math.pi)))* (torch.sin(x[:, 0] * (2 * math.pi))) + torch.randn(n) * 0.02,\n",
    "#     ], -1)\n",
    "\n",
    "    x = 5 * torch.rand(n)\n",
    "    \n",
    "    y = torch.stack([\n",
    "        torch.sin(3 * x) + torch.randn(n) * 0.02,\n",
    "        torch.cos(x) + torch.cos(2 * x) + torch.randn(n) * 0.02,\n",
    "        torch.sin(x) + torch.cos(x) + torch.randn(n) * 0.02,\n",
    "        torch.cos(x) * torch.cos(x) + torch.randn(n) * 0.02,\n",
    "    ], -1)\n",
    "\n",
    "#     train_x = torch.Tensor(x[:int(0.8*n), :])\n",
    "#     train_y = y[:int(0.8*n), :]\n",
    "\n",
    "#     test_x = torch.Tensor(x[int(0.8*n):, :])\n",
    "\n",
    "#     test_y = torch.Tensor(y[int(0.8*n):, :])\n",
    "\n",
    "    train_x = x[:int(0.8*n)]\n",
    "    train_y = y[:int(0.8*n)]\n",
    "\n",
    "    test_x = x[int(0.8*n): ]\n",
    "\n",
    "    test_y = y[int(0.8*n): ]\n",
    "\n",
    "#     # normalize features\n",
    "#     mean = train_x.mean(dim=-2, keepdim=True)\n",
    "#     std = train_x.std(dim=-2, keepdim=True) # + 1e-6 # prevent dividing by 0\n",
    "#     train_x = (train_x - mean) / std\n",
    "#     test_x = (test_x - mean) / std\n",
    "\n",
    "#     # normalize labels\n",
    "#     mean, std = train_y.mean(),train_y.std()\n",
    "#     train_y = (train_y - mean) / std\n",
    "#     test_y = (test_y - mean) / std\n",
    "\n",
    "#     norm_vec = (vec - mean) / std\n",
    "    \n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=Dval)\n",
    "    model = MultitaskGPModel(train_x, train_y, likelihood, num_base_kernels)\n",
    "    \n",
    "    start_time = time.time() # include the time of copying values onto gpu\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(f\"Use Cuda: {use_cuda}\")\n",
    "    if(use_cuda):\n",
    "        train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "    \n",
    "    \"\"\"train the model hyperparameters\"\"\"\n",
    "    import os\n",
    "    smoke_test = ('CI' in os.environ)\n",
    "    training_iterations = 2 if smoke_test else 90\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.09)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "#           if(i > training_iterations*0.8):\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "    \n",
    "    gpu_training_time.append(time.time() - start_time)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \"\"\" Making predictions with the model\"\"\"\n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Exact predictions\n",
    "    with torch.no_grad(): #, gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x) # no noise\n",
    "        covar = preds.covariance_matrix\n",
    "        gpu_exact_meancovar.append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # LOVE without cache\n",
    "        # Clear the cache from the previous computations\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x)\n",
    "        fast_covar = preds.covariance_matrix\n",
    "        gpu_love_meancovar.append(time.time() - start_time)\n",
    "        \n",
    "        \n",
    "#     \"\"\"\n",
    "#     Compute sum of squared difference LOVE diagonal covariance elements from exact diagonal elements \n",
    "#     (again divided by trace of exact covariance to make the quantity normalized), as a function\n",
    "#     of vector size\n",
    "#     \"\"\"\n",
    "    \n",
    "#     exactdiag = torch.diagonal(covar)\n",
    "#     lovediag = torch.diagonal(fast_covar)\n",
    "    \n",
    "# #     exactdiag = torch.diagonal(covar).log()\n",
    "# #     lovediag = torch.diagonal(fast_covar).log()\n",
    "#     diff = (exactdiag - lovediag).square().mean().sqrt()\n",
    "#     print(diff)\n",
    "#     diff = diff / exactdiag.square().mean().sqrt()\n",
    "#     love_covar_error.append(diff)\n",
    "#     print(f\"error: {diff}\")\n",
    "    \n",
    "    # LOVE with cache\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x)\n",
    "        fast_covar = preds.covariance_matrix\n",
    "        gpu_love_meancovar_cache.append(time.time() - start_time)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.613696098327637, 8.455718517303467, 8.370301961898804, 9.094841003417969, 9.149701356887817, 10.029205322265625, 13.803481817245483, 15.566254615783691, 27.517878770828247, 38.603108644485474, 47.328524351119995, 78.90387606620789]\n",
      "[0.5414190292358398, 0.40481114387512207, 0.29409217834472656, 1.0095553398132324, 4.690918445587158, 4.783432960510254, 21.339080572128296, 61.75691366195679, 134.4991636276245, 252.90403771400452, 425.3043465614319, 671.6062850952148]\n",
      "[0.032082319259643555, 0.5117297172546387, 0.40432190895080566, 0.546863317489624, 1.417698621749878, 0.5595638751983643, 1.4504857063293457, 1.524207592010498, 1.667036771774292, 1.8678596019744873, 2.2986080646514893, 3.460428237915039]\n",
      "[0.01564168930053711, 0.03661656379699707, 0.03747868537902832, 0.036977291107177734, 0.039435625076293945, 0.04876422882080078, 0.06599092483520508, 0.1306290626525879, 0.25631117820739746, 0.44233059883117676, 0.7250955104827881, 1.1036367416381836]\n"
     ]
    }
   ],
   "source": [
    "print(gpu_training_time)\n",
    "print(gpu_exact_meancovar)\n",
    "print(gpu_love_meancovar)\n",
    "print(gpu_love_meancovar_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # plot with various axes scales\n",
    "# plt.figure()\n",
    "\n",
    "# CPU vs GPU training\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(cpu_size_vec, cpu_training_time, 'r-', label='cpu training time')\n",
    "plt.plot(gpu_size_vec, gpu_training_time, 'b-', label='gpu training time')\n",
    "plt.ylabel('Time (second)')\n",
    "plt.xlabel('Input Size')\n",
    "plt.title('CPU vs. GPU Training Time')\n",
    "# plt.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictive Distribution Computation Time\n",
    "\n",
    "# # Initialize plots\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 6), sharey=True)\n",
    "\n",
    "plt.suptitle('Predictive Distribution Computation Time', fontweight='bold')\n",
    "\n",
    "ax1.plot(cpu_size_vec, cpu_exact_meancovar, 'r-', label='cpu exact')\n",
    "ax1.plot(gpu_size_vec, gpu_exact_meancovar, 'b-', label='gpu exact')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "ax2.plot(cpu_size_vec, cpu_love_meancovar, 'g-', label='cpu love no cache')\n",
    "ax2.plot(gpu_size_vec, gpu_love_meancovar, 'c-', label='gpu love no cache')\n",
    "ax2.legend()\n",
    "# plt.ylabel('Time')\n",
    "# plt.xlabel('Input Size')\n",
    "\n",
    "plt.subplot(133)\n",
    "ax3.plot(cpu_size_vec, cpu_love_meancovar_cache, 'y-', label='cpu love with cache')\n",
    "ax3.plot(gpu_size_vec, gpu_love_meancovar_cache, 'm-', label='gpu love with cache')\n",
    "ax3.legend()\n",
    "\n",
    "plt.setp([ax1,ax2,ax3], xlabel='Input Size')\n",
    "plt.setp(ax1, ylabel='Time (Second)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
