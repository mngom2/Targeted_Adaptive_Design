{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU vs. CPU Running Time Test\n",
    "\n",
    "We investigate the running time of GPytorch on CPU and GPU for training (150 iterations) and computing predictive distributions. For predictive distribution we also investigate the effect of LOVE approximation, so the scenarios are:\n",
    "- CPU Exact\n",
    "- GPU Exact\n",
    "- CPU with LOVE without cache\n",
    "- GPU with LOVE without cache\n",
    "- CPU with LOVE with cache\n",
    "- GPU with LOVE with cache\n",
    "\n",
    "We use a customized Multitask Kernel with variable number of Kronecker Product, the same Kernel as used in Targeted Adaptive Design. We use 4 dimensional input and output.\n",
    "\n",
    "The training and testing data are generated using sin and cos functions, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "# from Data_Gen_Script import VField\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from gpytorch.kernels import MultitaskKernel\n",
    "from gpytorch.constraints import Positive\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TensorProductKernel(MultitaskKernel):\n",
    "    \"\"\"\n",
    "    Class to get the tensorproduct kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_covar_module,  num_tasks, rank=1, pos_constraint = None, tri_constaint = None, task_covar_prior=None, **kwargs):\n",
    "        super().__init__(data_covar_module, num_tasks, rank, task_covar_prior = None, **kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, add_jitter = False, **params):\n",
    "        if last_dim_is_batch:\n",
    "            raise RuntimeError(\"MultitaskKernel does not accept the last_dim_is_batch argument.\")\n",
    "        covar_i = self.task_covar_module.covar_matrix #.evaluate()\n",
    "            \n",
    "        covar_i = covar_i.evaluate()\n",
    "        if len(x1.shape[:-2]):\n",
    "            covar_i = covar_i.repeat(*x1.shape[:-2], 1, 1)\n",
    "        covar_x = gpytorch.lazy.lazify(self.data_covar_module.forward(x1, x2, **params))#(self.data_covar_module.forward(x1, x2, **params))#\n",
    "        if (add_jitter == True):\n",
    "            covar_x = covar_x #+ (1e-6) * torch.eye(covar_x.shape[0])\n",
    "        res=gpytorch.lazy.KroneckerProductLazyTensor(covar_x, covar_i) #gpytorch.lazy.lazify(torch.kron(covar_x, covar_i))\n",
    "\n",
    "        return res.diag() if diag else res\n",
    "        \n",
    "        \n",
    "from copy import deepcopy\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from gpytorch.priors import Prior\n",
    "from gpytorch.kernels import Kernel\n",
    "from gpytorch.kernels import IndexKernel\n",
    "from gpytorch.constraints import Positive\n",
    "\n",
    "# This is the main Kernel to use\n",
    "\n",
    "class SepTensorProductKernel(Kernel):\n",
    "    \"\"\"\n",
    "    Class to get the tensorproduct kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_kernels: List, num_tasks: int, rank: Union[int, List] = 1, \n",
    "        task_covar_prior: Optional[Prior] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_kernels (:type: list of `Kernel` objects): A list of base kernels.\n",
    "            num_tasks (int): The number of output tasks to fit.\n",
    "            rank (int): Rank of index kernel to use for task covariance matrix for each\n",
    "                        of the base kernels.\n",
    "            task_covar_prior (:obj:`gpytorch.priors.Prior`): Prior to use for each\n",
    "                task kernel. See :class:`gpytorch.kernels.IndexKernel` for details.\n",
    "        \"\"\"\n",
    "        if len(base_kernels) < 1:\n",
    "            raise ValueError(\"At least one base kernel must be provided.\")\n",
    "        for k in base_kernels:\n",
    "            if not isinstance(k, Kernel):\n",
    "                raise ValueError(\"base_kernels must only contain Kernel objects\")\n",
    "        if not isinstance(rank, list):\n",
    "            rank = [rank] * len(base_kernels)\n",
    "\n",
    "        super(SepTensorProductKernel, self).__init__()\n",
    "        self.covar_module_list = ModuleList(\n",
    "            [\n",
    "                TensorProductKernel(base_kernel, num_tasks=num_tasks, rank=r, task_covar_prior=task_covar_prior)\n",
    "                for base_kernel, r in zip(base_kernels, rank)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        res = self.covar_module_list[0].forward(x1, x2, **params)\n",
    "        for m in self.covar_module_list[1:]:\n",
    "            res += m.forward(x1, x2, **params)\n",
    "        return res\n",
    "\n",
    "    def num_outputs_per_input(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Given `n` data points `x1` and `m` datapoints `x2`, this multitask kernel\n",
    "        returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.\n",
    "        \"\"\"\n",
    "        return self.covar_module_list[0].num_outputs_per_input(x1, x2)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        new_kernel = deepcopy(self)\n",
    "        new_kernel.covar_module_list = ModuleList(\n",
    "            [base_kernel.__getitem__(index) for base_kernel in self.covar_module_list]\n",
    "        )\n",
    "        return new_kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Initialization\n",
    "\"\"\"\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "              gpytorch.means.ConstantMean(), num_tasks=Dval\n",
    "        )\n",
    "        \n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(gpytorch.kernels.ScaleKernel(( gpytorch.kernels.RBFKernel() ))) \n",
    "            #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    " \n",
    "            \n",
    "        self.covar_module = SepTensorProductKernel(base_kernels,num_tasks = Dval)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "num_base_kernels = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_training_time = []\n",
    "cpu_exact_meancovar = []\n",
    "cpu_love_meancovar = []\n",
    "cpu_love_meancovar_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cpu_size_vec = [100,300,500,700,1000,1500,2000]\n",
    "# Nval = 4\n",
    "# Dval = 4\n",
    "\n",
    "# for size in cpu_size_vec:\n",
    "#     print(f\"data size: {size}\")\n",
    "#     \"\"\"Set up the training and testing data\"\"\"\n",
    "#     n = size # input size\n",
    "\n",
    "# #     x = 5 * torch.rand(n, Dval)\n",
    "\n",
    "# #     y = torch.stack([\n",
    "# #         torch.sin(x[:, 0] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "# #         torch.cos(x[:, 0] * (2 * math.pi)) + torch.cos(x[:, 2] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "# #         torch.sin(x[:, 2] * (2 * math.pi)) + torch.cos(x[:, 1] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "# #         (torch.cos(x[:, 3] * (2 * math.pi)))* (torch.sin(x[:, 0] * (2 * math.pi))) + torch.randn(n) * 0.02,\n",
    "# #     ], -1)\n",
    "\n",
    "#     x = 3 * torch.rand(n)\n",
    "    \n",
    "#     y = torch.stack([\n",
    "#         torch.sin(3 * x) + torch.randn(n) * 0.01,\n",
    "#         torch.cos(x) + torch.cos(2 * x) + torch.randn(n) * 0.01,\n",
    "#         torch.sin(x) + torch.cos(x) + torch.randn(n) * 0.01,\n",
    "#         torch.cos(x) * torch.cos(x) + torch.randn(n) * 0.01,\n",
    "#     ], -1)\n",
    "\n",
    "#     train_x = x[:int(0.8*n)]\n",
    "#     train_y = y[:int(0.8*n)]\n",
    "\n",
    "#     test_x = x[int(0.8*n): ]\n",
    "\n",
    "#     test_y = y[int(0.8*n): ]\n",
    "\n",
    "# #     # normalize features\n",
    "# #     mean = train_x.mean()\n",
    "# #     std = train_x.std() + 1e-6 # prevent dividing by 0\n",
    "# #     train_x = (train_x - mean) / std\n",
    "# #     test_x = (test_x - mean) / std\n",
    "\n",
    "# #     # normalize labels\n",
    "# #     mean, std = train_y.mean(),train_y.std()\n",
    "# #     train_y = (train_y - mean) / std\n",
    "# #     test_y = (test_y - mean) / std\n",
    "\n",
    "    \n",
    "    \n",
    "#     likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=Dval)\n",
    "#     model = MultitaskGPModel(train_x, train_y, likelihood, num_base_kernels)\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     \"\"\"train the model hyperparameters\"\"\"\n",
    "#     training_iterations = 150\n",
    "\n",
    "#     # Find optimal model hyperparameters\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "\n",
    "#     # Use the adam optimizer\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "#     # \"Loss\" for GPs - the marginal log likelihood\n",
    "#     mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "#     for i in range(training_iterations):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(train_x)\n",
    "#         loss = -mll(output, train_y)\n",
    "#         loss.backward()\n",
    "# #           if(i > training_iterations*0.8):\n",
    "#         print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     cpu_training_time.append(time.time() - start_time)\n",
    "    \n",
    "#     print()\n",
    "    \n",
    "#     \"\"\" Making predictions with the model\"\"\"\n",
    "#     # Set into eval mode\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "\n",
    "#     # Exact predictions\n",
    "#     with torch.no_grad(): #, gpytorch.settings.fast_pred_var():\n",
    "#         start_time = time.time()\n",
    "#         preds = model(test_x) # no noise\n",
    "#         covar = preds.covariance_matrix\n",
    "#         cpu_exact_meancovar.append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # LOVE without cache\n",
    "#         # Clear the cache from the previous computations\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "#     # Set into eval mode\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "\n",
    "#     with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#         start_time = time.time()\n",
    "#         preds = model(test_x)\n",
    "#         fast_covar = preds.covariance_matrix\n",
    "#         cpu_love_meancovar.append(time.time() - start_time)\n",
    "    \n",
    "#     # LOVE with cache\n",
    "#     with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#         start_time = time.time()\n",
    "#         preds = model(test_x)\n",
    "#         fast_covar = preds.covariance_matrix\n",
    "#         cpu_love_meancovar_cache.append(time.time() - start_time)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(cpu_training_time)\n",
    "print(cpu_exact_meancovar)\n",
    "print(cpu_love_meancovar)\n",
    "print(cpu_love_meancovar_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize plots\n",
    "# f, (y1_ax, y2_ax) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# # This contains predictions for both tasks, flattened out\n",
    "# # The first half of the predictions is for the first task\n",
    "# # The second half is for the second task\n",
    "\n",
    "# # Plot training data as black stars\n",
    "# y1_ax.plot(train_x[:, 0].detach().numpy(), train_y[:, 0].detach().numpy(), 'k*')\n",
    "# # Predictive mean as blue line\n",
    "# y1_ax.plot(test_x[:, 0].numpy(), preds.mean[:, 0].numpy(), 'b')\n",
    "# # Shade in confidence\n",
    "# # y1_ax.fill_between(test_x[:, 0].numpy(), lower[:, 0].numpy(), upper[:, 0].numpy(), alpha=0.5)\n",
    "# # y1_ax.set_ylim([-3, 3])\n",
    "# y1_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "# y1_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# # Plot training data as black stars\n",
    "# y2_ax.plot(train_x[:, 1].detach().numpy(), train_y[:, 1].detach().numpy(), 'k*')\n",
    "# # Predictive mean as blue line\n",
    "# y2_ax.plot(test_x[:, 1].numpy(), preds.mean[:, 1].numpy(), 'b')\n",
    "# # Shade in confidence\n",
    "# # y2_ax.fill_between(test_x[:, 1].numpy(), lower[:, 1].numpy(), upper[:, 1].numpy(), alpha=0.5)\n",
    "# # y2_ax.set_ylim([-3, 3])\n",
    "# y2_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "# y2_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_training_time = []\n",
    "gpu_exact_meancovar = []\n",
    "gpu_love_meancovar = []\n",
    "gpu_love_meancovar_cache = []\n",
    "love_covar_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 100\n",
      "Use Cuda: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /lus/theta-fs0/software/thetagpu/conda/2022-07-01/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 1.286\n",
      "Iter 2/100 - Loss: 1.242\n",
      "Iter 3/100 - Loss: 1.199\n",
      "Iter 4/100 - Loss: 1.157\n",
      "Iter 5/100 - Loss: 1.116\n",
      "Iter 6/100 - Loss: 1.076\n",
      "Iter 7/100 - Loss: 1.038\n",
      "Iter 8/100 - Loss: 0.999\n",
      "Iter 9/100 - Loss: 0.960\n",
      "Iter 10/100 - Loss: 0.920\n",
      "Iter 11/100 - Loss: 0.881\n",
      "Iter 12/100 - Loss: 0.841\n",
      "Iter 13/100 - Loss: 0.801\n",
      "Iter 14/100 - Loss: 0.760\n",
      "Iter 15/100 - Loss: 0.720\n",
      "Iter 16/100 - Loss: 0.681\n",
      "Iter 17/100 - Loss: 0.641\n",
      "Iter 18/100 - Loss: 0.602\n",
      "Iter 19/100 - Loss: 0.563\n",
      "Iter 20/100 - Loss: 0.523\n",
      "Iter 21/100 - Loss: 0.482\n",
      "Iter 22/100 - Loss: 0.441\n",
      "Iter 23/100 - Loss: 0.399\n",
      "Iter 24/100 - Loss: 0.357\n",
      "Iter 25/100 - Loss: 0.314\n",
      "Iter 26/100 - Loss: 0.272\n",
      "Iter 27/100 - Loss: 0.230\n",
      "Iter 28/100 - Loss: 0.188\n",
      "Iter 29/100 - Loss: 0.146\n",
      "Iter 30/100 - Loss: 0.104\n",
      "Iter 31/100 - Loss: 0.061\n",
      "Iter 32/100 - Loss: 0.018\n",
      "Iter 33/100 - Loss: -0.025\n",
      "Iter 34/100 - Loss: -0.068\n",
      "Iter 35/100 - Loss: -0.111\n",
      "Iter 36/100 - Loss: -0.154\n",
      "Iter 37/100 - Loss: -0.197\n",
      "Iter 38/100 - Loss: -0.240\n",
      "Iter 39/100 - Loss: -0.283\n",
      "Iter 40/100 - Loss: -0.326\n",
      "Iter 41/100 - Loss: -0.369\n",
      "Iter 42/100 - Loss: -0.412\n",
      "Iter 43/100 - Loss: -0.455\n",
      "Iter 44/100 - Loss: -0.497\n",
      "Iter 45/100 - Loss: -0.540\n",
      "Iter 46/100 - Loss: -0.582\n",
      "Iter 47/100 - Loss: -0.624\n",
      "Iter 48/100 - Loss: -0.666\n",
      "Iter 49/100 - Loss: -0.708\n",
      "Iter 50/100 - Loss: -0.750\n",
      "Iter 51/100 - Loss: -0.791\n",
      "Iter 52/100 - Loss: -0.833\n",
      "Iter 53/100 - Loss: -0.874\n",
      "Iter 54/100 - Loss: -0.914\n",
      "Iter 55/100 - Loss: -0.955\n",
      "Iter 56/100 - Loss: -0.995\n",
      "Iter 57/100 - Loss: -1.035\n",
      "Iter 58/100 - Loss: -1.075\n",
      "Iter 59/100 - Loss: -1.114\n",
      "Iter 60/100 - Loss: -1.153\n",
      "Iter 61/100 - Loss: -1.191\n",
      "Iter 62/100 - Loss: -1.229\n",
      "Iter 63/100 - Loss: -1.267\n",
      "Iter 64/100 - Loss: -1.304\n",
      "Iter 65/100 - Loss: -1.340\n",
      "Iter 66/100 - Loss: -1.376\n",
      "Iter 67/100 - Loss: -1.411\n",
      "Iter 68/100 - Loss: -1.446\n",
      "Iter 69/100 - Loss: -1.479\n",
      "Iter 70/100 - Loss: -1.512\n",
      "Iter 71/100 - Loss: -1.543\n",
      "Iter 72/100 - Loss: -1.574\n",
      "Iter 73/100 - Loss: -1.605\n",
      "Iter 74/100 - Loss: -1.634\n",
      "Iter 75/100 - Loss: -1.663\n",
      "Iter 76/100 - Loss: -1.690\n",
      "Iter 77/100 - Loss: -1.716\n",
      "Iter 78/100 - Loss: -1.741\n",
      "Iter 79/100 - Loss: -1.764\n",
      "Iter 80/100 - Loss: -1.788\n",
      "Iter 81/100 - Loss: -1.809\n",
      "Iter 82/100 - Loss: -1.829\n",
      "Iter 83/100 - Loss: -1.848\n",
      "Iter 84/100 - Loss: -1.866\n",
      "Iter 85/100 - Loss: -1.883\n",
      "Iter 86/100 - Loss: -1.899\n",
      "Iter 87/100 - Loss: -1.913\n",
      "Iter 88/100 - Loss: -1.927\n",
      "Iter 89/100 - Loss: -1.939\n",
      "Iter 90/100 - Loss: -1.950\n",
      "Iter 91/100 - Loss: -1.960\n",
      "Iter 92/100 - Loss: -1.969\n",
      "Iter 93/100 - Loss: -1.977\n",
      "Iter 94/100 - Loss: -1.985\n",
      "Iter 95/100 - Loss: -1.992\n",
      "Iter 96/100 - Loss: -1.998\n",
      "Iter 97/100 - Loss: -2.003\n",
      "Iter 98/100 - Loss: -2.008\n",
      "Iter 99/100 - Loss: -2.012\n",
      "Iter 100/100 - Loss: -2.016\n",
      "\n",
      "tensor([[ 6.9141e-05, -8.6427e-07,  8.9407e-08,  ..., -2.8312e-07,\n",
      "          1.4901e-08, -1.3411e-07],\n",
      "        [-8.7917e-07,  4.8161e-05,  4.7684e-07,  ...,  1.9073e-06,\n",
      "         -3.5763e-07,  2.6822e-06],\n",
      "        [ 0.0000e+00,  1.1921e-07,  3.3140e-05,  ..., -2.3842e-07,\n",
      "         -8.2254e-06,  7.3016e-07],\n",
      "        ...,\n",
      "        [-2.8312e-07,  1.4305e-06, -4.7684e-07,  ...,  7.1287e-05,\n",
      "          7.1526e-07,  1.2994e-05],\n",
      "        [ 0.0000e+00, -4.7684e-07, -7.7486e-06,  ...,  7.1526e-07,\n",
      "          4.6492e-05, -9.9093e-07],\n",
      "        [-1.1921e-07,  2.9802e-06,  7.3761e-07,  ...,  1.3232e-05,\n",
      "         -8.7917e-07,  4.7863e-05]], device='cuda:0')\n",
      "tensor([[ 7.1526e-05, -8.3447e-07, -5.9605e-08,  ..., -5.9605e-08,\n",
      "         -1.0431e-07, -2.7567e-07],\n",
      "        [-8.3447e-07,  5.0068e-05,  1.1921e-07,  ...,  3.5763e-06,\n",
      "          9.5367e-07,  4.1127e-06],\n",
      "        [-5.9605e-08,  1.1921e-07,  3.4094e-05,  ..., -9.5367e-07,\n",
      "         -8.9407e-06,  7.2271e-07],\n",
      "        ...,\n",
      "        [-5.9605e-08,  3.5763e-06, -9.5367e-07,  ...,  6.5088e-05,\n",
      "          2.3842e-06,  1.1206e-05],\n",
      "        [-1.0431e-07,  9.5367e-07, -8.9407e-06,  ...,  2.3842e-06,\n",
      "          4.3392e-05,  1.6391e-07],\n",
      "        [-2.7567e-07,  4.1127e-06,  7.2271e-07,  ...,  1.1206e-05,\n",
      "          1.6391e-07,  4.7565e-05]], device='cuda:0')\n",
      "tensor(3.0770e-06, device='cuda:0')\n",
      "error: -3.1970674991607666\n",
      "data size: 300\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.167\n",
      "Iter 2/100 - Loss: 1.122\n",
      "Iter 3/100 - Loss: 1.088\n",
      "Iter 4/100 - Loss: 1.051\n",
      "Iter 5/100 - Loss: 1.018\n",
      "Iter 6/100 - Loss: 0.973\n",
      "Iter 7/100 - Loss: 0.938\n",
      "Iter 8/100 - Loss: 0.908\n",
      "Iter 9/100 - Loss: 0.871\n",
      "Iter 10/100 - Loss: 0.834\n",
      "Iter 11/100 - Loss: 0.793\n",
      "Iter 12/100 - Loss: 0.748\n",
      "Iter 13/100 - Loss: 0.713\n",
      "Iter 14/100 - Loss: 0.674\n",
      "Iter 15/100 - Loss: 0.640\n",
      "Iter 16/100 - Loss: 0.604\n",
      "Iter 17/100 - Loss: 0.548\n",
      "Iter 18/100 - Loss: 0.511\n",
      "Iter 19/100 - Loss: 0.472\n",
      "Iter 20/100 - Loss: 0.428\n",
      "Iter 21/100 - Loss: 0.395\n",
      "Iter 22/100 - Loss: 0.338\n",
      "Iter 23/100 - Loss: 0.295\n",
      "Iter 24/100 - Loss: 0.253\n",
      "Iter 25/100 - Loss: 0.218\n",
      "Iter 26/100 - Loss: 0.170\n",
      "Iter 27/100 - Loss: 0.124\n",
      "Iter 28/100 - Loss: 0.086\n",
      "Iter 29/100 - Loss: 0.041\n",
      "Iter 30/100 - Loss: -0.001\n",
      "Iter 31/100 - Loss: -0.051\n",
      "Iter 32/100 - Loss: -0.096\n",
      "Iter 33/100 - Loss: -0.140\n",
      "Iter 34/100 - Loss: -0.193\n",
      "Iter 35/100 - Loss: -0.240\n",
      "Iter 36/100 - Loss: -0.266\n",
      "Iter 37/100 - Loss: -0.328\n",
      "Iter 38/100 - Loss: -0.370\n",
      "Iter 39/100 - Loss: -0.407\n",
      "Iter 40/100 - Loss: -0.446\n",
      "Iter 41/100 - Loss: -0.491\n",
      "Iter 42/100 - Loss: -0.532\n",
      "Iter 43/100 - Loss: -0.592\n",
      "Iter 44/100 - Loss: -0.638\n",
      "Iter 45/100 - Loss: -0.675\n",
      "Iter 46/100 - Loss: -0.730\n",
      "Iter 47/100 - Loss: -0.769\n",
      "Iter 48/100 - Loss: -0.809\n",
      "Iter 49/100 - Loss: -0.857\n",
      "Iter 50/100 - Loss: -0.904\n",
      "Iter 51/100 - Loss: -0.936\n",
      "Iter 52/100 - Loss: -0.995\n",
      "Iter 53/100 - Loss: -1.013\n",
      "Iter 54/100 - Loss: -1.042\n",
      "Iter 55/100 - Loss: -1.102\n",
      "Iter 56/100 - Loss: -1.140\n",
      "Iter 57/100 - Loss: -1.192\n",
      "Iter 58/100 - Loss: -1.226\n",
      "Iter 59/100 - Loss: -1.280\n",
      "Iter 60/100 - Loss: -1.299\n",
      "Iter 61/100 - Loss: -1.279\n",
      "Iter 62/100 - Loss: -1.335\n",
      "Iter 63/100 - Loss: -1.380\n",
      "Iter 64/100 - Loss: -1.397\n",
      "Iter 65/100 - Loss: -1.396\n",
      "Iter 66/100 - Loss: -1.464\n",
      "Iter 67/100 - Loss: -1.434\n",
      "Iter 68/100 - Loss: -1.472\n",
      "Iter 69/100 - Loss: -1.532\n",
      "Iter 70/100 - Loss: -1.513\n",
      "Iter 71/100 - Loss: -1.559\n",
      "Iter 72/100 - Loss: -1.555\n",
      "Iter 73/100 - Loss: -1.503\n",
      "Iter 74/100 - Loss: -1.563\n",
      "Iter 75/100 - Loss: -1.643\n",
      "Iter 76/100 - Loss: -1.651\n",
      "Iter 77/100 - Loss: -1.609\n",
      "Iter 78/100 - Loss: -1.670\n",
      "Iter 79/100 - Loss: -1.711\n",
      "Iter 80/100 - Loss: -1.695\n",
      "Iter 81/100 - Loss: -1.695\n",
      "Iter 82/100 - Loss: -1.752\n",
      "Iter 83/100 - Loss: -1.684\n",
      "Iter 84/100 - Loss: -1.774\n",
      "Iter 85/100 - Loss: -1.790\n",
      "Iter 86/100 - Loss: -1.737\n",
      "Iter 87/100 - Loss: -1.767\n",
      "Iter 88/100 - Loss: -1.752\n",
      "Iter 89/100 - Loss: -1.771\n",
      "Iter 90/100 - Loss: -1.792\n",
      "Iter 91/100 - Loss: -1.816\n",
      "Iter 92/100 - Loss: -1.790\n",
      "Iter 93/100 - Loss: -1.868\n",
      "Iter 94/100 - Loss: -1.838\n",
      "Iter 95/100 - Loss: -1.833\n",
      "Iter 96/100 - Loss: -1.817\n",
      "Iter 97/100 - Loss: -1.896\n",
      "Iter 98/100 - Loss: -1.786\n",
      "Iter 99/100 - Loss: -1.907\n",
      "Iter 100/100 - Loss: -1.875\n",
      "\n",
      "tensor([[ 1.0500e-02,  6.7880e-03,  8.8174e-03,  ...,  9.0675e-03,\n",
      "         -2.7468e-04,  5.0771e-03],\n",
      "        [ 5.5499e-03,  9.4314e-03,  6.6332e-03,  ..., -1.4350e-03,\n",
      "         -4.7861e-03, -3.2015e-03],\n",
      "        [ 3.3951e-03,  7.4217e-03,  7.9663e-03,  ..., -3.0884e-03,\n",
      "          2.2812e-03, -1.6730e-04],\n",
      "        ...,\n",
      "        [-1.9397e-04, -9.3132e-07, -2.2780e-03,  ...,  1.7291e-03,\n",
      "          6.1601e-04,  1.3192e-03],\n",
      "        [ 2.4981e-04, -4.3954e-03,  1.3166e-03,  ...,  1.3939e-03,\n",
      "          1.7231e-03,  1.1252e-04],\n",
      "        [ 4.2449e-04, -6.3197e-03, -2.8789e-03,  ...,  3.0639e-03,\n",
      "         -2.0139e-03,  1.7735e-03]], device='cuda:0')\n",
      "tensor([[ 8.2016e-05,  5.0664e-07,  9.8348e-07,  ..., -5.5647e-07,\n",
      "         -2.4308e-07, -1.5291e-07],\n",
      "        [ 5.0664e-07,  6.8665e-05,  3.6955e-06,  ..., -4.6194e-07,\n",
      "         -9.3970e-07,  5.2212e-08],\n",
      "        [ 9.8348e-07,  3.6955e-06,  7.0333e-05,  ..., -7.9069e-07,\n",
      "         -1.5423e-06, -2.4843e-07],\n",
      "        ...,\n",
      "        [-5.5647e-07, -4.6194e-07, -7.8976e-07,  ...,  1.5020e-05,\n",
      "         -1.9670e-06,  1.4901e-06],\n",
      "        [-2.4308e-07, -9.3877e-07, -1.5423e-06,  ..., -1.9670e-06,\n",
      "          1.0252e-05,  1.7881e-07],\n",
      "        [-1.5291e-07,  5.2329e-08, -2.4843e-07,  ...,  1.4901e-06,\n",
      "          1.7881e-07,  1.4812e-05]], device='cuda:0')\n",
      "tensor(0.0040, device='cuda:0')\n",
      "error: -0.0033399148378521204\n",
      "data size: 500\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.135\n",
      "Iter 2/100 - Loss: 1.100\n",
      "Iter 3/100 - Loss: 1.064\n",
      "Iter 4/100 - Loss: 1.030\n",
      "Iter 5/100 - Loss: 0.991\n",
      "Iter 6/100 - Loss: 0.962\n",
      "Iter 7/100 - Loss: 0.924\n",
      "Iter 8/100 - Loss: 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9/100 - Loss: 0.852\n",
      "Iter 10/100 - Loss: 0.811\n",
      "Iter 11/100 - Loss: 0.769\n",
      "Iter 12/100 - Loss: 0.730\n",
      "Iter 13/100 - Loss: 0.687\n",
      "Iter 14/100 - Loss: 0.650\n",
      "Iter 15/100 - Loss: 0.619\n",
      "Iter 16/100 - Loss: 0.570\n",
      "Iter 17/100 - Loss: 0.541\n",
      "Iter 18/100 - Loss: 0.494\n",
      "Iter 19/100 - Loss: 0.448\n",
      "Iter 20/100 - Loss: 0.411\n",
      "Iter 21/100 - Loss: 0.364\n",
      "Iter 22/100 - Loss: 0.332\n",
      "Iter 23/100 - Loss: 0.280\n",
      "Iter 24/100 - Loss: 0.240\n",
      "Iter 25/100 - Loss: 0.190\n",
      "Iter 26/100 - Loss: 0.150\n",
      "Iter 27/100 - Loss: 0.102\n",
      "Iter 28/100 - Loss: 0.062\n",
      "Iter 29/100 - Loss: 0.012\n",
      "Iter 30/100 - Loss: -0.033\n",
      "Iter 31/100 - Loss: -0.075\n",
      "Iter 32/100 - Loss: -0.127\n",
      "Iter 33/100 - Loss: -0.170\n",
      "Iter 34/100 - Loss: -0.217\n",
      "Iter 35/100 - Loss: -0.261\n",
      "Iter 36/100 - Loss: -0.306\n",
      "Iter 37/100 - Loss: -0.347\n",
      "Iter 38/100 - Loss: -0.394\n",
      "Iter 39/100 - Loss: -0.439\n",
      "Iter 40/100 - Loss: -0.488\n",
      "Iter 41/100 - Loss: -0.532\n",
      "Iter 42/100 - Loss: -0.578\n",
      "Iter 43/100 - Loss: -0.628\n",
      "Iter 44/100 - Loss: -0.680\n",
      "Iter 45/100 - Loss: -0.711\n",
      "Iter 46/100 - Loss: -0.749\n",
      "Iter 47/100 - Loss: -0.802\n",
      "Iter 48/100 - Loss: -0.853\n",
      "Iter 49/100 - Loss: -0.888\n",
      "Iter 50/100 - Loss: -0.938\n",
      "Iter 51/100 - Loss: -0.975\n",
      "Iter 52/100 - Loss: -1.029\n",
      "Iter 53/100 - Loss: -1.065\n",
      "Iter 54/100 - Loss: -1.119\n",
      "Iter 55/100 - Loss: -1.158\n",
      "Iter 56/100 - Loss: -1.175\n",
      "Iter 57/100 - Loss: -1.236\n",
      "Iter 58/100 - Loss: -1.252\n",
      "Iter 59/100 - Loss: -1.304\n",
      "Iter 60/100 - Loss: -1.346\n",
      "Iter 61/100 - Loss: -1.375\n",
      "Iter 62/100 - Loss: -1.389\n",
      "Iter 63/100 - Loss: -1.419\n",
      "Iter 64/100 - Loss: -1.453\n",
      "Iter 65/100 - Loss: -1.439\n",
      "Iter 66/100 - Loss: -1.479\n",
      "Iter 67/100 - Loss: -1.513\n",
      "Iter 68/100 - Loss: -1.483\n",
      "Iter 69/100 - Loss: -1.559\n",
      "Iter 70/100 - Loss: -1.555\n",
      "Iter 71/100 - Loss: -1.625\n",
      "Iter 72/100 - Loss: -1.595\n",
      "Iter 73/100 - Loss: -1.679\n",
      "Iter 74/100 - Loss: -1.699\n",
      "Iter 75/100 - Loss: -1.692\n",
      "Iter 76/100 - Loss: -1.713\n",
      "Iter 77/100 - Loss: -1.755\n",
      "Iter 78/100 - Loss: -1.718\n",
      "Iter 79/100 - Loss: -1.745\n",
      "Iter 80/100 - Loss: -1.760\n",
      "Iter 81/100 - Loss: -1.769\n",
      "Iter 82/100 - Loss: -1.745\n",
      "Iter 83/100 - Loss: -1.782\n",
      "Iter 84/100 - Loss: -1.756\n",
      "Iter 85/100 - Loss: -1.742\n",
      "Iter 86/100 - Loss: -1.849\n",
      "Iter 87/100 - Loss: -1.833\n",
      "Iter 88/100 - Loss: -1.786\n",
      "Iter 89/100 - Loss: -1.655\n",
      "Iter 90/100 - Loss: -1.732\n",
      "Iter 91/100 - Loss: -1.769\n",
      "Iter 92/100 - Loss: -1.754\n",
      "Iter 93/100 - Loss: -1.630\n",
      "Iter 94/100 - Loss: -1.705\n",
      "Iter 95/100 - Loss: -1.802\n",
      "Iter 96/100 - Loss: -1.817\n",
      "Iter 97/100 - Loss: -1.761\n",
      "Iter 98/100 - Loss: -1.697\n",
      "Iter 99/100 - Loss: -1.606\n",
      "Iter 100/100 - Loss: -1.717\n",
      "\n",
      "tensor([[-5.5981e-03,  5.7435e-04, -9.2437e-04,  ..., -7.0772e-04,\n",
      "          6.5104e-03, -6.1596e-04],\n",
      "        [ 1.3889e-03,  3.4859e-03,  1.1415e-02,  ..., -1.4450e-03,\n",
      "         -5.6464e-03, -8.8313e-04],\n",
      "        [ 3.2676e-03,  2.4008e-03,  3.0299e-03,  ..., -5.3480e-05,\n",
      "         -3.6252e-03, -3.5061e-03],\n",
      "        ...,\n",
      "        [-6.5109e-03, -1.6412e-03, -9.4985e-03,  ...,  3.3505e-03,\n",
      "          1.3307e-03,  1.7849e-03],\n",
      "        [ 2.0381e-03, -4.4924e-04, -4.5364e-03,  ...,  8.7589e-04,\n",
      "          3.3605e-03,  1.5355e-03],\n",
      "        [-2.9880e-03,  5.3835e-03, -7.7511e-03,  ..., -2.3280e-03,\n",
      "         -6.9157e-03,  5.9661e-03]], device='cuda:0')\n",
      "tensor([[-1.4938e-01, -1.7286e-02, -7.2525e-03,  ...,  4.9123e-02,\n",
      "          3.4435e-03,  1.6938e-02],\n",
      "        [-1.7286e-02, -2.2092e-03, -8.9711e-04,  ...,  5.7634e-03,\n",
      "          3.8360e-04,  1.9850e-03],\n",
      "        [-7.2525e-03, -8.9711e-04, -3.6895e-04,  ...,  2.3292e-03,\n",
      "          1.4600e-04,  8.0083e-04],\n",
      "        ...,\n",
      "        [ 4.9123e-02,  5.7633e-03,  2.3291e-03,  ..., -1.6806e-02,\n",
      "         -1.2629e-03, -5.8115e-03],\n",
      "        [ 3.4435e-03,  3.8357e-04,  1.4597e-04,  ..., -1.2629e-03,\n",
      "         -1.0002e-04, -4.3812e-04],\n",
      "        [ 1.6938e-02,  1.9850e-03,  8.0083e-04,  ..., -5.8115e-03,\n",
      "         -4.3812e-04, -1.9970e-03]], device='cuda:0')\n",
      "tensor(0.0998, device='cuda:0')\n",
      "error: 2.188699722290039\n",
      "data size: 700\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.129\n",
      "Iter 2/100 - Loss: 1.117\n",
      "Iter 3/100 - Loss: 1.086\n",
      "Iter 4/100 - Loss: 1.055\n",
      "Iter 5/100 - Loss: 1.020\n",
      "Iter 6/100 - Loss: 0.990\n",
      "Iter 7/100 - Loss: 0.955\n",
      "Iter 8/100 - Loss: 0.923\n",
      "Iter 9/100 - Loss: 0.887\n",
      "Iter 10/100 - Loss: 0.851\n",
      "Iter 11/100 - Loss: 0.814\n",
      "Iter 12/100 - Loss: 0.776\n",
      "Iter 13/100 - Loss: 0.739\n",
      "Iter 14/100 - Loss: 0.701\n",
      "Iter 15/100 - Loss: 0.663\n",
      "Iter 16/100 - Loss: 0.626\n",
      "Iter 17/100 - Loss: 0.588\n",
      "Iter 18/100 - Loss: 0.542\n",
      "Iter 19/100 - Loss: 0.503\n",
      "Iter 20/100 - Loss: 0.464\n",
      "Iter 21/100 - Loss: 0.420\n",
      "Iter 22/100 - Loss: 0.380\n",
      "Iter 23/100 - Loss: 0.338\n",
      "Iter 24/100 - Loss: 0.295\n",
      "Iter 25/100 - Loss: 0.248\n",
      "Iter 26/100 - Loss: 0.207\n",
      "Iter 27/100 - Loss: 0.165\n",
      "Iter 28/100 - Loss: 0.121\n",
      "Iter 29/100 - Loss: 0.074\n",
      "Iter 30/100 - Loss: 0.032\n",
      "Iter 31/100 - Loss: -0.013\n",
      "Iter 32/100 - Loss: -0.058\n",
      "Iter 33/100 - Loss: -0.102\n",
      "Iter 34/100 - Loss: -0.150\n",
      "Iter 35/100 - Loss: -0.193\n",
      "Iter 36/100 - Loss: -0.240\n",
      "Iter 37/100 - Loss: -0.283\n",
      "Iter 38/100 - Loss: -0.332\n",
      "Iter 39/100 - Loss: -0.378\n",
      "Iter 40/100 - Loss: -0.421\n",
      "Iter 41/100 - Loss: -0.466\n",
      "Iter 42/100 - Loss: -0.511\n",
      "Iter 43/100 - Loss: -0.562\n",
      "Iter 44/100 - Loss: -0.606\n",
      "Iter 45/100 - Loss: -0.652\n",
      "Iter 46/100 - Loss: -0.697\n",
      "Iter 47/100 - Loss: -0.741\n",
      "Iter 48/100 - Loss: -0.791\n",
      "Iter 49/100 - Loss: -0.833\n",
      "Iter 50/100 - Loss: -0.880\n",
      "Iter 51/100 - Loss: -0.923\n",
      "Iter 52/100 - Loss: -0.964\n",
      "Iter 53/100 - Loss: -1.019\n",
      "Iter 54/100 - Loss: -1.059\n",
      "Iter 55/100 - Loss: -1.108\n",
      "Iter 56/100 - Loss: -1.149\n",
      "Iter 57/100 - Loss: -1.191\n",
      "Iter 58/100 - Loss: -1.239\n",
      "Iter 59/100 - Loss: -1.277\n",
      "Iter 60/100 - Loss: -1.326\n",
      "Iter 61/100 - Loss: -1.366\n",
      "Iter 62/100 - Loss: -1.413\n",
      "Iter 63/100 - Loss: -1.453\n",
      "Iter 64/100 - Loss: -1.498\n",
      "Iter 65/100 - Loss: -1.538\n",
      "Iter 66/100 - Loss: -1.578\n",
      "Iter 67/100 - Loss: -1.613\n",
      "Iter 68/100 - Loss: -1.656\n",
      "Iter 69/100 - Loss: -1.693\n",
      "Iter 70/100 - Loss: -1.735\n",
      "Iter 71/100 - Loss: -1.768\n",
      "Iter 72/100 - Loss: -1.806\n",
      "Iter 73/100 - Loss: -1.852\n",
      "Iter 74/100 - Loss: -1.881\n",
      "Iter 75/100 - Loss: -1.912\n",
      "Iter 76/100 - Loss: -1.945\n",
      "Iter 77/100 - Loss: -1.981\n",
      "Iter 78/100 - Loss: -2.023\n",
      "Iter 79/100 - Loss: -2.079\n",
      "Iter 80/100 - Loss: -2.097\n",
      "Iter 81/100 - Loss: -2.137\n",
      "Iter 82/100 - Loss: -2.172\n",
      "Iter 83/100 - Loss: -2.196\n",
      "Iter 84/100 - Loss: -2.237\n",
      "Iter 85/100 - Loss: -2.273\n",
      "Iter 86/100 - Loss: -2.325\n",
      "Iter 87/100 - Loss: -2.357\n",
      "Iter 88/100 - Loss: -2.390\n",
      "Iter 89/100 - Loss: -2.420\n",
      "Iter 90/100 - Loss: -2.449\n",
      "Iter 91/100 - Loss: -2.470\n",
      "Iter 92/100 - Loss: -2.497\n",
      "Iter 93/100 - Loss: -2.506\n",
      "Iter 94/100 - Loss: -2.459\n",
      "Iter 95/100 - Loss: -2.507\n",
      "Iter 96/100 - Loss: -2.557\n",
      "Iter 97/100 - Loss: -2.572\n",
      "Iter 98/100 - Loss: -2.598\n",
      "Iter 99/100 - Loss: -2.617\n",
      "Iter 100/100 - Loss: -2.614\n",
      "\n",
      "tensor([[-0.0010,  0.0121,  0.0113,  ...,  0.0072,  0.0055, -0.0053],\n",
      "        [ 0.0097,  0.0020, -0.0061,  ...,  0.0079,  0.0094,  0.0008],\n",
      "        [-0.0046,  0.0169, -0.0376,  ...,  0.0106,  0.0062, -0.0006],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0062,  0.0100,  ..., -0.0107, -0.0149,  0.0139],\n",
      "        [ 0.0080,  0.0081,  0.0105,  ...,  0.0213, -0.0091,  0.0147],\n",
      "        [-0.0037, -0.0026, -0.0015,  ..., -0.0083, -0.0058, -0.0017]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.4628, -0.6699, -0.7841,  ..., -0.3048, -0.2445,  0.1358],\n",
      "        [-0.6699, -0.9720, -1.1377,  ..., -0.4426, -0.3544,  0.1976],\n",
      "        [-0.7841, -1.1377, -1.3317,  ..., -0.5181, -0.4150,  0.2312],\n",
      "        ...,\n",
      "        [-0.3048, -0.4426, -0.5181,  ..., -0.2018, -0.1616,  0.0901],\n",
      "        [-0.2445, -0.3544, -0.4150,  ..., -0.1616, -0.1301,  0.0717],\n",
      "        [ 0.1358,  0.1976,  0.2312,  ...,  0.0901,  0.0717, -0.0405]],\n",
      "       device='cuda:0')\n",
      "tensor(0.8141, device='cuda:0')\n",
      "error: 3.853971481323242\n",
      "data size: 1000\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.115\n",
      "Iter 2/100 - Loss: 1.104\n",
      "Iter 3/100 - Loss: 1.078\n",
      "Iter 4/100 - Loss: 1.043\n",
      "Iter 5/100 - Loss: 1.010\n",
      "Iter 6/100 - Loss: 0.979\n",
      "Iter 7/100 - Loss: 0.944\n",
      "Iter 8/100 - Loss: 0.911\n",
      "Iter 9/100 - Loss: 0.876\n",
      "Iter 10/100 - Loss: 0.840\n",
      "Iter 11/100 - Loss: 0.806\n",
      "Iter 12/100 - Loss: 0.767\n",
      "Iter 13/100 - Loss: 0.728\n",
      "Iter 14/100 - Loss: 0.690\n",
      "Iter 15/100 - Loss: 0.650\n",
      "Iter 16/100 - Loss: 0.612\n",
      "Iter 17/100 - Loss: 0.574\n",
      "Iter 18/100 - Loss: 0.532\n",
      "Iter 19/100 - Loss: 0.492\n",
      "Iter 20/100 - Loss: 0.452\n",
      "Iter 21/100 - Loss: 0.409\n",
      "Iter 22/100 - Loss: 0.367\n",
      "Iter 23/100 - Loss: 0.326\n",
      "Iter 24/100 - Loss: 0.283\n",
      "Iter 25/100 - Loss: 0.238\n",
      "Iter 26/100 - Loss: 0.196\n",
      "Iter 27/100 - Loss: 0.152\n",
      "Iter 28/100 - Loss: 0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29/100 - Loss: 0.065\n",
      "Iter 30/100 - Loss: 0.018\n",
      "Iter 31/100 - Loss: -0.023\n",
      "Iter 32/100 - Loss: -0.071\n",
      "Iter 33/100 - Loss: -0.118\n",
      "Iter 34/100 - Loss: -0.162\n",
      "Iter 35/100 - Loss: -0.206\n",
      "Iter 36/100 - Loss: -0.251\n",
      "Iter 37/100 - Loss: -0.298\n",
      "Iter 38/100 - Loss: -0.344\n",
      "Iter 39/100 - Loss: -0.390\n",
      "Iter 40/100 - Loss: -0.435\n",
      "Iter 41/100 - Loss: -0.483\n",
      "Iter 42/100 - Loss: -0.527\n",
      "Iter 43/100 - Loss: -0.572\n",
      "Iter 44/100 - Loss: -0.619\n",
      "Iter 45/100 - Loss: -0.665\n",
      "Iter 46/100 - Loss: -0.711\n",
      "Iter 47/100 - Loss: -0.757\n",
      "Iter 48/100 - Loss: -0.800\n",
      "Iter 49/100 - Loss: -0.848\n",
      "Iter 50/100 - Loss: -0.894\n",
      "Iter 51/100 - Loss: -0.939\n",
      "Iter 52/100 - Loss: -0.986\n",
      "Iter 53/100 - Loss: -1.031\n",
      "Iter 54/100 - Loss: -1.073\n",
      "Iter 55/100 - Loss: -1.120\n",
      "Iter 56/100 - Loss: -1.166\n",
      "Iter 57/100 - Loss: -1.210\n",
      "Iter 58/100 - Loss: -1.253\n",
      "Iter 59/100 - Loss: -1.294\n",
      "Iter 60/100 - Loss: -1.337\n",
      "Iter 61/100 - Loss: -1.384\n",
      "Iter 62/100 - Loss: -1.428\n",
      "Iter 63/100 - Loss: -1.468\n",
      "Iter 64/100 - Loss: -1.509\n",
      "Iter 65/100 - Loss: -1.551\n",
      "Iter 66/100 - Loss: -1.593\n",
      "Iter 67/100 - Loss: -1.633\n",
      "Iter 68/100 - Loss: -1.679\n",
      "Iter 69/100 - Loss: -1.711\n",
      "Iter 70/100 - Loss: -1.760\n",
      "Iter 71/100 - Loss: -1.793\n",
      "Iter 72/100 - Loss: -1.836\n",
      "Iter 73/100 - Loss: -1.878\n",
      "Iter 74/100 - Loss: -1.908\n",
      "Iter 75/100 - Loss: -1.952\n",
      "Iter 76/100 - Loss: -1.977\n",
      "Iter 77/100 - Loss: -2.016\n",
      "Iter 78/100 - Loss: -2.071\n",
      "Iter 79/100 - Loss: -2.107\n",
      "Iter 80/100 - Loss: -2.136\n",
      "Iter 81/100 - Loss: -2.200\n",
      "Iter 82/100 - Loss: -2.232\n",
      "Iter 83/100 - Loss: -2.273\n",
      "Iter 84/100 - Loss: -2.302\n",
      "Iter 85/100 - Loss: -2.341\n",
      "Iter 86/100 - Loss: -2.370\n",
      "Iter 87/100 - Loss: -2.370\n",
      "Iter 88/100 - Loss: -2.409\n",
      "Iter 89/100 - Loss: -2.448\n",
      "Iter 90/100 - Loss: -2.396\n",
      "Iter 91/100 - Loss: -2.453\n",
      "Iter 92/100 - Loss: -2.496\n",
      "Iter 93/100 - Loss: -2.485\n",
      "Iter 94/100 - Loss: -2.499\n",
      "Iter 95/100 - Loss: -2.513\n",
      "Iter 96/100 - Loss: -2.553\n",
      "Iter 97/100 - Loss: -2.581\n",
      "Iter 98/100 - Loss: -2.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0606849193573 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 99/100 - Loss: -2.385\n",
      "Iter 100/100 - Loss: -2.588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6783440113067627 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.6451173424720764 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.0336, -4.8401, -1.4165,  ..., -3.2789, -0.8091, -1.2897],\n",
      "        [-7.8555, 17.0430,  1.8291,  ..., 10.0891,  1.4181,  4.6633],\n",
      "        [-0.9008,  0.5843,  0.5845,  ...,  0.6769,  0.4557, -0.5959],\n",
      "        ...,\n",
      "        [-3.3321,  9.7346,  0.8020,  ...,  9.2769,  0.5736,  4.1698],\n",
      "        [-0.7226,  0.7263,  0.2673,  ...,  0.4668,  0.2777, -0.7035],\n",
      "        [-1.0832,  4.4287,  0.2176,  ...,  4.6524,  0.0996,  2.6682]],\n",
      "       device='cuda:0')\n",
      "tensor([[-1.3212e-01,  2.5601e-01,  3.2304e-03,  ...,  1.7650e-01,\n",
      "          7.1210e-03,  8.6710e-02],\n",
      "        [ 2.5601e-01, -5.2408e-01, -4.2719e-03,  ..., -3.6851e-01,\n",
      "         -1.2603e-02, -1.8283e-01],\n",
      "        [ 3.2304e-03, -4.2719e-03, -6.6185e-04,  ..., -5.8776e-04,\n",
      "         -6.0582e-04,  1.3554e-04],\n",
      "        ...,\n",
      "        [ 1.7650e-01, -3.6851e-01, -5.8770e-04,  ..., -2.7048e-01,\n",
      "         -6.8333e-03, -1.3598e-01],\n",
      "        [ 7.1209e-03, -1.2603e-02, -6.0582e-04,  ..., -6.8333e-03,\n",
      "         -7.2479e-04, -3.0262e-03],\n",
      "        [ 8.6710e-02, -1.8283e-01,  1.3554e-04,  ..., -1.3598e-01,\n",
      "         -3.0262e-03, -6.8664e-02]], device='cuda:0')\n",
      "tensor(7.8464, device='cuda:0')\n",
      "error: 0.022298911586403847\n",
      "data size: 1500\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.107\n",
      "Iter 2/100 - Loss: 1.097\n",
      "Iter 3/100 - Loss: 1.067\n",
      "Iter 4/100 - Loss: 1.035\n",
      "Iter 5/100 - Loss: 1.003\n",
      "Iter 6/100 - Loss: 0.971\n",
      "Iter 7/100 - Loss: 0.936\n",
      "Iter 8/100 - Loss: 0.903\n",
      "Iter 9/100 - Loss: 0.866\n",
      "Iter 10/100 - Loss: 0.830\n",
      "Iter 11/100 - Loss: 0.794\n",
      "Iter 12/100 - Loss: 0.757\n",
      "Iter 13/100 - Loss: 0.719\n",
      "Iter 14/100 - Loss: 0.682\n",
      "Iter 15/100 - Loss: 0.643\n",
      "Iter 16/100 - Loss: 0.604\n",
      "Iter 17/100 - Loss: 0.564\n",
      "Iter 18/100 - Loss: 0.524\n",
      "Iter 19/100 - Loss: 0.484\n",
      "Iter 20/100 - Loss: 0.442\n",
      "Iter 21/100 - Loss: 0.401\n",
      "Iter 22/100 - Loss: 0.360\n",
      "Iter 23/100 - Loss: 0.316\n",
      "Iter 24/100 - Loss: 0.273\n",
      "Iter 25/100 - Loss: 0.231\n",
      "Iter 26/100 - Loss: 0.187\n",
      "Iter 27/100 - Loss: 0.145\n",
      "Iter 28/100 - Loss: 0.098\n",
      "Iter 29/100 - Loss: 0.055\n",
      "Iter 30/100 - Loss: 0.010\n",
      "Iter 31/100 - Loss: -0.035\n",
      "Iter 32/100 - Loss: -0.080\n",
      "Iter 33/100 - Loss: -0.126\n",
      "Iter 34/100 - Loss: -0.171\n",
      "Iter 35/100 - Loss: -0.215\n",
      "Iter 36/100 - Loss: -0.260\n",
      "Iter 37/100 - Loss: -0.309\n",
      "Iter 38/100 - Loss: -0.355\n",
      "Iter 39/100 - Loss: -0.399\n",
      "Iter 40/100 - Loss: -0.445\n",
      "Iter 41/100 - Loss: -0.491\n",
      "Iter 42/100 - Loss: -0.538\n",
      "Iter 43/100 - Loss: -0.584\n",
      "Iter 44/100 - Loss: -0.630\n",
      "Iter 45/100 - Loss: -0.675\n",
      "Iter 46/100 - Loss: -0.721\n",
      "Iter 47/100 - Loss: -0.768\n",
      "Iter 48/100 - Loss: -0.814\n",
      "Iter 49/100 - Loss: -0.859\n",
      "Iter 50/100 - Loss: -0.904\n",
      "Iter 51/100 - Loss: -0.949\n",
      "Iter 52/100 - Loss: -0.995\n",
      "Iter 53/100 - Loss: -1.042\n",
      "Iter 54/100 - Loss: -1.089\n",
      "Iter 55/100 - Loss: -1.131\n",
      "Iter 56/100 - Loss: -1.175\n",
      "Iter 57/100 - Loss: -1.221\n",
      "Iter 58/100 - Loss: -1.267\n",
      "Iter 59/100 - Loss: -1.308\n",
      "Iter 60/100 - Loss: -1.352\n",
      "Iter 61/100 - Loss: -1.396\n",
      "Iter 62/100 - Loss: -1.437\n",
      "Iter 63/100 - Loss: -1.480\n",
      "Iter 64/100 - Loss: -1.526\n",
      "Iter 65/100 - Loss: -1.571\n",
      "Iter 66/100 - Loss: -1.610\n",
      "Iter 67/100 - Loss: -1.655\n",
      "Iter 68/100 - Loss: -1.701\n",
      "Iter 69/100 - Loss: -1.734\n",
      "Iter 70/100 - Loss: -1.790\n",
      "Iter 71/100 - Loss: -1.820\n",
      "Iter 72/100 - Loss: -1.859\n",
      "Iter 73/100 - Loss: -1.904\n",
      "Iter 74/100 - Loss: -1.945\n",
      "Iter 75/100 - Loss: -1.988\n",
      "Iter 76/100 - Loss: -2.038\n",
      "Iter 77/100 - Loss: -2.072\n",
      "Iter 78/100 - Loss: -2.102\n",
      "Iter 79/100 - Loss: -2.137\n",
      "Iter 80/100 - Loss: -2.179\n",
      "Iter 81/100 - Loss: -2.228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.346779465675354 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 82/100 - Loss: -2.149\n",
      "Iter 83/100 - Loss: -2.296\n",
      "Iter 84/100 - Loss: -2.327\n",
      "Iter 85/100 - Loss: -2.357\n",
      "Iter 86/100 - Loss: -2.388\n",
      "Iter 87/100 - Loss: -2.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.003901958465576 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/100 - Loss: -2.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.273878335952759 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/100 - Loss: -2.280\n",
      "Iter 90/100 - Loss: -2.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.455343723297119 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 91/100 - Loss: -2.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7052263021469116 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 92/100 - Loss: -2.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5231245756149292 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 93/100 - Loss: -2.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2894848585128784 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 94/100 - Loss: -2.354\n",
      "Iter 95/100 - Loss: -2.353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.509499549865723 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 96/100 - Loss: -2.360\n",
      "Iter 97/100 - Loss: -2.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.875657558441162 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 98/100 - Loss: -2.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2209928035736084 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 99/100 - Loss: -2.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.412660837173462 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100/100 - Loss: -2.348\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.910393238067627 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.9432263374328613 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31.6215, -3.2924,  3.9037,  ..., -3.1591,  3.6145,  4.7987],\n",
      "        [-3.2804, 16.0375,  4.5311,  ..., 15.6179,  4.5823, -8.6053],\n",
      "        [ 3.9010,  4.5575,  8.0438,  ...,  4.5570,  7.7015, -3.2313],\n",
      "        ...,\n",
      "        [-3.1429, 15.6235,  4.5327,  ..., 16.0000,  4.6017, -8.7859],\n",
      "        [ 3.5998,  4.5549,  7.6979,  ...,  4.5796,  7.7269, -3.1924],\n",
      "        [ 4.7789, -8.6051, -3.2201,  ..., -8.7858, -3.2000,  5.4692]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0118,  0.0020, -0.0029,  ...,  0.0020, -0.0027, -0.0019],\n",
      "        [ 0.0020, -0.0006,  0.0004,  ..., -0.0006,  0.0004,  0.0005],\n",
      "        [-0.0029,  0.0004, -0.0007,  ...,  0.0004, -0.0007, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0020, -0.0006,  0.0004,  ..., -0.0006,  0.0004,  0.0005],\n",
      "        [-0.0027,  0.0004, -0.0007,  ...,  0.0004, -0.0006, -0.0004],\n",
      "        [-0.0019,  0.0005, -0.0004,  ...,  0.0005, -0.0004, -0.0004]],\n",
      "       device='cuda:0')\n",
      "tensor(16.0409, device='cuda:0')\n",
      "error: 0.0002466136065777391\n",
      "data size: 2000\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.101\n",
      "Iter 2/100 - Loss: 1.092\n",
      "Iter 3/100 - Loss: 1.063\n",
      "Iter 4/100 - Loss: 1.031\n",
      "Iter 5/100 - Loss: 0.998\n",
      "Iter 6/100 - Loss: 0.966\n",
      "Iter 7/100 - Loss: 0.932\n",
      "Iter 8/100 - Loss: 0.899\n",
      "Iter 9/100 - Loss: 0.864\n",
      "Iter 10/100 - Loss: 0.828\n",
      "Iter 11/100 - Loss: 0.791\n",
      "Iter 12/100 - Loss: 0.754\n",
      "Iter 13/100 - Loss: 0.716\n",
      "Iter 14/100 - Loss: 0.679\n",
      "Iter 15/100 - Loss: 0.639\n",
      "Iter 16/100 - Loss: 0.601\n",
      "Iter 17/100 - Loss: 0.561\n",
      "Iter 18/100 - Loss: 0.520\n",
      "Iter 19/100 - Loss: 0.480\n",
      "Iter 20/100 - Loss: 0.439\n",
      "Iter 21/100 - Loss: 0.397\n",
      "Iter 22/100 - Loss: 0.355\n",
      "Iter 23/100 - Loss: 0.312\n",
      "Iter 24/100 - Loss: 0.269\n",
      "Iter 25/100 - Loss: 0.226\n",
      "Iter 26/100 - Loss: 0.183\n",
      "Iter 27/100 - Loss: 0.139\n",
      "Iter 28/100 - Loss: 0.095\n",
      "Iter 29/100 - Loss: 0.051\n",
      "Iter 30/100 - Loss: 0.005\n",
      "Iter 31/100 - Loss: -0.039\n",
      "Iter 32/100 - Loss: -0.084\n",
      "Iter 33/100 - Loss: -0.128\n",
      "Iter 34/100 - Loss: -0.174\n",
      "Iter 35/100 - Loss: -0.220\n",
      "Iter 36/100 - Loss: -0.265\n",
      "Iter 37/100 - Loss: -0.311\n",
      "Iter 38/100 - Loss: -0.357\n",
      "Iter 39/100 - Loss: -0.404\n",
      "Iter 40/100 - Loss: -0.449\n",
      "Iter 41/100 - Loss: -0.497\n",
      "Iter 42/100 - Loss: -0.541\n",
      "Iter 43/100 - Loss: -0.590\n",
      "Iter 44/100 - Loss: -0.634\n",
      "Iter 45/100 - Loss: -0.680\n",
      "Iter 46/100 - Loss: -0.726\n",
      "Iter 47/100 - Loss: -0.773\n",
      "Iter 48/100 - Loss: -0.820\n",
      "Iter 49/100 - Loss: -0.867\n",
      "Iter 50/100 - Loss: -0.912\n",
      "Iter 51/100 - Loss: -0.957\n",
      "Iter 52/100 - Loss: -1.002\n",
      "Iter 53/100 - Loss: -1.047\n",
      "Iter 54/100 - Loss: -1.093\n",
      "Iter 55/100 - Loss: -1.137\n",
      "Iter 56/100 - Loss: -1.181\n",
      "Iter 57/100 - Loss: -1.227\n",
      "Iter 58/100 - Loss: -1.273\n",
      "Iter 59/100 - Loss: -1.319\n",
      "Iter 60/100 - Loss: -1.367\n",
      "Iter 61/100 - Loss: -1.410\n",
      "Iter 62/100 - Loss: -1.459\n",
      "Iter 63/100 - Loss: -1.501\n",
      "Iter 64/100 - Loss: -1.542\n",
      "Iter 65/100 - Loss: -1.592\n",
      "Iter 66/100 - Loss: -1.630\n",
      "Iter 67/100 - Loss: -1.676\n",
      "Iter 68/100 - Loss: -1.719\n",
      "Iter 69/100 - Loss: -1.762\n",
      "Iter 70/100 - Loss: -1.804\n",
      "Iter 71/100 - Loss: -1.850\n",
      "Iter 72/100 - Loss: -1.882\n",
      "Iter 73/100 - Loss: -1.929\n",
      "Iter 74/100 - Loss: -1.967\n",
      "Iter 75/100 - Loss: -2.008\n",
      "Iter 76/100 - Loss: -2.053\n",
      "Iter 77/100 - Loss: -2.082\n",
      "Iter 78/100 - Loss: -2.114\n",
      "Iter 79/100 - Loss: -2.162\n",
      "Iter 80/100 - Loss: -2.201\n",
      "Iter 81/100 - Loss: -2.236\n",
      "Iter 82/100 - Loss: -2.272\n",
      "Iter 83/100 - Loss: -2.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4936801195144653 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 84/100 - Loss: -2.207\n",
      "Iter 85/100 - Loss: -2.353\n",
      "Iter 86/100 - Loss: -2.250\n",
      "Iter 87/100 - Loss: -2.445\n",
      "Iter 88/100 - Loss: -2.469\n",
      "Iter 89/100 - Loss: -2.292\n",
      "Iter 90/100 - Loss: -2.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3631525039672852 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 91/100 - Loss: -2.326\n",
      "Iter 92/100 - Loss: -2.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.37510347366333 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 93/100 - Loss: -2.349\n",
      "Iter 94/100 - Loss: -2.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2575603723526 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 95/100 - Loss: -2.363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0520668029785156 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 96/100 - Loss: -2.372\n",
      "Iter 97/100 - Loss: -2.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4442596435546875 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 98/100 - Loss: -2.378\n",
      "Iter 99/100 - Loss: -2.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.002423286437988 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100/100 - Loss: -2.385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.93405294418335 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.907345712184906 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[29.4083, -5.7319, -4.1715,  ..., -5.7051, -5.0488,  6.2976],\n",
      "        [-4.1798,  9.5809, -0.5638,  ...,  9.5203, -0.4189, -3.9827],\n",
      "        [-2.7312, -0.5183,  3.1773,  ..., -0.5194,  3.4376, -1.1350],\n",
      "        ...,\n",
      "        [-4.1544,  9.5399, -0.5662,  ...,  9.4853, -0.4211, -3.9812],\n",
      "        [-2.7183, -0.5168,  3.1604,  ..., -0.5175,  3.4200, -1.1324],\n",
      "        [ 4.5090, -3.9603, -0.8336,  ..., -3.9490, -1.1251,  5.3965]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0226,  0.0029,  0.0026,  ...,  0.0029,  0.0026, -0.0038],\n",
      "        [ 0.0029, -0.0006, -0.0003,  ..., -0.0006, -0.0003,  0.0006],\n",
      "        [ 0.0026, -0.0003, -0.0003,  ..., -0.0003, -0.0003,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0029, -0.0006, -0.0003,  ..., -0.0006, -0.0003,  0.0006],\n",
      "        [ 0.0026, -0.0003, -0.0003,  ..., -0.0003, -0.0003,  0.0004],\n",
      "        [-0.0038,  0.0006,  0.0004,  ...,  0.0006,  0.0004, -0.0007]],\n",
      "       device='cuda:0')\n",
      "tensor(17.1098, device='cuda:0')\n",
      "error: 0.0009766814764589071\n",
      "data size: 3000\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.096\n",
      "Iter 2/100 - Loss: 1.088\n",
      "Iter 3/100 - Loss: 1.058\n",
      "Iter 4/100 - Loss: 1.027\n",
      "Iter 5/100 - Loss: 0.995\n",
      "Iter 6/100 - Loss: 0.962\n",
      "Iter 7/100 - Loss: 0.928\n",
      "Iter 8/100 - Loss: 0.894\n",
      "Iter 9/100 - Loss: 0.860\n",
      "Iter 10/100 - Loss: 0.824\n",
      "Iter 11/100 - Loss: 0.787\n",
      "Iter 12/100 - Loss: 0.751\n",
      "Iter 13/100 - Loss: 0.713\n",
      "Iter 14/100 - Loss: 0.675\n",
      "Iter 15/100 - Loss: 0.636\n",
      "Iter 16/100 - Loss: 0.597\n",
      "Iter 17/100 - Loss: 0.556\n",
      "Iter 18/100 - Loss: 0.517\n",
      "Iter 19/100 - Loss: 0.475\n",
      "Iter 20/100 - Loss: 0.434\n",
      "Iter 21/100 - Loss: 0.393\n",
      "Iter 22/100 - Loss: 0.351\n",
      "Iter 23/100 - Loss: 0.308\n",
      "Iter 24/100 - Loss: 0.264\n",
      "Iter 25/100 - Loss: 0.221\n",
      "Iter 26/100 - Loss: 0.179\n",
      "Iter 27/100 - Loss: 0.134\n",
      "Iter 28/100 - Loss: 0.089\n",
      "Iter 29/100 - Loss: 0.045\n",
      "Iter 30/100 - Loss: 0.001\n",
      "Iter 31/100 - Loss: -0.044\n",
      "Iter 32/100 - Loss: -0.089\n",
      "Iter 33/100 - Loss: -0.134\n",
      "Iter 34/100 - Loss: -0.180\n",
      "Iter 35/100 - Loss: -0.226\n",
      "Iter 36/100 - Loss: -0.272\n",
      "Iter 37/100 - Loss: -0.317\n",
      "Iter 38/100 - Loss: -0.363\n",
      "Iter 39/100 - Loss: -0.410\n",
      "Iter 40/100 - Loss: -0.456\n",
      "Iter 41/100 - Loss: -0.502\n",
      "Iter 42/100 - Loss: -0.548\n",
      "Iter 43/100 - Loss: -0.595\n",
      "Iter 44/100 - Loss: -0.641\n",
      "Iter 45/100 - Loss: -0.687\n",
      "Iter 46/100 - Loss: -0.733\n",
      "Iter 47/100 - Loss: -0.780\n",
      "Iter 48/100 - Loss: -0.825\n",
      "Iter 49/100 - Loss: -0.872\n",
      "Iter 50/100 - Loss: -0.918\n",
      "Iter 51/100 - Loss: -0.963\n",
      "Iter 52/100 - Loss: -1.009\n",
      "Iter 53/100 - Loss: -1.054\n",
      "Iter 54/100 - Loss: -1.098\n",
      "Iter 55/100 - Loss: -1.145\n",
      "Iter 56/100 - Loss: -1.192\n",
      "Iter 57/100 - Loss: -1.236\n",
      "Iter 58/100 - Loss: -1.284\n",
      "Iter 59/100 - Loss: -1.328\n",
      "Iter 60/100 - Loss: -1.369\n",
      "Iter 61/100 - Loss: -1.414\n",
      "Iter 62/100 - Loss: -1.462\n",
      "Iter 63/100 - Loss: -1.508\n",
      "Iter 64/100 - Loss: -1.551\n",
      "Iter 65/100 - Loss: -1.599\n",
      "Iter 66/100 - Loss: -1.643\n",
      "Iter 67/100 - Loss: -1.687\n",
      "Iter 68/100 - Loss: -1.730\n",
      "Iter 69/100 - Loss: -1.772\n",
      "Iter 70/100 - Loss: -1.813\n",
      "Iter 71/100 - Loss: -1.856\n",
      "Iter 72/100 - Loss: -1.895\n",
      "Iter 73/100 - Loss: -1.936\n",
      "Iter 74/100 - Loss: -1.972\n",
      "Iter 75/100 - Loss: -2.015\n",
      "Iter 76/100 - Loss: -2.047\n",
      "Iter 77/100 - Loss: -2.082\n",
      "Iter 78/100 - Loss: -2.129\n",
      "Iter 79/100 - Loss: -2.166\n",
      "Iter 80/100 - Loss: -2.202\n",
      "Iter 81/100 - Loss: -2.249\n",
      "Iter 82/100 - Loss: -2.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7219960689544678 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 83/100 - Loss: -2.188\n",
      "Iter 84/100 - Loss: -2.367\n",
      "Iter 85/100 - Loss: -2.232\n",
      "Iter 86/100 - Loss: -2.262\n",
      "Iter 87/100 - Loss: -2.393\n",
      "Iter 88/100 - Loss: -2.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.951810359954834 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/100 - Loss: -2.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1620830297470093 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 90/100 - Loss: -2.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.034358501434326 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 91/100 - Loss: -2.307\n",
      "Iter 92/100 - Loss: -2.313\n",
      "Iter 93/100 - Loss: -2.317\n",
      "Iter 94/100 - Loss: -2.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4815093278884888 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 95/100 - Loss: -2.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.099392294883728 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 96/100 - Loss: -2.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9876108169555664 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 97/100 - Loss: -2.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3630259037017822 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 98/100 - Loss: -2.244\n",
      "Iter 99/100 - Loss: -2.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2240285873413086 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100/100 - Loss: -2.206\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2739453315734863 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.9013645648956299 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.7863, -3.0480, -0.9126,  ..., -2.2769, -1.3280, -0.4421],\n",
      "        [-2.3319,  4.4326, -0.8606,  ...,  1.9834,  0.0363, -0.0747],\n",
      "        [-0.7662, -0.5556,  1.8221,  ..., -0.1018,  0.7348,  0.0955],\n",
      "        ...,\n",
      "        [-2.2113,  2.3547, -0.0619,  ...,  5.0057,  0.2943, -0.3298],\n",
      "        [-0.9477, -0.1553,  0.5283,  ..., -0.9800,  1.4489, -0.1087],\n",
      "        [-0.5221,  0.0609,  0.1157,  ..., -0.2621,  0.0289,  0.1379]],\n",
      "       device='cuda:0')\n",
      "tensor([[-1.7385e-03,  6.4635e-04, -4.9800e-05,  ...,  5.2953e-04,\n",
      "         -1.0943e-04,  1.4451e-04],\n",
      "        [ 6.4635e-04, -2.3985e-04,  1.9193e-05,  ..., -1.9681e-04,\n",
      "          4.1410e-05, -5.3532e-05],\n",
      "        [-4.9800e-05,  1.9193e-05,  7.1526e-07,  ...,  1.6555e-05,\n",
      "         -5.6624e-06,  4.4517e-06],\n",
      "        ...,\n",
      "        [ 5.2953e-04, -1.9681e-04,  1.6555e-05,  ..., -1.6069e-04,\n",
      "          3.4571e-05, -4.3690e-05],\n",
      "        [-1.0943e-04,  4.1410e-05, -5.6624e-06,  ...,  3.4571e-05,\n",
      "         -7.1526e-06,  9.5516e-06],\n",
      "        [ 1.4451e-04, -5.3532e-05,  4.4517e-06,  ..., -4.3690e-05,\n",
      "          9.5516e-06, -9.6560e-06]], device='cuda:0')\n",
      "tensor(5.6862, device='cuda:0')\n",
      "error: 0.00010966652916977182\n",
      "data size: 4000\n",
      "Use Cuda: True\n",
      "Iter 1/100 - Loss: 1.093\n",
      "Iter 2/100 - Loss: 1.085\n",
      "Iter 3/100 - Loss: 1.055\n",
      "Iter 4/100 - Loss: 1.024\n",
      "Iter 5/100 - Loss: 0.993\n",
      "Iter 6/100 - Loss: 0.959\n",
      "Iter 7/100 - Loss: 0.926\n",
      "Iter 8/100 - Loss: 0.892\n",
      "Iter 9/100 - Loss: 0.857\n",
      "Iter 10/100 - Loss: 0.821\n",
      "Iter 11/100 - Loss: 0.785\n",
      "Iter 12/100 - Loss: 0.748\n",
      "Iter 13/100 - Loss: 0.710\n",
      "Iter 14/100 - Loss: 0.672\n",
      "Iter 15/100 - Loss: 0.634\n",
      "Iter 16/100 - Loss: 0.594\n",
      "Iter 17/100 - Loss: 0.555\n",
      "Iter 18/100 - Loss: 0.514\n",
      "Iter 19/100 - Loss: 0.474\n",
      "Iter 20/100 - Loss: 0.431\n",
      "Iter 21/100 - Loss: 0.390\n",
      "Iter 22/100 - Loss: 0.348\n",
      "Iter 23/100 - Loss: 0.306\n",
      "Iter 24/100 - Loss: 0.262\n",
      "Iter 25/100 - Loss: 0.219\n",
      "Iter 26/100 - Loss: 0.176\n",
      "Iter 27/100 - Loss: 0.131\n",
      "Iter 28/100 - Loss: 0.087\n",
      "Iter 29/100 - Loss: 0.043\n",
      "Iter 30/100 - Loss: -0.002\n",
      "Iter 31/100 - Loss: -0.047\n",
      "Iter 32/100 - Loss: -0.092\n",
      "Iter 33/100 - Loss: -0.138\n",
      "Iter 34/100 - Loss: -0.183\n",
      "Iter 35/100 - Loss: -0.229\n",
      "Iter 36/100 - Loss: -0.274\n",
      "Iter 37/100 - Loss: -0.320\n",
      "Iter 38/100 - Loss: -0.367\n",
      "Iter 39/100 - Loss: -0.413\n",
      "Iter 40/100 - Loss: -0.459\n",
      "Iter 41/100 - Loss: -0.506\n",
      "Iter 42/100 - Loss: -0.551\n",
      "Iter 43/100 - Loss: -0.598\n",
      "Iter 44/100 - Loss: -0.644\n",
      "Iter 45/100 - Loss: -0.690\n",
      "Iter 46/100 - Loss: -0.737\n",
      "Iter 47/100 - Loss: -0.783\n",
      "Iter 48/100 - Loss: -0.829\n",
      "Iter 49/100 - Loss: -0.876\n",
      "Iter 50/100 - Loss: -0.921\n",
      "Iter 51/100 - Loss: -0.967\n",
      "Iter 52/100 - Loss: -1.011\n",
      "Iter 53/100 - Loss: -1.058\n",
      "Iter 54/100 - Loss: -1.103\n",
      "Iter 55/100 - Loss: -1.150\n",
      "Iter 56/100 - Loss: -1.196\n",
      "Iter 57/100 - Loss: -1.243\n",
      "Iter 58/100 - Loss: -1.289\n",
      "Iter 59/100 - Loss: -1.335\n",
      "Iter 60/100 - Loss: -1.381\n",
      "Iter 61/100 - Loss: -1.429\n",
      "Iter 62/100 - Loss: -1.474\n",
      "Iter 63/100 - Loss: -1.518\n",
      "Iter 64/100 - Loss: -1.564\n",
      "Iter 65/100 - Loss: -1.608\n",
      "Iter 66/100 - Loss: -1.652\n",
      "Iter 67/100 - Loss: -1.694\n",
      "Iter 68/100 - Loss: -1.739\n",
      "Iter 69/100 - Loss: -1.782\n",
      "Iter 70/100 - Loss: -1.832\n",
      "Iter 71/100 - Loss: -1.868\n",
      "Iter 72/100 - Loss: -1.913\n",
      "Iter 73/100 - Loss: -1.953\n",
      "Iter 74/100 - Loss: -1.990\n",
      "Iter 75/100 - Loss: -2.036\n",
      "Iter 76/100 - Loss: -2.078\n",
      "Iter 77/100 - Loss: -2.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.509335994720459 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 78/100 - Loss: -2.069\n",
      "Iter 79/100 - Loss: -2.187\n",
      "Iter 80/100 - Loss: -2.222\n",
      "Iter 81/100 - Loss: -2.213\n",
      "Iter 82/100 - Loss: -2.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.585566759109497 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 83/100 - Loss: -2.162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.068086862564087 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 84/100 - Loss: -2.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3773711919784546 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 85/100 - Loss: -2.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3140848875045776 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 86/100 - Loss: -2.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.854015827178955 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 87/100 - Loss: -2.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3716779947280884 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 88/100 - Loss: -2.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.230764865875244 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 89/100 - Loss: -2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9505863189697266 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 90/100 - Loss: -2.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1394565105438232 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 91/100 - Loss: -2.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.940016031265259 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 92/100 - Loss: -2.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.256080150604248 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 93/100 - Loss: -2.180\n",
      "Iter 94/100 - Loss: -2.193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.727034330368042 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 95/100 - Loss: -2.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.480188846588135 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 96/100 - Loss: -2.241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.593756914138794 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 97/100 - Loss: -2.210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.972780704498291 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 98/100 - Loss: -2.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.860082626342773 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 99/100 - Loss: -2.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.32013463973999 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100/100 - Loss: -2.242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.804778575897217 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/grand/datascience/tiany/python_venv/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:321: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.7876563668251038 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.3188e+00, -6.7244e+00,  7.6170e+00,  ..., -5.5533e+00,\n",
      "          3.2573e+00, -1.5176e-01],\n",
      "        [-3.9314e+00,  9.4657e+00, -3.3255e+00,  ...,  2.4589e+00,\n",
      "         -1.2606e+00, -3.5882e-02],\n",
      "        [ 6.3331e+00, -4.1173e+00,  8.4479e+00,  ..., -3.3537e+00,\n",
      "          2.3230e+00,  6.9898e-03],\n",
      "        ...,\n",
      "        [-5.3531e-01,  1.3989e+00, -2.3749e-01,  ...,  6.5765e+00,\n",
      "         -3.5652e+00,  7.3150e-01],\n",
      "        [ 1.8400e+00, -3.8288e-01,  6.0291e-01,  ..., -4.1123e+00,\n",
      "          8.7446e+00, -4.2370e-01],\n",
      "        [ 2.9253e-01, -4.3768e-02,  2.0607e-01,  ...,  6.0666e-01,\n",
      "         -2.0390e-01,  9.2895e-01]], device='cuda:0')\n",
      "tensor([[-0.0103,  0.0055, -0.0049,  ..., -0.0071,  0.0147, -0.0034],\n",
      "        [ 0.0055, -0.0031,  0.0026,  ...,  0.0053, -0.0101,  0.0024],\n",
      "        [-0.0049,  0.0026, -0.0023,  ..., -0.0030,  0.0064, -0.0015],\n",
      "        ...,\n",
      "        [-0.0071,  0.0053, -0.0030,  ..., -0.0647,  0.0961, -0.0246],\n",
      "        [ 0.0147, -0.0101,  0.0064,  ...,  0.0961, -0.1446,  0.0369],\n",
      "        [-0.0034,  0.0024, -0.0015,  ..., -0.0246,  0.0369, -0.0094]],\n",
      "       device='cuda:0')\n",
      "tensor(11.5420, device='cuda:0')\n",
      "error: 0.010871585458517075\n"
     ]
    }
   ],
   "source": [
    "gpu_size_vec = [100,300,500,700,1000,1500,2000,3000,4000]#,5000,6000,7000]\n",
    "Nval = 4\n",
    "Dval = 4\n",
    "\n",
    "for size in gpu_size_vec:\n",
    "    print(f\"data size: {size}\")\n",
    "    \"\"\"Set up the training and testing data\"\"\"\n",
    "    n = size # input size\n",
    "\n",
    "#     x = 5 * torch.rand(n, Dval)\n",
    "\n",
    "#     y = torch.stack([\n",
    "#         torch.sin(x[:, 0] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         torch.cos(x[:, 0] * (2 * math.pi)) + torch.cos(x[:, 2] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         torch.sin(x[:, 2] * (2 * math.pi)) + torch.cos(x[:, 1] * (2 * math.pi)) + torch.randn(n) * 0.02,\n",
    "#         (torch.cos(x[:, 3] * (2 * math.pi)))* (torch.sin(x[:, 0] * (2 * math.pi))) + torch.randn(n) * 0.02,\n",
    "#     ], -1)\n",
    "\n",
    "    x = 5 * torch.rand(n)\n",
    "    \n",
    "    y = torch.stack([\n",
    "        torch.sin(3 * x) + torch.randn(n) * 0.02,\n",
    "        torch.cos(x) + torch.cos(2 * x) + torch.randn(n) * 0.02,\n",
    "        torch.sin(x) + torch.cos(x) + torch.randn(n) * 0.02,\n",
    "        torch.cos(x) * torch.cos(x) + torch.randn(n) * 0.02,\n",
    "    ], -1)\n",
    "\n",
    "#     train_x = torch.Tensor(x[:int(0.8*n), :])\n",
    "#     train_y = y[:int(0.8*n), :]\n",
    "\n",
    "#     test_x = torch.Tensor(x[int(0.8*n):, :])\n",
    "\n",
    "#     test_y = torch.Tensor(y[int(0.8*n):, :])\n",
    "\n",
    "    train_x = x[:int(0.8*n)]\n",
    "    train_y = y[:int(0.8*n)]\n",
    "\n",
    "    test_x = x[int(0.8*n): ]\n",
    "\n",
    "    test_y = y[int(0.8*n): ]\n",
    "\n",
    "#     # normalize features\n",
    "#     mean = train_x.mean(dim=-2, keepdim=True)\n",
    "#     std = train_x.std(dim=-2, keepdim=True) # + 1e-6 # prevent dividing by 0\n",
    "#     train_x = (train_x - mean) / std\n",
    "#     test_x = (test_x - mean) / std\n",
    "\n",
    "#     # normalize labels\n",
    "#     mean, std = train_y.mean(),train_y.std()\n",
    "#     train_y = (train_y - mean) / std\n",
    "#     test_y = (test_y - mean) / std\n",
    "\n",
    "#     norm_vec = (vec - mean) / std\n",
    "    \n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=Dval)\n",
    "    model = MultitaskGPModel(train_x, train_y, likelihood, num_base_kernels)\n",
    "    \n",
    "    start_time = time.time() # include the time of copying values onto gpu\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(f\"Use Cuda: {use_cuda}\")\n",
    "    if(use_cuda):\n",
    "        train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "    \n",
    "    \"\"\"train the model hyperparameters\"\"\"\n",
    "    import os\n",
    "    smoke_test = ('CI' in os.environ)\n",
    "    training_iterations = 2 if smoke_test else 100\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.09)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "#           if(i > training_iterations*0.8):\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "    \n",
    "    gpu_training_time.append(time.time() - start_time)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \"\"\" Making predictions with the model\"\"\"\n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Exact predictions\n",
    "    with torch.no_grad(): #, gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x) # no noise\n",
    "        covar = preds.covariance_matrix\n",
    "        gpu_exact_meancovar.append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # LOVE without cache\n",
    "        # Clear the cache from the previous computations\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x)\n",
    "        fast_covar = preds.covariance_matrix\n",
    "        gpu_love_meancovar.append(time.time() - start_time)\n",
    "        \n",
    "        \n",
    "#     \"\"\"\n",
    "#     Compute sum of squared difference LOVE diagonal covariance elements from exact diagonal elements \n",
    "#     (again divided by trace of exact covariance to make the quantity normalized), as a function\n",
    "#     of vector size\n",
    "#     \"\"\"\n",
    "    \n",
    "#     exactdiag = torch.diagonal(covar)\n",
    "#     lovediag = torch.diagonal(fast_covar)\n",
    "    print(covar)\n",
    "    print(fast_covar)\n",
    "    \n",
    "    exactdiag = torch.diagonal(covar)\n",
    "    lovediag = torch.diagonal(fast_covar)\n",
    "    diff = (exactdiag - lovediag).square().mean().sqrt()\n",
    "    diff = diff / exactdiag.square().mean().sqrt()\n",
    "    diff = diff.log()\n",
    "    love_covar_error.append(diff)\n",
    "    print(f\"error: {diff}\")\n",
    "    \n",
    "    # LOVE with cache\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        start_time = time.time()\n",
    "        preds = model(test_x)\n",
    "        fast_covar = preds.covariance_matrix\n",
    "        gpu_love_meancovar_cache.append(time.time() - start_time)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.867035627365112, 9.235561609268188, 10.500993728637695, 9.693816423416138, 11.418615579605103, 24.82183837890625, 23.0015971660614, 26.419338703155518, 40.362865686416626]\n",
      "[0.38283562660217285, 0.23735904693603516, 0.37119531631469727, 0.8079555034637451, 4.700577974319458, 11.02782917022705, 21.38694477081299, 61.8224663734436, 134.51374578475952]\n",
      "[0.03187155723571777, 0.33365297317504883, 0.4834730625152588, 0.4864842891693115, 1.4249203205108643, 1.626413106918335, 1.466770887374878, 1.5310571193695068, 1.8382620811462402]\n",
      "[0.015583276748657227, 0.03659701347351074, 0.03690624237060547, 0.036966562271118164, 0.03968024253845215, 0.04920148849487305, 0.06383752822875977, 0.12085819244384766, 0.2277364730834961]\n"
     ]
    }
   ],
   "source": [
    "print(gpu_training_time)\n",
    "print(gpu_exact_meancovar)\n",
    "print(gpu_love_meancovar)\n",
    "print(gpu_love_meancovar_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cpu_size_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # plot with various axes scales\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.figure()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# CPU vs GPU training\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mcpu_size_vec\u001b[49m, cpu_training_time, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu training time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(gpu_size_vec, gpu_training_time, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu training time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime (second)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cpu_size_vec' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # plot with various axes scales\n",
    "# plt.figure()\n",
    "\n",
    "# CPU vs GPU training\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(cpu_size_vec, cpu_training_time, 'r-', label='cpu training time')\n",
    "plt.plot(gpu_size_vec, gpu_training_time, 'b-', label='gpu training time')\n",
    "plt.ylabel('Time (second)')\n",
    "plt.xlabel('Input Size')\n",
    "plt.title('CPU vs. GPU Training Time')\n",
    "# plt.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Distribution Computation Time\n",
    "\n",
    "# # Initialize plots\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 6), sharey=True)\n",
    "\n",
    "plt.suptitle('Predictive Distribution Computation Time', fontweight='bold')\n",
    "\n",
    "ax1.plot(cpu_size_vec, cpu_exact_meancovar, 'r-', label='cpu exact')\n",
    "ax1.plot(gpu_size_vec, gpu_exact_meancovar, 'b-', label='gpu exact')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "ax2.plot(cpu_size_vec, cpu_love_meancovar, 'g-', label='cpu love no cache')\n",
    "ax2.plot(gpu_size_vec, gpu_love_meancovar, 'c-', label='gpu love no cache')\n",
    "ax2.legend()\n",
    "# plt.ylabel('Time')\n",
    "# plt.xlabel('Input Size')\n",
    "\n",
    "plt.subplot(133)\n",
    "ax3.plot(cpu_size_vec, cpu_love_meancovar_cache, 'y-', label='cpu love with cache')\n",
    "ax3.plot(gpu_size_vec, gpu_love_meancovar_cache, 'm-', label='gpu love with cache')\n",
    "ax3.legend()\n",
    "\n",
    "plt.setp([ax1,ax2,ax3], xlabel='Input Size')\n",
    "plt.setp(ax1, ylabel='Time (Second)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
