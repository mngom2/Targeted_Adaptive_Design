{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "sys.path.append(\"..\")\n",
    "import vvkernels as vvk\n",
    "import sep_vvkernels as svvk\n",
    "import vvk_rbfkernel as vvk_rbf\n",
    "import vvmeans as vvm\n",
    "import vvlikelihood as vvll\n",
    "from vfield import VField\n",
    "from LBFGS import FullBatchLBFGS\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = VField()\n",
    "f_target = vf.tgt_vec\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "def g_theta(sample_size, D):\n",
    "    loc = np.random.random_sample((sample_size,D))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "train_y = torch.zeros([sample_size, N])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    train_y[i] = Tensor(vf(train_x[i])) + torch.randn(Tensor(vf(train_x[i])).size()) *  math.sqrt(0.04)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = Tensor(vf(x[i])) + torch.randn(Tensor(vf(x[i])).size()) * math.sqrt(0.04)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criteria(tol_vector, f_target, lower_bound, upper_bound):\n",
    "    lower_tol_vector = f_target - tol_vector\n",
    "    upper_tol_vector = f_target + tol_vector\n",
    "    SUCCESS = True\n",
    "    FAILURE = False\n",
    "    for i in range(f_target.shape[0]):\n",
    "            if (lower_bound[i] < lower_tol_vector[i]) or  (upper_bound[i] > upper_tol_vector[i]):\n",
    "                SUCCESS = False  \n",
    "            if ((lower_bound[i] > upper_tol_vector[i]) or  (upper_bound[i] < lower_tol_vector[i])):\n",
    "                FAILURE = True\n",
    "    return SUCCESS, FAILURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)\n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(vvk_rbf.vvkRBFKernel())\n",
    "\n",
    "\n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "#\\         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "       # self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x,x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##hyperparameters optimization###\n",
    "# def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "#     likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "#     model = MultitaskGPModel(g_theta1, agg_data, likelihood,3)\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "    \n",
    "    \n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),  lr=0.07)  # Includes GaussianLikelihood parameters\n",
    "#     #mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "#     for i in range(training_iter):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(g_theta1)\n",
    "#         output_ll = likelihood(output)\n",
    "\n",
    "#         loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "#         #loss = -mll(output, agg_data)\n",
    "#         loss.backward(retain_graph=True)\n",
    "\n",
    "#         print('Iter %d/%d - Loss hyperparam: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "#         optimizer.step()\n",
    "\n",
    "#     return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_opti(g_theta1, agg_data, training_iter):\n",
    "    likelihood = vvll.TensorProductLikelihood(num_tasks = 2)\n",
    "\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,3)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = FullBatchLBFGS(model.parameters(), lr=1.)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    def closure():\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(g_theta1)\n",
    "        #output_ll = likelihood(output)\n",
    "\n",
    "        #loss = -likelihood.get_mll(agg_data,output_ll)\n",
    "        loss = -mll(output, agg_data)\n",
    "        print('Loss gp: %.3f' % ( loss))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    loss = closure()\n",
    "    loss.backward()\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        options = {'closure': closure, 'current_loss': loss, 'max_ls': 20}\n",
    "        loss, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "\n",
    "        if fail:\n",
    "            print('Convergence reached!')\n",
    "            break\n",
    "\n",
    " \n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class param_opti(nn.Module):\n",
    "    def __init__(self, sample):\n",
    "        super(param_opti, self).__init__()\n",
    "        #loc = np.random.random_sample((loc_size,2))\n",
    "        self.g_theta2 = nn.Parameter(Tensor(sample))\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return (self.g_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_param_opti(x,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_iter):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    _par = param_opti(loc_sample)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        g_theta2 = _par.forward()\n",
    "\n",
    "        loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "        loss1 = -1 * loss1\n",
    "        \n",
    "        return loss1\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(_par.parameters(), lr=1., history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    g_theta2 = _par.forward()\n",
    "\n",
    "    loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "    loss1 = -1 * loss1\n",
    "    \n",
    "    print('Loss theta: %.3f' % ( loss1))\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(_par.parameters(), lr=0.007)\n",
    "\n",
    "# #     for j in range(training_iter):\n",
    "# #         optimizer.zero_grad()\n",
    "# #         g_theta2 = _par.forward()\n",
    "\n",
    "# #         loss1, lower_bound, upper_bound = likelihood.get_ell(agg_data,f_target,x, g_theta1, g_theta2, model, likelihood)\n",
    "# #         loss1 = -1 * loss1\n",
    "# #         loss1.backward(retain_graph=True)\n",
    "# #         print('Iter %d/%d - Loss theta2: %.3f' % (j + 1, training_iter, loss1.item()))\n",
    "# #         optimizer.step()\n",
    "          \n",
    "    return loss1, g_theta2, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class design_opti(nn.Module):\n",
    "    def __init__(self, _x):\n",
    "        super(design_opti, self).__init__()\n",
    "#         a = _x[0,0]\n",
    "#         b = _x[0,1]\n",
    "#         self.x_design_1 = nn.Parameter(a)\n",
    "#         self.x_design_2 = nn.Parameter(b)\n",
    "        self.x_design = nn.Parameter(_x)\n",
    "    def forward(self):\n",
    "#         x_design = torch.zeros(1,2)\n",
    "#         x_design[0,0] = self.x_design_1\n",
    "#         x_design[0,1] = self.x_design_2\n",
    "        return (self.x_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_opti(x0,loc_sample, f_target, g_theta1, agg_data, model, likelihood, training_design_iter, training_param_iter, lr_new):\n",
    "    design = design_opti(x0)\n",
    "    loc_sample0 = loc_sample\n",
    "    x_d = design.forward()\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss2, g_theta2_out,lower_bound, upper_bound = conduct_param_opti(x_d,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_param_iter)\n",
    "        loss2.backward(retain_graph=True)\n",
    "#         print(x_d)\n",
    "#         print(lower_bound)\n",
    "#         print(upper_bound)\n",
    "       \n",
    "        return loss2\n",
    "        \n",
    "        \n",
    "        \n",
    "    optimizer = torch.optim.LBFGS(design.parameters(), lr=lr_new, history_size=100, max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    x_d = design.forward()\n",
    "    loss2, g_theta2_out,lower_bound, upper_bound = conduct_param_opti(x_d,loc_sample, f_target,g_theta1, agg_data, model, likelihood, training_param_iter) ##takethis out\n",
    "    print('Loss design: %.3f' % ( loss2))\n",
    "    print(optimizer.state)\n",
    "    return x_d, g_theta2_out, lower_bound, upper_bound\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.898\n",
      "Loss gp: 0.796\n",
      "Loss gp: 0.690\n",
      "Loss gp: 0.473\n",
      "Loss gp: 0.102\n",
      "Loss gp: 27.755\n",
      "Loss gp: 0.039\n",
      "Loss gp: 0.010\n",
      "Loss gp: 0.003\n",
      "Loss gp: -0.011\n",
      "Loss gp: -0.021\n",
      "Loss gp: -0.036\n",
      "Loss gp: -0.039\n",
      "Loss gp: -0.040\n",
      "Loss gp: -0.042\n",
      "Loss gp: -0.047\n",
      "Loss gp: -0.052\n",
      "Loss gp: -0.051\n",
      "Loss gp: -0.053\n",
      "Loss gp: -0.054\n",
      "Loss gp: -0.056\n",
      "Loss gp: -0.058\n",
      "Loss gp: -0.058\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.063\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "0\n",
      "Loss theta: 762.888\n",
      "Loss theta: -4.509\n",
      "Loss theta: -4.556\n",
      "Loss theta: -4.524\n",
      "Loss theta: -4.519\n",
      "Loss theta: -4.524\n",
      "Loss theta: -4.524\n",
      "Loss theta: -4.524\n",
      "Loss theta: -4.493\n",
      "Loss theta: -4.529\n",
      "Loss theta: -4.537\n",
      "Loss theta: -4.507\n",
      "Loss theta: -4.534\n",
      "Loss theta: -4.537\n",
      "Loss theta: 19.734\n",
      "Loss theta: -4.562\n",
      "Loss theta: -4.554\n",
      "Loss theta: -4.580\n",
      "Loss theta: -4.616\n",
      "Loss theta: -4.643\n",
      "Loss theta: -4.674\n",
      "Loss theta: -4.674\n",
      "Loss theta: -4.674\n",
      "Loss theta: -4.674\n",
      "Loss theta: -4.821\n",
      "Loss theta: -4.818\n",
      "Loss theta: -4.874\n",
      "Loss theta: -4.757\n",
      "Loss theta: -4.833\n",
      "Loss theta: -4.817\n",
      "Loss theta: -4.824\n",
      "Loss theta: -4.874\n",
      "Loss theta: -4.874\n",
      "Loss theta: -4.874\n",
      "Loss theta: -4.874\n",
      "Loss theta: -4.811\n",
      "Loss theta: -4.897\n",
      "Loss theta: -4.938\n",
      "Loss theta: -4.890\n",
      "Loss theta: -4.927\n",
      "Loss theta: -4.938\n",
      "Loss theta: -4.904\n",
      "Loss theta: -4.983\n",
      "Loss theta: -4.980\n",
      "Loss theta: -4.964\n",
      "Loss theta: -4.951\n",
      "Loss theta: -4.983\n",
      "Loss theta: -4.983\n",
      "Loss design: -4.983\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.2257, 0.0679]], requires_grad=True): {'func_evals': 47, 'n_iter': 13, 'al': [tensor(-1.9829e-10), tensor(4.0504e-05), tensor(1.4707e-07), tensor(-2.8484e-07), tensor(2.9315e-06), tensor(-9.1502e-07), tensor(1.2794e-05), tensor(-3.6224e-06), tensor(0.0001), tensor(2.5625), tensor(0.8363), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0053,  0.0033]), 't': 0, 'old_dirs': [tensor([-1827.4757, -1749.4731]), tensor([0.0032, 0.0171]), tensor([0.5759, 0.8849]), tensor([3.4428, 4.6835]), tensor([-0.3220, -0.4323]), tensor([-5.1715, -9.0242]), tensor([0.4233, 0.5946]), tensor([ 0.9459, -0.8377]), tensor([2.5220, 3.2618]), tensor([0.3757, 0.5976]), tensor([-0.6595, -0.8823])], 'old_stps': [tensor([-0.5105, -0.4895]), tensor([ 7.7214e-05, -1.0416e-05]), tensor([ 0.0151, -0.0025]), tensor([ 0.0471, -0.0098]), tensor([-0.0004, -0.0024]), tensor([ 0.0466, -0.0661]), tensor([ 0.0083, -0.0036]), tensor([ 0.0898, -0.0520]), tensor([ 0.0353, -0.0101]), tensor([-5.9765e-06,  4.4781e-06]), tensor([-0.0063,  0.0040])], 'ro': [tensor(0.0006), tensor(13744430.), tensor(153.1979), tensor(8.5902), tensor(857.0363), tensor(2.8134), tensor(739.1246), tensor(7.7806), tensor(17.8872), tensor(2321882.2500), tensor(1507.3800)], 'H_diag': tensor(0.0005), 'prev_flat_grad': tensor([-0.4115, -0.7939]), 'prev_loss': -4.982922077178955}})\n",
      "tensor(1.1756, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.5087],\n",
      "        [0.9420]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5662],\n",
      "        [1.0380]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2257, 0.0679]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.874\n",
      "Loss gp: 0.770\n",
      "Loss gp: 0.662\n",
      "Loss gp: 0.442\n",
      "Loss gp: 0.067\n",
      "Loss gp: 19.664\n",
      "Loss gp: 0.007\n",
      "Loss gp: -0.013\n",
      "Loss gp: -0.016\n",
      "Loss gp: -0.020\n",
      "Loss gp: -0.025\n",
      "Loss gp: -0.034\n",
      "Loss gp: -0.044\n",
      "Loss gp: -0.049\n",
      "Loss gp: -0.053\n",
      "Loss gp: -0.056\n",
      "Loss gp: -0.056\n",
      "Loss gp: -0.057\n",
      "Loss gp: -0.059\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.063\n",
      "Loss gp: -0.063\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.047\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.066\n",
      "Loss gp: -0.066\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.070\n",
      "Loss gp: -0.054\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.070\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.070\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.035\n",
      "Loss gp: -0.057\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.075\n",
      "Loss gp: -0.071\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "1\n",
      "Loss theta: 1.996\n",
      "Loss theta: 1116.445\n",
      "Loss theta: 0.263\n",
      "Loss theta: 2.719\n",
      "Loss theta: 2.761\n",
      "Loss theta: 1.325\n",
      "Loss theta: 1.529\n",
      "Loss theta: 0.263\n",
      "Loss theta: 0.263\n",
      "Loss theta: 0.263\n",
      "Loss theta: 2.607\n",
      "Loss theta: 1.760\n",
      "Loss theta: 0.617\n",
      "Loss theta: 0.263\n",
      "Loss theta: 0.263\n",
      "Loss theta: 0.263\n",
      "Loss theta: 0.263\n",
      "Loss design: 0.263\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.2445, 0.0935]], requires_grad=True): {'func_evals': 16, 'n_iter': 2, 'al': [tensor(-0.5948), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0115, -0.0150]), 't': 0, 'old_dirs': [tensor([20.0049, 25.5768])], 'old_stps': [tensor([0.0188, 0.0256])], 'ro': [tensor(0.9693)], 'H_diag': tensor(0.0010), 'prev_flat_grad': tensor([12.2189, 14.9765]), 'prev_loss': 0.2631092369556427}})\n",
      "tensor(1.1854, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.5287],\n",
      "        [0.9616]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5711],\n",
      "        [1.0465]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2445, 0.0935]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.877\n",
      "Loss gp: 0.773\n",
      "Loss gp: 0.664\n",
      "Loss gp: 0.443\n",
      "Loss gp: 0.069\n",
      "Loss gp: 15.609\n",
      "Loss gp: 0.007\n",
      "Loss gp: -0.010\n",
      "Loss gp: -0.015\n",
      "Loss gp: -0.026\n",
      "Loss gp: -0.033\n",
      "Loss gp: -0.044\n",
      "Loss gp: -0.049\n",
      "Loss gp: -0.052\n",
      "Loss gp: -0.054\n",
      "Loss gp: -0.055\n",
      "Loss gp: -0.056\n",
      "Loss gp: -0.058\n",
      "Loss gp: -0.056\n",
      "Loss gp: -0.059\n",
      "Loss gp: -0.059\n",
      "Loss gp: -0.059\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.063\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.063\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.032\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.057\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.068\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "2\n",
      "Loss theta: -5.992\n",
      "Loss theta: -4.074\n",
      "Loss theta: -3.247\n",
      "Loss theta: -6.052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss theta: -6.182\n",
      "Loss theta: -6.017\n",
      "Loss theta: -6.232\n",
      "Loss theta: -6.156\n",
      "Loss theta: -6.205\n",
      "Loss theta: -6.191\n",
      "Loss theta: -6.232\n",
      "Loss theta: -6.232\n",
      "Loss design: -6.232\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.2319, 0.0822]], requires_grad=True): {'func_evals': 11, 'n_iter': 4, 'al': [tensor(0.0239), tensor(0.3055), tensor(-0.1744), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0008,  0.0007]), 't': 0, 'old_dirs': [tensor([-10.0383, -11.5296]), tensor([2.1207, 2.3764]), tensor([-1.5433, -1.7511])], 'old_stps': [tensor([-0.0135, -0.0137]), tensor([0.0009, 0.0023]), tensor([-1.1748e-06, -2.2116e-07])], 'ro': [tensor(3.4142), tensor(134.8046), tensor(454494.2188)], 'H_diag': tensor(4.0386e-07), 'prev_flat_grad': tensor([-0.0727, -1.3485]), 'prev_loss': -6.231509685516357}})\n",
      "tensor(1.1684, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4962],\n",
      "        [0.9515]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5501],\n",
      "        [1.0385]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2319, 0.0822]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.870\n",
      "Loss gp: 0.765\n",
      "Loss gp: 0.656\n",
      "Loss gp: 0.433\n",
      "Loss gp: 0.058\n",
      "Loss gp: 13.584\n",
      "Loss gp: -0.002\n",
      "Loss gp: -0.017\n",
      "Loss gp: -0.021\n",
      "Loss gp: -0.031\n",
      "Loss gp: -0.037\n",
      "Loss gp: -0.051\n",
      "Loss gp: -0.058\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.063\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.066\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.066\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.070\n",
      "Loss gp: -0.071\n",
      "Loss gp: -0.071\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "3\n",
      "Loss theta: -6.408\n",
      "Loss theta: -4.273\n",
      "Loss theta: -5.109\n",
      "Loss theta: -6.399\n",
      "Loss theta: -6.383\n",
      "Loss theta: -6.385\n",
      "Loss theta: -6.388\n",
      "Loss theta: -6.408\n",
      "Loss theta: -6.408\n",
      "Loss theta: -6.408\n",
      "Loss design: -6.408\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.2319, 0.0822]], requires_grad=True): {'func_evals': 9, 'n_iter': 1, 'd': tensor([-3.0198, -2.8627]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([3.0198, 2.8627]), 'prev_loss': -6.408017635345459}})\n",
      "tensor(1.1755, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4968],\n",
      "        [0.9605]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5527],\n",
      "        [1.0450]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2319, 0.0822]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.881\n",
      "Loss gp: 0.775\n",
      "Loss gp: 0.666\n",
      "Loss gp: 0.442\n",
      "Loss gp: 0.060\n",
      "Loss gp: 22.053\n",
      "Loss gp: -0.001\n",
      "Loss gp: -0.025\n",
      "Loss gp: -0.031\n",
      "Loss gp: -0.040\n",
      "Loss gp: -0.046\n",
      "Loss gp: -0.054\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.071\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.075\n",
      "Loss gp: -0.076\n",
      "Loss gp: -0.077\n",
      "Loss gp: -0.077\n",
      "Loss gp: -0.078\n",
      "Loss gp: -0.078\n",
      "Loss gp: -0.079\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "4\n",
      "Loss theta: -6.249\n",
      "Loss theta: -3.787\n",
      "Loss theta: -4.689\n",
      "Loss theta: -6.296\n",
      "Loss theta: -6.311\n",
      "Loss theta: -6.296\n",
      "Loss theta: -6.288\n",
      "Loss theta: -6.300\n",
      "Loss theta: -6.311\n",
      "Loss theta: -6.311\n",
      "Loss theta: -6.311\n",
      "Loss design: -6.311\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.2302, 0.0742]], requires_grad=True): {'func_evals': 10, 'n_iter': 3, 'al': [tensor(0.1119), tensor(0.6296), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 0.0034, -0.0009]), 't': 0, 'old_dirs': [tensor([-4.3001, -5.5047]), tensor([1.0182, 1.1789])], 'old_stps': [tensor([-0.0042, -0.0091]), tensor([0.0024, 0.0012])], 'ro': [tensor(14.7129), tensor(261.3543)], 'H_diag': tensor(0.0016), 'prev_flat_grad': tensor([-1.1591,  0.3296]), 'prev_loss': -6.3111701011657715}})\n",
      "tensor(1.1713, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4926],\n",
      "        [0.9517]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5577],\n",
      "        [1.0378]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2302, 0.0742]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.862\n",
      "Loss gp: 0.755\n",
      "Loss gp: 0.644\n",
      "Loss gp: 0.418\n",
      "Loss gp: 0.035\n",
      "Loss gp: 15.813\n",
      "Loss gp: -0.023\n",
      "Loss gp: -0.040\n",
      "Loss gp: -0.043\n",
      "Loss gp: -0.048\n",
      "Loss gp: -0.054\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.077\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.086\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "5\n",
      "Loss theta: -6.151\n",
      "Loss theta: -3.479\n",
      "Loss theta: -4.893\n",
      "Loss theta: -6.221\n",
      "Loss theta: -6.240\n",
      "Loss theta: -6.255\n",
      "Loss theta: -6.256\n",
      "Loss theta: -6.256\n",
      "Loss theta: -6.263\n",
      "Loss theta: -6.264\n",
      "Loss theta: -6.263\n",
      "Loss theta: -6.255\n",
      "Loss theta: -6.252\n",
      "Loss theta: -6.279\n",
      "Loss theta: -6.270\n",
      "Loss theta: -6.277\n",
      "Loss theta: -6.264\n",
      "Loss theta: -6.279\n",
      "Loss theta: -6.279\n",
      "Loss design: -6.279\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.2555, 0.0263]], requires_grad=True): {'func_evals': 18, 'n_iter': 9, 'al': [tensor(7.4829e-05), tensor(-0.0020), tensor(-0.0132), tensor(-0.0071), tensor(-0.0401), tensor(-0.1563), tensor(0.2998), tensor(1.5251), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0028,  0.0020]), 't': 0, 'old_dirs': [tensor([-10.2325, -11.2047]), tensor([2.7065, 2.7151]), tensor([0.2567, 0.1375]), tensor([0.5998, 0.3579]), tensor([1.1134, 0.5136]), tensor([-0.6563, -1.0628]), tensor([-0.1240, -0.0785]), tensor([-0.0947, -0.1292])], 'old_stps': [tensor([-0.0160, -0.0215]), tensor([0.0058, 0.0045]), tensor([ 0.0021, -0.0007]), tensor([ 0.0105, -0.0061]), tensor([ 0.0199, -0.0159]), tensor([ 0.0056, -0.0091]), tensor([-0.0027,  0.0010]), tensor([-1.8630e-07,  1.7772e-08])], 'ro': [tensor(2.4762), tensor(35.9772), tensor(2180.0630), tensor(244.9390), tensor(71.3236), tensor(165.9072), tensor(3926.0833), tensor(65126072.)], 'H_diag': tensor(5.9840e-07), 'prev_flat_grad': tensor([0.1341, 0.0880]), 'prev_loss': -6.279232025146484}})\n",
      "tensor(1.1819, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4887],\n",
      "        [0.9577]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5592],\n",
      "        [1.0490]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2555, 0.0263]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.747\n",
      "Loss gp: 0.636\n",
      "Loss gp: 0.410\n",
      "Loss gp: 0.033\n",
      "Loss gp: 9.273\n",
      "Loss gp: -0.023\n",
      "Loss gp: -0.032\n",
      "Loss gp: -0.034\n",
      "Loss gp: -0.040\n",
      "Loss gp: -0.044\n",
      "Loss gp: -0.052\n",
      "Loss gp: -0.056\n",
      "Loss gp: -0.057\n",
      "Loss gp: -0.064\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.071\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.075\n",
      "Loss gp: -0.075\n",
      "Loss gp: -0.076\n",
      "Loss gp: -0.078\n",
      "Loss gp: -0.080\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.080\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.082\n",
      "Loss gp: -0.001\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.076\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.080\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.084\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "6\n",
      "Loss theta: -6.612\n",
      "Loss theta: -3.357\n",
      "Loss theta: -5.824\n",
      "Loss theta: -6.661\n",
      "Loss theta: -6.693\n",
      "Loss theta: -6.752\n",
      "Loss theta: -6.704\n",
      "Loss theta: -6.859\n",
      "Loss theta: -6.863\n",
      "Loss theta: -6.868\n",
      "Loss theta: -6.868\n",
      "Loss theta: -6.865\n",
      "Loss theta: -6.867\n",
      "Loss theta: -6.868\n",
      "Loss theta: -6.868\n",
      "Loss theta: -6.868\n",
      "Loss design: -6.868\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1488, 0.1339]], requires_grad=True): {'func_evals': 15, 'n_iter': 7, 'al': [tensor(-0.0001), tensor(-2.5086e-05), tensor(0.0009), tensor(0.0004), tensor(-0.0235), tensor(-0.1569), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0002,  0.0002]), 't': 0, 'old_dirs': [tensor([-4.5485, -3.4970]), tensor([-0.1230,  0.4847]), tensor([2.4197, 4.2747]), tensor([-3.7246, -2.5641]), tensor([1.3869, 1.1043]), tensor([-0.0074, -0.0454])], 'old_stps': [tensor([-0.0221, -0.0012]), tensor([-0.0205,  0.0285]), tensor([-0.0472,  0.0680]), tensor([-0.0230,  0.0150]), tensor([ 0.0054, -0.0019]), tensor([ 0.0007, -0.0009])], 'ro': [tensor(9.5260), tensor(61.1832), tensor(5.6633), tensor(21.2242), tensor(184.1567), tensor(26808.5039)], 'H_diag': tensor(0.0177), 'prev_flat_grad': tensor([0.0304, 0.0158]), 'prev_loss': -6.8683085441589355}})\n",
      "tensor(1.1698, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4848],\n",
      "        [0.9607]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5412],\n",
      "        [1.0447]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1488, 0.1339]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.853\n",
      "Loss gp: 0.745\n",
      "Loss gp: 0.634\n",
      "Loss gp: 0.406\n",
      "Loss gp: 0.025\n",
      "Loss gp: 10.638\n",
      "Loss gp: -0.032\n",
      "Loss gp: -0.043\n",
      "Loss gp: -0.046\n",
      "Loss gp: -0.053\n",
      "Loss gp: -0.058\n",
      "Loss gp: -0.066\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.076\n",
      "Loss gp: -0.077\n",
      "Loss gp: -0.079\n",
      "Loss gp: -0.082\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.086\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.078\n",
      "Loss gp: -0.086\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.088\n",
      "Loss gp: -0.088\n",
      "Loss gp: -0.088\n",
      "Loss gp: -0.088\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.082\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.090\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.091\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "7\n",
      "Loss theta: -6.834\n",
      "Loss theta: -4.676\n",
      "Loss theta: -5.223\n",
      "Loss theta: -6.822\n",
      "Loss theta: -6.646\n",
      "Loss theta: -6.797\n",
      "Loss theta: -6.830\n",
      "Loss theta: -6.834\n",
      "Loss theta: -6.834\n",
      "Loss theta: -6.834\n",
      "Loss design: -6.834\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1488, 0.1339]], requires_grad=True): {'func_evals': 9, 'n_iter': 1, 'd': tensor([-3.7120, -3.6388]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([3.7120, 3.6388]), 'prev_loss': -6.834260940551758}})\n",
      "tensor(1.1683, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4957],\n",
      "        [0.9661]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5419],\n",
      "        [1.0426]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1488, 0.1339]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.844\n",
      "Loss gp: 0.736\n",
      "Loss gp: 0.623\n",
      "Loss gp: 0.393\n",
      "Loss gp: 0.009\n",
      "Loss gp: 10.775\n",
      "Loss gp: -0.047\n",
      "Loss gp: -0.059\n",
      "Loss gp: -0.061\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.069\n",
      "Loss gp: -0.077\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.086\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.097\n",
      "Loss gp: -0.097\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.101\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "8\n",
      "Loss theta: -6.539\n",
      "Loss theta: -3.705\n",
      "Loss theta: -5.876\n",
      "Loss theta: -6.574\n",
      "Loss theta: -6.555\n",
      "Loss theta: -6.524\n",
      "Loss theta: -6.572\n",
      "Loss theta: -6.577\n",
      "Loss theta: -6.581\n",
      "Loss theta: -6.598\n",
      "Loss theta: -6.605\n",
      "Loss theta: -6.597\n",
      "Loss theta: -6.609\n",
      "Loss theta: -6.613\n",
      "Loss theta: -6.621\n",
      "Loss theta: -6.613\n",
      "Loss theta: -6.618\n",
      "Loss theta: -6.621\n",
      "Loss theta: -6.621\n",
      "Loss theta: -6.596\n",
      "Loss theta: -6.591\n",
      "Loss theta: -6.599\n",
      "Loss theta: -6.621\n",
      "Loss theta: -6.621\n",
      "Loss design: -6.621\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1947, 0.0776]], requires_grad=True): {'func_evals': 23, 'n_iter': 7, 'al': [tensor(-6.8397e-05), tensor(4.4849), tensor(-0.0001), tensor(0.0011), tensor(2.4999), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0048,  0.0051]), 't': 0, 'old_dirs': [tensor([-2.5502, -3.1097]), tensor([0.6047, 0.5662]), tensor([-1.6158, -2.7463]), tensor([-0.1325, -0.9737]), tensor([0.3964, 0.4318])], 'old_stps': [tensor([ 0.0013, -0.0087]), tensor([ 2.6515e-07, -1.6707e-07]), tensor([ 0.0289, -0.0308]), tensor([ 0.0178, -0.0190]), tensor([-0.0019,  0.0020])], 'ro': [tensor(42.3397), tensor(15211869.), tensor(26.2974), tensor(62.0884), tensor(8085.0439)], 'H_diag': tensor(0.0004), 'prev_flat_grad': tensor([-3.7031, -3.6182]), 'prev_loss': -6.62074089050293}})\n",
      "tensor(1.1621, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4812],\n",
      "        [0.9521]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5435],\n",
      "        [1.0348]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1947, 0.0776]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.845\n",
      "Loss gp: 0.736\n",
      "Loss gp: 0.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.393\n",
      "Loss gp: 0.008\n",
      "Loss gp: 11.124\n",
      "Loss gp: -0.048\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.062\n",
      "Loss gp: -0.067\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.079\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.088\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.097\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.100\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "9\n",
      "Loss theta: -5.675\n",
      "Loss theta: 725.928\n",
      "Loss theta: -5.702\n",
      "Loss theta: -3.633\n",
      "Loss theta: -5.746\n",
      "Loss theta: -5.762\n",
      "Loss theta: -5.792\n",
      "Loss theta: -5.974\n",
      "Loss theta: -5.993\n",
      "Loss theta: -5.996\n",
      "Loss theta: -5.984\n",
      "Loss theta: -5.997\n",
      "Loss theta: -5.986\n",
      "Loss theta: -6.006\n",
      "Loss theta: -5.989\n",
      "Loss theta: -5.993\n",
      "Loss theta: -6.005\n",
      "Loss theta: -6.009\n",
      "Loss theta: -5.990\n",
      "Loss theta: -6.009\n",
      "Loss theta: -5.993\n",
      "Loss theta: -6.000\n",
      "Loss theta: -6.002\n",
      "Loss theta: -6.007\n",
      "Loss theta: -6.012\n",
      "Loss theta: -6.012\n",
      "Loss theta: -5.988\n",
      "Loss theta: -5.998\n",
      "Loss theta: -5.988\n",
      "Loss theta: -6.007\n",
      "Loss theta: -6.012\n",
      "Loss theta: -6.012\n",
      "Loss theta: -6.012\n",
      "Loss design: -6.012\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[ 0.3294, -0.0229]], requires_grad=True): {'func_evals': 32, 'n_iter': 9, 'al': [tensor(0.0674), tensor(-0.2518), tensor(-0.0543), tensor(-0.2615), tensor(-1.1274), tensor(5.1028), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0069,  0.0132]), 't': 0, 'old_dirs': [tensor([6.1319, 6.8261]), tensor([-1.0454, -1.6212]), tensor([-0.3381, -3.5157]), tensor([1.4074, 0.5282]), tensor([0.4978, 0.4177]), tensor([0.1485, 0.0823])], 'old_stps': [tensor([0.0122, 0.0057]), tensor([ 0.0076, -0.0092]), tensor([ 0.0769, -0.0702]), tensor([ 0.0320, -0.0241]), tensor([ 0.0059, -0.0026]), tensor([5.3796e-05, 2.8765e-05])], 'ro': [tensor(8.8083), tensor(143.9241), tensor(4.5264), tensor(30.9759), tensor(540.1185), tensor(96547.2812)], 'H_diag': tensor(0.0004), 'prev_flat_grad': tensor([-0.5626, -0.7853]), 'prev_loss': -6.012006759643555}})\n",
      "tensor(1.1933, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4923],\n",
      "        [0.9557]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5699],\n",
      "        [1.0562]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3294, -0.0229]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.838\n",
      "Loss gp: 0.729\n",
      "Loss gp: 0.616\n",
      "Loss gp: 0.386\n",
      "Loss gp: 0.005\n",
      "Loss gp: 7.329\n",
      "Loss gp: -0.047\n",
      "Loss gp: -0.055\n",
      "Loss gp: -0.056\n",
      "Loss gp: -0.060\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.090\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.095\n",
      "Loss gp: -0.095\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.096\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "10\n",
      "Loss theta: -6.509\n",
      "Loss theta: 1159.246\n",
      "Loss theta: -6.516\n",
      "Loss theta: -3.929\n",
      "Loss theta: -6.510\n",
      "Loss theta: -6.542\n",
      "Loss theta: -6.547\n",
      "Loss theta: -6.559\n",
      "Loss theta: -6.599\n",
      "Loss theta: -6.625\n",
      "Loss theta: -6.604\n",
      "Loss theta: -6.598\n",
      "Loss theta: -6.593\n",
      "Loss theta: -6.625\n",
      "Loss theta: -6.625\n",
      "Loss design: -6.625\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.2536, 0.0502]], requires_grad=True): {'func_evals': 14, 'n_iter': 6, 'al': [tensor(-0.0134), tensor(-0.1035), tensor(0.0204), tensor(-0.1376), tensor(0.1955), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0018, -0.0006]), 't': 0, 'old_dirs': [tensor([1.2532, 1.3552]), tensor([2.2852, 2.8142]), tensor([-0.7144, -0.3437]), tensor([0.6604, 2.0889]), tensor([-2.3997, -2.0489])], 'old_stps': [tensor([0.0004, 0.0054]), tensor([-0.0044,  0.0098]), tensor([-0.0086,  0.0076]), tensor([-0.0371,  0.0339]), tensor([-0.0261,  0.0164])], 'ro': [tensor(126.6772), tensor(57.4861), tensor(284.8884), tensor(21.5867), tensor(34.4966)], 'H_diag': tensor(0.0029), 'prev_flat_grad': tensor([0.8478, 1.0029]), 'prev_loss': -6.625104904174805}})\n",
      "tensor(1.1719, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4870],\n",
      "        [0.9617]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5492],\n",
      "        [1.0429]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.2536, 0.0502]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.843\n",
      "Loss gp: 0.734\n",
      "Loss gp: 0.620\n",
      "Loss gp: 0.389\n",
      "Loss gp: 0.002\n",
      "Loss gp: 10.456\n",
      "Loss gp: -0.054\n",
      "Loss gp: -0.065\n",
      "Loss gp: -0.068\n",
      "Loss gp: -0.074\n",
      "Loss gp: -0.079\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.097\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.102\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.105\n",
      "Loss gp: -0.105\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.110\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.110\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.112\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "11\n",
      "Loss theta: -6.503\n",
      "Loss theta: -3.180\n",
      "Loss theta: -5.159\n",
      "Loss theta: -6.628\n",
      "Loss theta: -6.647\n",
      "Loss theta: -6.678\n",
      "Loss theta: -6.712\n",
      "Loss theta: -6.781\n",
      "Loss theta: -6.783\n",
      "Loss theta: -6.784\n",
      "Loss theta: -6.783\n",
      "Loss theta: -6.781\n",
      "Loss theta: -6.785\n",
      "Loss theta: -6.785\n",
      "Loss theta: -6.785\n",
      "Loss theta: -6.780\n",
      "Loss theta: -6.780\n",
      "Loss theta: -6.785\n",
      "Loss theta: -6.785\n",
      "Loss design: -6.785\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1683, 0.1233]], requires_grad=True): {'func_evals': 18, 'n_iter': 9, 'al': [tensor(4.5112e-06), tensor(-5.7900e-05), tensor(-0.0001), tensor(-0.0003), tensor(0.0022), tensor(0.0005), tensor(0.1981), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0002,  0.0004]), 't': 0, 'old_dirs': [tensor([-12.3463,  -9.9289]), tensor([1.5519, 1.1059]), tensor([3.1785, 2.8937]), tensor([0.6637, 1.1080]), tensor([-3.0572, -0.0033]), tensor([-1.9039, -1.8237]), tensor([0.4079, 0.3967])], 'old_stps': [tensor([-0.0246, -0.0133]), tensor([0.0014, 0.0055]), tensor([-0.0038,  0.0176]), tensor([-0.0101,  0.0146]), tensor([-0.0484,  0.0519]), tensor([ 0.0004, -0.0045]), tensor([-0.0002,  0.0013])], 'ro': [tensor(2.2937), tensor(120.7835), tensor(25.7732), tensor(105.7441), tensor(6.7589), tensor(136.5156), tensor(2471.4419)], 'H_diag': tensor(0.0012), 'prev_flat_grad': tensor([-0.0725, -0.0770]), 'prev_loss': -6.785495758056641}})\n",
      "tensor(1.1642, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4903],\n",
      "        [0.9610]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5439],\n",
      "        [1.0369]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1683, 0.1233]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.838\n",
      "Loss gp: 0.728\n",
      "Loss gp: 0.615\n",
      "Loss gp: 0.382\n",
      "Loss gp: -0.005\n",
      "Loss gp: 9.376\n",
      "Loss gp: -0.059\n",
      "Loss gp: -0.070\n",
      "Loss gp: -0.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -0.079\n",
      "Loss gp: -0.086\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.097\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.102\n",
      "Loss gp: -0.103\n",
      "Loss gp: -0.105\n",
      "Loss gp: -0.105\n",
      "Loss gp: -0.105\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.109\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "12\n",
      "Loss theta: -6.862\n",
      "Loss theta: -4.073\n",
      "Loss theta: -5.296\n",
      "Loss theta: -6.856\n",
      "Loss theta: -6.887\n",
      "Loss theta: -6.885\n",
      "Loss theta: -6.892\n",
      "Loss theta: -6.907\n",
      "Loss theta: -6.916\n",
      "Loss theta: -6.885\n",
      "Loss theta: -6.916\n",
      "Loss theta: -6.904\n",
      "Loss theta: -6.912\n",
      "Loss theta: -6.912\n",
      "Loss theta: -6.897\n",
      "Loss theta: -6.915\n",
      "Loss theta: -6.865\n",
      "Loss theta: -6.915\n",
      "Loss theta: -6.916\n",
      "Loss theta: -6.916\n",
      "Loss theta: -6.916\n",
      "Loss design: -6.916\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1668, 0.1205]], requires_grad=True): {'func_evals': 20, 'n_iter': 3, 'al': [tensor(1.4821), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0015, -0.0034]), 't': 0, 'old_dirs': [tensor([-1.8324, -1.9456])], 'old_stps': [tensor([-0.0013, -0.0020])], 'ro': [tensor(157.9799)], 'H_diag': tensor(0.0009), 'prev_flat_grad': tensor([2.1692, 3.2431]), 'prev_loss': -6.916019916534424}})\n",
      "tensor(1.1653, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4900],\n",
      "        [0.9695]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5417],\n",
      "        [1.0393]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1668, 0.1205]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.835\n",
      "Loss gp: 0.725\n",
      "Loss gp: 0.610\n",
      "Loss gp: 0.377\n",
      "Loss gp: -0.013\n",
      "Loss gp: 9.505\n",
      "Loss gp: -0.066\n",
      "Loss gp: -0.076\n",
      "Loss gp: -0.078\n",
      "Loss gp: -0.080\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.103\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.110\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "13\n",
      "Loss theta: -6.863\n",
      "Loss theta: -3.873\n",
      "Loss theta: -5.375\n",
      "Loss theta: -6.870\n",
      "Loss theta: -6.885\n",
      "Loss theta: -6.892\n",
      "Loss theta: -6.885\n",
      "Loss theta: -6.886\n",
      "Loss theta: -6.892\n",
      "Loss theta: -6.892\n",
      "Loss theta: -3.486\n",
      "Loss theta: -6.893\n",
      "Loss theta: -6.898\n",
      "Loss theta: -6.895\n",
      "Loss theta: -6.900\n",
      "Loss theta: -6.908\n",
      "Loss theta: -6.908\n",
      "Loss theta: -6.893\n",
      "Loss theta: -6.910\n",
      "Loss theta: -6.905\n",
      "Loss theta: -6.910\n",
      "Loss theta: -6.910\n",
      "Loss design: -6.910\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1942, 0.0908]], requires_grad=True): {'func_evals': 21, 'n_iter': 8, 'al': [tensor(1.6568e-05), tensor(-0.0013), tensor(0.0633), tensor(5.7174e-05), tensor(-0.0015), tensor(1.0272), tensor(-0.2084), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 1.1555e-05, -1.3021e-05]), 't': 0, 'old_dirs': [tensor([-4.0399, -4.2362]), tensor([1.7932, 1.7357]), tensor([-0.5696, -0.6352]), tensor([2.4473, 0.3902]), tensor([-0.5829,  0.0176]), tensor([-0.1525, -0.1619]), tensor([-0.5195, -0.4585])], 'old_stps': [tensor([-0.0024, -0.0049]), tensor([0.0016, 0.0007]), tensor([ 0.0009, -0.0008]), tensor([ 0.0411, -0.0370]), tensor([-0.0136,  0.0122]), tensor([-3.0052e-05,  2.6303e-05]), tensor([-1.6643e-06,  1.5450e-06])], 'ro': [tensor(32.8848), tensor(246.0039), tensor(35815.6445), tensor(11.6088), tensor(122.4332), tensor(3086552.), tensor(6400296.)], 'H_diag': tensor(3.2547e-07), 'prev_flat_grad': tensor([0.0856, 0.1133]), 'prev_loss': -6.910424709320068}})\n",
      "tensor(1.1636, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4835],\n",
      "        [0.9661]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5403],\n",
      "        [1.0382]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1942, 0.0908]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.832\n",
      "Loss gp: 0.721\n",
      "Loss gp: 0.606\n",
      "Loss gp: 0.370\n",
      "Loss gp: -0.025\n",
      "Loss gp: 12.473\n",
      "Loss gp: -0.080\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.110\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "14\n",
      "Loss theta: -7.055\n",
      "Loss theta: -3.824\n",
      "Loss theta: -6.070\n",
      "Loss theta: -7.042\n",
      "Loss theta: -7.045\n",
      "Loss theta: -7.044\n",
      "Loss theta: -7.043\n",
      "Loss theta: -7.055\n",
      "Loss theta: -7.055\n",
      "Loss theta: -7.055\n",
      "Loss design: -7.055\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1942, 0.0908]], requires_grad=True): {'func_evals': 9, 'n_iter': 1, 'd': tensor([-2.4806, -1.7176]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([2.4806, 1.7176]), 'prev_loss': -7.054917335510254}})\n",
      "tensor(1.1641, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4836],\n",
      "        [0.9723]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5357],\n",
      "        [1.0411]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1942, 0.0908]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.832\n",
      "Loss gp: 0.721\n",
      "Loss gp: 0.606\n",
      "Loss gp: 0.371\n",
      "Loss gp: -0.018\n",
      "Loss gp: 8.534\n",
      "Loss gp: -0.072\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.083\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.090\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.102\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -0.117\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "15\n",
      "Loss theta: -8.233\n",
      "Loss theta: -6.081\n",
      "Loss theta: -7.344\n",
      "Loss theta: -8.221\n",
      "Loss theta: -8.188\n",
      "Loss theta: -8.244\n",
      "Loss theta: -8.156\n",
      "Loss theta: -8.153\n",
      "Loss theta: -8.244\n",
      "Loss theta: -8.244\n",
      "Loss theta: -8.244\n",
      "Loss theta: -4.025\n",
      "Loss theta: -5.526\n",
      "Loss theta: -8.224\n",
      "Loss theta: -8.189\n",
      "Loss theta: -8.187\n",
      "Loss theta: -8.178\n",
      "Loss theta: -8.244\n",
      "Loss theta: -8.244\n",
      "Loss design: -8.244\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1942, 0.0908]], requires_grad=True): {'func_evals': 18, 'n_iter': 2, 'al': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-1.8601, -0.7890]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([1.8601, 0.7890]), 'prev_loss': -8.243976593017578}})\n",
      "tensor(1.1566, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4876],\n",
      "        [0.9723]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5242],\n",
      "        [1.0385]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1942, 0.0908]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.835\n",
      "Loss gp: 0.724\n",
      "Loss gp: 0.609\n",
      "Loss gp: 0.374\n",
      "Loss gp: -0.019\n",
      "Loss gp: 10.095\n",
      "Loss gp: -0.073\n",
      "Loss gp: -0.084\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.089\n",
      "Loss gp: -0.093\n",
      "Loss gp: -0.100\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.081\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "16\n",
      "Loss theta: -7.200\n",
      "Loss theta: -4.710\n",
      "Loss theta: -7.215\n",
      "Loss theta: -7.184\n",
      "Loss theta: -7.215\n",
      "Loss theta: -7.220\n",
      "Loss theta: -7.222\n",
      "Loss theta: -7.217\n",
      "Loss theta: -7.219\n",
      "Loss theta: -7.214\n",
      "Loss theta: -7.222\n",
      "Loss theta: -7.222\n",
      "Loss design: -7.222\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1651, 0.1203]], requires_grad=True): {'func_evals': 11, 'n_iter': 5, 'al': [tensor(-0.0216), tensor(0.0442), tensor(0.0573), tensor(0.6870), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([7.5457e-05, 5.6591e-04]), 't': 0, 'old_dirs': [tensor([-2.7889, -1.3704]), tensor([2.3561, 2.6199]), tensor([-0.4261, -0.7605]), tensor([-0.0043,  0.0420])], 'old_stps': [tensor([-0.0313,  0.0277]), tensor([-0.0028,  0.0075]), tensor([ 0.0056, -0.0064]), tensor([-0.0005,  0.0008])], 'ro': [tensor(20.2134), tensor(75.7655), tensor(399.9648), tensor(29313.8281)], 'H_diag': tensor(0.0191), 'prev_flat_grad': tensor([-0.1270, -0.1195]), 'prev_loss': -7.221731662750244}})\n",
      "tensor(1.1584, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4848],\n",
      "        [0.9681]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5325],\n",
      "        [1.0364]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1651, 0.1203]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.829\n",
      "Loss gp: 0.718\n",
      "Loss gp: 0.603\n",
      "Loss gp: 0.367\n",
      "Loss gp: -0.025\n",
      "Loss gp: 9.092\n",
      "Loss gp: -0.077\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.088\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.094\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.105\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "17\n",
      "Loss theta: -7.326\n",
      "Loss theta: 1861.067\n",
      "Loss theta: -7.334\n",
      "Loss theta: -7.360\n",
      "Loss theta: -7.362\n",
      "Loss theta: -7.374\n",
      "Loss theta: -7.377\n",
      "Loss theta: -7.378\n",
      "Loss theta: -7.377\n",
      "Loss theta: -7.379\n",
      "Loss theta: -7.378\n",
      "Loss theta: -7.380\n",
      "Loss theta: -7.378\n",
      "Loss theta: -7.380\n",
      "Loss theta: -7.380\n",
      "Loss theta: -7.381\n",
      "Loss theta: -7.381\n",
      "Loss theta: -7.379\n",
      "Loss theta: -7.378\n",
      "Loss theta: -7.381\n",
      "Loss theta: -7.381\n",
      "Loss design: -7.381\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1433, 0.1533]], requires_grad=True): {'func_evals': 20, 'n_iter': 9, 'al': [tensor(-0.0005), tensor(0.0018), tensor(-0.0468), tensor(-0.0033), tensor(-0.0264), tensor(-0.1226), tensor(-0.4125), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 4.7413e-05, -3.9818e-04]), 't': 0, 'old_dirs': [tensor([1.0217, 0.9156]), tensor([5.1658, 5.0806]), tensor([-0.3242, -0.2879]), tensor([-1.0655, -0.3782]), tensor([-0.2416,  0.3976]), tensor([0.2531, 0.3326]), tensor([0.5680, 0.5794])], 'old_stps': [tensor([0.0009, 0.0011]), tensor([0.0034, 0.0063]), tensor([-0.0008,  0.0005]), tensor([-0.0136,  0.0121]), tensor([-0.0118,  0.0116]), tensor([-0.0007,  0.0014]), tensor([8.5394e-04, 1.5419e-05])], 'ro': [tensor(543.1127), tensor(20.3050), tensor(8105.6104), tensor(100.9512), tensor(133.9819), tensor(3310.2793), tensor(2024.2637)], 'H_diag': tensor(0.0008), 'prev_flat_grad': tensor([0.2338, 0.2647]), 'prev_loss': -7.381358623504639}})\n",
      "tensor(1.1571, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4796],\n",
      "        [0.9716]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5255],\n",
      "        [1.0384]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1433, 0.1533]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.822\n",
      "Loss gp: 0.711\n",
      "Loss gp: 0.595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.358\n",
      "Loss gp: -0.035\n",
      "Loss gp: 8.544\n",
      "Loss gp: -0.087\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.097\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.102\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "18\n",
      "Loss theta: -6.670\n",
      "Loss theta: 168.091\n",
      "Loss theta: -6.681\n",
      "Loss theta: -1.132\n",
      "Loss theta: -6.682\n",
      "Loss theta: -6.682\n",
      "Loss theta: -6.699\n",
      "Loss theta: -6.710\n",
      "Loss theta: -6.708\n",
      "Loss theta: -6.705\n",
      "Loss theta: -6.705\n",
      "Loss theta: -6.709\n",
      "Loss theta: -6.710\n",
      "Loss theta: -6.710\n",
      "Loss design: -6.710\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1739, 0.1224]], requires_grad=True): {'func_evals': 13, 'n_iter': 4, 'al': [tensor(0.1521), tensor(1.1250), tensor(-0.2339), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0054,  0.0066]), 't': 0, 'old_dirs': [tensor([1.6452, 1.5905]), tensor([0.6915, 0.5592]), tensor([-1.8564, -3.7348])], 'old_stps': [tensor([0.0024, 0.0003]), tensor([ 0.0013, -0.0012]), tensor([ 0.0267, -0.0300])], 'ro': [tensor(226.7318), tensor(3561.7034), tensor(16.0595)], 'H_diag': tensor(0.0036), 'prev_flat_grad': tensor([-1.4579, -1.7871]), 'prev_loss': -6.709540843963623}})\n",
      "tensor(1.1633, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4875],\n",
      "        [0.9643]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5486],\n",
      "        [1.0335]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1739, 0.1224]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.825\n",
      "Loss gp: 0.714\n",
      "Loss gp: 0.598\n",
      "Loss gp: 0.361\n",
      "Loss gp: -0.033\n",
      "Loss gp: 9.052\n",
      "Loss gp: -0.085\n",
      "Loss gp: -0.095\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.101\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.052\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "19\n",
      "Loss theta: -7.127\n",
      "Loss theta: 1243.023\n",
      "Loss theta: -7.127\n",
      "Loss theta: -1.895\n",
      "Loss theta: -7.137\n",
      "Loss theta: -7.144\n",
      "Loss theta: -7.152\n",
      "Loss theta: -7.225\n",
      "Loss theta: -7.237\n",
      "Loss theta: -7.239\n",
      "Loss theta: -7.237\n",
      "Loss theta: -7.240\n",
      "Loss theta: -7.240\n",
      "Loss theta: -7.238\n",
      "Loss theta: -7.237\n",
      "Loss theta: -7.240\n",
      "Loss theta: -7.240\n",
      "Loss design: -7.240\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1187, 0.1864]], requires_grad=True): {'func_evals': 16, 'n_iter': 7, 'al': [tensor(0.0052), tensor(-0.0161), tensor(-0.0046), tensor(0.0023), tensor(0.2634), tensor(1.3958), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 0.0005, -0.0005]), 't': 0, 'old_dirs': [tensor([4.4956, 4.0023]), tensor([-1.3759, -1.0118]), tensor([-3.4953, -1.3690]), tensor([1.6933, 2.8581]), tensor([0.1140, 0.0377]), tensor([-0.0956, -0.0901])], 'old_stps': [tensor([0.0016, 0.0055]), tensor([-0.0047,  0.0033]), tensor([-0.0344,  0.0338]), tensor([-0.0191,  0.0229]), tensor([ 0.0014, -0.0016]), tensor([ 2.7247e-06, -3.8866e-06])], 'ro': [tensor(34.0159), tensor(327.3359), tensor(13.5169), tensor(30.1396), tensor(9757.9775), tensor(11113134.)], 'H_diag': tensor(5.2134e-06), 'prev_flat_grad': tensor([0.0450, 0.0638]), 'prev_loss': -7.240472793579102}})\n",
      "tensor(1.1559, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4728],\n",
      "        [0.9742]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5206],\n",
      "        [1.0395]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1187, 0.1864]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.831\n",
      "Loss gp: 0.720\n",
      "Loss gp: 0.604\n",
      "Loss gp: 0.368\n",
      "Loss gp: -0.027\n",
      "Loss gp: 9.694\n",
      "Loss gp: -0.080\n",
      "Loss gp: -0.091\n",
      "Loss gp: -0.092\n",
      "Loss gp: -0.095\n",
      "Loss gp: -0.099\n",
      "Loss gp: -0.105\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.110\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "20\n",
      "Loss theta: -7.135\n",
      "Loss theta: -3.107\n",
      "Loss theta: -4.787\n",
      "Loss theta: -7.143\n",
      "Loss theta: -7.142\n",
      "Loss theta: -7.143\n",
      "Loss theta: -7.147\n",
      "Loss theta: -7.149\n",
      "Loss theta: -7.148\n",
      "Loss theta: -7.145\n",
      "Loss theta: -7.149\n",
      "Loss theta: -7.149\n",
      "Loss theta: -7.152\n",
      "Loss theta: -7.151\n",
      "Loss theta: -7.153\n",
      "Loss theta: -7.151\n",
      "Loss theta: -7.153\n",
      "Loss theta: -7.153\n",
      "Loss design: -7.153\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1160, 0.1838]], requires_grad=True): {'func_evals': 17, 'n_iter': 5, 'al': [tensor(0.0005), tensor(-0.0148), tensor(1.6931), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-3.5773e-06,  5.4958e-06]), 't': 0, 'old_dirs': [tensor([-5.3206, -5.5704]), tensor([0.9315, 0.9969]), tensor([0.0618, 0.0764])], 'old_stps': [tensor([-0.0034, -0.0034]), tensor([0.0007, 0.0008]), tensor([4.8899e-07, 1.1428e-06])], 'ro': [tensor(26.9038), tensor(729.4396), tensor(8508687.)], 'H_diag': tensor(1.2174e-05), 'prev_flat_grad': tensor([-0.0446, -0.1550]), 'prev_loss': -7.15258264541626}})\n",
      "tensor(1.1563, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4702],\n",
      "        [0.9748]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5195],\n",
      "        [1.0405]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1160, 0.1838]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.820\n",
      "Loss gp: 0.708\n",
      "Loss gp: 0.591\n",
      "Loss gp: 0.352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -0.044\n",
      "Loss gp: 9.337\n",
      "Loss gp: -0.096\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "21\n",
      "Loss theta: -7.082\n",
      "Loss theta: -3.489\n",
      "Loss theta: -4.024\n",
      "Loss theta: -7.135\n",
      "Loss theta: -7.149\n",
      "Loss theta: -7.152\n",
      "Loss theta: -7.151\n",
      "Loss theta: -7.155\n",
      "Loss theta: -7.154\n",
      "Loss theta: -7.150\n",
      "Loss theta: -7.149\n",
      "Loss theta: -7.155\n",
      "Loss theta: -7.155\n",
      "Loss design: -7.155\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1100, 0.1758]], requires_grad=True): {'func_evals': 12, 'n_iter': 5, 'al': [tensor(0.0020), tensor(0.0808), tensor(1.1200), tensor(3.3943), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 0.0011, -0.0010]), 't': 0, 'old_dirs': [tensor([-12.4179, -13.8868]), tensor([3.8041, 4.2342]), tensor([-0.9533, -1.0255]), tensor([0.3202, 0.3549])], 'old_stps': [tensor([-0.0086, -0.0103]), tensor([0.0027, 0.0027]), tensor([ 1.2959e-05, -3.7702e-04]), tensor([ 5.0621e-06, -1.9550e-06])], 'ro': [tensor(4.0034), tensor(46.4950), tensor(2671.9062), tensor(1078737.3750)], 'H_diag': tensor(4.0576e-06), 'prev_flat_grad': tensor([-0.6516, -0.0778]), 'prev_loss': -7.155346870422363}})\n",
      "tensor(1.1572, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4716],\n",
      "        [0.9735]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5218],\n",
      "        [1.0404]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1100, 0.1758]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.816\n",
      "Loss gp: 0.704\n",
      "Loss gp: 0.587\n",
      "Loss gp: 0.347\n",
      "Loss gp: -0.051\n",
      "Loss gp: 9.636\n",
      "Loss gp: -0.103\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "22\n",
      "Loss theta: -6.735\n",
      "Loss theta: 2010.831\n",
      "Loss theta: -6.816\n",
      "Loss theta: -6.945\n",
      "Loss theta: -6.948\n",
      "Loss theta: -6.950\n",
      "Loss theta: -6.948\n",
      "Loss theta: -6.948\n",
      "Loss theta: -6.948\n",
      "Loss theta: -6.964\n",
      "Loss theta: -6.966\n",
      "Loss theta: -6.968\n",
      "Loss theta: -6.971\n",
      "Loss theta: -6.968\n",
      "Loss theta: -6.969\n",
      "Loss theta: -6.971\n",
      "Loss theta: -6.968\n",
      "Loss theta: -6.971\n",
      "Loss theta: -6.968\n",
      "Loss theta: -6.963\n",
      "Loss theta: -6.969\n",
      "Loss theta: -6.971\n",
      "Loss theta: -6.971\n",
      "Loss theta: -6.971\n",
      "Loss design: -6.971\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1486, 0.1609]], requires_grad=True): {'func_evals': 23, 'n_iter': 9, 'al': [tensor(-8.2949e-05), tensor(0.0001), tensor(-0.0145), tensor(-0.1085), tensor(-0.0204), tensor(-0.0163), tensor(-0.2605), tensor(-0.6320), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0021,  0.0011]), 't': 0, 'old_dirs': [tensor([3.1799, 3.1138]), tensor([16.3167, 16.3024]), tensor([-2.0779, -2.1469]), tensor([-0.1331, -0.1779]), tensor([-1.3587, -2.1790]), tensor([1.6375, 1.1084]), tensor([0.1077, 0.0181]), tensor([1.4284, 1.2976])], 'old_stps': [tensor([0.0023, 0.0021]), tensor([0.0110, 0.0091]), tensor([-0.0008, -0.0016]), tensor([ 0.0005, -0.0007]), tensor([ 0.0121, -0.0137]), tensor([ 0.0090, -0.0079]), tensor([ 0.0018, -0.0013]), tensor([ 0.0027, -0.0010])], 'ro': [tensor(71.7002), tensor(3.0499), tensor(192.8784), tensor(17514.4570), tensor(75.2282), tensor(167.7083), tensor(5978.3071), tensor(396.9649)], 'H_diag': tensor(0.0007), 'prev_flat_grad': tensor([0.8774, 0.7519]), 'prev_loss': -6.9712300300598145}})\n",
      "tensor(1.1606, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4806],\n",
      "        [0.9709]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5378],\n",
      "        [1.0361]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1486, 0.1609]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.818\n",
      "Loss gp: 0.705\n",
      "Loss gp: 0.588\n",
      "Loss gp: 0.348\n",
      "Loss gp: -0.052\n",
      "Loss gp: 10.896\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "23\n",
      "Loss theta: -6.817\n",
      "Loss theta: -0.348\n",
      "Loss theta: -4.688\n",
      "Loss theta: -6.825\n",
      "Loss theta: -6.829\n",
      "Loss theta: -6.831\n",
      "Loss theta: -6.836\n",
      "Loss theta: -6.833\n",
      "Loss theta: -6.830\n",
      "Loss theta: -6.830\n",
      "Loss theta: -6.836\n",
      "Loss theta: -6.836\n",
      "Loss theta: -6.836\n",
      "Loss design: -6.836\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1433, 0.1625]], requires_grad=True): {'func_evals': 12, 'n_iter': 5, 'al': [tensor(0.0071), tensor(-0.2156), tensor(-3.0009), tensor(-0.0979), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0016,  0.0011]), 't': 0, 'old_dirs': [tensor([-8.6256, -9.3598]), tensor([3.5084, 3.7995]), tensor([0.1531, 0.2036]), tensor([1.4195, 1.8192])], 'old_stps': [tensor([-0.0050, -0.0044]), tensor([0.0019, 0.0022]), tensor([-0.0002,  0.0005]), tensor([-0.0020,  0.0033])], 'ro': [tensor(11.8324), tensor(67.1337), tensor(13178.3994), tensor(306.4186)], 'H_diag': tensor(0.0006), 'prev_flat_grad': tensor([1.8434, 1.1868]), 'prev_loss': -6.835644245147705}})\n",
      "tensor(1.1634, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4841],\n",
      "        [0.9691]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5460],\n",
      "        [1.0349]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1433, 0.1625]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.820\n",
      "Loss gp: 0.708\n",
      "Loss gp: 0.591\n",
      "Loss gp: 0.352\n",
      "Loss gp: -0.047\n",
      "Loss gp: 9.423\n",
      "Loss gp: -0.098\n",
      "Loss gp: -0.108\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "24\n",
      "Loss theta: -8.437\n",
      "Loss theta: -4.732\n",
      "Loss theta: -7.494\n",
      "Loss theta: -8.429\n",
      "Loss theta: -8.446\n",
      "Loss theta: -8.439\n",
      "Loss theta: -8.442\n",
      "Loss theta: -8.459\n",
      "Loss theta: -8.459\n",
      "Loss theta: -8.437\n",
      "Loss theta: -8.430\n",
      "Loss theta: -8.449\n",
      "Loss theta: -8.459\n",
      "Loss theta: -8.459\n",
      "Loss design: -8.459\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1426, 0.1619]], requires_grad=True): {'func_evals': 13, 'n_iter': 3, 'al': [tensor(0.1855), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-1.9084e-04, -5.8031e-05]), 't': 0, 'old_dirs': [tensor([-1.3771, -1.4469])], 'old_stps': [tensor([-0.0007, -0.0006])], 'ro': [tensor(534.8552)], 'H_diag': tensor(0.0005), 'prev_flat_grad': tensor([0.3756, 0.1345]), 'prev_loss': -8.458513259887695}})\n",
      "tensor(1.1490, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4827],\n",
      "        [0.9740]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5164],\n",
      "        [1.0340]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1426, 0.1619]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.817\n",
      "Loss gp: 0.705\n",
      "Loss gp: 0.587\n",
      "Loss gp: 0.347\n",
      "Loss gp: -0.055\n",
      "Loss gp: 10.673\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "25\n",
      "Loss theta: -6.877\n",
      "Loss theta: 2376.319\n",
      "Loss theta: -6.899\n",
      "Loss theta: -6.911\n",
      "Loss theta: -6.919\n",
      "Loss theta: -6.913\n",
      "Loss theta: -6.907\n",
      "Loss theta: -6.913\n",
      "Loss theta: -6.919\n",
      "Loss theta: -6.919\n",
      "Loss design: -6.919\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1461, 0.1663]], requires_grad=True): {'func_evals': 9, 'n_iter': 4, 'al': [tensor(-0.0060), tensor(-0.0088), tensor(-0.4266), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0004, -0.0005]), 't': 0, 'old_dirs': [tensor([1.8608, 1.8886]), tensor([3.8725, 3.9376]), tensor([3.2212, 3.3320])], 'old_stps': [tensor([0.0007, 0.0008]), tensor([0.0020, 0.0024]), tensor([0.0008, 0.0013])], 'ro': [tensor(366.0400), tensor(57.3970), tensor(144.4275)], 'H_diag': tensor(0.0003), 'prev_flat_grad': tensor([1.6728, 1.2344]), 'prev_loss': -6.919051647186279}})\n",
      "tensor(1.1591, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4862],\n",
      "        [0.9703]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5435],\n",
      "        [1.0315]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1461, 0.1663]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.813\n",
      "Loss gp: 0.700\n",
      "Loss gp: 0.583\n",
      "Loss gp: 0.343\n",
      "Loss gp: -0.053\n",
      "Loss gp: 7.625\n",
      "Loss gp: -0.103\n",
      "Loss gp: -0.111\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.131\n",
      "Loss gp: 0.224\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "26\n",
      "Loss theta: -7.718\n",
      "Loss theta: -4.087\n",
      "Loss theta: -3.649\n",
      "Loss theta: -7.721\n",
      "Loss theta: -7.800\n",
      "Loss theta: -7.757\n",
      "Loss theta: -7.792\n",
      "Loss theta: -7.800\n",
      "Loss theta: -7.800\n",
      "Loss theta: -7.800\n",
      "Loss theta: -7.800\n",
      "Loss design: -7.800\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1424, 0.1626]], requires_grad=True): {'func_evals': 10, 'n_iter': 3, 'al': [tensor(0.0005), tensor(0.0996), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([4.7303e-05, 1.9828e-04]), 't': 0, 'old_dirs': [tensor([ -9.9513, -10.6128]), tensor([2.2775, 2.3996])], 'old_stps': [tensor([-0.0049, -0.0051]), tensor([0.0012, 0.0013])], 'ro': [tensor(9.6855), tensor(170.7162)], 'H_diag': tensor(0.0005), 'prev_flat_grad': tensor([-0.0927, -0.3577]), 'prev_loss': -7.799617290496826}})\n",
      "tensor(1.1477, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4799],\n",
      "        [0.9755]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5123],\n",
      "        [1.0345]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1424, 0.1626]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.813\n",
      "Loss gp: 0.700\n",
      "Loss gp: 0.583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.342\n",
      "Loss gp: -0.056\n",
      "Loss gp: 8.088\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "27\n",
      "Loss theta: -7.614\n",
      "Loss theta: 2496.947\n",
      "Loss theta: -7.622\n",
      "Loss theta: -7.605\n",
      "Loss theta: -7.629\n",
      "Loss theta: -7.630\n",
      "Loss theta: -7.622\n",
      "Loss theta: -7.630\n",
      "Loss theta: -7.630\n",
      "Loss theta: -7.623\n",
      "Loss theta: -7.626\n",
      "Loss theta: -7.628\n",
      "Loss theta: -7.630\n",
      "Loss theta: -7.630\n",
      "Loss design: -7.630\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1425, 0.1627]], requires_grad=True): {'func_evals': 13, 'n_iter': 3, 'al': [tensor(-0.5920), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-6.6244e-05, -6.8387e-05]), 't': 0, 'old_dirs': [tensor([1.6212, 1.7666])], 'old_stps': [tensor([1.2997e-04, 9.8951e-05])], 'ro': [tensor(2593.9167)], 'H_diag': tensor(6.7057e-05), 'prev_flat_grad': tensor([0.8306, 1.2152]), 'prev_loss': -7.6296210289001465}})\n",
      "tensor(1.1515, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4838],\n",
      "        [0.9741]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5245],\n",
      "        [1.0326]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1425, 0.1627]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.813\n",
      "Loss gp: 0.701\n",
      "Loss gp: 0.583\n",
      "Loss gp: 0.343\n",
      "Loss gp: -0.054\n",
      "Loss gp: 7.803\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "28\n",
      "Loss theta: -6.927\n",
      "Loss theta: -0.828\n",
      "Loss theta: -5.071\n",
      "Loss theta: -6.921\n",
      "Loss theta: -6.934\n",
      "Loss theta: -6.936\n",
      "Loss theta: -6.934\n",
      "Loss theta: -6.936\n",
      "Loss theta: -6.962\n",
      "Loss theta: -6.944\n",
      "Loss theta: -6.949\n",
      "Loss theta: -6.954\n",
      "Loss theta: -6.962\n",
      "Loss theta: -6.962\n",
      "Loss theta: -6.962\n",
      "Loss theta: -6.962\n",
      "Loss theta: -6.962\n",
      "Loss theta: -6.962\n",
      "Loss theta: -6.945\n",
      "Loss theta: -6.938\n",
      "Loss theta: -6.963\n",
      "Loss theta: -6.950\n",
      "Loss theta: -6.963\n",
      "Loss theta: -6.963\n",
      "Loss theta: -6.963\n",
      "Loss theta: -6.963\n",
      "Loss design: -6.963\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1420, 0.1612]], requires_grad=True): {'func_evals': 25, 'n_iter': 5, 'al': [tensor(0.0390), tensor(0.0426), tensor(3.0652), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 2.8728e-05, -2.5217e-05]), 't': 0, 'old_dirs': [tensor([-3.2167, -3.9106]), tensor([2.3046, 2.7630]), tensor([-0.3783, -0.4451])], 'old_stps': [tensor([-0.0011, -0.0017]), tensor([0.0004, 0.0003]), tensor([-1.2564e-07, -1.5435e-07])], 'ro': [tensor(98.0207), tensor(590.2927), tensor(8604282.)], 'H_diag': tensor(3.4066e-07), 'prev_flat_grad': tensor([0.7812, 1.6722]), 'prev_loss': -6.962517738342285}})\n",
      "tensor(1.1592, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4838],\n",
      "        [0.9708]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5416],\n",
      "        [1.0325]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1420, 0.1612]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.812\n",
      "Loss gp: 0.699\n",
      "Loss gp: 0.581\n",
      "Loss gp: 0.340\n",
      "Loss gp: -0.058\n",
      "Loss gp: 8.159\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "29\n",
      "Loss theta: -8.063\n",
      "Loss theta: -6.510\n",
      "Loss theta: -5.455\n",
      "Loss theta: -8.352\n",
      "Loss theta: -8.641\n",
      "Loss theta: -8.600\n",
      "Loss theta: -8.539\n",
      "Loss theta: -8.421\n",
      "Loss theta: -8.556\n",
      "Loss theta: -8.641\n",
      "Loss theta: -8.641\n",
      "Loss design: -8.641\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1216, 0.1330]], requires_grad=True): {'func_evals': 10, 'n_iter': 3, 'al': [tensor(0.0001), tensor(0.6791), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([0.0061, 0.0069]), 't': 0, 'old_dirs': [tensor([-34.4976, -46.7102]), tensor([5.5624, 7.9061])], 'old_stps': [tensor([-0.0284, -0.0389]), tensor([0.0081, 0.0108])], 'ro': [tensor(0.3575), tensor(7.6909)], 'H_diag': tensor(0.0014), 'prev_flat_grad': tensor([-4.2320, -5.0282]), 'prev_loss': -8.640727043151855}})\n",
      "tensor(1.1420, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4815],\n",
      "        [0.9674]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5146],\n",
      "        [1.0270]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1216, 0.1330]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.813\n",
      "Loss gp: 0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.583\n",
      "Loss gp: 0.342\n",
      "Loss gp: -0.055\n",
      "Loss gp: 7.399\n",
      "Loss gp: -0.104\n",
      "Loss gp: -0.112\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "30\n",
      "Loss theta: -7.276\n",
      "Loss theta: 2817.173\n",
      "Loss theta: -7.361\n",
      "Loss theta: -7.513\n",
      "Loss theta: -7.497\n",
      "Loss theta: -7.496\n",
      "Loss theta: -7.473\n",
      "Loss theta: -7.513\n",
      "Loss theta: -7.498\n",
      "Loss theta: -7.513\n",
      "Loss theta: -7.492\n",
      "Loss theta: -7.513\n",
      "Loss theta: -7.513\n",
      "Loss design: -7.513\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1377, 0.1503]], requires_grad=True): {'func_evals': 12, 'n_iter': 4, 'al': [tensor(-0.0032), tensor(0.0028), tensor(41.2541), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 9.4765e-06, -6.8174e-06]), 't': 0, 'old_dirs': [tensor([2.3275, 2.8396]), tensor([13.9653, 17.0414]), tensor([-0.0466, -0.0726])], 'old_stps': [tensor([0.0026, 0.0029]), tensor([0.0136, 0.0143]), tensor([-7.8857e-09, -1.2744e-08])], 'ro': [tensor(69.5949), tensor(2.3050), tensor(7.7318e+08)], 'H_diag': tensor(1.7359e-07), 'prev_flat_grad': tensor([1.6668, 3.1556]), 'prev_loss': -7.513423919677734}})\n",
      "tensor(1.1540, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4882],\n",
      "        [0.9739]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5289],\n",
      "        [1.0333]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1377, 0.1503]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.810\n",
      "Loss gp: 0.697\n",
      "Loss gp: 0.579\n",
      "Loss gp: 0.339\n",
      "Loss gp: -0.058\n",
      "Loss gp: 6.979\n",
      "Loss gp: -0.107\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "31\n",
      "Loss theta: -7.834\n",
      "Loss theta: -4.485\n",
      "Loss theta: -5.165\n",
      "Loss theta: -7.837\n",
      "Loss theta: -7.846\n",
      "Loss theta: -7.852\n",
      "Loss theta: -7.845\n",
      "Loss theta: -7.852\n",
      "Loss theta: -7.845\n",
      "Loss theta: -7.846\n",
      "Loss theta: -7.841\n",
      "Loss theta: -7.852\n",
      "Loss theta: -7.852\n",
      "Loss design: -7.852\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1357, 0.1468]], requires_grad=True): {'func_evals': 12, 'n_iter': 4, 'al': [tensor(0.0044), tensor(-0.1338), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-1.6009e-05, -2.7592e-04]), 't': 0, 'old_dirs': [tensor([-5.9344, -7.5699]), tensor([2.1140, 2.6450])], 'old_stps': [tensor([-0.0032, -0.0045]), tensor([0.0011, 0.0013])], 'ro': [tensor(18.9177), tensor(175.2221)], 'H_diag': tensor(0.0005), 'prev_flat_grad': tensor([0.0289, 0.5833]), 'prev_loss': -7.852476596832275}})\n",
      "tensor(1.1508, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4834],\n",
      "        [0.9740]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5210],\n",
      "        [1.0336]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1357, 0.1468]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.808\n",
      "Loss gp: 0.695\n",
      "Loss gp: 0.577\n",
      "Loss gp: 0.336\n",
      "Loss gp: -0.060\n",
      "Loss gp: 6.634\n",
      "Loss gp: -0.109\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "32\n",
      "Loss theta: -6.615\n",
      "Loss theta: 2767.051\n",
      "Loss theta: -6.628\n",
      "Loss theta: -0.209\n",
      "Loss theta: -6.770\n",
      "Loss theta: -6.785\n",
      "Loss theta: -6.778\n",
      "Loss theta: -6.772\n",
      "Loss theta: -6.781\n",
      "Loss theta: -6.785\n",
      "Loss theta: -6.785\n",
      "Loss design: -6.785\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1434, 0.1576]], requires_grad=True): {'func_evals': 10, 'n_iter': 3, 'al': [tensor(0.0201), tensor(0.2950), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0008,  0.0003]), 't': 0, 'old_dirs': [tensor([17.6669, 20.9680]), tensor([-1.2268, -1.3945])], 'old_stps': [tensor([0.0087, 0.0112]), tensor([-0.0010, -0.0004])], 'ro': [tensor(2.5831), tensor(558.3940)], 'H_diag': tensor(0.0005), 'prev_flat_grad': tensor([ 0.7850, -0.6126]), 'prev_loss': -6.784503936767578}})\n",
      "tensor(1.1573, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4943],\n",
      "        [0.9665]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5493],\n",
      "        [1.0263]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1434, 0.1576]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.698\n",
      "Loss gp: 0.580\n",
      "Loss gp: 0.339\n",
      "Loss gp: -0.058\n",
      "Loss gp: 6.730\n",
      "Loss gp: -0.106\n",
      "Loss gp: -0.113\n",
      "Loss gp: -0.114\n",
      "Loss gp: -0.115\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "33\n",
      "Loss theta: -6.738\n",
      "Loss theta: -0.244\n",
      "Loss theta: -3.889\n",
      "Loss theta: -6.752\n",
      "Loss theta: -6.753\n",
      "Loss theta: -6.759\n",
      "Loss theta: -6.763\n",
      "Loss theta: -6.762\n",
      "Loss theta: -6.758\n",
      "Loss theta: -6.760\n",
      "Loss theta: -6.756\n",
      "Loss theta: -6.763\n",
      "Loss theta: -6.763\n",
      "Loss design: -6.763\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1363, 0.1600]], requires_grad=True): {'func_evals': 12, 'n_iter': 5, 'al': [tensor(0.0916), tensor(-1.6835), tensor(-1.5518), tensor(1.1828), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0065,  0.0054]), 't': 0, 'old_dirs': [tensor([-5.8805, -7.1647]), tensor([0.4520, 0.5807]), tensor([0.7326, 1.0428]), tensor([0.6764, 1.1411])], 'old_stps': [tensor([-0.0029, -0.0028]), tensor([-0.0001,  0.0005]), tensor([-0.0011,  0.0016]), tensor([-0.0030,  0.0031])], 'ro': [tensor(26.9301), tensor(3988.4097), tensor(1124.6145), tensor(657.9622)], 'H_diag': tensor(0.0009), 'prev_flat_grad': tensor([2.0781, 1.4406]), 'prev_loss': -6.763210296630859}})\n",
      "tensor(1.1591, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4960],\n",
      "        [0.9670]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5507],\n",
      "        [1.0276]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1363, 0.1600]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.809\n",
      "Loss gp: 0.695\n",
      "Loss gp: 0.577\n",
      "Loss gp: 0.336\n",
      "Loss gp: -0.062\n",
      "Loss gp: 6.909\n",
      "Loss gp: -0.110\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.121\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "34\n",
      "Loss theta: -6.726\n",
      "Loss theta: -0.285\n",
      "Loss theta: -2.217\n",
      "Loss theta: -6.788\n",
      "Loss theta: -6.808\n",
      "Loss theta: -6.796\n",
      "Loss theta: -6.791\n",
      "Loss theta: -6.802\n",
      "Loss theta: -6.808\n",
      "Loss theta: -6.808\n",
      "Loss design: -6.808\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1299, 0.1548]], requires_grad=True): {'func_evals': 9, 'n_iter': 3, 'al': [tensor(0.0245), tensor(-0.1028), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0006,  0.0002]), 't': 0, 'old_dirs': [tensor([-15.0781, -18.2905]), tensor([4.0411, 5.1051])], 'old_stps': [tensor([-0.0073, -0.0073]), tensor([0.0010, 0.0021])], 'ro': [tensor(4.0898), tensor(66.9228)], 'H_diag': tensor(0.0004), 'prev_flat_grad': tensor([ 1.9753, -0.2236]), 'prev_loss': -6.808419704437256}})\n",
      "tensor(1.1585, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4920],\n",
      "        [0.9683]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5491],\n",
      "        [1.0278]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1299, 0.1548]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.805\n",
      "Loss gp: 0.692\n",
      "Loss gp: 0.573\n",
      "Loss gp: 0.331\n",
      "Loss gp: -0.068\n",
      "Loss gp: 6.830\n",
      "Loss gp: -0.116\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.124\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "35\n",
      "Loss theta: -7.071\n",
      "Loss theta: -3.774\n",
      "Loss theta: -2.555\n",
      "Loss theta: -6.777\n",
      "Loss theta: -7.108\n",
      "Loss theta: -6.818\n",
      "Loss theta: -6.976\n",
      "Loss theta: -6.988\n",
      "Loss theta: -7.108\n",
      "Loss theta: -7.108\n",
      "Loss design: -7.108\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1293, 0.1541]], requires_grad=True): {'func_evals': 9, 'n_iter': 2, 'al': [tensor(1.6062), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0009, -0.0013]), 't': 0, 'old_dirs': [tensor([-3.3255, -4.0534])], 'old_stps': [tensor([-0.0006, -0.0008])], 'ro': [tensor(193.9785)], 'H_diag': tensor(0.0002), 'prev_flat_grad': tensor([4.9460, 6.8117]), 'prev_loss': -7.108438491821289}})\n",
      "tensor(1.1517, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4969],\n",
      "        [0.9772]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5266],\n",
      "        [1.0319]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1293, 0.1541]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.809\n",
      "Loss gp: 0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.577\n",
      "Loss gp: 0.336\n",
      "Loss gp: -0.062\n",
      "Loss gp: 6.680\n",
      "Loss gp: -0.110\n",
      "Loss gp: -0.117\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.120\n",
      "Loss gp: -0.122\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "36\n",
      "Loss theta: -6.664\n",
      "Loss theta: 2781.625\n",
      "Loss theta: -6.667\n",
      "Loss theta: -6.664\n",
      "Loss theta: -6.666\n",
      "Loss theta: -6.645\n",
      "Loss theta: -6.684\n",
      "Loss theta: -6.647\n",
      "Loss theta: -6.684\n",
      "Loss theta: -6.684\n",
      "Loss theta: -6.684\n",
      "Loss theta: -6.684\n",
      "Loss design: -6.684\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1298, 0.1548]], requires_grad=True): {'func_evals': 11, 'n_iter': 3, 'al': [tensor(-0.1190), tensor(3.3830), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-1.5037e-05,  1.9059e-05]), 't': 0, 'old_dirs': [tensor([1.9401, 2.2713]), tensor([1.5360, 1.8414])], 'old_stps': [tensor([0.0005, 0.0008]), tensor([5.6886e-07, 1.4512e-06])], 'ro': [tensor(369.8814), tensor(282006.0625)], 'H_diag': tensor(6.1670e-07), 'prev_flat_grad': tensor([-3.4037, -6.9321]), 'prev_loss': -6.684178829193115}})\n",
      "tensor(1.1520, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4966],\n",
      "        [0.9619]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5510],\n",
      "        [1.0193]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1298, 0.1548]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.805\n",
      "Loss gp: 0.691\n",
      "Loss gp: 0.573\n",
      "Loss gp: 0.330\n",
      "Loss gp: -0.070\n",
      "Loss gp: 7.117\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "37\n",
      "Loss theta: -6.830\n",
      "Loss theta: 2498.777\n",
      "Loss theta: -6.821\n",
      "Loss theta: -6.836\n",
      "Loss theta: -6.828\n",
      "Loss theta: -6.830\n",
      "Loss theta: -6.836\n",
      "Loss theta: 2.350\n",
      "Loss theta: 53.553\n",
      "Loss theta: -6.290\n",
      "Loss theta: -6.845\n",
      "Loss theta: -6.854\n",
      "Loss theta: -6.882\n",
      "Loss theta: -6.860\n",
      "Loss theta: -6.836\n",
      "Loss theta: -6.882\n",
      "Loss theta: -6.812\n",
      "Loss theta: -6.878\n",
      "Loss theta: -6.876\n",
      "Loss theta: -6.882\n",
      "Loss theta: -6.882\n",
      "Loss theta: -2.561\n",
      "Loss theta: -6.856\n",
      "Loss theta: -6.874\n",
      "Loss theta: -6.869\n",
      "Loss theta: -6.830\n",
      "Loss theta: -6.854\n",
      "Loss theta: -6.882\n",
      "Loss theta: -6.882\n",
      "Loss design: -6.882\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1280, 0.1618]], requires_grad=True): {'func_evals': 28, 'n_iter': 6, 'al': [tensor(1.5599), tensor(13.0828), tensor(21.4126), tensor(23.9860), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.6603,  0.5524]), 't': 0, 'old_dirs': [tensor([3.9391, 4.5839]), tensor([1.5028, 1.9485]), tensor([1.7271, 2.5229]), tensor([-2.9054, -3.4727])], 'old_stps': [tensor([0.0013, 0.0029]), tensor([-0.0005,  0.0014]), tensor([-0.0025,  0.0026]), tensor([-1.6018e-05,  1.2999e-05])], 'ro': [tensor(54.7060), tensor(542.9118), tensor(441.8869), tensor(714740.8750)], 'H_diag': tensor(6.8246e-08), 'prev_flat_grad': tensor([ 1.3687, -0.8951]), 'prev_loss': -6.881577968597412}})\n",
      "tensor(1.1549, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4944],\n",
      "        [0.9678]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5487],\n",
      "        [1.0239]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1280, 0.1618]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.804\n",
      "Loss gp: 0.690\n",
      "Loss gp: 0.571\n",
      "Loss gp: 0.327\n",
      "Loss gp: -0.074\n",
      "Loss gp: 7.577\n",
      "Loss gp: -0.123\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "38\n",
      "Loss theta: -6.673\n",
      "Loss theta: 2948.278\n",
      "Loss theta: -6.713\n",
      "Loss theta: -6.785\n",
      "Loss theta: -6.794\n",
      "Loss theta: -6.798\n",
      "Loss theta: -6.831\n",
      "Loss theta: -6.828\n",
      "Loss theta: -6.817\n",
      "Loss theta: -6.823\n",
      "Loss theta: -6.815\n",
      "Loss theta: -6.831\n",
      "Loss theta: -6.831\n",
      "Loss design: -6.831\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1146, 0.1879]], requires_grad=True): {'func_evals': 12, 'n_iter': 6, 'al': [tensor(-0.0257), tensor(-0.0720), tensor(-1.5163), tensor(-1.8387), tensor(0.2915), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0078,  0.0057]), 't': 0, 'old_dirs': [tensor([3.2180, 3.7484]), tensor([12.2162, 14.4437]), tensor([1.8599, 2.3418]), tensor([0.6727, 1.0641]), tensor([1.7090, 4.0583])], 'old_stps': [tensor([0.0010, 0.0014]), tensor([0.0034, 0.0065]), tensor([-0.0004,  0.0012]), tensor([-0.0020,  0.0022]), tensor([-0.0154,  0.0147])], 'ro': [tensor(117.3273), tensor(7.3968), tensor(463.5343), tensor(987.5496), tensor(29.8382)], 'H_diag': tensor(0.0017), 'prev_flat_grad': tensor([5.3258, 4.9030]), 'prev_loss': -6.831353664398193}})\n",
      "tensor(1.1556, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4987],\n",
      "        [0.9708]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5493],\n",
      "        [1.0244]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1146, 0.1879]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.807\n",
      "Loss gp: 0.693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.574\n",
      "Loss gp: 0.332\n",
      "Loss gp: -0.069\n",
      "Loss gp: 7.461\n",
      "Loss gp: -0.118\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "39\n",
      "Loss theta: -6.798\n",
      "Loss theta: 1.337\n",
      "Loss theta: -1.753\n",
      "Loss theta: -6.816\n",
      "Loss theta: -6.819\n",
      "Loss theta: -6.829\n",
      "Loss theta: -6.825\n",
      "Loss theta: -6.830\n",
      "Loss theta: -6.848\n",
      "Loss theta: -6.849\n",
      "Loss theta: -6.856\n",
      "Loss theta: -6.835\n",
      "Loss theta: -6.851\n",
      "Loss theta: -6.850\n",
      "Loss theta: -6.856\n",
      "Loss theta: -6.856\n",
      "Loss design: -6.856\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0913, 0.2028]], requires_grad=True): {'func_evals': 15, 'n_iter': 7, 'al': [tensor(0.0054), tensor(-0.1521), tensor(-0.4618), tensor(-0.1141), tensor(-0.4443), tensor(0.3255), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-0.0011, -0.0001]), 't': 0, 'old_dirs': [tensor([-10.9845, -13.2322]), tensor([1.7590, 2.1805]), tensor([0.2222, 0.3801]), tensor([0.6286, 1.5739]), tensor([1.0825, 2.1449]), tensor([-1.6188, -1.1563])], 'old_stps': [tensor([-0.0037, -0.0035]), tensor([0.0002, 0.0008]), tensor([-0.0005,  0.0006]), tensor([-0.0054,  0.0059]), tensor([-0.0069,  0.0063]), tensor([-0.0070,  0.0047])], 'ro': [tensor(11.4497), tensor(474.9344), tensor(6971.5957), tensor(167.2364), tensor(164.8981), tensor(169.7063)], 'H_diag': tensor(0.0015), 'prev_flat_grad': tensor([1.6063, 1.9855]), 'prev_loss': -6.856351852416992}})\n",
      "tensor(1.1549, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4955],\n",
      "        [0.9682]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5481],\n",
      "        [1.0242]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0913, 0.2028]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.806\n",
      "Loss gp: 0.692\n",
      "Loss gp: 0.573\n",
      "Loss gp: 0.330\n",
      "Loss gp: -0.070\n",
      "Loss gp: 7.323\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.129\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "40\n",
      "Loss theta: -6.837\n",
      "Loss theta: 0.961\n",
      "Loss theta: -2.586\n",
      "Loss theta: -6.868\n",
      "Loss theta: -6.864\n",
      "Loss theta: -6.868\n",
      "Loss theta: -6.870\n",
      "Loss theta: -6.870\n",
      "Loss theta: -6.873\n",
      "Loss theta: -6.868\n",
      "Loss theta: -6.873\n",
      "Loss theta: -6.873\n",
      "Loss design: -6.873\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0882, 0.1990]], requires_grad=True): {'func_evals': 11, 'n_iter': 4, 'al': [tensor(0.0002), tensor(-0.0036), tensor(3.1160), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([1.6544e-07, 4.0940e-07]), 't': 0, 'old_dirs': [tensor([ -9.4319, -11.8723]), tensor([0.4724, 0.6272]), tensor([0.2417, 0.3456])], 'old_stps': [tensor([-0.0031, -0.0038]), tensor([3.0963e-06, 4.3532e-06]), tensor([7.2060e-08, 1.1812e-07])], 'ro': [tensor(13.3090), tensor(238512.4219), tensor(17170110.)], 'H_diag': tensor(3.2745e-07), 'prev_flat_grad': tensor([-0.7185, -1.0980]), 'prev_loss': -6.8730998039245605}})\n",
      "tensor(1.1536, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4923],\n",
      "        [0.9662]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5467],\n",
      "        [1.0236]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0882, 0.1990]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.804\n",
      "Loss gp: 0.689\n",
      "Loss gp: 0.570\n",
      "Loss gp: 0.327\n",
      "Loss gp: -0.075\n",
      "Loss gp: 7.594\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "41\n",
      "Loss theta: -6.644\n",
      "Loss theta: 3118.308\n",
      "Loss theta: -6.656\n",
      "Loss theta: -6.669\n",
      "Loss theta: -6.678\n",
      "Loss theta: -6.678\n",
      "Loss theta: -6.688\n",
      "Loss theta: -6.667\n",
      "Loss theta: -6.688\n",
      "Loss theta: -6.655\n",
      "Loss theta: -6.688\n",
      "Loss theta: -6.688\n",
      "Loss design: -6.688\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0907, 0.2018]], requires_grad=True): {'func_evals': 11, 'n_iter': 5, 'al': [tensor(-0.0045), tensor(0.0002), tensor(0.0110), tensor(-0.6342), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 4.6076e-06, -1.1789e-05]), 't': 0, 'old_dirs': [tensor([0.8072, 1.0291]), tensor([10.3867, 12.7751]), tensor([-4.8746, -5.9447]), tensor([0.7488, 0.9677])], 'old_stps': [tensor([0.0005, 0.0006]), tensor([0.0036, 0.0040]), tensor([-0.0016, -0.0018]), tensor([1.0521e-05, 4.8255e-06])], 'ro': [tensor(990.3289), tensor(11.2474), tensor(54.1512), tensor(79691.6094)], 'H_diag': tensor(8.3809e-06), 'prev_flat_grad': tensor([0.3906, 0.7975]), 'prev_loss': -6.6875104904174805}})\n",
      "tensor(1.1536, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4997],\n",
      "        [0.9653]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5510],\n",
      "        [1.0212]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0907, 0.2018]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.803\n",
      "Loss gp: 0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: 0.570\n",
      "Loss gp: 0.326\n",
      "Loss gp: -0.076\n",
      "Loss gp: 7.326\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "42\n",
      "Loss theta: -7.011\n",
      "Loss theta: 0.909\n",
      "Loss theta: -3.677\n",
      "Loss theta: -7.008\n",
      "Loss theta: -7.027\n",
      "Loss theta: -7.019\n",
      "Loss theta: -7.022\n",
      "Loss theta: -7.025\n",
      "Loss theta: -7.027\n",
      "Loss theta: -7.027\n",
      "Loss design: -7.027\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0890, 0.1992]], requires_grad=True): {'func_evals': 9, 'n_iter': 2, 'al': [tensor(0.0491), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 0.0002, -0.0004]), 't': 0, 'old_dirs': [tensor([-5.3099, -6.5424])], 'old_stps': [tensor([-0.0016, -0.0026])], 'ro': [tensor(38.7715)], 'H_diag': tensor(0.0004), 'prev_flat_grad': tensor([-0.6248,  0.8799]), 'prev_loss': -7.027176380157471}})\n",
      "tensor(1.1542, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4873],\n",
      "        [0.9692]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5425],\n",
      "        [1.0264]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0890, 0.1992]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.801\n",
      "Loss gp: 0.686\n",
      "Loss gp: 0.567\n",
      "Loss gp: 0.323\n",
      "Loss gp: -0.078\n",
      "Loss gp: 6.724\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.151\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.152\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.154\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.153\n",
      "Loss gp: -0.154\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "43\n",
      "Loss theta: -6.615\n",
      "Loss theta: 3425.959\n",
      "Loss theta: -6.464\n",
      "Loss theta: -6.473\n",
      "Loss theta: -6.612\n",
      "Loss theta: -6.629\n",
      "Loss theta: -6.629\n",
      "Loss theta: -0.203\n",
      "Loss theta: 8.511\n",
      "Loss theta: 630.032\n",
      "Loss theta: -6.591\n",
      "Loss theta: -6.456\n",
      "Loss theta: -6.556\n",
      "Loss theta: -6.604\n",
      "Loss theta: -6.629\n",
      "Loss theta: -6.629\n",
      "Loss design: -6.629\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0890, 0.1992]], requires_grad=True): {'func_evals': 15, 'n_iter': 2, 'al': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([7.8138, 6.5209]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([-7.8138, -6.5209]), 'prev_loss': -6.628775596618652}})\n",
      "tensor(1.1379, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4981],\n",
      "        [0.9624]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5301],\n",
      "        [1.0145]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0890, 0.1992]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.806\n",
      "Loss gp: 0.692\n",
      "Loss gp: 0.574\n",
      "Loss gp: 0.331\n",
      "Loss gp: -0.070\n",
      "Loss gp: 7.165\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "44\n",
      "Loss theta: -7.097\n",
      "Loss theta: 3516.440\n",
      "Loss theta: -7.115\n",
      "Loss theta: -7.129\n",
      "Loss theta: -7.127\n",
      "Loss theta: -7.117\n",
      "Loss theta: -7.132\n",
      "Loss theta: -7.129\n",
      "Loss theta: -7.121\n",
      "Loss theta: -7.130\n",
      "Loss theta: -7.132\n",
      "Loss theta: -7.132\n",
      "Loss design: -7.132\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0915, 0.2012]], requires_grad=True): {'func_evals': 11, 'n_iter': 4, 'al': [tensor(-0.0622), tensor(-0.1027), tensor(0.8532), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 8.6287e-05, -5.2236e-05]), 't': 0, 'old_dirs': [tensor([2.6193, 3.5456]), tensor([3.2661, 4.2597]), tensor([1.5331, 2.1031])], 'old_stps': [tensor([0.0007, 0.0007]), tensor([0.0018, 0.0013]), tensor([1.1337e-05, 4.2352e-06])], 'ro': [tensor(228.3040), tensor(87.7437), tensor(38039.6211)], 'H_diag': tensor(3.8811e-06), 'prev_flat_grad': tensor([-1.9570, -0.0573]), 'prev_loss': -7.1322479248046875}})\n",
      "tensor(1.1511, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4847],\n",
      "        [0.9703]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5380],\n",
      "        [1.0253]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0915, 0.2012]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.799\n",
      "Loss gp: 0.685\n",
      "Loss gp: 0.565\n",
      "Loss gp: 0.322\n",
      "Loss gp: -0.080\n",
      "Loss gp: 6.553\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.135\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gp: -0.145\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.149\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Loss gp: -0.150\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "45\n",
      "Loss theta: -7.073\n",
      "Loss theta: 3349.622\n",
      "Loss theta: -7.057\n",
      "Loss theta: -7.064\n",
      "Loss theta: -7.061\n",
      "Loss theta: -7.071\n",
      "Loss theta: -7.073\n",
      "Loss theta: -7.073\n",
      "Loss design: -7.073\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0915, 0.2012]], requires_grad=True): {'func_evals': 7, 'n_iter': 1, 'd': tensor([5.2301, 5.5783]), 't': 0, 'old_dirs': [], 'old_stps': [], 'ro': [], 'H_diag': 1, 'prev_flat_grad': tensor([-5.2301, -5.5783]), 'prev_loss': -7.072710037231445}})\n",
      "tensor(1.1476, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4895],\n",
      "        [0.9649]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5406],\n",
      "        [1.0200]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0915, 0.2012]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.800\n",
      "Loss gp: 0.686\n",
      "Loss gp: 0.567\n",
      "Loss gp: 0.323\n",
      "Loss gp: -0.078\n",
      "Loss gp: 6.460\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.147\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Loss gp: -0.148\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "46\n",
      "Loss theta: -7.018\n",
      "Loss theta: 1.048\n",
      "Loss theta: 0.800\n",
      "Loss theta: -7.136\n",
      "Loss theta: -7.153\n",
      "Loss theta: -7.146\n",
      "Loss theta: -7.143\n",
      "Loss theta: -7.145\n",
      "Loss theta: -7.153\n",
      "Loss theta: -7.153\n",
      "Loss design: -7.153\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.0867, 0.1933]], requires_grad=True): {'func_evals': 9, 'n_iter': 3, 'al': [tensor(0.0099), tensor(0.0580), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 0.0005, -0.0002]), 't': 0, 'old_dirs': [tensor([-21.0331, -28.1400]), tensor([3.5581, 5.0341])], 'old_stps': [tensor([-0.0063, -0.0093]), tensor([0.0015, 0.0014])], 'ro': [tensor(2.5387), tensor(82.6484)], 'H_diag': tensor(0.0003), 'prev_flat_grad': tensor([-1.2868,  0.8563]), 'prev_loss': -7.15252161026001}})\n",
      "tensor(1.1515, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4848],\n",
      "        [0.9711]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5375],\n",
      "        [1.0260]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.0867, 0.1933]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.803\n",
      "Loss gp: 0.689\n",
      "Loss gp: 0.570\n",
      "Loss gp: 0.327\n",
      "Loss gp: -0.072\n",
      "Loss gp: 5.932\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.127\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.133\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.138\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "47\n",
      "Loss theta: -7.171\n",
      "Loss theta: 0.064\n",
      "Loss theta: -2.055\n",
      "Loss theta: -7.185\n",
      "Loss theta: -7.190\n",
      "Loss theta: -7.206\n",
      "Loss theta: -7.198\n",
      "Loss theta: -7.200\n",
      "Loss theta: -7.200\n",
      "Loss theta: -7.200\n",
      "Loss theta: -6.879\n",
      "Loss theta: -7.300\n",
      "Loss theta: -7.306\n",
      "Loss theta: -7.304\n",
      "Loss theta: -7.293\n",
      "Loss theta: -7.318\n",
      "Loss theta: -7.318\n",
      "Loss theta: -7.318\n",
      "Loss theta: -7.301\n",
      "Loss theta: -7.314\n",
      "Loss theta: -7.313\n",
      "Loss theta: -7.318\n",
      "Loss theta: -7.318\n",
      "Loss design: -7.318\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1358, 0.1541]], requires_grad=True): {'func_evals': 22, 'n_iter': 7, 'al': [tensor(-0.0007), tensor(-0.0347), tensor(0.8157), tensor(0.0391), tensor(-0.3806), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([ 0.0017, -0.0013]), 't': 0, 'old_dirs': [tensor([ -8.5638, -11.3277]), tensor([4.5854, 6.1695]), tensor([-1.7794, -2.4129]), tensor([6.7416, 3.5660]), tensor([-2.9754, -3.5774])], 'old_stps': [tensor([-0.0008, -0.0031]), tensor([0.0009, 0.0007]), tensor([ 0.0007, -0.0006]), tensor([ 0.0496, -0.0369]), tensor([-0.0012,  0.0008])], 'ro': [tensor(23.5169), tensor(121.5549), tensor(6987.6406), tensor(4.9427), tensor(1461.4752)], 'H_diag': tensor(3.1603e-05), 'prev_flat_grad': tensor([0.2224, 0.6700]), 'prev_loss': -7.317636966705322}})\n",
      "tensor(1.1493, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4791],\n",
      "        [0.9760]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5300],\n",
      "        [1.0274]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1358, 0.1541]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.804\n",
      "Loss gp: 0.689\n",
      "Loss gp: 0.570\n",
      "Loss gp: 0.327\n",
      "Loss gp: -0.072\n",
      "Loss gp: 6.038\n",
      "Loss gp: -0.119\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.126\n",
      "Loss gp: -0.128\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.136\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.139\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "48\n",
      "Loss theta: -7.189\n",
      "Loss theta: 3363.497\n",
      "Loss theta: -7.229\n",
      "Loss theta: 5.939\n",
      "Loss theta: -7.227\n",
      "Loss theta: -7.237\n",
      "Loss theta: -7.232\n",
      "Loss theta: -7.237\n",
      "Loss theta: -7.225\n",
      "Loss theta: -7.240\n",
      "Loss theta: -7.255\n",
      "Loss theta: -7.235\n",
      "Loss theta: -7.250\n",
      "Loss theta: -7.255\n",
      "Loss theta: -7.257\n",
      "Loss theta: -7.229\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.256\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.273\n",
      "Loss theta: -7.241\n",
      "Loss theta: -7.247\n",
      "Loss theta: -7.283\n",
      "Loss theta: -7.269\n",
      "Loss theta: -7.283\n",
      "Loss theta: -7.283\n",
      "Loss design: -7.283\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1366, 0.1554]], requires_grad=True): {'func_evals': 34, 'n_iter': 5, 'al': [tensor(-0.0037), tensor(0.0020), tensor(3.1848), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-1.0014e-06, -9.3498e-07]), 't': 0, 'old_dirs': [tensor([2.1900, 3.0978]), tensor([5.4265, 7.1139]), tensor([-0.7532, -0.9357])], 'old_stps': [tensor([0.0004, 0.0006]), tensor([0.0003, 0.0007]), tensor([-2.2794e-07, -3.6319e-07])], 'ro': [tensor(369.3947), tensor(152.2022), tensor(1955050.5000)], 'H_diag': tensor(3.5453e-07), 'prev_flat_grad': tensor([2.6014, 2.8527]), 'prev_loss': -7.282955646514893}})\n",
      "tensor(1.1497, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4898],\n",
      "        [0.9738]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5368],\n",
      "        [1.0243]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1554]], requires_grad=True)\n",
      "START HYPERPARAMETERS optimization\n",
      "Loss gp: 0.798\n",
      "Loss gp: 0.683\n",
      "Loss gp: 0.564\n",
      "Loss gp: 0.320\n",
      "Loss gp: -0.078\n",
      "Loss gp: 5.521\n",
      "Loss gp: -0.125\n",
      "Loss gp: -0.130\n",
      "Loss gp: -0.131\n",
      "Loss gp: -0.132\n",
      "Loss gp: -0.134\n",
      "Loss gp: -0.137\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.140\n",
      "Loss gp: -0.141\n",
      "Loss gp: -0.142\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.143\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.144\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.145\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Loss gp: -0.146\n",
      "Convergence reached!\n",
      "END HYPERPARAMETERS optimization\n",
      "49\n",
      "Loss theta: -7.277\n",
      "Loss theta: 0.936\n",
      "Loss theta: -3.964\n",
      "Loss theta: -7.293\n",
      "Loss theta: -7.299\n",
      "Loss theta: -7.306\n",
      "Loss theta: -7.308\n",
      "Loss theta: -7.300\n",
      "Loss theta: -7.306\n",
      "Loss theta: -7.315\n",
      "Loss theta: -7.308\n",
      "Loss theta: -7.325\n",
      "Loss theta: -7.210\n",
      "Loss theta: -7.325\n",
      "Loss theta: -7.326\n",
      "Loss theta: -7.305\n",
      "Loss theta: -7.320\n",
      "Loss theta: -7.323\n",
      "Loss theta: -7.326\n",
      "Loss theta: -7.326\n",
      "Loss design: -7.326\n",
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[0.1351, 0.1544]], requires_grad=True): {'func_evals': 19, 'n_iter': 5, 'al': [tensor(0.0073), tensor(-0.1203), tensor(0.0608), tensor(1.2805), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'd': tensor([-1.8625e-04, -1.7236e-05]), 't': 0, 'old_dirs': [tensor([-7.3420, -9.2836]), tensor([0.3940, 0.5434]), tensor([3.0033, 3.9985]), tensor([-0.6748, -0.8451])], 'old_stps': [tensor([-0.0022, -0.0024]), tensor([0.0008, 0.0013]), tensor([9.0570e-05, 2.2440e-04]), tensor([-9.2047e-05, -5.6092e-05])], 'ro': [tensor(25.7065), tensor(1007.3057), tensor(855.2403), tensor(9131.3369)], 'H_diag': tensor(9.3640e-05), 'prev_flat_grad': tensor([1.1202, 0.6618]), 'prev_loss': -7.326108932495117}})\n",
      "tensor(1.1482, grad_fn=<CopyBackwards>)\n",
      "tensor([[0.4884],\n",
      "        [0.9727]], grad_fn=<CopySlices>)\n",
      "tensor([[0.5353],\n",
      "        [1.0235]], grad_fn=<CopySlices>)\n",
      "tensor([[0.4950],\n",
      "        [0.9950]])\n",
      "tensor([[0.5050],\n",
      "        [1.0050]])\n",
      "Parameter containing:\n",
      "tensor([[0.1351, 0.1544]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "iter_hp = 90\n",
    "iter_design = 40 \n",
    "iter_param = 50\n",
    "\n",
    "\n",
    "f_target = Tensor(vf.tgt_vec) \n",
    "f_target = f_target.reshape(f_target.shape[0],1) \n",
    "tol_vector = 0.005 * torch.ones(f_target.shape)\n",
    "\n",
    "\n",
    "loc_size = 10\n",
    "loc_sample = np.random.random_sample((loc_size,2))\n",
    "g_theta2_vec = (Tensor(loc_sample)).flatten()\n",
    "\n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "\n",
    "\n",
    "x0 = Tensor(np.array([0.5,0.7])) \n",
    "x0 = x0.reshape(1,2)\n",
    "x00 = x0 \n",
    "vec_x = x00 \n",
    "\n",
    "\n",
    "lr_new = 1.\n",
    "\n",
    "SUCCESS = False \n",
    "FAILURE = False \n",
    "iter = 0 \n",
    "tol = 0.009 \n",
    "while(SUCCESS == False and iter < 50):\n",
    "    print('START HYPERPARAMETERS optimization')\n",
    "    model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp)\n",
    "\n",
    "    print('END HYPERPARAMETERS optimization')\n",
    "    print(iter)\n",
    "    \n",
    "    x0_new,g_theta2,lower_bound, upper_bound = conduct_design_opti(x0, loc_sample, f_target, g_theta1, agg_data, model, likelihood, iter_design,iter_param, lr_new)\n",
    "    g_theta2_vec = torch.cat([g_theta2_vec, g_theta2.flatten()], 0)\n",
    "    print(torch.norm(upper_bound - tol_vector))\n",
    "#     if (torch.norm(upper_bound - tol_vector) <= 0.1 ):\n",
    "#         print('bkhjghf')\n",
    "#         lr_new = lr_new * 0.1\n",
    "    print(lower_bound)\n",
    "    print(upper_bound)\n",
    "    print(f_target-tol_vector)\n",
    "    print(f_target+tol_vector)\n",
    "    loc_sample = np.random.random_sample((loc_size,2))\n",
    "    x0 = x0_new #Tensor(np.random.random_sample((1,2))) #x0_new\n",
    "    vec_x = torch.cat([vec_x, x0_new])\n",
    "    g_theta2_detach = g_theta2.detach()\n",
    "    new_data = vfield_(g_theta2_detach)\n",
    "    agg_data = torch.cat([agg_data, new_data.flatten()], 0)\n",
    "    g_theta1= torch.cat([g_theta1, g_theta2], 0)\n",
    "\n",
    "    SUCCESS, FAILURE = stopping_criteria(tol_vector, f_target, lower_bound, upper_bound)\n",
    "    iter = iter + 1\n",
    "    print(x0_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52541146 1.06348491]\n",
      "tensor([[0.1566, 0.1873]])\n",
      "tensor([[0.5375, 1.0537]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "#x0 = Tensor(np.array([0.1937, 0.1257]))\n",
    "#x0 = Tensor(np.array([0.1885, 0.1038]))\n",
    "\n",
    "x0 = Tensor(np.array([0.1566, 0.1873])) # lr = .2, samp_size = 5\n",
    "print(vf(x0))\n",
    "x0 = x0.reshape(1,2)\n",
    "print(x0)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "pr = likelihood(model(x0))\n",
    "print(pr.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "res = res + torch.ones(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1020])\n"
     ]
    }
   ],
   "source": [
    "print(g_theta2_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[70, 2]' is invalid for input of size 1020",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-47f5976ceebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_theta2_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[70, 2]' is invalid for input of size 1020"
     ]
    }
   ],
   "source": [
    "v2 = g_theta2_vec.reshape(70,2)\n",
    "print(vec_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v2)\n",
    "np.savetxt('g_theta2_01.txt', v2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt('vec_x_01.txt', vec_x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
