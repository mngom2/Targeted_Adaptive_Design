{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn  import functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(\"..\")\n",
    "from kernels import vvkernels as vvk, sep_vvkernels as svvk, vvk_rbfkernel as vvk_rbf\n",
    "from means import vvmeans as vvm\n",
    "from likelihood import vvlikelihood as vvll\n",
    "from mlikelihoods import MarginalLogLikelihood as exmll\n",
    "from predstrategies import GPprediction\n",
    "from utils import ObjFun, get_vertices, stopping_criteria\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3380, 0.3502], dtype=torch.float32)\n",
      "tensor([[ 2.4683, -2.8006],\n",
      "        [-2.1043, -0.8821],\n",
      "        [-1.9342,  1.9788],\n",
      "        [ 1.4148,  1.0389],\n",
      "        [ 2.0117,  1.7284],\n",
      "        [ 0.5297,  0.4172],\n",
      "        [-0.6897, -1.4173],\n",
      "        [ 0.5013, -2.9818],\n",
      "        [-0.8649, -0.7607],\n",
      "        [ 0.4400,  0.7817],\n",
      "        [ 0.2724,  0.6959],\n",
      "        [ 1.9166, -2.1338],\n",
      "        [-0.3922,  2.3453],\n",
      "        [ 2.4806, -0.4953],\n",
      "        [-2.7516, -0.3190],\n",
      "        [-2.4444, -0.9558],\n",
      "        [-0.3199,  2.5898],\n",
      "        [-0.1106, -2.2790],\n",
      "        [-1.1771, -2.5949],\n",
      "        [ 0.2845, -1.3441],\n",
      "        [ 2.7202, -2.1631],\n",
      "        [ 0.6443,  2.2437],\n",
      "        [-1.4896, -1.4143],\n",
      "        [ 2.9410, -1.4842],\n",
      "        [-1.8677, -2.5650],\n",
      "        [-0.3051, -2.8640],\n",
      "        [ 2.8369, -2.4092],\n",
      "        [ 0.2516,  0.4043],\n",
      "        [-0.2993,  1.3328],\n",
      "        [-1.0095,  0.2817],\n",
      "        [-2.3846, -1.9468],\n",
      "        [-2.5929, -1.3840],\n",
      "        [ 0.3319,  0.1781],\n",
      "        [ 1.3162, -1.0153],\n",
      "        [-1.7985, -2.0253],\n",
      "        [ 1.6911,  0.1581],\n",
      "        [-2.7489,  2.4318],\n",
      "        [-1.8012,  0.8754],\n",
      "        [-0.4914,  0.7251],\n",
      "        [ 1.1450, -0.7584],\n",
      "        [ 0.8697, -0.1172],\n",
      "        [-1.9649, -1.7697],\n",
      "        [ 0.0297,  2.0883],\n",
      "        [ 0.3692, -0.6846],\n",
      "        [ 0.3102,  1.4390],\n",
      "        [ 0.7038, -1.4106],\n",
      "        [-1.1235,  1.5121],\n",
      "        [-1.5247, -1.8062],\n",
      "        [-1.0302, -0.2175],\n",
      "        [-0.3606, -1.1111],\n",
      "        [ 0.2061,  1.7615],\n",
      "        [ 2.7914,  0.0647],\n",
      "        [-1.4020,  0.6409],\n",
      "        [ 2.6516, -2.0937],\n",
      "        [ 2.0103,  1.7957],\n",
      "        [ 1.7625, -1.6090],\n",
      "        [ 1.6599, -1.2708],\n",
      "        [-0.7487,  2.7223],\n",
      "        [ 1.2536, -1.6650],\n",
      "        [ 1.1197,  1.5042],\n",
      "        [-1.2306, -0.7783],\n",
      "        [-1.5657, -2.8142],\n",
      "        [-0.9187, -0.3778],\n",
      "        [-1.5974,  1.9606],\n",
      "        [-1.3924,  2.3700],\n",
      "        [-2.9665, -0.2926],\n",
      "        [ 1.7667,  2.5358],\n",
      "        [ 1.6249,  2.9770],\n",
      "        [ 0.9980, -2.9208],\n",
      "        [-1.8314,  1.3516],\n",
      "        [ 1.2726, -1.5129],\n",
      "        [ 2.9487, -0.2216],\n",
      "        [ 0.7531, -2.0514],\n",
      "        [-2.4633,  2.5916],\n",
      "        [ 1.1412,  2.5399],\n",
      "        [-0.3014,  1.0806],\n",
      "        [ 1.4405, -0.8217],\n",
      "        [ 2.7504, -0.6660],\n",
      "        [-2.9447,  2.6056],\n",
      "        [ 1.9070,  0.8877],\n",
      "        [ 2.3338, -0.9544],\n",
      "        [ 2.9667, -1.7552],\n",
      "        [-1.1545, -0.5694],\n",
      "        [-2.3103, -1.9896],\n",
      "        [-1.0900,  2.4443],\n",
      "        [ 0.4512,  2.8702],\n",
      "        [-2.7596,  1.9320],\n",
      "        [-2.1673, -1.3234],\n",
      "        [-1.5983,  2.7492],\n",
      "        [ 0.8806,  0.8598],\n",
      "        [ 0.0221, -1.5131],\n",
      "        [ 2.9763, -0.5419],\n",
      "        [ 0.4721, -0.5662],\n",
      "        [ 0.9086, -1.8062],\n",
      "        [ 1.8636,  2.8011],\n",
      "        [-0.9548,  0.4501],\n",
      "        [-1.4459,  2.9703],\n",
      "        [-0.9893,  2.4039],\n",
      "        [-2.4004, -0.5189],\n",
      "        [ 1.1034,  1.5592]])\n"
     ]
    }
   ],
   "source": [
    "vf = ObjFun()\n",
    "f_target = vf.tgt_vec\n",
    "print(f_target)\n",
    "sample_size = 100\n",
    "D = vf.D\n",
    "N = vf.N\n",
    "\n",
    "vf.low = -3.\n",
    "vf.high = 3.\n",
    "\n",
    "high_minus_low = vf.high- vf.low\n",
    "def g_theta(sample_size, D):\n",
    "    loc = high_minus_low  * np.random.random_sample((sample_size,2)) + vf.low#(np.random.uniform(low=vf.low, high=vf.high, size=(sample_size, D)))\n",
    "    return Tensor(loc)\n",
    "train_x = g_theta(sample_size, D)\n",
    "#train_x = Tensor([[-1.5, 1.5], [1.5, -1.5]])\n",
    "print(train_x)\n",
    "noise_value = 0.0004 #noise_free = 0.\n",
    "def vfield_(x):\n",
    "    x = x.reshape(x.shape[0],D)\n",
    "    out = torch.zeros(x.shape[0], N)\n",
    "    out = vf(x[:,0], x[:,1]) + torch.randn(Tensor(vf(x[:,0], x[:,1])).size()) * math.sqrt(noise_value)\n",
    "    return out #/torch.max(out)\n",
    "\n",
    "train_y = vfield_(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x #loc #torch.linspace(0, 1, 10)\n",
    "y_train = train_y #v  #torch.stack([torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,], -1)\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_base_kernels):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        a = torch.ones(2,2)\n",
    "        chol_q = torch.tril(a)\n",
    "        self.mean_module = vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)  #vvm.TensorProductSubMean(gpytorch.means.LinearMean(2), num_tasks = 2)#vvm.TensorProductSubMean(gpytorch.means.ConstantMean(), num_tasks = 2)  # \n",
    "        base_kernels = []\n",
    "        for i in range(num_base_kernels):\n",
    "            base_kernels.append(( gpytorch.kernels.RBFKernel() )) #gpytorch.kernels.PolynomialKernel(4)  ##gpytorch.kernels.MaternKernel()# (vvk_rbf.vvkRBFKernel())\n",
    "#         base_kernels2 = []\n",
    "#         for i in range(num_base_kernels):\n",
    "#             base_kernels2.append(gpytorch.kernels.PolynomialKernel(5))  \n",
    "            \n",
    "        self.covar_module = svvk.SepTensorProductKernel(base_kernels,num_tasks = 2)\n",
    "        #self.covar_module = gpytorch.kernels.LCMKernel(base_kernels,num_tasks = 2)\n",
    "\n",
    "#\\         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "       # self.covar_module = vvk.TensorProductKernel(vvk_rbf.vvkRBFKernel(), a[0,0], a[1,0], a[1,1], num_tasks = 2, rank =1,  task_covar_prior=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ###hyperparameters optimization###\n",
    "def hyper_opti(g_theta1, agg_data, training_iter,num_base_kernels,noise_value, current_model = None, current_likelihood = None):\n",
    "    noises = torch.ones(agg_data.shape[0]) * (noise_value) #  torch.zeros(agg_data.shape[0]) # \n",
    "    noises = noises.reshape(g_theta1.shape[0], 2)\n",
    "    \n",
    "#     if (current_model is not None):\n",
    "#         likelihood = current_likelihood #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises)  #\n",
    "\n",
    "#         model = current_model#.get_fantasy_model(g_theta1, agg_data) #MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#         model.set_train_data(g_theta1, agg_data,  strict=False)\n",
    "#     else:\n",
    "#         likelihood = vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2)#vvll.FixedNoiseMultitaskGaussianLikelihood(2, noises) #\n",
    "#         model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "        \n",
    "    likelihood =  vvll.FixedNoiseMultitaskGaussianLikelihood(noises) #vvll.TensorProductLikelihood(num_tasks = 2) #\n",
    "    model = MultitaskGPModel(g_theta1, agg_data, likelihood,num_base_kernels)\n",
    "#     model.double()\n",
    "#     likelihood.double()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #, weight_decay=0.001)  # Includes GaussianLikelihood parameters\n",
    "    mll = exmll(likelihood, model)\n",
    "   # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    #mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "#         loss, inv_quad = likelihood.get_mll(agg_data,g_theta1, model, likelihood, noise_value)\n",
    "#         loss = -loss\n",
    "      #  output = model(g_theta1)\n",
    " #   Calc loss and backprop gradients\n",
    "        #loss = -mll(output, agg_data)\n",
    "        loss, chi_square = mll(agg_data,g_theta1, model, likelihood, noise_value)\n",
    "        loss = -1. * loss\n",
    "#         print('df is %.3f' %agg_data.shape[0] +'and chi_square %.3f' %chi_square) \n",
    "        print('loss is %.3f' %loss)\n",
    "        df = agg_data.shape[0]\n",
    "        chi_square = chi_square.clone().detach()\n",
    "        \n",
    "        p_val = 1. - stats.chi2.cdf(chi_square, df)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "#         if (p_val > 0.99999):\n",
    "#             return model, likelihood\n",
    "        \n",
    "\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -0.148\n",
      "loss is -0.377\n",
      "loss is -0.602\n",
      "loss is -0.818\n",
      "loss is -1.017\n",
      "loss is -1.192\n",
      "loss is -1.332\n",
      "loss is -1.428\n",
      "loss is -1.471\n",
      "loss is -1.463\n",
      "loss is -1.424\n",
      "loss is -1.384\n",
      "loss is -1.374\n",
      "loss is -1.399\n",
      "loss is -1.447\n",
      "loss is -1.501\n",
      "loss is -1.545\n",
      "loss is -1.571\n",
      "loss is -1.579\n",
      "loss is -1.576\n",
      "loss is -1.571\n",
      "loss is -1.573\n",
      "loss is -1.584\n",
      "loss is -1.606\n",
      "loss is -1.636\n",
      "loss is -1.670\n",
      "loss is -1.700\n",
      "loss is -1.722\n",
      "loss is -1.735\n",
      "loss is -1.743\n",
      "loss is -1.754\n",
      "loss is -1.771\n",
      "loss is -1.795\n",
      "loss is -1.818\n",
      "loss is -1.837\n",
      "loss is -1.849\n",
      "loss is -1.854\n",
      "loss is -1.860\n",
      "loss is -1.872\n",
      "loss is -1.891\n",
      "loss is -1.911\n",
      "loss is -1.926\n",
      "loss is -1.936\n",
      "loss is -1.944\n",
      "loss is -1.954\n",
      "loss is -1.968\n",
      "loss is -1.984\n",
      "loss is -1.990\n",
      "loss is -1.995\n",
      "loss is -2.006\n",
      "loss is -2.019\n",
      "loss is -2.027\n",
      "loss is -2.028\n",
      "loss is -2.036\n",
      "loss is -2.047\n",
      "loss is -2.049\n",
      "loss is -2.052\n",
      "loss is -2.058\n",
      "loss is -2.064\n",
      "loss is -2.064\n",
      "loss is -2.069\n",
      "loss is -2.072\n",
      "loss is -2.074\n",
      "loss is -2.077\n",
      "loss is -2.081\n",
      "loss is -2.081\n",
      "loss is -2.083\n",
      "loss is -2.087\n",
      "loss is -2.086\n",
      "loss is -2.089\n",
      "loss is -2.092\n",
      "loss is -2.093\n",
      "loss is -2.094\n",
      "loss is -2.096\n",
      "loss is -2.098\n",
      "loss is -2.098\n",
      "loss is -2.100\n",
      "loss is -2.101\n",
      "loss is -2.102\n",
      "loss is -2.104\n",
      "loss is -2.105\n",
      "loss is -2.105\n",
      "loss is -2.107\n",
      "loss is -2.108\n",
      "loss is -2.108\n",
      "loss is -2.108\n",
      "loss is -2.109\n",
      "loss is -2.108\n",
      "loss is -2.109\n",
      "loss is -2.111\n",
      "loss is -2.111\n",
      "loss is -2.112\n",
      "loss is -2.112\n",
      "loss is -2.113\n",
      "loss is -2.114\n",
      "loss is -2.114\n",
      "loss is -2.114\n",
      "loss is -2.115\n",
      "loss is -2.115\n",
      "loss is -2.116\n",
      "loss is -2.116\n",
      "loss is -2.116\n",
      "loss is -2.117\n",
      "loss is -2.117\n",
      "loss is -2.117\n",
      "loss is -2.118\n",
      "loss is -2.118\n",
      "loss is -2.118\n",
      "loss is -2.119\n",
      "loss is -2.119\n",
      "loss is -2.119\n",
      "loss is -2.120\n",
      "loss is -2.120\n",
      "loss is -2.120\n",
      "loss is -2.120\n",
      "loss is -2.121\n",
      "loss is -2.121\n",
      "loss is -2.121\n",
      "loss is -2.121\n",
      "loss is -2.121\n",
      "loss is -2.121\n",
      "loss is -2.121\n",
      "loss is -2.117\n",
      "loss is -2.107\n",
      "loss is -2.110\n",
      "loss is -2.122\n",
      "loss is -2.111\n",
      "loss is -2.120\n",
      "loss is -2.118\n",
      "loss is -2.117\n",
      "loss is -2.121\n",
      "loss is -2.117\n",
      "loss is -2.123\n",
      "loss is -2.118\n",
      "loss is -2.123\n",
      "loss is -2.119\n",
      "loss is -2.123\n",
      "loss is -2.120\n",
      "loss is -2.123\n",
      "loss is -2.122\n",
      "loss is -2.123\n",
      "loss is -2.123\n",
      "loss is -2.123\n",
      "loss is -2.124\n",
      "loss is -2.123\n",
      "loss is -2.124\n",
      "loss is -2.123\n",
      "loss is -2.125\n",
      "loss is -2.123\n",
      "loss is -2.125\n",
      "loss is -2.124\n",
      "loss is -2.125\n",
      "loss is -2.125\n",
      "loss is -2.124\n",
      "loss is -2.125\n",
      "loss is -2.125\n",
      "loss is -2.125\n",
      "loss is -2.125\n",
      "loss is -2.125\n",
      "loss is -2.125\n",
      "loss is -2.125\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.126\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.126\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.127\n",
      "loss is -2.128\n",
      "loss is -2.128\n",
      "loss is -2.128\n",
      "loss is -2.128\n",
      "loss is -2.128\n",
      "loss is -2.128\n",
      "loss is -2.129\n",
      "loss is -2.129\n",
      "loss is -2.130\n",
      "loss is -2.130\n",
      "loss is -2.130\n",
      "loss is -2.128\n"
     ]
    }
   ],
   "source": [
    "iter_hp = 200\n",
    "\n",
    "num_base_kernels = 6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_target = f_target.reshape(2,1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g_theta1 = x_train\n",
    "agg_data = y_train.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "cur_model = None\n",
    "cur_likelihood = None\n",
    "\n",
    "\n",
    "\n",
    "model, likelihood = hyper_opti(g_theta1,agg_data,iter_hp,num_base_kernels,noise_value, current_model = cur_model, current_likelihood = cur_likelihood)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooiiiib\n",
      "tensor([[0.8731, 0.5664]])\n",
      "tensor([[ 0.8731,  0.5664],\n",
      "        [-0.3929, -0.3277],\n",
      "        [ 0.1632,  0.3333],\n",
      "        [-0.1885, -0.4165],\n",
      "        [-0.7430, -0.0559],\n",
      "        [-0.9464,  0.3909],\n",
      "        [ 0.6872, -0.3770],\n",
      "        [ 0.4608,  0.5156],\n",
      "        [ 0.5210,  0.7621],\n",
      "        [ 0.6418, -0.6243],\n",
      "        [-0.4407,  0.1266],\n",
      "        [-0.2890, -0.0111],\n",
      "        [ 0.0717,  0.8106],\n",
      "        [ 0.2333, -0.8968],\n",
      "        [ 0.2710, -0.1219],\n",
      "        [-0.0174, -0.4012],\n",
      "        [-0.8329, -0.0578],\n",
      "        [ 0.2581,  0.8180],\n",
      "        [ 0.6606, -0.8927],\n",
      "        [-0.3964, -0.7127],\n",
      "        [-0.9927, -0.1078],\n",
      "        [-0.6344,  0.3009],\n",
      "        [-0.7357, -0.5184],\n",
      "        [-0.9433,  0.5205],\n",
      "        [ 0.2129,  0.9496],\n",
      "        [-0.1910,  0.5101],\n",
      "        [-0.9870,  0.5820],\n",
      "        [ 0.1765, -0.5181],\n",
      "        [ 0.7720, -0.3344],\n",
      "        [ 0.7360, -0.5933],\n",
      "        [ 0.0787,  0.7088],\n",
      "        [-0.7667,  0.7035],\n",
      "        [ 0.4420,  0.9784],\n",
      "        [ 0.5938, -0.4767],\n",
      "        [ 0.8416, -0.5884],\n",
      "        [-0.0600,  0.7473],\n",
      "        [-0.4192, -0.5123],\n",
      "        [-0.2136,  0.8913],\n",
      "        [-0.8632,  0.1931],\n",
      "        [-0.2789,  0.8495],\n",
      "        [ 0.7306, -0.0207],\n",
      "        [ 0.1452, -0.0661],\n",
      "        [-0.1272, -0.9304],\n",
      "        [ 0.9346,  0.6364],\n",
      "        [ 0.7991, -0.0066],\n",
      "        [ 0.0446, -0.8594],\n",
      "        [-0.7628,  0.5028],\n",
      "        [ 0.4421, -0.5403],\n",
      "        [-0.9615, -0.2845],\n",
      "        [-0.1366, -0.3969],\n",
      "        [ 0.4661,  0.5646]])\n",
      "enddd\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vd/tt7x9mbn35b5wxq8_t6p0mt80000gn/T/ipykernel_65569/329674896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdist_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdist_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtad_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtad_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtad_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtad_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vd/tt7x9mbn35b5wxq8_t6p0mt80000gn/T/ipykernel_65569/329674896.py\u001b[0m in \u001b[0;36mtad_obj\u001b[0;34m(x2, g_theta2)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mg_theta2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_theta2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQf12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_theta2_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "x0 = Tensor(np.array([0.8731, 0.5664])) # 1./3. * Tensor(high_minus_low  * np.random.random_sample((1,2)) + vf.low) #\n",
    "x0 = x0.reshape(1,2) #\n",
    "fun = partial(likelihood.get_ell, agg_data,f_target, x0, g_theta1, model, likelihood, noise_value)\n",
    "\n",
    "\n",
    "\n",
    "def tad_obj(x2, g_theta2):\n",
    "    \n",
    "    g_theta2_ = torch.cat([x2, g_theta2], 0)\n",
    "\n",
    "    loss2, pf1, Qf1, Qf12, data_fit = fun(g_theta2_)\n",
    "    return -loss2.detach()\n",
    "err = 1e-5\n",
    "tad_vec = torch.empty((1,1))\n",
    "dist_vec = torch.empty((1,1))\n",
    "g_theta2 = 1./3. *  Tensor(6.  * np.random.random_sample((50,2)) - 3.)\n",
    "for i in range(60):\n",
    "    \n",
    "    x_plot = x0 + err\n",
    "#     print(x_plot)\n",
    "    dist = torch.norm(x0-x_plot).reshape(1,1)\n",
    "    dist_vec = torch.cat([dist_vec, dist], 0)\n",
    "    tad_val = tad_obj(x_plot, g_theta2).reshape(1,1)\n",
    "    tad_vec = torch.cat([tad_vec, tad_val], 0)\n",
    "    err = err*1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "\n",
    "ax.plot(np.array(dist_vec[1:]), (tad_vec[1:]),'-o',color = 'blue', markersize=12)\n",
    "#ax.set_xlim(-15.1, torch.max(data_fit_vec_plot)+0.1)\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xlabel('distance')\n",
    "ax.set_ylabel('TAD')\n",
    "#ax.plot(np.array(range(2,iter+2)), data_fit_vec_plot, 'rx-')\n",
    "#plt.savefig('figures/TADvsDIST.eps',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tad_obj2(g_theta2_):\n",
    "    \n",
    "\n",
    "    loss2, pf1, Qf1, Qf12, data_fit = fun(g_theta2_)\n",
    "    return -loss2.detach()\n",
    "err = 1e-5\n",
    "tad_vec = torch.empty((1,1))\n",
    "dist_vec = torch.empty((1,1))\n",
    "#g_theta2 = 1./3. *  Tensor(6.  * np.random.random_sample((3,2)) - 3.)\n",
    "for i in range(60):\n",
    "    \n",
    "    x_plot = (g_theta2[0] + err).reshape(1,2)\n",
    "    g_theta2_ = torch.cat([x_plot, g_theta2], 0)\n",
    "    print(x_plot)\n",
    "    dist = torch.norm(g_theta2[0].reshape(1,2)-x_plot).reshape(1,1)\n",
    "    dist_vec = torch.cat([dist_vec, dist], 0)\n",
    "    tad_val = tad_obj2(g_theta2_).reshape(1,1)\n",
    "    tad_vec = torch.cat([tad_vec, tad_val], 0)\n",
    "    err = err*1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"pdf.use14corefonts\"] = True\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "#ax.plot(np.array(range(2,iter+2)), torch.log(entropy_vec_plot), '+-')\n",
    "\n",
    "ax.plot(np.array(dist_vec[1:]), (tad_vec[1:]),'-o',color = 'blue', markersize=12)\n",
    "#ax.set_xlim(-15.1, torch.max(data_fit_vec_plot)+0.1)\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xlabel('distance')\n",
    "ax.set_ylabel('TAD')\n",
    "#ax.plot(np.array(range(2,iter+2)), data_fit_vec_plot, 'rx-')\n",
    "#plt.savefig('figures/TADvsDIST.eps',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Tensor([[2.5000, 1.5498]])\n",
    "x2 = Tensor([[1.5323, 1.7661]])\n",
    "res = torch.norm(x1 - x2, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9916])\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.norm(x1 - x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0836)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
